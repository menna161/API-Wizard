import logging
import networkx as nx
import numpy as np
from scipy import optimize, spatial
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors


def discover_facts(X, model, top_n=10, strategy='random_uniform', max_candidates=100, target_rel=None, seed=0):
    "\n    Discover new facts from an existing knowledge graph.\n\n    You should use this function when you already have a model trained on a knowledge graph and you want to\n    discover potentially true statements in that knowledge graph.\n\n    The general procedure of this function is to generate a set of candidate statements :math:`C` according to some\n    sampling strategy ``strategy``, then rank them against a set of corruptions using the\n    :meth:`ampligraph.evaluation.evaluate_performance` function.\n    Candidates that appear in the ``top_n`` ranked statements of this procedure are returned as likely true\n    statements.\n\n    The majority of the strategies are implemented with the same underlying principle of searching for\n    candidate statements:\n\n    - from among the less frequent entities (`'entity_frequency'`),\n    - less connected entities (`'graph_degree'`, `'cluster_coefficient'`),\n    - | less frequent local graph structures (`'cluster_triangles'`, `'cluster_squares'`), on the assumption that\n        densely connected entities are less likely to have missing true statements.\n    - | The remaining strategies (`'random_uniform'`, `'exhaustive'`) generate candidate statements by a random\n        sampling of entities and relations or exhaustively, respectively.\n\n    .. warning::\n        Due to the significant amount of computation required to evaluate all triples using the 'exhaustive' strategy,\n        we do not recommend its use at this time.\n\n    The function will automatically filter entities that have not been seen by the model, and operates on\n    the assumption that the model provided has been fit on the data ``X`` (determined heuristically), although ``X``\n    may be a subset of the original data, in which case a warning is shown.\n\n    The ``target_rel`` argument indicates what relation to generate candidate statements for. If this is set to ``None``\n    then all target relations will be considered for sampling.\n\n    Parameters\n    ----------\n\n    X : ndarray of shape (n, 3)\n        The input knowledge graph used to train ``model``, or a subset of it.\n    model : EmbeddingModel\n        The trained model that will be used to score candidate facts.\n    top_n : int\n        The cutoff position in ranking to consider a candidate triple as true positive.\n    strategy: str\n        The candidates generation strategy:\n\n        - `'random_uniform'` : generates `N` candidates (:math:`N <= max_candidates`) based on a uniform sampling of\n            entities.\n        - `'entity_frequency'` : generates candidates by weighted sampling of entities using entity frequency.\n        - `'graph_degree'` : generates candidates by weighted sampling of entities with graph degree.\n        - `'cluster_coefficient'` : generates candidates by weighted sampling entities with clustering coefficient.\n        - `'cluster_triangles'` : generates candidates by weighted sampling entities with cluster triangles.\n        - `'cluster_squares'` : generates candidates by weighted sampling entities with cluster squares.\n\n    max_candidates: int or float\n        The maximum numbers of candidates generated by ``strategy``.\n        Can be an absolute number or a percentage [0,1] of the size of the `X` parameter.\n    target_rel : str or list(str)\n        Target relations to focus on. The function will discover facts only for that specific relation types.\n        If `None`, the function attempts to discover new facts for all relation types in the graph.\n    seed : int\n        Seed to use for reproducible results.\n\n\n    Returns\n    -------\n    X_pred : ndarray, shape (n, 3)\n        A list of new facts predicted to be true.\n\n    Example\n    -------\n    >>> import requests\n    >>> from ampligraph.latent_features import ScoringBasedEmbeddingModel\n    >>> from ampligraph.datasets import load_from_csv\n    >>> from ampligraph.discovery import discover_facts\n    >>> # Game of Thrones relations dataset\n    >>> url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/GoT.csv'\n    >>> open('GoT.csv', 'wb').write(requests.get(url).content)\n    >>> X = load_from_csv('.', 'GoT.csv', sep=',')\n    >>> model = ScoringBasedEmbeddingModel(eta=5,\n    >>>                                      k=300,\n    >>>                                      scoring_type='ComplEx')\n    >>> model.compile(optimizer='adam', loss='multiclass_nll')\n    >>> model.fit(X,\n    >>>              batch_size=100,\n    >>>              epochs=10,\n    >>>              validation_freq=50,\n    >>>              validation_batch_size=100,\n    >>>              validation_data = dataset['valid'])\n    >>> discover_facts(X,\n    >>>                model,\n    >>>                top_n=100,\n    >>>                strategy='random_uniform',\n    >>>                max_candidates=100,\n    >>>                target_rel='ALLIED_WITH',\n    >>>                seed=0)\n    Epoch 1/10\n    33/33 [==============================] - 1s 27ms/step - loss: 177.7778\n    Epoch 2/10\n    33/33 [==============================] - 0s 6ms/step - loss: 177.4795\n    Epoch 3/10\n    33/33 [==============================] - 0s 6ms/step - loss: 176.9654\n    Epoch 4/10\n    33/33 [==============================] - 0s 6ms/step - loss: 175.8453\n    Epoch 5/10\n    33/33 [==============================] - 0s 6ms/step - loss: 173.4385\n    Epoch 6/10\n    33/33 [==============================] - 0s 6ms/step - loss: 168.8143\n    Epoch 7/10\n    33/33 [==============================] - 0s 6ms/step - loss: 161.2919\n    Epoch 8/10\n    33/33 [==============================] - 0s 6ms/step - loss: 151.3496\n    Epoch 9/10\n    33/33 [==============================] - 0s 6ms/step - loss: 140.4268\n    Epoch 10/10\n    33/33 [==============================] - 0s 5ms/step - loss: 129.8206\n    3175 triples containing invalid keys skipped!\n    (array([['House Nymeros Martell of Sunspear', 'ALLIED_WITH',\n             'House Mallister of Seagard'],\n            ['Ben', 'ALLIED_WITH', 'House Mallister of Seagard'],\n            ['Selwyn Tarth', 'ALLIED_WITH', 'House Mallister of Seagard'],\n            ['Clarence Charlton', 'ALLIED_WITH', 'House Woods'],\n            ['Selwyn Tarth', 'ALLIED_WITH', 'House Woods'],\n            ['Dacks', 'ALLIED_WITH', 'Titus Peake'],\n            ['Barra', 'ALLIED_WITH', 'Titus Peake'],\n            ['House Chelsted', 'ALLIED_WITH', 'Denys Darklyn'],\n            ['Crow Spike Keep', 'ALLIED_WITH', 'Denys Darklyn'],\n            ['Selwyn Tarth', 'ALLIED_WITH', 'Denys Darklyn'],\n            ['House Chelsted', 'ALLIED_WITH', 'House Belmore of Strongsong'],\n            ['Barra', 'ALLIED_WITH', 'House Belmore of Strongsong'],\n            ['Walder Frey', 'ALLIED_WITH', 'House Belmore of Strongsong']],\n           dtype=object),\n     array([ 2. , 53. , 73. , 42. , 18. , 59.5, 86. , 76.5, 31. , 60.5, 31.5,\n            32. , 24. ]))\n    "
    if model.is_backward:
        model = model.model
    if (not model.is_fitted):
        msg = 'Model is not fitted.'
        logger.error(msg)
        raise ValueError(msg)
    if (strategy not in ['random_uniform', 'entity_frequency', 'graph_degree', 'cluster_coefficient', 'cluster_triangles', 'cluster_squares']):
        msg = ('%s is not a valid strategy.' % strategy)
        logger.error(msg)
        raise ValueError(msg)
    if (strategy == 'exhaustive'):
        msg = 'Strategy is `exhaustive`, ignoring max_candidates.'
        logger.info(msg)
    if isinstance(max_candidates, float):
        logger.debug('Converting max_candidates float value {} to int value {}'.format(max_candidates, int((max_candidates * len(X)))))
        max_candidates = int((max_candidates * len(X)))
    if isinstance(target_rel, str):
        target_rel = [target_rel]
    if (target_rel is None):
        msg = 'No target relation specified. Using all relations to generate candidate statements.'
        logger.info(msg)
        rel_list = [x for x in model.data_indexer.backend.get_all_relations()]
    else:
        missing_rels = []
        for rel in target_rel:
            if (rel not in model.data_indexer.backend.get_all_relations()):
                missing_rels.append(rel)
        if (len(missing_rels) > 0):
            msg = 'Target relation(s) not found in model: {}'.format(missing_rels)
            logger.error(msg)
            raise ValueError(msg)
        rel_list = [target_rel]
    np.random.seed(seed)
    discoveries = []
    discovery_ranks = []
    for relation in rel_list:
        logger.info(('Generating candidates for relation: %s' % relation))
        candidates = generate_candidates(X, strategy, relation, max_candidates, seed=seed)
        logger.debug(('Generated %d candidate statements.' % len(candidates)))
        ranks = model.evaluate(candidates, use_filter={'test': X}, corrupt_side='s,o', verbose=False)
        avg_ranks = np.mean(ranks, axis=1)
        preds = (np.array(avg_ranks) <= top_n)
        discoveries.append(candidates[preds])
        discovery_ranks.append(avg_ranks[preds])
    logger.info(('Discovered %d facts' % len(discoveries)))
    return (np.hstack(discoveries), np.hstack(discovery_ranks))
