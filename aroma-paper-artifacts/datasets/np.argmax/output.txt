------------------------- example 1 ------------------------ 
def pick_move_greedily(pi):
    return np.argmax(pi)

------------------------- example 2 ------------------------ 
def forward(self, v1):
    return np.argmax(v1, axis=1)

------------------------- example 3 ------------------------ 
def pick_move_probabilistically(pi):
    r = random.random()
    s = 0
    for move in range(len(pi)):
// your code ...

    return np.argmax(pi)

------------------------- example 4 ------------------------ 
def predict_one(self, sample):
    if (not self.trained):
        sys.stderr.write('Model should be trained or loaded before doing predict\n')
        sys.exit((- 1))
    return np.argmax(self.model.predict(np.array([sample])))

------------------------- example 5 ------------------------ 
def get_most_frequent_category(cat_array):
    cats = np.zeros(NUM_CATEGORIES)
    for i in range(NUM_CATEGORIES):
        cats[i] = len(cat_array[(cat_array == i)])
    return np.argmax(cats)

examples  ||  representativeness  ||  number of lines  || number of comments 
example1  ||          2           ||        2         ||         0        
example2  ||          2           ||        2         ||         0        
example3  ||          2           ||        6         ||         1        
example4  ||          2           ||        5         ||         0        
example5  ||          2           ||        5         ||         0        

avg       ||          2.0           ||        4.0         ||         0.2        

idx = 0:------------------- similar code ------------------ index = 21, score = 6.0 
def pick_move_greedily(pi):
    return np.argmax(pi)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.argmax

idx = 1:------------------- similar code ------------------ index = 22, score = 6.0 
def forward(self, v1):
    return np.argmax(v1, axis=1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return np.argmax

idx = 2:------------------- similar code ------------------ index = 25, score = 6.0 
def pick_move_probabilistically(pi):
    r = random.random()
    s = 0
    for move in range(len(pi)):
        s += pi[move]
        if (s >= r):
            return move
    return np.argmax(pi)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.argmax

idx = 3:------------------- similar code ------------------ index = 31, score = 6.0 
def get_data_sample(self, i=0):
    '\n        :param i: the attack target label id\n        '
    if (i > (self.total_classes - 2)):
        assert False
    self.X_origin = self.all_orig_img[i:(i + 1)]
    self.orig_img_id = self.all_orig_img_id[i:(i + 1)][0]
    self.input_label = self.all_orig_labels_int[i:(i + 1)][0]
    X_orig_img_file = os.path.join(self.results_folder, f'X_{self.dataset_name}_origin_{self.input_label}_id{self.orig_img_id}')
    if ('imagenet' in self.dataset_name):
        np.save(X_orig_img_file, np.array([0]))
    else:
        np.save(X_orig_img_file, self.X_origin)
    target_label_vector = self.all_target_labels[i:(i + 1)]
    self.target_label = np.argmax(target_label_vector, 1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        np
 =  ... .argmax

idx = 4:------------------- similar code ------------------ index = 38, score = 6.0 
def convert_model(model: 'chainer.Chain', args=[]):
    values.reset_field_and_attributes()
    utils.reset_guid()
    values.function_converters.clear()
    values.builtin_function_converters.clear()
    values.instance_converters.clear()

    def instance_converter(m, i):
        if links_builtin.is_builtin_chainer_link(i):
            return links_builtin.ChainerLinkInstance(m, i)
        if isinstance(i, chainer.ChainList):
            module = values.Object(values.ModuleValue(sys.modules[i.__module__]))
            return links_builtin.ChainerChainListInstance(module, i)
        if isinstance(i, chainer.Link):
            module = values.Object(values.ModuleValue(sys.modules[i.__module__]))
            return links_builtin.ChainerChainInstance(module, i)
        return None
    values.instance_converters.append(instance_converter)
    custom_functions_module = values.Object(values.ModuleValue(custom_functions))
    functions_onnx_module = values.Object(values.ModuleValue(functions_onnx))

    def ret_same(funcArgs):
        return functions.generate_value_with_same_type(funcArgs.keywords['x'].get_value())
    values.function_converters[functions_onnx.onnx_abs] = values.FuncValue(functions_builtin.ChainerFunction(functions_onnx.onnx_abs, ret_value_func=ret_same), None, module=functions_onnx_module)
    c_variable = values.FuncValue(functions_ndarray.NDArrayFunction(), None)
    values.function_converters[chainer.Variable] = c_variable

    def add_chainer_function(func, ret_value_func=None):
        if (ret_value_func is None):
            f = values.FuncValue(functions_builtin.ChainerFunction(func), None)
        else:
            f = values.FuncValue(functions_builtin.ChainerFunction(func, ret_value_func=ret_value_func), None)
        values.function_converters[func] = f

    def ret_tuple(funcArgs=None):
        ret = values.TupleValue()
        ret.vtype = values.TensorValue
        return ret
    for f in F.__dict__.items():
        if inspect.isfunction(f[1]):
            values.function_converters[f[1]] = values.FuncValue(functions.UnimplementedFunction(f[1]), None)
    add_chainer_function(F.elu)
    add_chainer_function(F.leaky_relu)
    add_chainer_function(F.log_softmax)
    add_chainer_function(F.relu)
    add_chainer_function(F.selu)
    add_chainer_function(F.sigmoid)
    add_chainer_function(F.softmax)
    add_chainer_function(F.tanh)
    add_chainer_function(F.softmax_cross_entropy)
    add_chainer_function(F.pad_sequence)
    add_chainer_function(F.average_pooling_2d)
    add_chainer_function(F.unpooling_2d)
    add_chainer_function(F.reshape)
    add_chainer_function(F.transpose)
    add_chainer_function(F.split_axis, ret_value_func=ret_tuple)
    add_chainer_function(F.hstack)
    add_chainer_function(F.vstack)
    add_chainer_function(F.stack)
    add_chainer_function(F.separate, ret_value_func=ret_tuple)
    add_chainer_function(F.squeeze)
    add_chainer_function(F.swapaxes)
    add_chainer_function(F.dropout)
    add_chainer_function(F.concat)
    add_chainer_function(F.matmul)
    add_chainer_function(F.max_pooling_2d)
    add_chainer_function(F.resize_images)
    add_chainer_function(F.broadcast_to)
    add_chainer_function(F.expand_dims)
    add_chainer_function(F.local_response_normalization)
    add_chainer_function(F.mean)
    add_chainer_function(F.average)
    add_chainer_function(F.sum)
    add_chainer_function(F.maximum)
    add_chainer_function(F.minimum)
    add_chainer_function(F.max)
    add_chainer_function(F.min)
    values.function_converters[F.absolute] = values.FuncValue(functions.UserDefinedFunction(custom_functions.chainer_absolute), None, module=custom_functions_module)
    add_chainer_function(F.sin)
    add_chainer_function(F.sinh)
    add_chainer_function(F.sign)
    add_chainer_function(F.cos)
    add_chainer_function(F.cosh)
    add_chainer_function(F.tan)
    add_chainer_function(F.tanh)
    add_chainer_function(F.arcsin)
    add_chainer_function(F.arccos)
    add_chainer_function(F.arctan)
    add_chainer_function(F.exp)
    add_chainer_function(F.log)
    add_chainer_function(F.sqrt)
    add_chainer_function(F.clip)
    values.function_converters[F.argmax] = values.FuncValue(functions_builtin.ChainerArgminmaxFunction(F.argmax), None)
    values.function_converters[F.argmin] = values.FuncValue(functions_builtin.ChainerArgminmaxFunction(F.argmin), None)
    values.function_converters[F.clipped_relu] = values.FuncValue(functions.UserDefinedFunction(custom_functions.chainer_clipped_relu), None, module=custom_functions_module)
    if (int(chainer.__version__[0]) >= 6):
        add_chainer_function(F.roi_max_pooling_2d)
        add_chainer_function(F.roi_average_pooling_2d)
        add_chainer_function(F.roi_max_align_2d)
    add_chainer_function(F.roi_average_align_2d)
    f_array = values.FuncValue(functions_ndarray.NDArrayFunction(), None)
    f_zeros = values.FuncValue(functions_ndarray.NDArrayZerosFunction(), None)
    f_full = values.FuncValue(functions_ndarray.NDArrayFullFunction(), None)
    f_ceil = values.FuncValue(functions_ndarray.NDArrayCeilFunction(), None)
    f_cumsum = values.FuncValue(functions_ndarray.NDArrayCumsumFunction(), None)
    f_maximum = values.FuncValue(functions_ndarray.NDArrayChainerFunction(functions_ndarray.dummy_maximum), None)
    f_minimum = values.FuncValue(functions_ndarray.NDArrayChainerFunction(functions_ndarray.dummy_minimum), None)
    f_argmax = values.FuncValue(functions_ndarray.NDarrayArgminmaxFunction(functions_ndarray.dummy_argmax), None)
    f_argmin = values.FuncValue(functions_ndarray.NDarrayArgminmaxFunction(functions_ndarray.dummy_argmin), None)
    f_round = values.FuncValue(functions_ndarray.NDarrayRoundFunction(functions_ndarray.dummy_round), None)
    f_sqrt = values.FuncValue(functions_ndarray.NDarraySqrtFunction(functions_ndarray.dummy_sqrt), None)
    f_stack = values.FuncValue(functions_ndarray.NDarrayStackFunction(functions_ndarray.dummy_stack), None)
    f_reshape = values.FuncValue(functions_ndarray.NDarrayReshapeFunction(functions_ndarray.dummy_reshape), None)
    f_transpose = values.FuncValue(functions_ndarray.NDarrayTransposeFunction(functions_ndarray.dummy_transpose), None)
    f_int32 = values.FuncValue(functions_ndarray.NDArrayInt32(), None)
    f_float32 = values.FuncValue(functions_ndarray.NDArrayFloat32(), None)
    values.function_converters[np.array] = f_array
    values.function_converters[np.zeros] = f_zeros
    values.function_converters[np.full] = f_full
    values.function_converters[np.ceil] = f_ceil
    values.function_converters[np.cumsum] = f_cumsum
    values.function_converters[np.int32] = f_int32
    values.function_converters[np.float32] = f_float32
    values.function_converters[np.maximum] = f_maximum
    values.function_converters[np.minimum] = f_minimum
    values.function_converters[np.argmax] = f_argmax
    values.function_converters[np.argmin] = f_argmin
    values.function_converters[np.round] = f_round
    values.function_converters[np.sqrt] = f_sqrt
    values.function_converters[np.stack] = f_stack
    values.function_converters[np.reshape] = f_reshape
    values.function_converters[np.transpose] = f_transpose
    values.function_converters[np.clip] = values.FuncValue(functions.UserDefinedFunction(custom_functions.numpy_clip), None, module=custom_functions_module)
    values.function_converters[np.absolute] = values.FuncValue(functions.UserDefinedFunction(custom_functions.numpy_absolute), None, module=custom_functions_module)
    values.function_converters[custom_functions.check_attribute_value] = values.FuncValue(functions.CheckAttributeValueFunction(), None, module=custom_functions_module)
    values.function_converters[custom_functions.check_attribute_scalar] = values.FuncValue(functions.CheckAttributeScalarFunction(), None, module=custom_functions_module)
    values.builtin_function_converters['abs'] = values.FuncValue(functions.UserDefinedFunction(custom_functions.builtin_absolute), None, module=custom_functions_module)
    m_range = values.FuncValue(functions_builtin.RangeFunction(), None)
    values.builtin_function_converters['range'] = m_range
    m_len = values.FuncValue(functions_builtin.LenFunction(), None)
    values.builtin_function_converters['len'] = m_len
    values.function_converters[six.moves.range] = m_range
    m_list = values.FuncValue(functions_builtin.ListFunction(), None)
    values.builtin_function_converters['list'] = m_list
    m_print = values.FuncValue(functions_builtin.PrintFunction(), None)
    values.builtin_function_converters['print'] = m_print
    m_getattr = values.FuncValue(functions_builtin.GetAttrFunction(), None)
    values.builtin_function_converters['getattr'] = m_getattr
    m_hasattr = values.FuncValue(functions_builtin.HasAttrFunction(), None)
    values.builtin_function_converters['hasattr'] = m_hasattr
    m_to_gpu = values.FuncValue(functions_builtin.CopyFunction(cuda.to_gpu), None)
    values.function_converters[cuda.to_gpu] = m_to_gpu
    m_to_cpu = values.FuncValue(functions_builtin.CopyFunction(cuda.to_cpu), None)
    values.function_converters[cuda.to_cpu] = m_to_cpu

    def add_veval_flag_function(name: 'str', func):
        f = values.FuncValue(functions_builtin.VEvalContextFunction(func), None)
        values.builtin_function_converters[name] = f
    add_veval_flag_function('eval_as_written_target', flags.eval_as_written_target)
    add_veval_flag_function('ignore_branch', flags.ignore_branch)
    add_veval_flag_function('for_unroll', flags.for_unroll)
    default_module = values.Object(values.ModuleValue(sys.modules[model.__module__]))
    model_inst = values.parse_instance(default_module, '', model)
    forward_func = model_inst.try_get_and_store_obj('forward', None)
    finput = functions.FunctionArgInput()
    value_args = []
    ind = 0
    node_input = nodes.NodeInput('input')
    for arg in args:
        varg = values.parse_instance(default_module, '', arg, None)
        varg.name = ('in_' + str(ind))
        varg.get_value().name = ('in_' + str(ind))
        varg.get_value().internal_value = None
        finput.inputs.append(varg)
        value_args.append(varg.get_value())
        ind += 1
    node_input.set_outputs(value_args)
    graph = Graph()
    graph.root_graph = graph
    graph.add_node(node_input)
    forward_func_value = forward_func.get_value()
    ret = forward_func_value.func.vcall(default_module, graph, forward_func_value.obj, finput).get_obj()
    assert ((ret is None) or isinstance(ret, values.Object))

    def try_get_value(value) -> 'values.Value':
        if isinstance(value, values.Value):
            return value
        if isinstance(value, values.Object):
            return value.get_value()
        if isinstance(value, values.Attribute):
            return value.get_obj().get_value()
    if ((ret is None) or isinstance(ret, values.NoneValue)):
        if config.show_warnings:
            print('Failed to compile. output is None.')
        return (value_args, None, graph)
    ret_ = []
    if isinstance(ret.get_value(), values.TupleValue):
        if (ret.get_value().internal_value is not None):
            for v in ret.get_value().internal_value:
                assert (v is not None)
                ret_.append(try_get_value(v))
        else:
            ret_ = [ret.get_value()]
    elif isinstance(ret, list):
        ret_ = [r.get_value() for r in ret]
    else:
        ret_ = [ret.get_value()]
    for v in value_args:
        graph.add_input_value(v)
    for v in ret_:
        graph.add_output_value(v)
    return (value_args, ret_, graph)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
     ... . ... [np.argmax]

idx = 5:------------------- similar code ------------------ index = 89, score = 6.0 
def predict_one(self, sample):
    if (not self.trained):
        sys.stderr.write('Model should be trained or loaded before doing predict\n')
        sys.exit((- 1))
    return np.argmax(self.model.predict(np.array([sample])))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return np.argmax

idx = 6:------------------- similar code ------------------ index = 33, score = 6.0 
def forward(self, v1):
    return np.argmax(v1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return np.argmax

idx = 7:------------------- similar code ------------------ index = 20, score = 6.0 
def get_most_frequent_category(cat_array):
    cats = np.zeros(NUM_CATEGORIES)
    for i in range(NUM_CATEGORIES):
        cats[i] = len(cat_array[(cat_array == i)])
    return np.argmax(cats)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.argmax

idx = 8:------------------- similar code ------------------ index = 37, score = 6.0 
def sample(p):
    return np.argmax(np.random.multinomial(1, p))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.argmax

idx = 9:------------------- similar code ------------------ index = 66, score = 6.0 
def sample(p):
    return np.argmax(np.random.multinomial(1, p))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.argmax

idx = 10:------------------- similar code ------------------ index = 60, score = 6.0 
def sample(p):
    return np.argmax(np.random.multinomial(1, p))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.argmax

idx = 11:------------------- similar code ------------------ index = 49, score = 6.0 
def sample(p):
    return np.argmax(np.random.multinomial(1, p))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.argmax

idx = 12:------------------- similar code ------------------ index = 117, score = 6.0 
def select_br_a(self, pub_obses, range_idxs, legal_actions_lists, explore=False):
    if (explore and (np.random.random() < self._eps)):
        return np.array([legal_actions[np.random.randint(len(legal_actions))] for legal_actions in legal_actions_lists])
    with torch.no_grad():
        self.eval()
        range_idxs = torch.tensor(range_idxs, dtype=torch.long, device=self.device)
        q = self._net(pub_obses=pub_obses, range_idxs=range_idxs, legal_action_masks=rl_util.batch_get_legal_action_mask_torch(n_actions=self._env_bldr.N_ACTIONS, legal_actions_lists=legal_actions_lists, device=self.device, dtype=torch.float32)).cpu().numpy()
        for b in range(q.shape[0]):
            illegal_actions = [i for i in self._n_actions_arranged if (i not in legal_actions_lists[b])]
            if (len(illegal_actions) > 0):
                illegal_actions = np.array(illegal_actions)
                q[(b, illegal_actions)] = (- 1e+20)
        return np.argmax(q, axis=1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    with:
        return np.argmax

idx = 13:------------------- similar code ------------------ index = 112, score = 6.0 
def solve_lambda_x(A, y, lambs_exponent=np.arange((- 10), 5, 0.01)):
    '\n    Minimize the Lagrangian L(x,lambda) = || A*x - y||**2 + lambda*||x||**2 based on the idea of Tikhonov Regularization.\n    This program is used to roughly estimate the parameters x and the Lagrange multiplier lambda. \n    The L-curve method is applied to get the proper Lagrangian multiplier.\n\n    Usage:\n    estimate_lamb,estimate_x,log10_residual_norm,log10_solution_norm,curvature = solve_lambda_x(A,y) \n\n    Inputs:\n    A -> [float 2d array] Design matrix\n    y -> [float array] Measurements\n\n    Parameters:\n    lambs_exponent -> [optional, float 3d/4d array, default = np.arange(-10,5,0.01)] Exponent for lambda with base of 10\n    \n    Outputs:\n    estimate_lamb -> [float] Lagrange multiplier\n    estimate_x -> [float array] Estimated parameters\n    log10_residual_norm -> [float array] log10(||A*x-y||) with lambda taking 10**lambs_exponent\n    log10_solution_norm -> [float array] log10(||x||) with lambda taking 10**lambs_exponent\n    curvature -> [float array] curvature of the L-curve, where the ordinate of the curve is log10_solution_norm and the abscissa is log10_residual_norm.\n\n    For more information, please refer to \n    (1) [NumPy/SciPy Recipes for Data Science: Regularized Least Squares Optimization](https://www.researchgate.net/publication/274138835_NumPy_SciPy_Recipes_for_Data_Science_Regularized_Least_Squares_Optimization)\n    (2) [Choosing the Regularization Parameter](http://www2.compute.dtu.dk/~pcha/DIP/chap5.pdf)\n    '
    np.seterr(divide='ignore', invalid='ignore')
    m = A.shape[1]
    (log10_residual_norm, log10_solution_norm) = ([], [])
    lambs = np.float_power(10, lambs_exponent)
    for lamb in lambs:
        x = np.dot(inv((np.dot(A.T, A) + (lamb * np.eye(m)))), np.dot(A.T, y))
        residual_norm = norm((np.dot(A, x) - y))
        solution_norm = norm(x)
        log10_residual_norm.append(np.log10(residual_norm))
        log10_solution_norm.append(np.log10(solution_norm))
    log10_residual_norm = np.array(log10_residual_norm)
    log10_solution_norm = np.array(log10_solution_norm)
    g1 = np.gradient(log10_solution_norm, log10_residual_norm)
    g1[np.isnan(g1)] = (- np.inf)
    g2 = np.gradient(g1, log10_residual_norm)
    g2[np.isnan(g2)] = np.inf
    curvature = (np.abs(g2) / ((1 + (g1 ** 2)) ** 1.5))
    curvature[np.isnan(curvature)] = 0
    curvature[np.isinf(curvature)] = 0
    index_curvature_max = np.argmax(curvature)
    estimate_lamb = lambs[index_curvature_max]
    estimate_x = np.dot(inv((np.dot(A.T, A) + (estimate_lamb * np.eye(m)))), np.dot(A.T, y))
    return (estimate_lamb, estimate_x, log10_residual_norm, log10_solution_norm, curvature)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    np
     ...  =  ... .argmax

idx = 14:------------------- similar code ------------------ index = 7, score = 6.0 
def sample(p):
    return np.argmax(np.random.multinomial(1, p))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.argmax

idx = 15:------------------- similar code ------------------ index = 65, score = 6.0 
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .csv files (or other data files) for the task.')
    parser.add_argument('--bert_model', default=None, type=str, required=True, help='Bert pre-trained model selected in the list: bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese.')
    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model checkpoints will be written.')
    parser.add_argument('--save_model_name', default='model', type=str, required=True, help='The output model name where the model checkpoints will be written.')
    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \nSequences longer than this will be truncated, and sequences shorter \nthan this will be padded.')
    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')
    parser.add_argument('--wsc', action='store_true', help='Whether to run training with wsc.')
    parser.add_argument('--swag_transfer', action='store_true', help='Whether to run training with swag.')
    parser.add_argument('--inhouse', action='store_true', help='Whether to run eval on the inhouse train/dev set.')
    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')
    parser.add_argument('--do_test', action='store_true', help='Whether to run test on the test set.')
    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')
    parser.add_argument('--train_batch_size', default=32, type=int, help='Total batch size for training.')
    parser.add_argument('--eval_batch_size', default=8, type=int, help='Total batch size for eval.')
    parser.add_argument('--epoch_suffix', default=0, type=int, help='Epoch suffix number.')
    parser.add_argument('--learning_rate', default=0.0001, type=float, help='The initial learning rate for Adam.')
    parser.add_argument('--mlp_hidden_dim', default=64, type=int, help='mlp_hidden_dim.')
    parser.add_argument('--mlp_dropout', default=0.1, type=float, help='hidden drop out')
    parser.add_argument('--weight_decay', default=0.01, type=float, help='Weight decay for optimization')
    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')
    parser.add_argument('--warmup_proportion', default=0.1, type=float, help='Proportion of training to perform linear learning rate warmup for. E.g., 0.1 = 10%% of training.')
    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')
    parser.add_argument('--local_rank', type=int, default=(- 1), help='local_rank for distributed training on gpus')
    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')
    parser.add_argument('--patience', type=int, default=5, help='early stop epoch nums on dev')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')
    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit float precision instead of 32-bit')
    parser.add_argument('--loss_scale', type=float, default=0, help='Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\n0 (default value): dynamic loss scaling.\nPositive power of 2: static loss scaling value.\n')
    args = parser.parse_args()
    print('torch.cuda.is_available()', torch.cuda.is_available())
    if ((args.local_rank == (- 1)) or args.no_cuda):
        device = torch.device(('cuda' if torch.cuda.is_available() else 'cpu'))
        n_gpu = torch.cuda.device_count()
    else:
        torch.cuda.set_device(args.local_rank)
        device = torch.device('cuda', args.local_rank)
        n_gpu = 1
        torch.distributed.init_process_group(backend='nccl')
    logger.info('device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}'.format(device, n_gpu, bool((args.local_rank != (- 1))), args.fp16))
    if (args.gradient_accumulation_steps < 1):
        raise ValueError('Invalid gradient_accumulation_steps parameter: {}, should be >= 1'.format(args.gradient_accumulation_steps))
    args.train_batch_size = (args.train_batch_size // args.gradient_accumulation_steps)
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if (n_gpu > 0):
        torch.cuda.manual_seed_all(args.seed)
    if ((not args.do_train) and (not args.do_eval) and (not args.do_test)):
        raise ValueError('At least one of `do_train` or `do_eval` must be True.')
    if (os.path.exists(args.output_dir) and os.listdir(args.output_dir)):
        print('WARNING: Output directory ({}) already exists and is not empty.'.format(args.output_dir))
    if (not os.path.exists(args.output_dir)):
        os.makedirs(args.output_dir)
    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)
    train_examples = None
    num_train_optimization_steps = None
    if args.do_train:
        ori_train_examples = read_csqa_examples(os.path.join(args.data_dir, 'train_rand_split.jsonl'))
        ori_dev_examples = read_csqa_examples(os.path.join(args.data_dir, 'dev_rand_split.jsonl'))
        ori_test_examples = read_csqa_examples(os.path.join(args.data_dir, 'train2_rand_split.jsonl'))
        if args.inhouse:
            train_examples = ori_train_examples[0:850]
            test_examples = ori_train_examples[8500:]
            dev_examples = ori_dev_examples[:]
        else:
            train_examples = ori_train_examples[:]
            dev_examples = ori_dev_examples[:]
        num_train_optimization_steps = (int(((len(train_examples) / args.train_batch_size) / args.gradient_accumulation_steps)) * args.num_train_epochs)
        if (args.local_rank != (- 1)):
            num_train_optimization_steps = (num_train_optimization_steps // torch.distributed.get_world_size())
    model = BertForMultipleChoice.from_pretrained(args.bert_model, cache_dir=os.path.join(PYTORCH_PRETRAINED_BERT_CACHE, 'distributed_{}'.format(args.local_rank)), num_choices=5, mlp_hidden_dim=args.mlp_hidden_dim, mlp_dropout=args.mlp_dropout)
    if args.fp16:
        model.half()
    model.to(device)
    if (args.local_rank != (- 1)):
        try:
            from apex.parallel import DistributedDataParallel as DDP
        except ImportError:
            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.')
        model = DDP(model)
    elif (n_gpu > 1):
        model = torch.nn.DataParallel(model)
    param_optimizer = list(model.named_parameters())
    param_optimizer = [n for n in param_optimizer if ('pooler' not in n[0])]
    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [{'params': [p for (n, p) in param_optimizer if (not any(((nd in n) for nd in no_decay)))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in param_optimizer if any(((nd in n) for nd in no_decay))], 'weight_decay': 0.0}]
    if args.fp16:
        try:
            from apex.optimizers import FP16_Optimizer
            from apex.optimizers import FusedAdam
        except ImportError:
            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.')
        optimizer = FusedAdam(optimizer_grouped_parameters, lr=args.learning_rate, bias_correction=False, max_grad_norm=1.0)
        if (args.loss_scale == 0):
            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)
        else:
            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)
    else:
        optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=args.warmup_proportion, t_total=num_train_optimization_steps)
    global_step = 0
    if args.do_train:
        train_features = convert_examples_to_features(train_examples, tokenizer, args.max_seq_length, True)
        dev_features = convert_examples_to_features(dev_examples, tokenizer, args.max_seq_length, True)
        train_dataloader = get_train_dataloader(train_features, args)
        dev_dataloader = get_eval_dataloader(dev_features, args)
        if args.inhouse:
            test_features = convert_examples_to_features(test_examples, tokenizer, args.max_seq_length, True)
            test_dataloader = get_eval_dataloader(test_features, args)
        logger.info('***** Running training *****')
        logger.info('  Num examples = %d', len(train_examples))
        logger.info('  Batch size = %d', args.train_batch_size)
        logger.info('  Num steps = %d', num_train_optimization_steps)
        logger.info('')
        logger.info('  Num train features = %d', len(train_features))
        logger.info('  Num dev features = %d', len(dev_features))
        best_dev_accuracy = 0
        best_dev_epoch = 0
        no_up = 0
        epoch_tqdm = trange(int(args.num_train_epochs), desc='Epoch')
        for epoch in epoch_tqdm:
            model.train()
            tr_loss = 0
            (nb_tr_examples, nb_tr_steps) = (0, 0)
            for (step, batch) in enumerate(tqdm(train_dataloader, desc='Iteration')):
                batch = tuple((t.to(device) for t in batch))
                (input_ids, input_mask, segment_ids, label_ids) = batch
                loss = model(input_ids, segment_ids, input_mask, label_ids)
                if (n_gpu > 1):
                    loss = loss.mean()
                if (args.fp16 and (args.loss_scale != 1.0)):
                    loss = (loss * args.loss_scale)
                if (args.gradient_accumulation_steps > 1):
                    loss = (loss / args.gradient_accumulation_steps)
                tr_loss += loss.item()
                nb_tr_examples += input_ids.size(0)
                nb_tr_steps += 1
                if args.fp16:
                    optimizer.backward(loss)
                else:
                    loss.backward()
                if (((step + 1) % args.gradient_accumulation_steps) == 0):
                    if args.fp16:
                        lr_this_step = (args.learning_rate * warmup_linear((global_step / num_train_optimization_steps), args.warmup_proportion))
                        for param_group in optimizer.param_groups:
                            param_group['lr'] = lr_this_step
                    optimizer.step()
                    optimizer.zero_grad()
                    global_step += 1
            (dev_loss, dev_accuracy) = evaluate(model, device, dev_dataloader, desc='Evaluate Dev')
            if args.inhouse:
                (test_loss, test_accuracy) = evaluate(model, device, test_dataloader, desc='Evaluate Test')
            if (dev_accuracy > best_dev_accuracy):
                best_dev_accuracy = dev_accuracy
                best_dev_epoch = (epoch + 1)
                no_up = 0
                model_to_save = (model.module if hasattr(model, 'module') else model)
                output_model_file = os.path.join(args.output_dir, (args.save_model_name + ('.bin.%d' % epoch)))
                torch.save(model_to_save.state_dict(), output_model_file)
                output_config_file = os.path.join(args.output_dir, (args.save_model_name + '.config'))
                with open(output_config_file, 'w') as fpp:
                    fpp.write(model_to_save.config.to_json_string())
            else:
                no_up += 1
            tqdm.write(('\t ***** Eval results (Epoch %s) *****' % str((epoch + 1))))
            tqdm.write(('\t dev_accuracy = %s' % str(dev_accuracy)))
            tqdm.write('')
            if args.inhouse:
                tqdm.write(('\t test_accuracy = %s' % str(test_accuracy)))
                tqdm.write('')
            tqdm.write(('\t best_dev_accuracy = %s' % str(best_dev_accuracy)))
            tqdm.write(('\t best_dev_epoch = %s' % str(best_dev_epoch)))
            tqdm.write(('\t no_up = %s' % str(no_up)))
            tqdm.write('')
            if (no_up >= args.patience):
                epoch_tqdm.close()
                break
    model.to(device)
    if (args.do_eval and ((args.local_rank == (- 1)) or (torch.distributed.get_rank() == 0))):
        output_model_file = os.path.join(args.output_dir, (args.save_model_name + ('.bin.%d' % args.epoch_suffix)))
        output_config_file = os.path.join(args.output_dir, (args.save_model_name + '.config'))
        config = BertConfig(output_config_file)
        model = BertForMultipleChoice(config, num_choices=5, mlp_hidden_dim=args.mlp_hidden_dim, mlp_dropout=args.mlp_dropout)
        model.load_state_dict(torch.load(output_model_file))
        model.to(device)
        if args.wsc:
            eval_examples = read_csqa_examples('../datasets/wsc.jsonl')
        elif args.swag_transfer:
            eval_examples = read_csqa_examples('../datasets/swagaf/data/val.jsonl')
        else:
            eval_examples = read_csqa_examples(os.path.join(args.data_dir, 'dev_rand_split.jsonl'))
        eval_features = convert_examples_to_features(eval_examples, tokenizer, args.max_seq_length, True)
        if args.inhouse:
            eval_examples_test = read_csqa_examples(os.path.join(args.data_dir, 'train_rand_split.jsonl'))[8500:]
            eval_features_test = convert_examples_to_features(eval_examples_test, tokenizer, args.max_seq_length, True)
        logger.info('***** Running evaluation *****')
        logger.info('  Num examples = %d', len(eval_examples))
        logger.info('  Batch size = %d', args.eval_batch_size)
        all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long)
        all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long)
        all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long)
        all_label = torch.tensor([f.label for f in eval_features], dtype=torch.long)
        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)
        eval_sampler = SequentialSampler(eval_data)
        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)
        model.eval()
        (eval_loss, eval_accuracy) = (0, 0)
        (nb_eval_steps, nb_eval_examples) = (0, 0)
        test_outputs = []
        for (input_ids, input_mask, segment_ids, label_ids) in tqdm(eval_dataloader, desc='evaluating'):
            input_ids = input_ids.to(device)
            input_mask = input_mask.to(device)
            segment_ids = segment_ids.to(device)
            label_ids = label_ids.to(device)
            with torch.no_grad():
                tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)
                logits = model(input_ids, segment_ids, input_mask)
            logits = logits.detach().cpu().numpy()
            outputs = np.argmax(logits, axis=1)
            test_outputs += list(outputs)
            label_ids = label_ids.to('cpu').numpy()
            tmp_eval_accuracy = accuracy(logits, label_ids)
            eval_loss += tmp_eval_loss.mean().item()
            eval_accuracy += tmp_eval_accuracy
            nb_eval_examples += input_ids.size(0)
            nb_eval_steps += 1
        eval_loss = (eval_loss / nb_eval_steps)
        eval_accuracy = (eval_accuracy / nb_eval_examples)
        if args.wsc:
            result = {'eval_accuracy': eval_accuracy}
            logger.info('***** Eval results *****')
            for key in sorted(result.keys()):
                logger.info('  %s = %s', key, str(result[key]))
            test_output_file = os.path.join(args.output_dir, (args.save_model_name + '_wsc_prediction.csv'))
            with open(test_output_file, 'w') as fout:
                with open(os.path.join('../datasets/wsc.jsonl'), 'r', encoding='utf-8') as fin:
                    examples = []
                    for (i, line) in enumerate(fin.readlines()):
                        csqa_json = json.loads(line)
                        label_pred = chr((ord('A') + test_outputs[i]))
                        if (label_pred in ['C', 'E']):
                            label_pred = 'A'
                        if (label_pred in ['D']):
                            label_pred = 'B'
                        fout.write((((csqa_json['id'] + ',') + str(label_pred)) + '\n'))
        elif args.swag_transfer:
            result = {'eval_accuracy': eval_accuracy}
            logger.info('***** Eval results *****')
            for key in sorted(result.keys()):
                logger.info('  %s = %s', key, str(result[key]))
            test_output_file = os.path.join(args.output_dir, (args.save_model_name + '_swag_val.csv'))
            with open(test_output_file, 'w') as fout:
                with open(os.path.join('../datasets/swagaf/data/val.jsonl'), 'r', encoding='utf-8') as fin:
                    examples = []
                    for (i, line) in enumerate(fin.readlines()):
                        csqa_json = json.loads(line)
                        label_pred = chr((ord('A') + test_outputs[i]))
                        if (label_pred == 'E'):
                            label_pred = 'A'
                        fout.write((((csqa_json['id'] + ',') + str(label_pred)) + '\n'))
        elif args.inhouse:
            dev_result = {'dev_eval_accuracy': eval_accuracy}
            logger.info('***** Running evaluation *****')
            logger.info('  Num examples = %d', len(eval_examples))
            logger.info('  Batch size = %d', args.eval_batch_size)
            all_input_ids = torch.tensor(select_field(eval_features_test, 'input_ids'), dtype=torch.long)
            all_input_mask = torch.tensor(select_field(eval_features_test, 'input_mask'), dtype=torch.long)
            all_segment_ids = torch.tensor(select_field(eval_features_test, 'segment_ids'), dtype=torch.long)
            all_label = torch.tensor([f.label for f in eval_features_test], dtype=torch.long)
            eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)
            eval_sampler = SequentialSampler(eval_data)
            eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)
            model.eval()
            (eval_loss, eval_accuracy) = (0, 0)
            (nb_eval_steps, nb_eval_examples) = (0, 0)
            test_outputs = []
            for (input_ids, input_mask, segment_ids, label_ids) in eval_dataloader:
                input_ids = input_ids.to(device)
                input_mask = input_mask.to(device)
                segment_ids = segment_ids.to(device)
                label_ids = label_ids.to(device)
                with torch.no_grad():
                    tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)
                    logits = model(input_ids, segment_ids, input_mask)
                logits = logits.detach().cpu().numpy()
                outputs = np.argmax(logits, axis=1)
                test_outputs += list(outputs)
                label_ids = label_ids.to('cpu').numpy()
                tmp_eval_accuracy = accuracy(logits, label_ids)
                eval_loss += tmp_eval_loss.mean().item()
                eval_accuracy += tmp_eval_accuracy
                nb_eval_examples += input_ids.size(0)
                nb_eval_steps += 1
            eval_loss = (eval_loss / nb_eval_steps)
            eval_accuracy = (eval_accuracy / nb_eval_examples)
            test_result = {'test_eval_accuracy': eval_accuracy}
            with open(output_eval_file, 'w') as writer:
                logger.info('***** Eval results *****')
                for key in sorted(result.keys()):
                    logger.info('  %s = %s', key, str(result[key]))
                    writer.write(('%s = %s\n' % (key, str(result[key]))))
        else:
            result = {'eval_accuracy': eval_accuracy}
            test_output_file = os.path.join(args.output_dir, (args.save_model_name + '_dev_output.csv'))
            with open(test_output_file, 'w') as fout:
                with open(os.path.join(args.data_dir, 'dev_rand_split.jsonl'), 'r', encoding='utf-8') as fin:
                    examples = []
                    for (i, line) in enumerate(fin.readlines()):
                        csqa_json = json.loads(line)
                        label_pred = chr((ord('A') + test_outputs[i]))
                        fout.write((((csqa_json['id'] + ',') + str(label_pred)) + '\n'))
            output_eval_file = os.path.join(args.output_dir, (args.save_model_name + '_res_on_dev.txt'))
            with open(output_eval_file, 'w') as writer:
                logger.info('***** Eval results *****')
                for key in sorted(result.keys()):
                    logger.info('  %s = %s', key, str(result[key]))
                    writer.write(('%s = %s\n' % (key, str(result[key]))))
    if (args.do_test and ((args.local_rank == (- 1)) or (torch.distributed.get_rank() == 0))):
        output_model_file = os.path.join(args.output_dir, (args.save_model_name + ('.bin.%d' % args.epoch_suffix)))
        output_config_file = os.path.join(args.output_dir, (args.save_model_name + '.config'))
        config = BertConfig(output_config_file)
        model = BertForMultipleChoice(config, num_choices=5, mlp_hidden_dim=args.mlp_hidden_dim, mlp_dropout=args.mlp_dropout)
        model.load_state_dict(torch.load(output_model_file))
        model.to(device)
        eval_examples = read_csqa_examples(os.path.join(args.data_dir, 'test_rand_split_no_answers.jsonl'), have_answer=False)
        eval_features = convert_examples_to_features(eval_examples, tokenizer, args.max_seq_length, True)
        logger.info('***** Running evaluation *****')
        logger.info('  Num examples = %d', len(eval_examples))
        logger.info('  Batch size = %d', args.eval_batch_size)
        all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long)
        all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long)
        all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long)
        all_label = torch.tensor([f.label for f in eval_features], dtype=torch.long)
        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)
        eval_sampler = SequentialSampler(eval_data)
        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)
        model.eval()
        test_outputs = []
        for (input_ids, input_mask, segment_ids, label_ids) in eval_dataloader:
            input_ids = input_ids.to(device)
            input_mask = input_mask.to(device)
            segment_ids = segment_ids.to(device)
            with torch.no_grad():
                logits = model(input_ids, segment_ids, input_mask)
            logits = logits.detach().cpu().numpy()
            outputs = np.argmax(logits, axis=1)
            test_outputs += list(outputs)
        test_output_file = os.path.join(args.output_dir, (args.save_model_name + '_test_output.csv'))
        with open(test_output_file, 'w') as fout:
            with open(os.path.join(args.data_dir, 'test_rand_split_no_answers.jsonl'), 'r', encoding='utf-8') as fin:
                examples = []
                for (i, line) in enumerate(fin.readlines()):
                    csqa_json = json.loads(line)
                    label_pred = chr((ord('A') + test_outputs[i]))
                    fout.write((((csqa_json['id'] + ',') + str(label_pred)) + '\n'))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
        for in:
             ...  = np.argmax

idx = 16:------------------- similar code ------------------ index = 72, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 17:------------------- similar code ------------------ index = 43, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 18:------------------- similar code ------------------ index = 100, score = 6.0 
def spacenet(predictions, ground_truth):
    pred_types = set([type(pred) for pred in predictions])
    truth_types = set([type(truth) for truth in ground_truth])
    if ((len(pred_types) == 0) or (len(truth_types) == 0)):
        return {'tp': 0, 'fp': 0, 'fn': 0}
    elif ((len(pred_types) != 1) or (len(truth_types) != 1)):
        raise Exception()
    pred_type = pred_types.pop()
    if (pred_type == shapely.geometry.polygon.Polygon):
        pass
    elif (pred_type == dict):
        predictions = [shapely.geometry.shape(g) for g in predictions]
    else:
        raise Exception()
    truth_type = truth_types.pop()
    if (truth_type == shapely.geometry.polygon.Polygon):
        pass
    elif (truth_type == dict):
        ground_truth = [shapely.geometry.shape(g) for g in ground_truth]
    else:
        raise Exception()
    tree = shapely.strtree.STRtree(ground_truth)

    def make_valid(p):
        if p.is_valid:
            return p
        else:
            (minx, miny, maxx, maxy) = p.bounds
            stretch = max((maxx - minx), (maxy - miny))
            return p.buffer((stretch * 0.01))

    def iou(a, b):
        a = make_valid(a)
        b = make_valid(b)
        a_and_b = a.intersection(b).area
        a_or_b = a.union(b).area
        return (a_and_b / a_or_b)

    def not_already_matched(a):
        return (not hasattr(a, 'iou_matched'))
    tp = 0
    fp = 0
    fn = 0
    for pred in predictions:
        results = list(filter(not_already_matched, tree.query(pred)))
        scores = list(map((lambda a: iou(a, pred)), results))
        if (len(scores) > 0):
            argmax = np.argmax(scores)
            if (max(scores) > 0.5):
                results[argmax].iou_matched = True
                tp = (tp + 1)
            else:
                fp = (fp + 1)
        else:
            fp = (fp + 1)
    fn = len(list(filter(not_already_matched, ground_truth)))
    return {'tp': tp, 'fp': fp, 'fn': fn}

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in  ... :
        if:
             ...  = np.argmax

idx = 19:------------------- similar code ------------------ index = 80, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 20:------------------- similar code ------------------ index = 96, score = 6.0 
def evaluate(self, model, train_df, valid_df, test_df):
    valid_probs = model.get_preds(ds_type=DatasetType.Valid, ordered=True)[0].cpu().numpy()
    test_probs = model.get_preds(ds_type=DatasetType.Test, ordered=True)[0].cpu().numpy()
    train_probs = model.get_preds(ds_type=DatasetType.Train, ordered=True)[0].cpu().numpy()
    self._preds = []

    def multipreds2preds(preds, threshold=0.5):
        bs = preds.shape[0]
        return np.concatenate([probs, (np.ones((bs, 1)) * threshold)], axis=(- 1)).argmax((- 1))
    for (prefix, tdf, probs) in zip(['train', 'valid', 'test'], [train_df, valid_df, test_df], [train_probs, valid_probs, test_probs]):
        if (self.sigmoid and (not self.irrelevant_as_class)):
            preds = multipreds2preds(probs)
        else:
            preds = np.argmax(probs, axis=1)
        true_y = tdf['label']
        self._set_results(prefix, preds, true_y)
        self._preds.append(probs)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
        if:        else:
             ...  = np.argmax

idx = 21:------------------- similar code ------------------ index = 40, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 22:------------------- similar code ------------------ index = 57, score = 6.0 
def synthesize(self, texts=None, tokens=None, base_path=None, paths=None, speaker_ids=None, start_of_sentence=None, end_of_sentence=True, pre_word_num=0, post_word_num=0, pre_surplus_idx=0, post_surplus_idx=1, use_short_concat=False, manual_attention_mode=0, base_alignment_path=None, librosa_trim=False, attention_trim=True, isKorean=True):
    if (type(texts) == str):
        texts = [texts]
    if ((texts is not None) and (tokens is None)):
        sequences = np.array([text_to_sequence(text) for text in texts])
        sequences = _prepare_inputs(sequences)
    elif (tokens is not None):
        sequences = tokens
    if (paths is None):
        paths = ([None] * len(sequences))
    if (texts is None):
        texts = ([None] * len(sequences))
    time_str = get_time()

    def plot_and_save_parallel(wavs, alignments, use_manual_attention, mels):
        items = list(enumerate(zip(wavs, alignments, paths, texts, sequences, mels)))
        fn = partial(plot_graph_and_save_audio, base_path=base_path, start_of_sentence=start_of_sentence, end_of_sentence=end_of_sentence, pre_word_num=pre_word_num, post_word_num=post_word_num, pre_surplus_idx=pre_surplus_idx, post_surplus_idx=post_surplus_idx, use_short_concat=use_short_concat, use_manual_attention=use_manual_attention, librosa_trim=librosa_trim, attention_trim=attention_trim, time_str=time_str, isKorean=isKorean)
        return parallel_run(fn, items, desc='plot_graph_and_save_audio', parallel=False)
    input_lengths = [(np.argmax((a == 1)) + 1) for a in sequences]
    fetches = [self.model.linear_outputs, self.model.alignments, self.model.mel_outputs]
    feed_dict = {self.model.inputs: sequences, self.model.input_lengths: input_lengths}
    if (base_alignment_path is None):
        feed_dict.update({self.model.manual_alignments: np.zeros([1, 1, 1]), self.model.is_manual_attention: False})
    else:
        manual_alignments = []
        alignment_path = os.path.join(os.path.basename(base_path), base_alignment_path)
        for idx in range(len(sequences)):
            numpy_path = '{}{}.npy'.format(alignment_path, idx)
            manual_alignments.append(np.load(numpy_path))
        alignments_T = np.transpose(manual_alignments, [0, 2, 1])
        feed_dict.update({self.model.manual_alignments: alignments_T, self.model.is_manual_attention: True})
    if (speaker_ids is not None):
        if (type(speaker_ids) == dict):
            speaker_embed_table = sess.run(self.model.speaker_embed_table)
            speaker_embed = [(speaker_ids[speaker_id] * speaker_embed_table[speaker_id]) for speaker_id in speaker_ids]
            feed_dict.update({self.model.speaker_embed_table: np.tile()})
        else:
            feed_dict[self.model.speaker_id] = speaker_ids
    (wavs, alignments, mels) = self.sess.run(fetches, feed_dict=feed_dict)
    results = plot_and_save_parallel(wavs, alignments, use_manual_attention=False, mels=mels)
    if (manual_attention_mode > 0):
        if (manual_attention_mode == 1):
            alignments_T = np.transpose(alignments, [0, 2, 1])
            new_alignments = np.zeros_like(alignments_T)
            for idx in range(len(alignments)):
                argmax = alignments[idx].argmax(1)
                new_alignments[idx][(argmax, range(len(argmax)))] = 1
        elif (manual_attention_mode == 2):
            new_alignments = np.transpose(alignments, [0, 2, 1])
            for idx in range(len(alignments)):
                var = np.var(new_alignments[idx], 1)
                mean_var = var[:input_lengths[idx]].mean()
                new_alignments[idx] = np.power(new_alignments[idx], 2)
        elif (manual_attention_mode == 3):
            new_alignments = np.transpose(alignments, [0, 2, 1])
            for idx in range(len(alignments)):
                argmax = alignments[idx].argmax(1)
                new_alignments[idx][(argmax, range(len(argmax)))] = 1
        feed_dict.update({self.model.manual_alignments: new_alignments, self.model.is_manual_attention: True})
        (new_wavs, new_alignments) = self.sess.run(fetches, feed_dict=feed_dict)
        results = plot_and_save_parallel(new_wavs, new_alignments, True)
    return results

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = [(np.argmax +  ... )]

idx = 23:------------------- similar code ------------------ index = 93, score = 6.0 
def generate_attack_data_set(data, num_sample, img_offset, model, attack_type='targeted', random_target_class=None, shift_index=False):
    '\n    Generate the data for conducting attack. Only select the data being classified correctly.\n    '
    orig_img = []
    orig_labels = []
    target_labels = []
    orig_img_id = []
    pred_labels = np.argmax(model.model.predict(data.test_data), axis=1)
    true_labels = np.argmax(data.test_labels, axis=1)
    correct_data_indices = np.where([(1 if (x == y) else 0) for (x, y) in zip(pred_labels, true_labels)])
    print('Total testing data:{}, correct classified data:{}'.format(len(data.test_labels), len(correct_data_indices[0])))
    data.test_data = data.test_data[correct_data_indices]
    data.test_labels = data.test_labels[correct_data_indices]
    true_labels = true_labels[correct_data_indices]
    np.random.seed(img_offset)
    class_num = data.test_labels.shape[1]
    for sample_index in range(num_sample):
        if (attack_type == 'targeted'):
            if (random_target_class is not None):
                np.random.seed(0)
                seq_imagenet = [1, 4, 6, 8, 9, 13, 15, 16, 19, 20, 22, 24, 25, 27, 28, 30, 34, 35, 36, 37, 38, 44, 49, 51, 56, 59, 60, 61, 62, 63, 67, 68, 70, 71, 74, 75, 76, 77, 78, 79, 82, 84, 85, 87, 88, 91, 94, 96, 97, 99]
                seq = [seq_imagenet[(img_offset + sample_index)]]
                while (seq == true_labels[(img_offset + sample_index)]):
                    seq = np.random.choice(random_target_class, 1)
            else:
                seq = list(range(class_num))
                seq.remove(true_labels[(img_offset + sample_index)])
            for s in seq:
                if (shift_index and (s == 0)):
                    s += 1
                orig_img.append(data.test_data[(img_offset + sample_index)])
                target_labels.append(np.eye(class_num)[s])
                orig_labels.append(data.test_labels[(img_offset + sample_index)])
                orig_img_id.append((img_offset + sample_index))
        elif (attack_type == 'untargeted'):
            orig_img.append(data.test_data[(img_offset + sample_index)])
            target_labels.append(data.test_labels[(img_offset + sample_index)])
            orig_labels.append(data.test_labels[(img_offset + sample_index)])
            orig_img_id.append((img_offset + sample_index))
    orig_img = np.array(orig_img)
    target_labels = np.array(target_labels)
    orig_labels = np.array(orig_labels)
    orig_img_id = np.array(orig_img_id)
    return (orig_img, target_labels, orig_labels, orig_img_id)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 24:------------------- similar code ------------------ index = 39, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 25:------------------- similar code ------------------ index = 120, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 26:------------------- similar code ------------------ index = 128, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 27:------------------- similar code ------------------ index = 2, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 28:------------------- similar code ------------------ index = 69, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 29:------------------- similar code ------------------ index = 54, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 30:------------------- similar code ------------------ index = 51, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 31:------------------- similar code ------------------ index = 63, score = 6.0 
def getCoords(image):
    (contours, _) = cv2.findContours(preprocess, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    all_contours = sorted(contours, key=cv2.contourArea, reverse=True)
    polygon = all_contours[0]
    sums = []
    diffs = []
    for point in polygon:
        for (x, y) in point:
            sums.append((x + y))
            diffs.append((x - y))
    top_left = polygon[np.argmin(sums)].squeeze()
    bottom_right = polygon[np.argmax(sums)].squeeze()
    top_right = polygon[np.argmax(diffs)].squeeze()
    bottom_left = polygon[np.argmin(diffs)].squeeze()
    return np.array([top_left, top_right, bottom_right, bottom_left], dtype=np.float32)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  =  ... [np.argmax]

idx = 32:------------------- similar code ------------------ index = 55, score = 5.0 
def preds_for_cell_content_max(test_df, probs, group_by=['cell_content']):
    test_df = test_df.copy()
    probs_df = pd.DataFrame(probs, index=test_df.index)
    test_df = pd.concat([test_df, probs_df], axis=1)
    grouped_preds = np.argmax(test_df.groupby(group_by)[probs_df.columns].max().values, axis=1)
    grouped_counts = test_df.groupby(group_by)['label'].count()
    results = pd.DataFrame({'true': test_df.groupby(group_by)['label'].agg((lambda x: x.value_counts().index[0])), 'pred': grouped_preds, 'counts': grouped_counts})
    return results

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 33:------------------- similar code ------------------ index = 53, score = 5.0 
def update_current_market(self):
    '\n            Market distribution control rule.\n            Choose market by capital proportional robot distribution,\n            ref http://ensrationis.com/smart-factory-and-capital/\n        '
    rospy.loginfo('Input market list is %s', self.market_list)
    cap = [self.investors.call().supply(m) for m in self.market_list]
    rospy.loginfo('Capitalization vector is %s', cap)
    rob = [len(self.robots[m]) for m in self.market_list]
    rospy.loginfo('Real robot distribution is %s', rob)
    err = distribution_error(np.array(cap), np.array(rob))
    rospy.loginfo('Robot distribution error is %s', err)
    maxi = np.argmax(err)
    rospy.loginfo('Maximal error index is %d', maxi)
    self.market.publish(self.market_list[maxi])

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.argmax

idx = 34:------------------- similar code ------------------ index = 48, score = 5.0 
def ilp(training=True):
    'Run the ILP task using the ILP model.'
    (goals, vgoals) = (['f(X)'], list())
    for g in goals:
        v = np.zeros((1, 1, 4, (len(CHAR_IDX) + 1)))
        for (i, c) in enumerate(g):
            v[(0, 0, i, CHAR_IDX[c])] = 1
        vgoals.append(v)
    model = build_model('ilp', 'weights/ilp.h5', char_size=(len(CHAR_IDX) + 1), training=training, goals=vgoals, num_preds=1, pred_len=4)
    model.summary()
    traind = LogicSeq.from_file('data/ilp_train.txt', ARGS.batch_size, pad=ARGS.pad)
    testd = LogicSeq.from_file('data/ilp_test.txt', ARGS.batch_size, pad=ARGS.pad)
    if training:
        callbacks = [C.ModelCheckpoint(filepath='weights/ilp.h5', verbose=1, save_best_only=True, save_weights_only=True), C.TerminateOnNaN()]
        model.fit_generator(traind, epochs=200, callbacks=callbacks, validation_data=testd, shuffle=True)
    else:
        ctx = 'b(h).v(O):-c(O).c(a).'
        ctx = ctx.split('.')[:(- 1)]
        ctx = [(r + '.') for r in ctx]
        dgen = LogicSeq([[(ctx, 'f(h).', 0)]], 1, False, False)
        print('TEMPLATES:')
        outs = model.predict_on_batch(dgen[0])
        (ts, out) = (outs[0], outs[(- 1)])
        print(ts)
        ts = np.argmax(ts[0], axis=(- 1))
        ts = np.vectorize((lambda i: IDX_CHAR[i]))(ts)
        print(ts)
        print('CTX:', ctx)
        for o in outs[1:(- 1)]:
            print(o)
        print('OUT:', out)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if  ... :    else:
         ...  = np.argmax

idx = 35:------------------- similar code ------------------ index = 52, score = 5.0 
def test_model(model, tdf):
    probs = model(tdf['text'])
    preds = np.argmax(probs, axis=1)
    print('Results of categorisation on text fagment level')
    print(metrics(preds, tdf.label))
    print('Results per cell_content grouped using majority voting')
    results = preds_for_cell_content(tdf, probs)
    print(metrics(results['pred'], results['true']))
    print('Results per cell_content grouped with multi category mean')
    results = preds_for_cell_content_multi(tdf, probs)
    print(metrics(results['pred'], results['true']))
    print('Results per cell_content grouped with multi category mean - only on fragments from the same paper that the coresponding table')
    results = preds_for_cell_content_multi(tdf[tdf.this_paper], probs[tdf.this_paper])
    print(metrics(results['pred'], results['true']))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 36:------------------- similar code ------------------ index = 41, score = 5.0 
def np_library_matching(self, ids_predicted, ids_observed, y_predicted, y_observed, y_query):
    ids_library = np.concatenate([ids_predicted, ids_observed])
    np_library = self.np_normalize_rows(np.concatenate([y_predicted, y_observed], axis=0))
    np_similarities = np.dot(np_library, np.transpose(y_query))
    np_predictions = np.argmax(np_similarities, axis=0)
    np_predicted_ids = [ids_library[i] for i in np_predictions]
    return np_predicted_ids

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 37:------------------- similar code ------------------ index = 42, score = 5.0 
def export_segmentations_postprocess(indir, outdir):
    maybe_mkdir_p(outdir)
    niftis = subfiles(indir, suffix='nii.gz', join=False)
    for n in niftis:
        print('\n', n)
        identifier = str(n.split('_')[(- 1)][:(- 7)])
        outfname = join(outdir, ('test-segmentation-%s.nii' % identifier))
        img = sitk.ReadImage(join(indir, n))
        img_npy = sitk.GetArrayFromImage(img)
        (lmap, num_objects) = label((img_npy > 0).astype(int))
        sizes = []
        for o in range(1, (num_objects + 1)):
            sizes.append((lmap == o).sum())
        mx = (np.argmax(sizes) + 1)
        print(sizes)
        img_npy[(lmap != mx)] = 0
        img_new = sitk.GetImageFromArray(img_npy)
        img_new.CopyInformation(img)
        sitk.WriteImage(img_new, outfname)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in  ... :
         ...  = (np.argmax +  ... )

idx = 38:------------------- similar code ------------------ index = 50, score = 5.0 
@torch.no_grad()
def predict(self, eval_examples):
    'predicts the class for a list of known samples'
    eval_features = tools.convert_examples_to_features(eval_examples, self.label_list, self.max_seq_length, self.tokenizer)
    eval_examples_count = len(eval_examples)
    logger.info('***** Running evaluation *****')
    logger.info('  Num examples = %d', eval_examples_count)
    logger.info('  Batch size = %d', args.eval_batch_size)
    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)
    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)
    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)
    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)
    eval_sampler = SequentialSampler(eval_data)
    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)
    self.model.eval()
    if self.fp16:
        self.model.half()
    predictions = np.array([])
    truth = np.array([f.label_id for f in eval_features], dtype=int)
    for (input_ids, input_mask, segment_ids) in tqdm(eval_dataloader, desc='test batches'):
        input_ids = input_ids.to(self.device)
        input_mask = input_mask.to(self.device)
        segment_ids = segment_ids.to(self.device)
        logits = self.model(input_ids, segment_ids, input_mask)
        logits = logits.detach().cpu().numpy()
        predictions = np.append(predictions, np.argmax(logits, axis=1))
    lookup = self.processor.get_labels()
    map_classes = (lambda x: [lookup[int(item)] for item in x])
    return (map_classes(truth), map_classes(predictions))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
         ...  =  ... . ... ( ... , np.argmax)

idx = 39:------------------- similar code ------------------ index = 44, score = 5.0 
def train_actor(actor_model, critic_model, state_transitions, num_actor_training_samples, num_actions):
    random_actions = []
    for i in range(num_actor_training_samples):
        random_actions.append(((np.random.rand(num_actions) * 2) - 1))
    random_states = [s.state for s in state_transitions]
    for i in range(len(random_states)):
        with torch.no_grad():
            act = actor_model(torch.Tensor(random_states[i]).to(actor_model.device)).cpu().detach().numpy()
            random_actions.append(act)
    best_state_action = []
    for i_states in range(len(random_states)):
        QAs = []
        for i_actions in range(len(random_actions)):
            with torch.no_grad():
                qval = critic_model(torch.Tensor(torch.cat((torch.Tensor(random_states[i_states]), torch.Tensor(random_actions[i_actions])), 0)).to(critic_model.device)).cpu()
                QAs.append(qval)
        best_state_action.append(sars(random_states[i_states], random_actions[np.argmax(QAs)], 0.0, None, False, np.max(QAs)))
    t_random_states = torch.stack([torch.Tensor(s.state) for s in best_state_action]).to(actor_model.device)
    target_actions = torch.stack([torch.Tensor(s.action) for s in best_state_action]).to(actor_model.device)
    actor_model.zero_grad()
    predicted_actions = actor_model(t_random_states)
    loss = F.smooth_l1_loss(predicted_actions, target_actions).mean()
    loss.backward()
    actor_model.opt.step()
    return loss

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ... . ... ( ... (,  ... [np.argmax],  ... , None, False,))

idx = 40:------------------- similar code ------------------ index = 45, score = 5.0 
def visualize(img, seg_pred, exist_pred):
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    lane_img = np.zeros_like(img)
    color = np.array([[255, 125, 0], [0, 255, 0], [0, 0, 255], [0, 255, 255]], dtype='uint8')
    coord_mask = np.argmax(seg_pred, axis=0)
    for i in range(0, 4):
        if (exist_pred[(0, i)] > 0.5):
            lane_img[(coord_mask == (i + 1))] = color[i]
    img = cv2.addWeighted(src1=lane_img, alpha=0.8, src2=img, beta=1.0, gamma=0.0)
    return img

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 41:------------------- similar code ------------------ index = 36, score = 5.0 
def train_phase(predictor, train, valid, args):
    print('# classes:', train.n_classes)
    print('# samples:')
    print('-- train:', len(train))
    print('-- valid:', len(valid))
    train_batchsize = min((args.batchsize * len(args.gpu)), len(train))
    valid_batchsize = args.batchsize
    train_iter = chainer.iterators.MultiprocessIterator(train, train_batchsize)
    valid_iter = chainer.iterators.SerialIterator(valid, valid_batchsize, repeat=False, shuffle=True)
    class_weight = None
    lossfun = partial(softmax_cross_entropy, normalize=False, class_weight=class_weight)
    model = Classifier(predictor, lossfun=lossfun)
    if (args.gpu[0] >= 0):
        chainer.backends.cuda.get_device_from_id(args.gpu[0]).use()
        if (len(args.gpu) == 1):
            model.to_gpu()
    optimizer = chainer.optimizers.Adam(alpha=args.lr, beta1=0.9, beta2=0.999, eps=1e-08, amsgrad=False)
    optimizer.setup(model)
    if (args.decay > 0):
        optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(args.decay))
    if (len(args.gpu) == 1):
        updater = training.updaters.StandardUpdater(train_iter, optimizer, device=args.gpu[0])
    else:
        devices = {'main': args.gpu[0]}
        for (idx, g) in enumerate(args.gpu[1:]):
            devices[('slave_%d' % idx)] = g
        updater = training.updaters.ParallelUpdater(train_iter, optimizer, devices=devices)
    frequency = (max((args.iteration // 20), 1) if (args.frequency == (- 1)) else max(1, args.frequency))
    stop_trigger = triggers.EarlyStoppingTrigger(monitor='validation/main/loss', max_trigger=(args.iteration, 'iteration'), check_trigger=(frequency, 'iteration'), patients=(np.inf if (args.pinfall == (- 1)) else max(1, args.pinfall)))
    trainer = training.Trainer(updater, stop_trigger, out=args.out)
    transforms = {'x': (lambda x: x), 'y': (lambda x: np.argmax(x, axis=0)), 't': (lambda x: x)}
    cmap = np.array([[0, 0, 0], [0, 0, 1]])
    cmaps = {'x': None, 'y': cmap, 't': cmap}
    clims = {'x': 'minmax', 'y': None, 't': None}
    visualizer = ImageVisualizer(transforms=transforms, cmaps=cmaps, clims=clims)
    valid_file = os.path.join('validation', 'iter_{.updater.iteration:08}.png')
    trainer.extend(Validator(valid_iter, model, valid_file, visualizer=visualizer, n_vis=20, device=args.gpu[0]), trigger=(frequency, 'iteration'))
    trainer.extend(extensions.dump_graph('main/loss'))
    trainer.extend(extensions.snapshot(filename='snapshot_iter_{.updater.iteration:08}.npz'), trigger=(frequency, 'iteration'))
    trainer.extend(extensions.snapshot_object(predictor, 'predictor_iter_{.updater.iteration:08}.npz'), trigger=(frequency, 'iteration'))
    log_keys = ['main/loss', 'validation/main/loss', 'main/accuracy', 'validation/main/accuracy']
    trainer.extend(LogReport(keys=log_keys))
    if extensions.PlotReport.available():
        for plot_key in ['loss', 'accuracy']:
            plot_keys = [key for key in log_keys if key.split('/')[(- 1)].startswith(plot_key)]
            trainer.extend(extensions.PlotReport(plot_keys, 'iteration', file_name=(plot_key + '.png'), trigger=(frequency, 'iteration')))
    trainer.extend(PrintReport(((['iteration'] + log_keys) + ['elapsed_time']), n_step=100))
    trainer.extend(extensions.ProgressBar())
    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)
    trainer.run()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = { ... :,  ... : (lambda  ... : np.argmax),  ... :}

idx = 42:------------------- similar code ------------------ index = 47, score = 5.0 
def test(t_data, t_label, test_iterations=1, evalate=False):
    assert (test_data.shape[0] == test_label.shape[0])
    y_predict_class = model['predict_class_number']
    (overAllAcc, avgAcc, averageAccClass) = ([], [], [])
    for _ in range(test_iterations):
        pred_class = []
        for t in tqdm(t_data):
            t = np.expand_dims(t, axis=0)
            feed_dict_test = {img_entry: t, prob: 1.0}
            prediction = session.run(y_predict_class, feed_dict=feed_dict_test)
            pred_class.append(prediction)
        true_class = np.argmax(t_label, axis=1)
        conMatrix = confusion_matrix(true_class, pred_class)
        classArray = []
        for c in range(len(conMatrix)):
            recallScore = (conMatrix[c][c] / sum(conMatrix[c]))
            classArray += [recallScore]
        averageAccClass.append(classArray)
        avgAcc.append((sum(classArray) / len(classArray)))
        overAllAcc.append(accuracy_score(true_class, pred_class))
    averageAccClass = np.transpose(averageAccClass)
    meanPerClass = np.mean(averageAccClass, axis=1)
    showClassTable(meanPerClass, title='Class accuracy')
    print(('Average Accuracy: ' + str(np.mean(avgAcc))))
    print(('Overall Accuracy: ' + str(np.mean(overAllAcc))))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ...  = np.argmax

idx = 43:------------------- similar code ------------------ index = 46, score = 5.0 
def forward(self, x, t):
    h = self.bn1(self.conv1(x))
    h = F.max_pooling_2d(F.relu(h), 3, stride=2)
    h = self.res2(h)
    h = self.res3(h)
    h = self.res4(h)
    h = self.res5(h)
    h = F.average_pooling_2d(h, 7, stride=1)
    h = self.fc(h)
    loss = self.softmax_cross_entropy(h, t)
    if self.compute_accuracy:
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, np.argmax(t, axis=1))}, self)
    else:
        chainer.report({'loss': loss}, self)
    return loss

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ... . ... ({ ... :  ... ,  ... :  ... . ... ( ... , np.argmax)},  ... )

idx = 44:------------------- similar code ------------------ index = 29, score = 5.0 
def assignOffspring(self, species, pop, p):
    "Assigns number of offspring to each species based on fitness sharing.\n  NOTE: Ordinal rather than the cardinal fitness of canonical NEAT is used.\n\n  Args:\n    species - (Species) - this generation's species\n      .members    - [Ind]   - individuals in species\n    pop     - [Ind]     - individuals with species assigned\n      .fitness    - (float) - performance on task (higher is better)\n    p       - (Dict)    - algorithm hyperparameters\n\n  Returns:\n    species - (Species) - This generation's species\n      .nOffspring - (int) - number of children to produce\n  "
    nSpecies = len(species)
    if (nSpecies == 1):
        species[0].offspring = p['popSize']
    else:
        popFit = np.asarray([ind.fitness for ind in pop])
        popRank = tiedRank(popFit)
        if (p['select_rankWeight'] == 'exp'):
            rankScore = (1 / popRank)
        elif (p['select_rankWeight'] == 'lin'):
            rankScore = (1 + abs((popRank - len(popRank))))
        else:
            print('Invalid rank weighting (using linear)')
            rankScore = (1 + abs((popRank - len(popRank))))
        specId = np.asarray([ind.species for ind in pop])
        speciesFit = np.zeros((nSpecies, 1))
        speciesTop = np.zeros((nSpecies, 1))
        for iSpec in range(nSpecies):
            if (not np.any((specId == iSpec))):
                speciesFit[iSpec] = 0
            else:
                speciesFit[iSpec] = np.mean(rankScore[(specId == iSpec)])
                speciesTop[iSpec] = np.max(popFit[(specId == iSpec)])
                if (speciesTop[iSpec] > species[iSpec].bestFit):
                    species[iSpec].bestFit = speciesTop[iSpec]
                    bestId = np.argmax(popFit[(specId == iSpec)])
                    species[iSpec].bestInd = species[iSpec].members[bestId]
                    species[iSpec].lastImp = 0
                else:
                    species[iSpec].lastImp += 1
                if (species[iSpec].lastImp > p['spec_dropOffAge']):
                    speciesFit[iSpec] = 0
        if (sum(speciesFit) == 0):
            speciesFit = np.ones((nSpecies, 1))
            print('WARN: Entire population stagnant, continuing without extinction')
        offspring = bestIntSplit(speciesFit, p['popSize'])
        for iSpec in range(nSpecies):
            species[iSpec].nOffspring = offspring[iSpec]
    species[:] = [s for s in species if (s.nOffspring != 0)]
    return species

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:    else:
        for  ...  in:
            if:            else:
                if:
                     ...  = np.argmax

idx = 45:------------------- similar code ------------------ index = 35, score = 5.0 
def lossfunc(state, net_kw_const={}, run_kw_const={}, validate=True, val_patience=np.inf, test=False, numepochs=100, dataset_code='T', run_network_kw={}, penalize='t_epoch', wc=0.1, tbar_epoch=1, numparams_bar=4000000.0, problem_type='classification'):
    "\n    *** Wrapper function for run_network. Given a net, find its model search loss and other statistics ***\n    \n    Net is described using:\n        state : Params being optimized over\n        net_kw_const, run_kw_const : Params not being optimized over\n    These combine to form net_kw and run_kw for run_network\n        \n    Parameters of run_network which might change according to lossfunc are described using:\n        validate, val_patience, test, numepochs\n        \n    Other parameters of run_network which are not expected to change are given in run_network_kw\n        Example: data, input_size, output_size, num_workers, pin_memory, wt_init, bias_init, verbose\n        \n    penalize: Either 't_epoch' or 'numparams' (CNNs only support t_epoch)\n    wc : weightage given to complexity term\n    tbar_epoch : used to normalize training time\n    numparams_bar: Used to normalize number of parameters\n    \n    Returns: loss_stats dictionary. Most important key is 'loss', which gives model search loss\n    "
    net_kw = {**net_kw_const, **{key: state[key] for key in state.keys() if (key in net_kws_defaults)}}
    run_kw = {**run_kw_const, **{key: state[key] for key in state.keys() if (key in run_kws_defaults)}}
    if ('weight_decay' not in state.keys()):
        run_kw['weight_decay'] = default_weight_decay(dataset_code=dataset_code, input_size=run_network_kw['input_size'], output_size=run_network_kw['output_size'], net_kw=net_kw)
    (net, recs) = run_network(net_kw=net_kw, run_kw=run_kw, validate=validate, val_patience=val_patience, test=test, numepochs=numepochs, **run_network_kw)
    numparams = get_numparams(input_size=run_network_kw['input_size'], output_size=run_network_kw['output_size'], net_kw=net_kw)
    loss_stats = {}
    if (problem_type == 'classification'):
        (acc, ep) = (np.max(recs['val_accs']), (np.argmax(recs['val_accs']) + 1))
        fp = ((100 - acc) / 100.0)
        loss_stats['best_val_acc'] = np.max(recs['val_accs'])
    elif (problem_type == 'regression'):
        (loss, ep) = (np.min(recs['val_losses']), (np.argmin(recs['val_losses']) + 1))
        scale = 10
        fp = (loss * scale)
        loss_stats['best_val_loss'] = np.min(recs['val_losses'])
    fc = ((recs['t_epoch'] / tbar_epoch) if (penalize == 't_epoch') else (numparams / numparams_bar))
    loss_stats['loss'] = np.log10((fp + (wc * fc)))
    loss_stats['t_epoch'] = recs['t_epoch']
    loss_stats['numparams'] = numparams
    return loss_stats

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
 = (, (np.argmax +  ... ))

idx = 46:------------------- similar code ------------------ index = 14, score = 5.0 
if (__name__ == '__main__'):
    n_neurons = 784
    n_timesteps = 4
    initial_conditions = read_image_activities('./mnist-class3.csv')
    initial_weights = read_weights('./mlp_layer1-2_weights.csv')
    timestep_to_weights = {1: read_weights('./mlp_layer2-3_weights.csv'), 2: read_weights('./mlp_layer3-4_weights.csv'), 3: {}}
    timestep_to_activation = {1: relu, 2: relu, 3: identity}
    network = ntm.Network()
    [network.add_edge(i, j) for i in range(n_neurons) for j in range(n_neurons)]
    set_weights(network, initial_weights)

    def activity_rule(ctx):
        V = 0
        for neighbour_label in ctx.neighbour_labels:
            V += (ctx.connection_states[neighbour_label][0]['weight'] * ctx.activities[neighbour_label])
        activity = timestep_to_activation[ctx.timestep](V)
        return activity

    def topology_rule(ctx):
        curr_network = ctx.network
        new_weights = timestep_to_weights[ctx.timestep]
        set_weights(curr_network, new_weights)
        return curr_network
    trajectory = ntm.evolve(network, initial_conditions=initial_conditions, activity_rule=activity_rule, topology_rule=topology_rule, update_order=ntm.UpdateOrder.ACTIVITIES_FIRST, timesteps=n_timesteps)
    vals = [trajectory[(- 1)].activities[i] for i in range(10)]
    plt.title(('predicted class: %s' % np.argmax(np.log(softmax(vals)))))
    plt.imshow(np.array([initial_conditions[i] for i in range(n_neurons)]).reshape((28, 28)), cmap='gray_r')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
     ... . ... (( ...  % np.argmax))

idx = 47:------------------- similar code ------------------ index = 1, score = 5.0 
def model_prediction(model, inputs):
    prob = model.model.predict(inputs)
    predicted_class = np.argmax(prob)
    prob_str = np.array2string(prob).replace('\n', '')
    return (prob, predicted_class, prob_str)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 48:------------------- similar code ------------------ index = 3, score = 5.0 
def ddk_gaussian(filter_type, D, visible=None):
    "\n    Given a specific type of DDK filter and the maximum SHC degree number, evaluate the 'equivalent' Gaussian filter radius. \n    Different Gaussian filter radius corresponds to a different correlation between DDK filer and Gaussian filter.\n    According to the rule that the largest correlation corresponds to the 'equivalent' radius, this program try to find out and visualize them.\n\n    Usage:\n    ddk_gausian('DDK5',96)\n    ddk_gausian('DDK3',60,'visible')\n\n    Inputs:\n    filter_type -> [str] Types of DDK filter. Available options are 'DDK1' to 'DDK8'.\n    D -> [int] Degree and order of SHC\n\n    Parameters:\n    visible -> [optional, str, default = None] If None, the visualization of the Point Spreading Function(PSF), DDK filtered PSF, Gaussian filtered PSF at the northpole as well as their cross section will be closed. If 'visible', they will be visualized by outputing an image.\n    \n    Outputs:\n    Print the 'equivalent' Gaussian filter radius and the correlation coefficient between the given DDK filer and the Gaussian filter with the 'equivalent' radius. \n    Also, images of the Point Spreading Function(PSF), DDK filtered PSF, Gaussian filtered PSF at the northpole as well as their cross section can be generated in the figures directory. \n    "
    cap_at_equator_grids_class = SHGrid.from_cap(0.1, 0, 0, D)
    cap_at_equator_coeffs_class = cap_at_equator_grids_class.expand()
    cap_at_northpole_coeffs_class = cap_at_equator_coeffs_class.rotate(0, 90, 0)
    cap_at_northpole_grids_class = cap_at_northpole_coeffs_class.expand()
    cap_at_northpole_coeffs = cap_at_northpole_coeffs_class.coeffs
    cap_at_northpole_grids = cap_at_northpole_grids_class.data
    filt_SHC_ddk = filter_ddk(filter_type, cap_at_northpole_coeffs)
    ddk_power = np.sum((filt_SHC_ddk ** 2))
    corrs = []
    rs = range(20, 550, 5)
    for r in rs:
        filt_SHC_gau = filter_gaussian(r, cap_at_northpole_coeffs)
        gau_power = np.sum((filt_SHC_gau ** 2))
        ddk_gau_crosspower = np.sum((filt_SHC_ddk * filt_SHC_gau))
        alpha = (ddk_gau_crosspower / np.sqrt((ddk_power * gau_power)))
        corrs.append(alpha)
    corrs = np.array(corrs)
    max_corrs_index = np.argmax(corrs)
    filt_SHC_gau = filter_gaussian(rs[max_corrs_index], cap_at_northpole_coeffs)
    print('Correlation: {:.4f}\nApproximate equivalent Gaussian filter radius for {:s}: {:d}'.format(corrs[max_corrs_index], filter_type, rs[max_corrs_index]))
    thetas = np.arange(0, 20, 0.1)
    (value, value_ddk, value_gau) = ([], [], [])
    for theta in thetas:
        value.append(MakeGridPoint(cap_at_northpole_coeffs, (90 - theta), 0))
        value_ddk.append(MakeGridPoint(filt_SHC_ddk, (90 - theta), 0))
        value_gau.append(MakeGridPoint(filt_SHC_gau, (90 - theta), 0))
    value = np.array(value)
    value_ddk = np.array(value_ddk)
    value_gau = np.array(value_gau)
    if (visible is not None):
        fig_dir = 'figures/'
        if (not path.exists(fig_dir)):
            makedirs(fig_dir)
        cap_at_northpole_grids_ddk = MakeGridDH(filt_SHC_ddk, sampling=2)
        cap_at_northpole_grids_gau = MakeGridDH(filt_SHC_gau, sampling=2)
        (lons, lats) = (cap_at_northpole_grids_class.lons(), cap_at_northpole_grids_class.lats())
        fig_name = 'raw_ddk_gau.png'
        (fig_name1, fig_name2, fig_name3) = ('psf_raw.png', 'psf_ddk.png', 'psf_gau.png')
        magnify = 1000.0
        plot_at_northpole(lons, lats, cap_at_northpole_grids, (fig_dir + fig_name1), '[mm w.e.]', magnify)
        plot_at_northpole(lons, lats, cap_at_northpole_grids_ddk, (fig_dir + fig_name2), '[mm w.e.]', magnify)
        plot_at_northpole(lons, lats, cap_at_northpole_grids_gau, (fig_dir + fig_name3), '[mm w.e.]', magnify)
        fig = plt.figure(dpi=200)
        ax = fig.add_subplot(111)
        ax.plot(thetas, (value * magnify), 'y', label='raw')
        ax.plot(thetas, (value_ddk * magnify), 'b--', label='ddk')
        ax.plot(thetas, (value_gau * magnify), 'r--', label='gaussian')
        ax.set_xlabel('$\\theta$ [deg]', size='small')
        ax.set_ylabel('[mm w.e.]', size='small')
        ax.legend()
        plt.savefig((fig_dir + fig_name), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 49:------------------- similar code ------------------ index = 4, score = 5.0 
def accuracy(outputs, labels):
    '\n    Compute the accuracy, given the outputs and labels for all images.\n\n    Returns: (float) accuracy in [0,1]\n    '
    outputs = np.argmax(outputs, axis=1)
    return (np.sum((outputs == labels)) / float(labels.size))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 50:------------------- similar code ------------------ index = 5, score = 5.0 
def makeChord(self):
    line = gic.GraphicsCollection()
    color = QtGui.QColor(52, 235, 122, 255)
    line.pen.setColor(color)
    line.pen.setWidthF(2.5)
    line.pen.setCosmetic(True)
    line.pen.setStyle(QtCore.Qt.CustomDashLine)
    stroke = 10
    dot = 1
    space = 5
    line.pen.setDashPattern([stroke, space, dot, space])
    index_min = np.argmin(self.raw_coordinates[0])
    index_max = np.argmax(self.raw_coordinates[0])
    x1 = self.raw_coordinates[0][index_min]
    y1 = self.raw_coordinates[1][index_min]
    x2 = self.raw_coordinates[0][index_max]
    y2 = self.raw_coordinates[1][index_max]
    line.Line(x1, y1, x2, y2)
    self.chord = GraphicsItem.GraphicsItem(line)
    self.chord.setZValue(99)
    self.chord.setAcceptHoverEvents(False)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.argmax

idx = 51:------------------- similar code ------------------ index = 6, score = 5.0 
def plan_experiment(self):
    use_nonzero_mask_for_normalization = self.determine_whether_to_use_mask_for_norm()
    print('Are we using the nonzero mask for normalizaion?', use_nonzero_mask_for_normalization)
    spacings = self.dataset_properties['all_spacings']
    sizes = self.dataset_properties['all_sizes']
    all_classes = self.dataset_properties['all_classes']
    modalities = self.dataset_properties['modalities']
    num_modalities = len(list(modalities.keys()))
    target_spacing = self.get_target_spacing()
    new_shapes = [((np.array(i) / target_spacing) * np.array(j)) for (i, j) in zip(spacings, sizes)]
    max_spacing_axis = np.argmax(target_spacing)
    remaining_axes = [i for i in list(range(3)) if (i != max_spacing_axis)]
    self.transpose_forward = ([max_spacing_axis] + remaining_axes)
    self.transpose_backward = [np.argwhere((np.array(self.transpose_forward) == i))[0][0] for i in range(3)]
    median_shape = np.median(np.vstack(new_shapes), 0)
    print('the median shape of the dataset is ', median_shape)
    max_shape = np.max(np.vstack(new_shapes), 0)
    print('the max shape in the dataset is ', max_shape)
    min_shape = np.min(np.vstack(new_shapes), 0)
    print('the min shape in the dataset is ', min_shape)
    print("we don't want feature maps smaller than ", self.unet_featuremap_min_edge_length, ' in the bottleneck')
    self.plans_per_stage = list()
    target_spacing_transposed = np.array(target_spacing)[self.transpose_forward]
    median_shape_transposed = np.array(median_shape)[self.transpose_forward]
    print('the transposed median shape of the dataset is ', median_shape_transposed)
    print('generating configuration for 3d_fullres')
    self.plans_per_stage.append(self.get_properties_for_stage(target_spacing_transposed, target_spacing_transposed, median_shape_transposed, len(self.list_of_cropped_npz_files), num_modalities, (len(all_classes) + 1)))
    architecture_input_voxels_here = np.prod(self.plans_per_stage[(- 1)]['patch_size'], dtype=np.int64)
    if ((np.prod(self.plans_per_stage[(- 1)]['median_patient_size_in_voxels'], dtype=np.int64) / architecture_input_voxels_here) < self.how_much_of_a_patient_must_the_network_see_at_stage0):
        more = False
    else:
        more = True
    if more:
        print('generating configuration for 3d_lowres')
        lowres_stage_spacing = deepcopy(target_spacing)
        num_voxels = np.prod(median_shape, dtype=np.int64)
        while (num_voxels > (self.how_much_of_a_patient_must_the_network_see_at_stage0 * architecture_input_voxels_here)):
            max_spacing = max(lowres_stage_spacing)
            if np.any(((max_spacing / lowres_stage_spacing) > 2)):
                lowres_stage_spacing[((max_spacing / lowres_stage_spacing) > 2)] *= 1.01
            else:
                lowres_stage_spacing *= 1.01
            num_voxels = np.prod(((target_spacing / lowres_stage_spacing) * median_shape), dtype=np.int64)
            lowres_stage_spacing_transposed = np.array(lowres_stage_spacing)[self.transpose_forward]
            new = self.get_properties_for_stage(lowres_stage_spacing_transposed, target_spacing_transposed, median_shape_transposed, len(self.list_of_cropped_npz_files), num_modalities, (len(all_classes) + 1))
            architecture_input_voxels_here = np.prod(new['patch_size'], dtype=np.int64)
        if ((2 * np.prod(new['median_patient_size_in_voxels'], dtype=np.int64)) < np.prod(self.plans_per_stage[0]['median_patient_size_in_voxels'], dtype=np.int64)):
            self.plans_per_stage.append(new)
    self.plans_per_stage = self.plans_per_stage[::(- 1)]
    self.plans_per_stage = {i: self.plans_per_stage[i] for i in range(len(self.plans_per_stage))}
    print(self.plans_per_stage)
    print('transpose forward', self.transpose_forward)
    print('transpose backward', self.transpose_backward)
    normalization_schemes = self.determine_normalization_scheme()
    (only_keep_largest_connected_component, min_size_per_class, min_region_size_per_class) = (None, None, None)
    plans = {'num_stages': len(list(self.plans_per_stage.keys())), 'num_modalities': num_modalities, 'modalities': modalities, 'normalization_schemes': normalization_schemes, 'dataset_properties': self.dataset_properties, 'list_of_npz_files': self.list_of_cropped_npz_files, 'original_spacings': spacings, 'original_sizes': sizes, 'preprocessed_data_folder': self.preprocessed_output_folder, 'num_classes': len(all_classes), 'all_classes': all_classes, 'base_num_features': self.unet_base_num_features, 'use_mask_for_norm': use_nonzero_mask_for_normalization, 'keep_only_largest_region': only_keep_largest_connected_component, 'min_region_size_per_class': min_region_size_per_class, 'min_size_per_class': min_size_per_class, 'transpose_forward': self.transpose_forward, 'transpose_backward': self.transpose_backward, 'data_identifier': self.data_identifier, 'plans_per_stage': self.plans_per_stage, 'preprocessor_name': self.preprocessor_name, 'conv_per_stage': self.conv_per_stage}
    self.plans = plans
    self.save_my_plans()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.argmax

idx = 52:------------------- similar code ------------------ index = 8, score = 5.0 
def _plot_reconstruction(self, updater, fetched):
    inp = fetched['inp']
    output = fetched['output']
    prediction = fetched.get('prediction', None)
    targets = fetched.get('targets', None)
    sqrt_N = int(np.ceil(np.sqrt(self.N)))
    fig_height = 20
    fig_width = (4.5 * fig_height)
    (fig, axes) = plt.subplots(sqrt_N, (3 * sqrt_N), figsize=(fig_width, fig_height))
    for (n, (pred, gt)) in enumerate(zip(output, inp)):
        i = int((n / sqrt_N))
        j = int((n % sqrt_N))
        ax = axes[(i, (3 * j))]
        ax.set_axis_off()
        self.imshow(ax, gt)
        if (targets is not None):
            _target = targets[n]
            _prediction = prediction[n]
            title = 'target={}, prediction={}'.format(np.argmax(_target), np.argmax(_prediction))
            ax.set_title(title)
        ax = axes[(i, ((3 * j) + 1))]
        ax.set_axis_off()
        self.imshow(ax, pred)
        ax = axes[(i, ((3 * j) + 2))]
        ax.set_axis_off()
        diff = (np.abs((gt - pred)).sum(2) / 3)
        self.imshow(ax, diff)
    plt.subplots_adjust(left=0, right=1, top=0.9, bottom=0, wspace=0.1, hspace=0.2)
    self.savefig('sampled_reconstruction', fig, updater)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
        if:
             ...  =  ... . ... (np.argmax,)

idx = 53:------------------- similar code ------------------ index = 9, score = 5.0 
def accuracy(output, labels):
    corr_output = np.argmax(output, axis=1)
    acc = (np.sum((corr_output == labels)) / float(labels.size))
    return acc

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 54:------------------- similar code ------------------ index = 10, score = 5.0 
def main(opt):
    (TRAIN, VALIDATION, TEST) = maybeExtract(opt.data, opt.patch_size)
    (training_data, training_label) = (TRAIN[0], TRAIN[1])
    (validation_data, validation_label) = (VALIDATION[0], VALIDATION[1])
    (test_data, test_label) = (TEST[0], TEST[1])
    print('\nData shapes')
    print(('training_data shape' + str(training_data.shape)))
    print((('training_label shape' + str(training_label.shape)) + '\n'))
    print(('validation_data shape' + str(validation_data.shape)))
    print((('validation_label shape' + str(validation_label.shape)) + '\n'))
    print(('test_data shape' + str(test_data.shape)))
    print((('test_label shape' + str(test_label.shape)) + '\n'))
    SIZE = training_data.shape[0]
    HEIGHT = training_data.shape[1]
    WIDTH = training_data.shape[2]
    CHANNELS = training_data.shape[3]
    N_PARALLEL_BAND = number_of_band[opt.data]
    NUM_CLASS = training_label.shape[1]
    EPOCHS = opt.epoch
    BATCH = opt.batch_size
    graph = tf.Graph()
    with graph.as_default():
        img_entry = tf.placeholder(tf.float32, shape=[None, WIDTH, HEIGHT, CHANNELS])
        img_label = tf.placeholder(tf.uint8, shape=[None, NUM_CLASS])
        image_true_class = tf.argmax(img_label, axis=1)
        prob = tf.placeholder(tf.float32)
        model = net(img_entry, prob, HEIGHT, WIDTH, CHANNELS, N_PARALLEL_BAND, NUM_CLASS)
        final_layer = model['dense3']
        with tf.name_scope('loss'):
            cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer, labels=img_label)
            cost = tf.reduce_mean(cross_entropy)
        with tf.name_scope('adam_optimizer'):
            optimizer = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(cost)
        with tf.name_scope('accuracy'):
            predict_class = model['predict_class_number']
            correction = tf.equal(predict_class, image_true_class)
        accuracy = tf.reduce_mean(tf.cast(correction, tf.float32))
        saver = tf.train.Saver()
        with tf.Session(graph=graph) as session:
            session.run(tf.global_variables_initializer())

            def test(t_data, t_label, test_iterations=1, evalate=False):
                assert (test_data.shape[0] == test_label.shape[0])
                y_predict_class = model['predict_class_number']
                (overAllAcc, avgAcc, averageAccClass) = ([], [], [])
                for _ in range(test_iterations):
                    pred_class = []
                    for t in tqdm(t_data):
                        t = np.expand_dims(t, axis=0)
                        feed_dict_test = {img_entry: t, prob: 1.0}
                        prediction = session.run(y_predict_class, feed_dict=feed_dict_test)
                        pred_class.append(prediction)
                    true_class = np.argmax(t_label, axis=1)
                    conMatrix = confusion_matrix(true_class, pred_class)
                    classArray = []
                    for c in range(len(conMatrix)):
                        recallScore = (conMatrix[c][c] / sum(conMatrix[c]))
                        classArray += [recallScore]
                    averageAccClass.append(classArray)
                    avgAcc.append((sum(classArray) / len(classArray)))
                    overAllAcc.append(accuracy_score(true_class, pred_class))
                averageAccClass = np.transpose(averageAccClass)
                meanPerClass = np.mean(averageAccClass, axis=1)
                showClassTable(meanPerClass, title='Class accuracy')
                print(('Average Accuracy: ' + str(np.mean(avgAcc))))
                print(('Overall Accuracy: ' + str(np.mean(overAllAcc))))

            def train(num_iterations, train_batch_size=50):
                maxValidRate = 0
                for i in range((num_iterations + 1)):
                    print(('Optimization Iteration: ' + str(i)))
                    for x in range((int((SIZE / train_batch_size)) + 1)):
                        train_batch = training_data[(x * train_batch_size):((x + 1) * train_batch_size)]
                        train_batch_label = training_label[(x * train_batch_size):((x + 1) * train_batch_size)]
                        feed_dict_train = {img_entry: train_batch, img_label: train_batch_label, prob: 0.5}
                        (_, loss_val) = session.run([optimizer, cross_entropy], feed_dict=feed_dict_train)
                    if ((i % 15) == 0):
                        acc = session.run(accuracy, feed_dict={img_entry: validation_data, img_label: validation_label, prob: 1.0})
                        print('Model Performance, Validation accuracy: ', (acc * 100))
                        if (maxValidRate < acc):
                            location = i
                            maxValidRate = acc
                            saver.save(session, ((('./Trained_model/' + str(opt.data)) + '/the3dnetwork-') + opt.data))
                        print('Maximum validation accuracy: ', acc, ' at epoch ', location)
                        test(validation_data, validation_label, 1)

            def count_param():
                total_parameters = 0
                for variable in tf.trainable_variables():
                    shape = variable.get_shape()
                    variable_parameters = 1
                    for dim in shape:
                        variable_parameters *= dim.value
                    total_parameters += variable_parameters
                print(((('Trainable parameters: ' + '\x1b[92m') + str(total_parameters)) + '\x1b[0m'))
            count_param()
            train(num_iterations=EPOCHS, train_batch_size=BATCH)
            test(test_data, test_label, test_iterations=1)
            print(('End session ' + str(opt.data)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    with:
        with:
            def  ... ():
                for  ...  in:
                     ...  = np.argmax

idx = 55:------------------- similar code ------------------ index = 11, score = 5.0 
def _update_model(self, X_all, Y_all_raw, itr=0):
    '\n        :param X_all: observed input data\n        :param Y_all_raw: observed output raw data\n        :param itr: BO iteration counter\n        '
    if self.normalize_Y:
        Y_all = ((Y_all_raw - Y_all_raw.mean()) / Y_all_raw.std())
    else:
        Y_all = Y_all_raw
    if self.sparse.startswith('SUB'):
        (X_all, Y_all) = subset_select(X_all, Y_all, select_metric=self.sparse)
    if (self.model is None):
        self.input_dim = X_all.shape[1]
        self.input_dim_opt_ex = list(range(self.input_dim))
    else:
        self.model.set_XY(X_all, Y_all)
    if ((itr % int((self.update_interval * 8))) == 0):
        input_dim_permutate_list = [np.random.permutation(range(self.input_dim)) for i in range(self.n_decomp)]
        input_dim_permutate_list.append(self.input_dim_opt_ex)
        if ('ADD' in self.sparse):
            print('learn the decomposition with subset observed data')
            (X_ob, Y_ob) = subset_select_for_learning(X_all, Y_all, select_metric=self.sparse)
        else:
            (X_ob, Y_ob) = (X_all, Y_all)
        ll_list = []
        submodel_list = []
        for (i, input_dim_i) in enumerate(input_dim_permutate_list):
            (sub_model_i, ll_i) = self._create_model_sub(X_ob, Y_ob, input_dim_i)
            print(f'll for decom {i} ={ll_i}')
            ll_list.append(ll_i)
            submodel_list.append(sub_model_i)
        mlh_idx = np.argmax(ll_list)
        self.model = submodel_list[mlh_idx]
        self.model.set_XY(X_all, Y_all)
        input_dim_opt = input_dim_permutate_list[mlh_idx]
        self.active_dims_list = split(input_dim_opt, self.n_sub)
        self.model_kern_list = [self.model.kern.__dict__[f'k{k_indx}'] for k_indx in range(self.n_sub)]
        self.input_dim_opt_ex = input_dim_opt.copy()
        print(f'opt_decom={mlh_idx}')
    if ((itr % self.update_interval) == 0):
        self.model.optimize_restarts(num_restarts=self.optimize_restarts, optimizer=self.optimizer, max_iters=self.max_iters, verbose=self.verbose)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.argmax

idx = 56:------------------- similar code ------------------ index = 12, score = 5.0 
if (__name__ == '__main__'):
    (epoch_num, batch_size, train_type, train_percent, dev_percent, learn_rate, model_type, device) = parse_args()
    print('\n=====Arguments====')
    print('epoch num {}'.format(epoch_num))
    print('batch size {}'.format(batch_size))
    print('train type {}'.format(train_type))
    print('train percent {}'.format(train_percent))
    print('dev percent {}'.format(dev_percent))
    print('learn rate {}'.format(learn_rate))
    print('model type {}'.format(model_type))
    print('device {}'.format(device))
    print('=====Arguments====\n')
    if ((train_percent + dev_percent) >= 1.0):
        print('ERROR! Train data percentage plus dev data percentage is {}! Make sure the sum is below 1.0!'.format((train_percent + dev_percent)))
        exit(1)
    BERT_VEC_LENGTH = 1024
    (deep_model, optimiser) = build_model(model_type, (BERT_VEC_LENGTH * 2), learn_rate)
    if ('gpu' in device):
        deep_model.to('cuda')
    sorted_scores = read_sorted_scores()
    (train, dev, test, all) = parse_split_data(sorted_scores, train_percent, dev_percent)
    train_pairs = build_pairs(train)
    dev_pairs = build_pairs(dev)
    test_pairs = build_pairs(test)
    print(len(train_pairs), len(dev_pairs), len(test_pairs))
    with open('data/doc_summ_bert_vectors.pkl', 'rb') as ff:
        all_vec_dic = pickle.load(ff)
    pcc_list = []
    weights_list = []
    for ii in range(epoch_num):
        print('\n=====EPOCH {}====='.format(ii))
        loss = pair_train_rewarder(all_vec_dic, train_pairs, deep_model, optimiser, batch_size, device)
        print('--> loss', loss)
        results = test_rewarder(all_vec_dic, dev, deep_model, device)
        for metric in results:
            print('{}\t{}'.format(metric, np.mean(results[metric])))
        pcc_list.append(np.mean(results['pcc']))
        weights_list.append(copy.deepcopy(deep_model.state_dict()))
    idx = np.argmax(pcc_list)
    best_result = pcc_list[idx]
    print('\n======Best results come from epoch no. {}====='.format(idx))
    deep_model.load_state_dict(weights_list[idx])
    test_results = test_rewarder(all_vec_dic, test, deep_model, device)
    print('Its performance on the test set is:')
    for metric in test_results:
        print('{}\t{}'.format(metric, np.mean(test_results[metric])))
    model_weight_name = 'pcc{0:.4f}_'.format(np.mean(test_results['pcc']))
    model_weight_name += 'epoch{}_batch{}_{}_trainPercent{}_lrate{}_{}.model'.format(epoch_num, batch_size, train_type, train_percent, learn_rate, model_type)
    torch.save(weights_list[idx], os.path.join(MODEL_WEIGHT_DIR, model_weight_name))
    print('\nbest model weight saved to: {}'.format(os.path.join(MODEL_WEIGHT_DIR, model_weight_name)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
     ...  = np.argmax

idx = 57:------------------- similar code ------------------ index = 13, score = 5.0 
def test_classification():
    _categorical_cmaps = {'y': _default_cmap, 't': _default_cmap}
    _categorical_clims = {'x': (0.0, 1.0)}
    _categorical_transforms = {'x': (lambda x: x), 'y': (lambda x: np.argmax(x, axis=0)), 't': (lambda x: np.argmax(x, axis=0))}
    visualizer = ImageVisualizer(transforms=_categorical_transforms, cmaps=_categorical_cmaps, clims=_categorical_clims)
    x = np.random.rand(3, 100, 200)
    y = np.random.rand(10, 100, 200)
    t = np.random.rand(10, 100, 200)
    for _ in range(3):
        visualizer.add_example(x, y, t)
    visualizer.save('test_classification.png')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = { ... :,  ... : (lambda  ... : np.argmax),  ... :}

idx = 58:------------------- similar code ------------------ index = 15, score = 5.0 
def ErrorRateAt95Recall(labels, scores):
    distances = (1.0 / (scores + 1e-08))
    recall_point = 0.95
    labels = labels[np.argsort(distances)]
    threshold_index = np.argmax((np.cumsum(labels) >= (recall_point * np.sum(labels))))
    FP = np.sum((labels[:threshold_index] == 0))
    TN = np.sum((labels[threshold_index:] == 0))
    return (float(FP) / float((FP + TN)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 59:------------------- similar code ------------------ index = 34, score = 5.0 
def _update_periodogram(self, replace_idx, freqs_fine, pers_fine):
    new_best = np.argmax(pers_fine[0])
    if (pers_fine[0][new_best] > self.per[replace_idx]):
        self.freq[replace_idx] = freqs_fine[new_best]
        self.per[replace_idx] = pers_fine[0][new_best]
        for filter_name in self.filter_names:
            self.per_single_band[filter_name][replace_idx] = pers_fine[1][filter_name][new_best]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 60:------------------- similar code ------------------ index = 16, score = 5.0 
def swap_flip_dimensions(cosine_matrix, image, header=None):
    swap = np.argmax(abs(cosine_matrix), axis=0)
    flip = np.sum(cosine_matrix, axis=0)
    image = np.transpose(image, tuple(swap))
    image = image[tuple((slice(None, None, int(f)) for f in flip))]
    if (header is None):
        return image
    header['spacing'] = tuple((header['spacing'][s] for s in swap))
    header['direction'] = np.eye(3)
    return (image, header)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 61:------------------- similar code ------------------ index = 17, score = 5.0 
def evaluation(model, dataset, device, save_mask=True, plot_roc=True, print_metric=True):
    '\n    Function to perform an evaluation of a trained model. We compute different metrics show in the dictionary\n    to_plot_metrics and plot the ROC over different thresholds.\n    :param model: a trained model\n    :param dataset: dataset of images\n    :param device: GPU or CPU. Used to transfer the dataset to the right device.\n    :param save_mask: Boolean to call or not saveMask to plot the mask predicted by the model\n    :param plot_roc: Boolean to plot and save the ROC computer over the different thresholds\n    :param print_metric: Boolean to plot or not the different metrics computed over the thresholds\n    :return: the dictionary containing the metrics\n    '
    model.eval()
    loss = 0
    last_masks = ([None] * len(dataset))
    last_truths = ([None] * len(dataset))
    thesholds = [0, 1e-07, 1e-06, 5e-06, 1e-05, 2.5e-05, 5e-05, 0.0001, 0.00025, 0.0005, 0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.2, 0.4, 0.6, 0.8, 1]
    n_thesholds = len(thesholds)
    to_plot_metrics = dict([('F1', np.zeros(n_thesholds)), ('Recall', np.zeros(n_thesholds)), ('Precision', np.zeros(n_thesholds)), ('TP', np.zeros(n_thesholds)), ('TN', np.zeros(n_thesholds)), ('FP', np.zeros(n_thesholds)), ('FN', np.zeros(n_thesholds)), ('AUC', 0), ('TPR', np.zeros(n_thesholds)), ('FPR', np.zeros(n_thesholds))])
    with tqdm(desc=f'Validation', unit='img') as progress_bar:
        for (i, (image, ground_truth)) in enumerate(dataset):
            image = image[(0, ...)]
            ground_truth = ground_truth[(0, ...)]
            last_truths[i] = ground_truth
            image = image.to(device)
            ground_truth = ground_truth.to(device)
            with torch.no_grad():
                mask_predicted = model(image)
            last_masks[i] = mask_predicted
            progress_bar.set_postfix(**{'loss': loss})
            bce_weight = torch.Tensor([1, 8]).to(device)
            loss += compute_loss(mask_predicted, ground_truth, bce_weight=bce_weight)
            get_metrics(mask_predicted[(0, 0)], ground_truth[0], to_plot_metrics, thesholds)
            progress_bar.update()
    if save_mask:
        save_masks(last_masks, last_truths, str(device), max_img=50, shuffle=False, color='red', filename='mask_predicted_test.png', threshold=thesholds[np.argmax(to_plot_metrics['F1'])])
    if print_metric:
        print_metrics(to_plot_metrics, len(dataset), 'test set')
    nb_images = len(dataset)
    for (k, v) in to_plot_metrics.items():
        to_plot_metrics[k] = (v / nb_images)
    if plot_roc:
        plt.title('Receiver Operating Characteristic')
        plt.plot(to_plot_metrics['FPR'], to_plot_metrics['TPR'], 'b', label=('AUC = %0.2f' % to_plot_metrics['AUC']))
        plt.legend(loc='lower right')
        plt.plot([0, 1], [0, 1], 'r--')
        plt.xlim([0, 1])
        plt.ylim([0, 1])
        plt.ylabel('True Positive Rate')
        plt.xlabel('False Positive Rate')
        plt.savefig('ROC.png')
        plt.show()
        plt.close('ROC.png')
    loss /= len(dataset)
    to_plot_metrics['loss'] = loss
    to_plot_metrics['best_threshold'] = thesholds[np.argmax(to_plot_metrics['F1'])]
    return to_plot_metrics

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if  ... :
         ... ( ... ,  ... ,,,,,,  ... = ... [np.argmax])

idx = 62:------------------- similar code ------------------ index = 18, score = 5.0 
def train_actor(actor_model, critic_model, state_transitions, num_actor_training_samples, num_actions):
    random_actions = []
    for i in range(num_actor_training_samples):
        random_actions.append(((np.random.rand(num_actions) * 2) - 1))
    random_states = [s.state for s in state_transitions]
    for i in range(len(random_states)):
        with torch.no_grad():
            act = actor_model(torch.Tensor(random_states[i]).to(actor_model.device)).cpu().detach().numpy()
            random_actions.append(act)
    best_state_action = []
    for i_states in range(len(random_states)):
        QAs = []
        for i_actions in range(len(random_actions)):
            with torch.no_grad():
                qval = critic_model(torch.Tensor(torch.cat((torch.Tensor(random_states[i_states]), torch.Tensor(random_actions[i_actions])), 0)).to(critic_model.device)).cpu()
                QAs.append(qval)
        best_state_action.append(sars(random_states[i_states], random_actions[np.argmax(QAs)], 0.0, None, False, np.max(QAs)))
    t_random_states = torch.stack([torch.Tensor(s.state) for s in best_state_action]).to(actor_model.device)
    target_actions = torch.stack([torch.Tensor(s.action) for s in best_state_action]).to(actor_model.device)
    actor_model.zero_grad()
    predicted_actions = actor_model(t_random_states)
    loss = F.smooth_l1_loss(predicted_actions, target_actions).mean()
    loss.backward()
    actor_model.opt.step()
    return loss

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ... . ... ( ... (,  ... [np.argmax],  ... , None, False,))

idx = 63:------------------- similar code ------------------ index = 19, score = 5.0 
def validate(self, X_test, y_test):
    acc = (np.argmax(self.predict_proba(X_test), axis=1) == y_test).mean()
    return acc

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = (np.argmax ==  ... )

idx = 64:------------------- similar code ------------------ index = 23, score = 5.0 
def get_best_frequency(self, fid=None):
    if (fid is None):
        best_idx = np.argmax(self.per)
    else:
        best_idx = np.argmax(self.per_single_band[fid])
    return self.freq[best_idx]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.argmax

idx = 65:------------------- similar code ------------------ index = 24, score = 5.0 
def add(self, predicted, target):
    '\n        Computes the confusion matrix of K x K size where K is no of classes\n\n        Paramaters:\n            predicted (tensor): Can be an N x K tensor of predicted scores obtained from\n                the model for N examples and K classes or an N-tensor of\n                integer values between 0 and K-1.\n            target (tensor): Can be a N-tensor of integer values assumed to be integer\n                values between 0 and K-1 or N x K tensor, where targets are\n                assumed to be provided as one-hot vectors\n        '
    predicted = predicted.cpu().numpy()
    target = target.cpu().numpy()
    assert (predicted.shape[0] == target.shape[0]), 'number of targets and predicted outputs do not match'
    if (np.ndim(predicted) != 1):
        assert (predicted.shape[1] == self.k), 'number of predictions does not match size of confusion matrix'
        predicted = np.argmax(predicted, 1)
    else:
        assert ((predicted.max() < self.k) and (predicted.min() >= 0)), 'predicted values are not between 1 and k'
    onehot_target = (np.ndim(target) != 1)
    if onehot_target:
        assert (target.shape[1] == self.k), 'Onehot target does not match size of confusion matrix'
        assert ((target >= 0).all() and (target <= 1).all()), 'in one-hot encoding, target values should be 0 or 1'
        assert (target.sum(1) == 1).all(), 'multi-label setting is not supported'
        target = np.argmax(target, 1)
    else:
        assert ((predicted.max() < self.k) and (predicted.min() >= 0)), 'predicted values are not between 0 and k-1'
    x = (predicted + (self.k * target))
    bincount_2d = np.bincount(x.astype(np.int32), minlength=(self.k ** 2))
    assert (bincount_2d.size == (self.k ** 2))
    conf = bincount_2d.reshape((self.k, self.k))
    self.conf += conf

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.argmax

idx = 66:------------------- similar code ------------------ index = 26, score = 5.0 
def train_actor(actor_model, critic_model, noisy_actor_model, state_transitions, num_actor_training_samples, num_actions):
    random_actions = []
    for i in range(num_actor_training_samples):
        random_actions.append(((np.random.rand(num_actions) * 2) - 1))
    for i in range(len(state_transitions)):
        random_actions.append(state_transitions[i].action)
    random_states = [s.state for s in state_transitions]
    for i in range(len(random_states)):
        with torch.no_grad():
            act = actor_model(torch.Tensor(random_states[i]).to(actor_model.device)).cpu().detach().numpy()
            random_actions.append(act)
            act = noisy_actor_model(torch.Tensor(random_states[i]).to(noisy_actor_model.device)).cpu().detach().numpy()
            random_actions.append(act)
    best_state_action = []
    for i_states in range(len(random_states)):
        QAs = []
        for i_actions in range(len(random_actions)):
            with torch.no_grad():
                qval = critic_model(torch.Tensor(torch.cat((torch.Tensor(random_states[i_states]), torch.Tensor(random_actions[i_actions])), 0)).to(critic_model.device)).cpu()
                QAs.append(qval)
        best_state_action.append(sars(random_states[i_states], random_actions[np.argmax(QAs)], 0.0, None, False, np.max(QAs)))
    t_random_states = torch.stack([torch.Tensor(s.state) for s in best_state_action]).to(actor_model.device)
    target_actions = torch.stack([torch.Tensor(s.action) for s in best_state_action]).to(actor_model.device)
    actor_model.zero_grad()
    predicted_actions = actor_model(t_random_states)
    loss = F.smooth_l1_loss(predicted_actions, target_actions).mean()
    loss.backward()
    actor_model.opt.step()
    return loss

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ... . ... ( ... (,  ... [np.argmax],  ... , None, False,))

idx = 67:------------------- similar code ------------------ index = 27, score = 5.0 
def gatherData(self, pop, species):
    fitness = [ind.fitness for ind in pop]
    peakfit = [ind.fitMax for ind in pop]
    nodes = np.asarray([np.shape(ind.node)[1] for ind in pop])
    conns = np.asarray([ind.nConn for ind in pop])
    if (len(self.x_scale) is 0):
        self.x_scale = np.append(self.x_scale, len(pop))
    else:
        self.x_scale = np.append(self.x_scale, (self.x_scale[(- 1)] + len(pop)))
    self.elite.append(pop[np.argmax(fitness)])
    if (len(self.best) is 0):
        self.best = copy.deepcopy(self.elite)
    elif (self.elite[(- 1)].fitness > self.best[(- 1)].fitness):
        self.best = np.append(self.best, copy.deepcopy(self.elite[(- 1)]))
        self.newBest = True
    else:
        self.best = np.append(self.best, copy.deepcopy(self.best[(- 1)]))
        self.newBest = False
    self.node_med = np.append(self.node_med, np.median(nodes))
    self.conn_med = np.append(self.conn_med, np.median(conns))
    self.fit_med = np.append(self.fit_med, np.median(fitness))
    self.fit_max = np.append(self.fit_max, self.elite[(- 1)].fitness)
    self.fit_top = np.append(self.fit_top, self.best[(- 1)].fitness)
    self.fit_peak = np.append(self.fit_peak, self.best[(- 1)].fitMax)
    if (len(self.objVals) == 0):
        self.objVals = np.c_[(fitness, peakfit, conns)]
    else:
        self.objVals = np.c_[(self.objVals, np.c_[(fitness, peakfit, conns)])]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ... . ... ( ... [np.argmax])

idx = 68:------------------- similar code ------------------ index = 28, score = 5.0 
def evaluate(generator, model, iou_threshold=0.5, score_threshold=0.05, max_detections=100, save_path=None):
    ' Evaluate a given dataset using a given model.\n\t# Arguments\n\t\tgenerator       : The generator that represents the dataset to evaluate.\n\t\tmodel           : The model to evaluate.\n\t\tiou_threshold   : The threshold used to consider when a detection is positive or negative.\n\t\tscore_threshold : The score confidence threshold to use for detections.\n\t\tmax_detections  : The maximum number of detections to use per image.\n\t\tsave_path       : The path to save images with visualized detections to.\n\t# Returns\n\t\tA dict mapping class names to mAP scores.\n\t'
    (all_detections, all_inferences) = _get_detections(generator, model, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path)
    all_annotations = _get_annotations(generator)
    average_precisions = {}
    for label in range(generator.num_classes()):
        if (not generator.has_label(label)):
            continue
        false_positives = np.zeros((0,))
        true_positives = np.zeros((0,))
        scores = np.zeros((0,))
        num_annotations = 0.0
        for i in range(generator.size()):
            detections = all_detections[i][label]
            annotations = all_annotations[i][label]
            num_annotations += annotations.shape[0]
            detected_annotations = []
            for d in detections:
                scores = np.append(scores, d[4])
                if (annotations.shape[0] == 0):
                    false_positives = np.append(false_positives, 1)
                    true_positives = np.append(true_positives, 0)
                    continue
                overlaps = compute_overlap(np.expand_dims(d, axis=0), annotations)
                assigned_annotation = np.argmax(overlaps, axis=1)
                max_overlap = overlaps[(0, assigned_annotation)]
                if ((max_overlap >= iou_threshold) and (assigned_annotation not in detected_annotations)):
                    false_positives = np.append(false_positives, 0)
                    true_positives = np.append(true_positives, 1)
                    detected_annotations.append(assigned_annotation)
                else:
                    false_positives = np.append(false_positives, 1)
                    true_positives = np.append(true_positives, 0)
        if (num_annotations == 0):
            average_precisions[label] = (0, 0)
            continue
        indices = np.argsort((- scores))
        false_positives = false_positives[indices]
        true_positives = true_positives[indices]
        false_positives = np.cumsum(false_positives)
        true_positives = np.cumsum(true_positives)
        recall = (true_positives / num_annotations)
        precision = (true_positives / np.maximum((true_positives + false_positives), np.finfo(np.float64).eps))
        average_precision = _compute_ap(recall, precision)
        average_precisions[label] = (average_precision, num_annotations)
        inference_time = (np.sum(all_inferences) / generator.size())
    print_results(generator, average_precisions, inference_time)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
        for  ...  in:
            for  ...  in  ... :
                 ...  = np.argmax

idx = 69:------------------- similar code ------------------ index = 30, score = 5.0 
def plan_experiment(self):
    use_nonzero_mask_for_normalization = self.determine_whether_to_use_mask_for_norm()
    print('Are we using the nonzero maks for normalizaion?', use_nonzero_mask_for_normalization)
    spacings = self.dataset_properties['all_spacings']
    sizes = self.dataset_properties['all_sizes']
    all_classes = self.dataset_properties['all_classes']
    modalities = self.dataset_properties['modalities']
    num_modalities = len(list(modalities.keys()))
    target_spacing = self.get_target_spacing()
    new_shapes = np.array([((np.array(i) / target_spacing) * np.array(j)) for (i, j) in zip(spacings, sizes)])
    max_spacing_axis = np.argmax(target_spacing)
    remaining_axes = [i for i in list(range(3)) if (i != max_spacing_axis)]
    self.transpose_forward = ([max_spacing_axis] + remaining_axes)
    self.transpose_backward = [np.argwhere((np.array(self.transpose_forward) == i))[0][0] for i in range(3)]
    median_shape = np.median(np.vstack(new_shapes), 0)
    print('the median shape of the dataset is ', median_shape)
    max_shape = np.max(np.vstack(new_shapes), 0)
    print('the max shape in the dataset is ', max_shape)
    min_shape = np.min(np.vstack(new_shapes), 0)
    print('the min shape in the dataset is ', min_shape)
    print("we don't want feature maps smaller than ", self.unet_featuremap_min_edge_length, ' in the bottleneck')
    self.plans_per_stage = []
    target_spacing_transposed = np.array(target_spacing)[self.transpose_forward]
    median_shape_transposed = np.array(median_shape)[self.transpose_forward]
    print('the transposed median shape of the dataset is ', median_shape_transposed)
    self.plans_per_stage.append(self.get_properties_for_stage(target_spacing_transposed, target_spacing_transposed, median_shape_transposed, num_cases=len(self.list_of_cropped_npz_files), num_modalities=num_modalities, num_classes=(len(all_classes) + 1)))
    print(self.plans_per_stage)
    self.plans_per_stage = self.plans_per_stage[::(- 1)]
    self.plans_per_stage = {i: self.plans_per_stage[i] for i in range(len(self.plans_per_stage))}
    normalization_schemes = self.determine_normalization_scheme()
    (only_keep_largest_connected_component, min_size_per_class, min_region_size_per_class) = (None, None, None)
    plans = {'num_stages': len(list(self.plans_per_stage.keys())), 'num_modalities': num_modalities, 'modalities': modalities, 'normalization_schemes': normalization_schemes, 'dataset_properties': self.dataset_properties, 'list_of_npz_files': self.list_of_cropped_npz_files, 'original_spacings': spacings, 'original_sizes': sizes, 'preprocessed_data_folder': self.preprocessed_output_folder, 'num_classes': len(all_classes), 'all_classes': all_classes, 'base_num_features': self.unet_base_num_features, 'use_mask_for_norm': use_nonzero_mask_for_normalization, 'keep_only_largest_region': only_keep_largest_connected_component, 'min_region_size_per_class': min_region_size_per_class, 'min_size_per_class': min_size_per_class, 'transpose_forward': self.transpose_forward, 'transpose_backward': self.transpose_backward, 'data_identifier': self.data_identifier, 'plans_per_stage': self.plans_per_stage, 'preprocessor_name': self.preprocessor_name}
    self.plans = plans
    self.save_my_plans()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.argmax

idx = 70:------------------- similar code ------------------ index = 56, score = 5.0 
def test(test_loader, data_list, model, classes, mean, std, base_size, crop_h, crop_w, scales, gray_folder, color_folder, colors):
    logger.info('>>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>')
    data_time = AverageMeter()
    batch_time = AverageMeter()
    model.eval()
    end = time.time()
    for (i, (input, _)) in enumerate(test_loader):
        data_time.update((time.time() - end))
        input = np.squeeze(input.numpy(), axis=0)
        image = np.transpose(input, (1, 2, 0))
        (h, w, _) = image.shape
        if (base_size == 0):
            base_size = max(h, w)
        prediction = np.zeros((h, w, classes), dtype=float)
        for scale in scales:
            long_size = round((scale * base_size))
            new_h = long_size
            new_w = long_size
            if (h > w):
                new_w = round(((long_size / float(h)) * w))
            else:
                new_h = round(((long_size / float(w)) * h))
            image_scale = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
            prediction += scale_process(model, image_scale, classes, crop_h, crop_w, h, w, mean, std)
        prediction /= len(scales)
        prediction = np.argmax(prediction, axis=2)
        batch_time.update((time.time() - end))
        end = time.time()
        if ((((i + 1) % 10) == 0) or ((i + 1) == len(test_loader))):
            logger.info('Test: [{}/{}] Data {data_time.val:.3f} ({data_time.avg:.3f}) Batch {batch_time.val:.3f} ({batch_time.avg:.3f}).'.format((i + 1), len(test_loader), data_time=data_time, batch_time=batch_time))
        check_makedirs(gray_folder)
        check_makedirs(color_folder)
        gray = np.uint8(prediction)
        color = colorize(gray, colors)
        (image_path, _) = data_list[i]
        image_name = image_path.split('/')[(- 1)].split('.')[0]
        gray_path = os.path.join(gray_folder, (image_name + '.png'))
        color_path = os.path.join(color_folder, (image_name + '.png'))
        cv2.imwrite(gray_path, gray)
        color.save(color_path)
    logger.info('<<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
         ...  = np.argmax

idx = 71:------------------- similar code ------------------ index = 130, score = 5.0 
def match_target_hypo(args, target_outfile, hypo_outfile):
    'combine scores from the LM and bitext models, and write the top scoring hypothesis to a file'
    if (len(args.weight1) == 1):
        res = score_target_hypo(args, args.weight1[0], args.weight2[0], args.weight3[0], args.lenpen[0], target_outfile, hypo_outfile, True, args.normalize)
        rerank_scores = [res]
    else:
        print('launching pool')
        with Pool(32) as p:
            rerank_scores = p.starmap(score_target_hypo, [(args, args.weight1[i], args.weight2[i], args.weight3[i], args.lenpen[i], target_outfile, hypo_outfile, False, args.normalize) for i in range(len(args.weight1))])
    if (len(rerank_scores) > 1):
        best_index = np.argmax(rerank_scores)
        best_score = rerank_scores[best_index]
        print('best score', best_score)
        print('best lenpen', args.lenpen[best_index])
        print('best weight1', args.weight1[best_index])
        print('best weight2', args.weight2[best_index])
        print('best weight3', args.weight3[best_index])
        return (args.lenpen[best_index], args.weight1[best_index], args.weight2[best_index], args.weight3[best_index], best_score)
    else:
        return (args.lenpen[0], args.weight1[0], args.weight2[0], args.weight3[0], rerank_scores[0])

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.argmax

idx = 72:------------------- similar code ------------------ index = 58, score = 5.0 
def val(epoch):
    global best_val_loss
    print('Val Epoch: {}'.format(epoch))
    net.eval()
    val_loss = 0
    val_loss_seg = 0
    val_loss_exist = 0
    progressbar = tqdm(range(len(val_loader)))
    with torch.no_grad():
        for (batch_idx, sample) in enumerate(val_loader):
            img = sample['img'].to(device)
            segLabel = sample['segLabel'].to(device)
            exist = sample['exist'].to(device)
            (seg_pred, exist_pred, loss_seg, loss_exist, loss) = net(img, segLabel, exist)
            if isinstance(net, torch.nn.DataParallel):
                loss_seg = loss_seg.sum()
                loss_exist = loss_exist.sum()
                loss = loss.sum()
            gap_num = 5
            if (((batch_idx % gap_num) == 0) and (batch_idx < (50 * gap_num))):
                origin_imgs = []
                seg_pred = seg_pred.detach().cpu().numpy()
                exist_pred = exist_pred.detach().cpu().numpy()
                for b in range(len(img)):
                    img_name = sample['img_name'][b]
                    img = cv2.imread(img_name)
                    img = transform_val_img({'img': img})['img']
                    lane_img = np.zeros_like(img)
                    color = np.array([[255, 125, 0], [0, 255, 0], [0, 0, 255], [0, 255, 255]], dtype='uint8')
                    coord_mask = np.argmax(seg_pred[b], axis=0)
                    for i in range(0, 4):
                        if (exist_pred[(b, i)] > 0.5):
                            lane_img[(coord_mask == (i + 1))] = color[i]
                    img = cv2.addWeighted(src1=lane_img, alpha=0.8, src2=img, beta=1.0, gamma=0.0)
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    lane_img = cv2.cvtColor(lane_img, cv2.COLOR_BGR2RGB)
                    cv2.putText(lane_img, '{}'.format([(1 if (exist_pred[(b, i)] > 0.5) else 0) for i in range(4)]), (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (255, 255, 255), 2)
                    origin_imgs.append(img)
                    origin_imgs.append(lane_img)
                tensorboard.image_summary('img_{}'.format(batch_idx), origin_imgs, epoch)
            val_loss += loss.item()
            val_loss_seg += loss_seg.item()
            val_loss_exist += loss_exist.item()
            progressbar.set_description('batch loss: {:.3f}'.format(loss.item()))
            progressbar.update(1)
    progressbar.close()
    iter_idx = ((epoch + 1) * len(train_loader))
    tensorboard.scalar_summary('val_loss', val_loss, iter_idx)
    tensorboard.scalar_summary('val_loss_seg', val_loss_seg, iter_idx)
    tensorboard.scalar_summary('val_loss_exist', val_loss_exist, iter_idx)
    tensorboard.writer.flush()
    print('------------------------\n')
    if (val_loss < best_val_loss):
        best_val_loss = val_loss
        save_name = os.path.join(exp_dir, (exp_name + '.pth'))
        copy_name = os.path.join(exp_dir, (exp_name + '_best.pth'))
        shutil.copyfile(save_name, copy_name)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    with:
        for in:
            if:
                for  ...  in:
                     ...  = np.argmax

idx = 73:------------------- similar code ------------------ index = 111, score = 5.0 
def locate_probable_sentence(text, out, inter, cmp):
    sentences = split_sentences(text)
    sentences = list(filter((lambda x: (x != '')), sentences))
    dict_of_words = list(map(generate_word_dict, sentences))
    point_arr = ([None] * len(sentences))
    idx = 0
    for d in dict_of_words:
        points = 0
        points += reduce((lambda y, x: ((y + 1) if (x in d) else y)), out.split(' '), 0)
        points += reduce((lambda y, x: ((y + 1) if (x in d) else y)), inter.split(' '), 0)
        points += reduce((lambda y, x: ((y + 1) if (x in d) else y)), inter.split(' '), 0)
        point_arr[idx] = points
        idx += 1
    loc_best = np.argmax(point_arr)
    likely_sentence = sentences[loc_best]
    return (likely_sentence, point_arr)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 74:------------------- similar code ------------------ index = 98, score = 5.0 
def evaluate(args, model, eval_dataset, prefix='', eval_output_dir='/tmp/out'):
    results = {}
    eval_task = args.task_name
    if ((not os.path.exists(eval_output_dir)) and (args.local_rank in [(- 1), 0])):
        os.makedirs(eval_output_dir)
    args.eval_batch_size = (args.per_gpu_eval_batch_size * max(1, args.n_gpu))
    eval_sampler = (SequentialSampler(eval_dataset) if (args.local_rank == (- 1)) else DistributedSampler(eval_dataset))
    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)
    logger.info('***** Running evaluation {} *****'.format(prefix))
    logger.info('  Num examples = %d', len(eval_dataset))
    logger.info('  Batch size = %d', args.eval_batch_size)
    eval_loss = 0.0
    nb_eval_steps = 0
    preds = None
    out_label_ids = None
    mb = progress_bar(eval_dataloader)
    for batch in mb:
        model.eval()
        batch = tuple((t.to(args.device) for t in batch))
        with torch.no_grad():
            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}
            if (args.model_type != 'distilbert'):
                inputs['token_type_ids'] = (batch[2] if (args.model_type in ['bert', 'xlnet']) else None)
            outputs = model(**inputs)
            (tmp_eval_loss, logits) = outputs[:2]
            eval_loss += tmp_eval_loss.mean().item()
        nb_eval_steps += 1
        if (preds is None):
            preds = logits.detach().cpu().numpy()
            out_label_ids = inputs['labels'].detach().cpu().numpy()
        else:
            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)
            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)
    eval_loss = (eval_loss / nb_eval_steps)
    if (args.output_mode == 'classification'):
        preds = np.argmax(preds, axis=1)
    elif (args.output_mode == 'regression'):
        preds = np.squeeze(preds)
    result = compute_metrics(eval_task, preds, out_label_ids)
    results.update(result)
    results['loss'] = eval_loss
    output_eval_file = os.path.join(eval_output_dir, prefix, 'eval_results.txt')
    with open(output_eval_file, 'w') as writer:
        logger.info('***** Eval results {} *****'.format(prefix))
        for key in sorted(result.keys()):
            logger.info('  %s = %s', key, str(result[key]))
            writer.write(('%s = %s\n' % (key, str(result[key]))))
    return results

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.argmax

idx = 75:------------------- similar code ------------------ index = 99, score = 5.0 
def assignOffspring(self, species, pop, p):
    "Assigns number of offspring to each species based on fitness sharing.\n  NOTE: Ordinal rather than the cardinal fitness of canonical NEAT is used.\n\n  Args:\n    species - (Species) - this generation's species\n      .members    - [Ind]   - individuals in species\n    pop     - [Ind]     - individuals with species assigned\n      .fitness    - (float) - performance on task (higher is better)\n    p       - (Dict)    - algorithm hyperparameters\n\n  Returns:\n    species - (Species) - This generation's species\n      .nOffspring - (int) - number of children to produce\n  "
    nSpecies = len(species)
    if (nSpecies == 1):
        species[0].offspring = p['popSize']
    else:
        popFit = np.asarray([ind.fitness for ind in pop])
        popRank = tiedRank(popFit)
        if (p['select_rankWeight'] == 'exp'):
            rankScore = (1 / popRank)
        elif (p['select_rankWeight'] == 'lin'):
            rankScore = (1 + abs((popRank - len(popRank))))
        else:
            print('Invalid rank weighting (using linear)')
            rankScore = (1 + abs((popRank - len(popRank))))
        specId = np.asarray([ind.species for ind in pop])
        speciesFit = np.zeros((nSpecies, 1))
        speciesTop = np.zeros((nSpecies, 1))
        for iSpec in range(nSpecies):
            if (not np.any((specId == iSpec))):
                speciesFit[iSpec] = 0
            else:
                speciesFit[iSpec] = np.mean(rankScore[(specId == iSpec)])
                speciesTop[iSpec] = np.max(popFit[(specId == iSpec)])
                if (speciesTop[iSpec] > species[iSpec].bestFit):
                    species[iSpec].bestFit = speciesTop[iSpec]
                    bestId = np.argmax(popFit[(specId == iSpec)])
                    species[iSpec].bestInd = species[iSpec].members[bestId]
                    species[iSpec].lastImp = 0
                else:
                    species[iSpec].lastImp += 1
                if (species[iSpec].lastImp > p['spec_dropOffAge']):
                    speciesFit[iSpec] = 0
        if (sum(speciesFit) == 0):
            speciesFit = np.ones((nSpecies, 1))
            print('WARN: Entire population stagnant, continuing without extinction')
        offspring = bestIntSplit(speciesFit, p['popSize'])
        for iSpec in range(nSpecies):
            species[iSpec].nOffspring = offspring[iSpec]
    species[:] = [s for s in species if (s.nOffspring != 0)]
    return species

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:    else:
        for  ...  in:
            if:            else:
                if:
                     ...  = np.argmax

idx = 76:------------------- similar code ------------------ index = 101, score = 5.0 
def get_freer_gpu():
    '\n    Find which gpu is free\n    '
    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')
    memory_available = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]
    return int(np.argmax(memory_available))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    return  ... (np.argmax)

idx = 77:------------------- similar code ------------------ index = 102, score = 5.0 
def train_actor(actor_model, critic_model, state_transitions, num_actor_training_samples, num_actions):
    random_actions = []
    random_states = [s.state for s in state_transitions]
    for i in range(len(random_states)):
        with torch.no_grad():
            act = actor_model(torch.Tensor(random_states[i]).to(actor_model.device)).cpu().detach().numpy()
            random_actions.append(act)
            for j in range(num_actor_training_samples):
                noise = np.random.normal(0, 0.15, num_actions)
                random_actions.append((act + noise))
    best_state_action = []
    for i_states in range(len(random_states)):
        QAs = []
        for i_actions in range(len(random_actions)):
            with torch.no_grad():
                qval = critic_model(torch.Tensor(torch.cat((torch.Tensor(random_states[i_states]), torch.Tensor(random_actions[i_actions])), 0)).to(critic_model.device)).cpu()
                QAs.append(qval)
        best_state_action.append(sars(random_states[i_states], random_actions[np.argmax(QAs)], 0.0, None, False, np.max(QAs), None, None))
    t_random_states = torch.stack([torch.Tensor(s.state) for s in best_state_action]).to(actor_model.device)
    target_actions = torch.stack([torch.Tensor(s.action) for s in best_state_action]).to(actor_model.device)
    actor_model.zero_grad()
    predicted_actions = actor_model(t_random_states)
    loss = F.smooth_l1_loss(predicted_actions, target_actions).mean()
    loss.backward()
    actor_model.opt.step()
    return loss

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ... . ... ( ... (,  ... [np.argmax],  ... , None, False,, None, None))

idx = 78:------------------- similar code ------------------ index = 103, score = 5.0 
def evaluate(generator, retinanet, iou_threshold=0.5, score_threshold=0.05, max_detections=100, save_path=None):
    ' Evaluate a given dataset using a given retinanet.\n    # Arguments\n        generator       : The generator that represents the dataset to evaluate.\n        retinanet           : The retinanet to evaluate.\n        iou_threshold   : The threshold used to consider when a detection is positive or negative.\n        score_threshold : The score confidence threshold to use for detections.\n        max_detections  : The maximum number of detections to use per image.\n        save_path       : The path to save images with visualized detections to.\n    # Returns\n        A dict mapping class names to mAP scores.\n    '
    all_detections = _get_detections(generator, retinanet, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path)
    all_annotations = _get_annotations(generator)
    average_precisions = {}
    for label in range(generator.num_classes()):
        false_positives = np.zeros((0,))
        true_positives = np.zeros((0,))
        scores = np.zeros((0,))
        num_annotations = 0.0
        for i in range(len(generator)):
            detections = all_detections[i][label]
            annotations = all_annotations[i][label]
            num_annotations += annotations.shape[0]
            detected_annotations = []
            for d in detections:
                scores = np.append(scores, d[4])
                if (annotations.shape[0] == 0):
                    false_positives = np.append(false_positives, 1)
                    true_positives = np.append(true_positives, 0)
                    continue
                overlaps = compute_overlap(np.expand_dims(d, axis=0), annotations)
                assigned_annotation = np.argmax(overlaps, axis=1)
                max_overlap = overlaps[(0, assigned_annotation)]
                if ((max_overlap >= iou_threshold) and (assigned_annotation not in detected_annotations)):
                    false_positives = np.append(false_positives, 0)
                    true_positives = np.append(true_positives, 1)
                    detected_annotations.append(assigned_annotation)
                else:
                    false_positives = np.append(false_positives, 1)
                    true_positives = np.append(true_positives, 0)
        if (num_annotations == 0):
            average_precisions[label] = (0, 0)
            continue
        indices = np.argsort((- scores))
        false_positives = false_positives[indices]
        true_positives = true_positives[indices]
        false_positives = np.cumsum(false_positives)
        true_positives = np.cumsum(true_positives)
        recall = (true_positives / num_annotations)
        precision = (true_positives / np.maximum((true_positives + false_positives), np.finfo(np.float64).eps))
        average_precision = _compute_ap(recall, precision)
        average_precisions[label] = (average_precision, num_annotations)
    print('\nmAP:')
    for label in range(generator.num_classes()):
        label_name = generator.label_to_name(label)
        print('{}: {}'.format(label_name, average_precisions[label][0]))
    return average_precisions

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
        for  ...  in:
            for  ...  in  ... :
                 ...  = np.argmax

idx = 79:------------------- similar code ------------------ index = 104, score = 5.0 
def log(self):
    'This function is executed after every optimization. So it can be used \n        to interact with the system during training.'
    idx = np.argmax(self.population_returns)
    reward = self.population_returns[(idx, 0)]
    print(f'Reward: {reward}')
    if (reward == 500):
        self.best = self.population_parameters[idx]
        self.terminate()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.argmax

idx = 80:------------------- similar code ------------------ index = 105, score = 5.0 
def export_csv_separate(exp_batch, variables_to_export, task_list):
    root_path = '_logs'
    experiments = os.listdir(os.path.join(root_path, exp_batch))
    if ('episodes_fully_completed' not in set(variables_to_export)):
        raise ValueError(' export csv needs the episodes fully completed param on variables')
    csv_outfile = os.path.join(root_path, exp_batch, 'result.csv')
    with open(csv_outfile, 'w') as f:
        f.write('experiment,environment')
        for variable in variables_to_export:
            f.write((',%s' % variable))
        f.write('\n')
    experiment_list = []
    for exp in experiments:
        if os.path.isdir(os.path.join(root_path, exp_batch, exp)):
            experiments_logs = os.listdir(os.path.join(root_path, exp_batch, exp))
            scenario = []
            for log in experiments_logs:
                dicts_to_write = {}
                for task in task_list:
                    dicts_to_write.update({task: {}})
                for task in task_list:
                    if (('drive' in log) and ('_csv' in log)):
                        csv_file_path = os.path.join(root_path, exp_batch, exp, log, (('control_output_' + task) + '.csv'))
                        if (not os.path.exists(csv_file_path)):
                            continue
                        control_csv = read_summary_csv(csv_file_path)
                        if (control_csv is None):
                            continue
                        print(control_csv)
                        position_of_max_success = np.argmax(control_csv['episodes_fully_completed'])
                        print(dicts_to_write)
                        for variable in variables_to_export:
                            dicts_to_write[task].update({variable: control_csv[variable][position_of_max_success]})
                scenario.append(dicts_to_write)
            experiment_list.append(scenario)
    print(' FULL DICT')
    print(experiment_list)
    with open(csv_outfile, 'a') as f:
        for exp in experiments:
            print('EXP ', exp)
            if os.path.isdir(os.path.join(root_path, exp_batch, exp)):
                experiments_logs = os.listdir(os.path.join(root_path, exp_batch, exp))
                count = 0
                for log in experiments_logs:
                    if (('drive' in log) and ('_csv' in log)):
                        f.write(('%s,%s' % (exp, log.split('_')[1])))
                        for variable in variables_to_export:
                            f.write(',')
                            for task in task_list:
                                if experiment_list[experiments.index(exp)][count][task]:
                                    f.write(('%.2f/' % experiment_list[experiments.index(exp)][count][task][variable]))
                        f.write('\n')
                    count += 1

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in  ... :
        if:
            for  ...  in  ... :
                for  ...  in  ... :
                    if:
                         ...  = np.argmax

idx = 81:------------------- similar code ------------------ index = 106, score = 5.0 
def add(self, predicted, target):
    'Computes the confusion matrix\n\n        The shape of the confusion matrix is K x K, where K is the number\n        of classes.\n\n        Keyword arguments:\n        - predicted (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n        predicted scores obtained from the model for N examples and K classes,\n        or an N-tensor/array of integer values between 0 and K-1.\n        - target (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n        ground-truth classes for N examples and K classes, or an N-tensor/array\n        of integer values between 0 and K-1.\n\n        '
    (_, predicted) = predicted.max(1)
    predicted = predicted.view((- 1))
    target = target.view((- 1))
    if torch.is_tensor(predicted):
        predicted = predicted.cpu().numpy()
    if torch.is_tensor(target):
        target = target.cpu().numpy()
    assert (predicted.shape[0] == target.shape[0]), 'number of targets and predicted outputs do not match'
    if (np.ndim(predicted) != 1):
        assert (predicted.shape[1] == self.num_classes), 'number of predictions does not match size of confusion matrix'
        predicted = np.argmax(predicted, 1)
    else:
        assert ((predicted.max() < self.num_classes) and (predicted.min() >= 0)), 'predicted values are not between 0 and k-1'
    if (np.ndim(target) != 1):
        assert (target.shape[1] == self.num_classes), 'Onehot target does not match size of confusion matrix'
        assert ((target >= 0).all() and (target <= 1).all()), 'in one-hot encoding, target values should be 0 or 1'
        assert (target.sum(1) == 1).all(), 'multi-label setting is not supported'
        target = np.argmax(target, 1)
    else:
        assert ((target.max() < self.num_classes) and (target.min() >= 0)), 'target values are not between 0 and k-1'
    x = (predicted + (self.num_classes * target))
    bincount_2d = np.bincount(x.astype(np.int32), minlength=(self.num_classes ** 2))
    assert (bincount_2d.size == (self.num_classes ** 2))
    conf = bincount_2d.reshape((self.num_classes, self.num_classes))
    self.conf += conf

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.argmax

idx = 82:------------------- similar code ------------------ index = 107, score = 5.0 
def get_label_conf(y_vec):
    '\n    Returns the confidence and the label of the most probable class given a vector of class confidences\n    :param y_vec: (np.ndarray) vector of class confidences, nb of instances as first dimension\n    :return: (np.ndarray, np.ndarray) confidences and labels\n    '
    assert (len(y_vec.shape) == 2)
    (confs, labels) = (np.amax(y_vec, axis=1), np.argmax(y_vec, axis=1))
    return (confs, labels)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = (, np.argmax)

idx = 83:------------------- similar code ------------------ index = 108, score = 5.0 
def train_actor(actor_model, critic_model, state_transitions, num_actor_training_samples, num_actions):
    random_actions = []
    for i in range(num_actor_training_samples):
        random_actions.append(((np.random.rand(num_actions) * 2) - 1))
    random_states = [s.state for s in state_transitions]
    for i in range(len(random_states)):
        with torch.no_grad():
            act = actor_model(torch.Tensor(random_states[i]).to(actor_model.device)).cpu().detach().numpy()
            random_actions.append(act)
            for j in range(num_actor_training_samples):
                noise = np.random.normal(0, 0.15, num_actions)
                random_actions.append((act + noise))
    best_state_action = []
    for i_states in range(len(random_states)):
        QAs = []
        for i_actions in range(len(random_actions)):
            with torch.no_grad():
                qval = critic_model(torch.Tensor(torch.cat((torch.Tensor(random_states[i_states]), torch.Tensor(random_actions[i_actions])), 0)).to(critic_model.device)).cpu()
                QAs.append(qval)
        best_state_action.append(sars(random_states[i_states], random_actions[np.argmax(QAs)], 0.0, None, False, np.max(QAs), None, None))
    t_random_states = torch.stack([torch.Tensor(s.state) for s in best_state_action]).to(actor_model.device)
    target_actions = torch.stack([torch.Tensor(s.action) for s in best_state_action]).to(actor_model.device)
    actor_model.zero_grad()
    predicted_actions = actor_model(t_random_states)
    loss = F.smooth_l1_loss(predicted_actions, target_actions).mean()
    loss.backward()
    actor_model.opt.step()
    return loss

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ... . ... ( ... (,  ... [np.argmax],  ... , None, False,, None, None))

idx = 84:------------------- similar code ------------------ index = 109, score = 5.0 
def train_actor(actor_model, critic_model, noisy_actor_model, state_transitions, num_actor_training_samples, num_actions):
    random_actions = []
    for i in range(num_actor_training_samples):
        random_actions.append(((np.random.rand(num_actions) * 2) - 1))
    for i in range(len(state_transitions)):
        random_actions.append(state_transitions[i].action)
    random_states = [s.state for s in state_transitions]
    for i in range(len(random_states)):
        with torch.no_grad():
            act = actor_model(torch.Tensor(random_states[i]).to(actor_model.device)).cpu().detach().numpy()
            random_actions.append(act)
            act = noisy_actor_model(torch.Tensor(random_states[i]).to(noisy_actor_model.device)).cpu().detach().numpy()
            random_actions.append(act)
    best_state_action = []
    for i_states in range(len(random_states)):
        QAs = []
        for i_actions in range(len(random_actions)):
            with torch.no_grad():
                qval = critic_model(torch.Tensor(torch.cat((torch.Tensor(random_states[i_states]), torch.Tensor(random_actions[i_actions])), 0)).to(critic_model.device)).cpu()
                QAs.append(qval)
        best_state_action.append(sars(random_states[i_states], random_actions[np.argmax(QAs)], 0.0, None, False, np.max(QAs)))
    t_random_states = torch.stack([torch.Tensor(s.state) for s in best_state_action]).to(actor_model.device)
    target_actions = torch.stack([torch.Tensor(s.action) for s in best_state_action]).to(actor_model.device)
    actor_model.zero_grad()
    predicted_actions = actor_model(t_random_states)
    loss = F.smooth_l1_loss(predicted_actions, target_actions).mean()
    loss.backward()
    actor_model.opt.step()
    return loss

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ... . ... ( ... (,  ... [np.argmax],  ... , None, False,))

idx = 85:------------------- similar code ------------------ index = 110, score = 5.0 
def choose_best(self, problem_data, labels, parallel=False, batch_size=stg.JOBLIB_BATCH_SIZE, use_cache=True):
    '\n        Choose best strategy between provided ones\n\n        Parameters\n        ----------\n        labels : list\n            Strategy labels to compare.\n        parallel : bool, optional\n            Perform `n_best` strategies evaluation in parallel.\n            True by default.\n        use_cache : bool, optional\n            Use solver cache if available. True by default.\n\n        Returns\n        -------\n        dict\n            Results as a dictionary.\n        '
    n_best = self._learner.options['n_best']
    x = []
    time = []
    infeas = []
    cost = []
    strategies = [self.encoding[label] for label in labels]
    cache = ([None] * n_best)
    if (self._solver_cache and use_cache):
        cache = [self._solver_cache[label] for label in labels]
    n_jobs = (u.get_n_processes() if parallel else 1)
    results = Parallel(n_jobs=n_jobs, batch_size=batch_size)((delayed(self._problem.solve)(problem_data, strategy=strategies[j], cache=cache[j]) for j in range(n_best)))
    x = [r['x'] for r in results]
    time = [r['time'] for r in results]
    infeas = [r['infeasibility'] for r in results]
    cost = [r['cost'] for r in results]
    infeas = np.array(infeas)
    cost = np.array(cost)
    idx_filter = np.where((infeas <= stg.INFEAS_TOL))[0]
    if (len(idx_filter) > 0):
        if (self._problem.sense() == Minimize):
            idx_pick = idx_filter[np.argmin(cost[idx_filter])]
        elif (self._problem.sense() == Maximize):
            idx_pick = idx_filter[np.argmax(cost[idx_filter])]
        else:
            e.value_error('Objective type not understood')
    else:
        idx_pick = np.argmin(infeas)
    result = {}
    result['x'] = x[idx_pick]
    result['time'] = np.sum(time)
    result['strategy'] = strategies[idx_pick]
    result['cost'] = cost[idx_pick]
    result['infeasibility'] = infeas[idx_pick]
    return result

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
        if:        elif:
             ...  =  ... [np.argmax]

idx = 86:------------------- similar code ------------------ index = 113, score = 5.0 
@njit
def _compute_valid_splitting_indices(t, min_leaf):
    'Compute valid split indices for treatment array *t* given *min_leaf*.\n\n    Given an array *t* of treatment status and an integer *min_leaf* --denoting\n    the minimum number of allowed observations of each type in a leaf node--\n    computes a sequence of indices on which we can split *t* and get that each\n    resulting side contains a minimum of *min_leaf* treated and untreated\n    observations. Returns an empty sequence if no split is possible.\n\n    Args:\n        t (np.array): 1d array containing the treatment status as treated =\n            True and untreated = False.\n        min_leaf (int): Minimum number of observations of each type (treated,\n            untreated) allowed in a leaf; has to be greater than 1.\n\n    Returns:\n        out (np.array): a sequence of indices representing valid splitting\n            points.\n\n    '
    out = np.arange(0)
    n = len(t)
    if (n < (2 * min_leaf)):
        return out
    left_index_treated = np.argmax((np.cumsum(t) == min_leaf))
    if (left_index_treated == 0):
        return out
    left_index_untreated = np.argmax((np.cumsum((~ t)) == min_leaf))
    if (left_index_untreated == 0):
        return out
    tmparray = np.array([left_index_treated, left_index_untreated])
    left = np.max(tmparray)
    right_index_treated = np.argmax((np.cumsum(t[::(- 1)]) == min_leaf))
    if (right_index_treated == 0):
        return out
    right_index_untreated = np.argmax((np.cumsum((~ t[::(- 1)])) == min_leaf))
    if (right_index_untreated == 0):
        return out
    tmparray = np.array([right_index_treated, right_index_untreated])
    right = (n - np.max(tmparray))
    if (left > (right - 1)):
        return out
    else:
        out = np.arange(left, (right - 1))
        return out

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 87:------------------- similar code ------------------ index = 59, score = 5.0 
if (__name__ == '__main__'):
    result_json_path = sys.argv[1]
    video_root_path = sys.argv[2]
    dst_directory_path = sys.argv[3]
    if (not os.path.exists(dst_directory_path)):
        subprocess.call('mkdir -p {}'.format(dst_directory_path), shell=True)
    class_name_path = sys.argv[4]
    temporal_unit = int(sys.argv[5])
    with open(result_json_path, 'r') as f:
        results = json.load(f)
    with open(class_name_path, 'r') as f:
        class_names = []
        for row in f:
            class_names.append(row[:(- 1)])
    for index in range(len(results)):
        video_path = os.path.join(video_root_path, results[index]['video'])
        print(video_path)
        clips = results[index]['clips']
        unit_classes = []
        unit_segments = []
        if (temporal_unit == 0):
            unit = len(clips)
        else:
            unit = temporal_unit
        for i in range(0, len(clips), unit):
            n_elements = min(unit, (len(clips) - i))
            scores = np.array(clips[i]['scores'])
            for j in range(i, min((i + unit), len(clips))):
                scores += np.array(clips[i]['scores'])
            scores /= n_elements
            unit_classes.append(class_names[np.argmax(scores)])
            unit_segments.append([clips[i]['segment'][0], clips[((i + n_elements) - 1)]['segment'][1]])
        if os.path.exists('tmp'):
            subprocess.call('rm -rf tmp', shell=True)
        subprocess.call('mkdir tmp', shell=True)
        subprocess.call('ffmpeg -i {} tmp/image_%05d.jpg'.format(video_path), shell=True)
        fps = get_fps(video_path, 'tmp')
        for i in range(len(unit_classes)):
            for j in range(unit_segments[i][0], (unit_segments[i][1] + 1)):
                image = Image.open('tmp/image_{:05}.jpg'.format(j)).convert('RGB')
                min_length = min(image.size)
                font_size = int((min_length * 0.05))
                font = ImageFont.truetype(os.path.join(os.path.dirname(__file__), 'SourceSansPro-Regular.ttf'), font_size)
                d = ImageDraw.Draw(image)
                textsize = d.textsize(unit_classes[i], font=font)
                x = int((font_size * 0.5))
                y = int((font_size * 0.25))
                x_offset = x
                y_offset = y
                rect_position = (x, y, ((x + textsize[0]) + (x_offset * 2)), ((y + textsize[1]) + (y_offset * 2)))
                d.rectangle(rect_position, fill=(30, 30, 30))
                d.text(((x + x_offset), (y + y_offset)), unit_classes[i], font=font, fill=(235, 235, 235))
                image.save('tmp/image_{:05}_pred.jpg'.format(j))
        dst_file_path = os.path.join(dst_directory_path, video_path.split('/')[(- 1)])
        subprocess.call('ffmpeg -y -r {} -i tmp/image_%05d_pred.jpg -b:v 1000k {}'.format(fps, dst_file_path), shell=True)
        if os.path.exists('tmp'):
            subprocess.call('rm -rf tmp', shell=True)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
    for  ...  in:
        for  ...  in:
             ... . ... ( ... [np.argmax])

idx = 88:------------------- similar code ------------------ index = 114, score = 5.0 
def random_targets(labels, nb_classes):
    '\n    Given a set of correct labels, randomly choose target labels different from the original ones. These can be\n    one-hot encoded or integers.\n\n    :param labels: The correct labels\n    :type labels: `np.ndarray`\n    :param nb_classes: The number of classes for this model\n    :type nb_classes: `int`\n    :return: An array holding the randomly-selected target classes, one-hot encoded.\n    :rtype: `np.ndarray`\n    '
    if (len(labels.shape) > 1):
        labels = np.argmax(labels, axis=1)
    result = np.zeros(labels.shape)
    for class_ind in range(nb_classes):
        other_classes = list(range(nb_classes))
        other_classes.remove(class_ind)
        in_cl = (labels == class_ind)
        result[in_cl] = np.random.choice(other_classes)
    return to_categorical(result, nb_classes)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.argmax

idx = 89:------------------- similar code ------------------ index = 115, score = 5.0 
def evaluate(self, model, train_df, valid_df, test_df):
    for (prefix, tdf) in zip(['train', 'valid', 'test'], [train_df, valid_df, test_df]):
        probs = model.predict_proba(tdf['text'])
        preds = np.argmax(probs, axis=1)
        if (self.merge_fragments and (self.merge_type != 'concat')):
            if (self.merge_type == 'vote_maj'):
                vote_results = preds_for_cell_content(tdf, probs)
            elif (self.merge_type == 'vote_avg'):
                vote_results = preds_for_cell_content_multi(tdf, probs)
            elif (self.merge_type == 'vote_max'):
                vote_results = preds_for_cell_content_max(tdf, probs)
            preds = vote_results['pred']
            true_y = vote_results['true']
        else:
            true_y = tdf['label']
            true_y_ext = tdf['cell_type'].apply((lambda x: label_map_ext.get(x, 0)))
        self._set_results(prefix, preds, true_y, true_y_ext)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
         ...  = np.argmax

idx = 90:------------------- similar code ------------------ index = 116, score = 5.0 
def test(t_data, t_label, test_iterations=1, evalate=False):
    assert (test_data.shape[0] == test_label.shape[0])
    y_predict_class = model['predict_class_number']
    (overAllAcc, avgAcc, averageAccClass) = ([], [], [])
    for _ in range(test_iterations):
        pred_class = []
        for t in tqdm(t_data):
            t = np.expand_dims(t, axis=0)
            feed_dict_test = {img_entry: t, prob: 1.0}
            prediction = session.run(y_predict_class, feed_dict=feed_dict_test)
            pred_class.append(prediction)
        true_class = np.argmax(t_label, axis=1)
        conMatrix = confusion_matrix(true_class, pred_class)
        classArray = []
        for c in range(len(conMatrix)):
            recallScore = (conMatrix[c][c] / sum(conMatrix[c]))
            classArray += [recallScore]
        averageAccClass.append(classArray)
        avgAcc.append((sum(classArray) / len(classArray)))
        overAllAcc.append(accuracy_score(true_class, pred_class))
    averageAccClass = np.transpose(averageAccClass)
    meanPerClass = np.mean(averageAccClass, axis=1)
    showClassTable(meanPerClass, title='Class accuracy')
    print(('Average Accuracy: ' + str(np.mean(avgAcc))))
    print(('Overall Accuracy: ' + str(np.mean(overAllAcc))))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ...  = np.argmax

idx = 91:------------------- similar code ------------------ index = 118, score = 5.0 
def evaluate(self, model, train_df, valid_df, test_df):
    data = self.get_databunch(train_df, valid_df, test_df)
    valid_probs = get_preds(self, model, data.valid_ds, ordered=True)[0].cpu().numpy()
    test_probs = get_preds(self, model, data.test_ds, ordered=True)[0].cpu().numpy()
    train_probs = get_preds(self, model, data.train_ds, ordered=True)[0].cpu().numpy()
    self._preds = []
    for (prefix, tdf, probs) in zip(['train', 'valid', 'test'], [train_df, valid_df, test_df], [train_probs, valid_probs, test_probs]):
        preds = np.argmax(probs, axis=1)
        if (self.merge_fragments and (self.merge_type != 'concat')):
            if (self.merge_type == 'vote_maj'):
                vote_results = preds_for_cell_content(tdf, probs)
            elif (self.merge_type == 'vote_avg'):
                vote_results = preds_for_cell_content_multi(tdf, probs)
            elif (self.merge_type == 'vote_max'):
                vote_results = preds_for_cell_content_max(tdf, probs)
            preds = vote_results['pred']
            true_y = vote_results['true']
        else:
            true_y = tdf['label']
            print(true_y.shape)
        self._set_results(prefix, preds, true_y)
        self._preds.append(probs)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
         ...  = np.argmax

idx = 92:------------------- similar code ------------------ index = 119, score = 5.0 
def evaluate(self, model, train_df, valid_df, test_df):
    valid_probs = model.get_preds(ds_type=DatasetType.Valid, ordered=True)[0].cpu().numpy()
    test_probs = model.get_preds(ds_type=DatasetType.Test, ordered=True)[0].cpu().numpy()
    train_probs = model.get_preds(ds_type=DatasetType.Train, ordered=True)[0].cpu().numpy()
    self._preds = []
    for (prefix, tdf, probs) in zip(['train', 'valid', 'test'], [train_df, valid_df, test_df], [train_probs, valid_probs, test_probs]):
        preds = np.argmax(probs, axis=1)
        if (self.merge_fragments and (self.merge_type != 'concat')):
            if (self.merge_type == 'vote_maj'):
                vote_results = preds_for_cell_content(tdf, probs)
            elif (self.merge_type == 'vote_avg'):
                vote_results = preds_for_cell_content_multi(tdf, probs)
            elif (self.merge_type == 'vote_max'):
                vote_results = preds_for_cell_content_max(tdf, probs)
            preds = vote_results['pred']
            true_y = vote_results['true']
        else:
            true_y = tdf['label']
            true_y_ext = tdf['cell_type'].apply((lambda x: label_map_ext.get(x, 0)))
        self._set_results(prefix, preds, true_y, true_y_ext)
        self._preds.append(probs)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
         ...  = np.argmax

idx = 93:------------------- similar code ------------------ index = 121, score = 5.0 
def train_actor(actor_model, critic_model, state_transitions, num_actor_training_samples, num_actions):
    random_actions = []
    for i in range(num_actor_training_samples):
        random_actions.append(((np.random.rand(num_actions) * 2) - 1))
    random_states = [s.state for s in state_transitions]
    for i in range(len(random_states)):
        with torch.no_grad():
            act = actor_model(torch.Tensor(random_states[i]).to(actor_model.device)).cpu().detach().numpy()
            random_actions.append(act)
    best_state_action = []
    for i_states in range(len(random_states)):
        QAs = []
        for i_actions in range(len(random_actions)):
            with torch.no_grad():
                qval = critic_model(torch.Tensor(torch.cat((torch.Tensor(random_states[i_states]), torch.Tensor(random_actions[i_actions])), 0)).to(critic_model.device)).cpu()
                QAs.append(qval)
        best_state_action.append(sars(random_states[i_states], random_actions[np.argmax(QAs)], 0.0, None, False, np.max(QAs)))
    t_random_states = torch.stack([torch.Tensor(s.state) for s in best_state_action]).to(actor_model.device)
    target_actions = torch.stack([torch.Tensor(s.action) for s in best_state_action]).to(actor_model.device)
    actor_model.zero_grad()
    predicted_actions = actor_model(t_random_states)
    loss = F.smooth_l1_loss(predicted_actions, target_actions).mean()
    loss.backward()
    actor_model.opt.step()
    return loss

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ... . ... ( ... (,  ... [np.argmax],  ... , None, False,))

idx = 94:------------------- similar code ------------------ index = 122, score = 5.0 
def export_csv(exp_batch, variables_to_export):
    root_path = '_logs'
    experiments = os.listdir(os.path.join(root_path, exp_batch))
    if ('episodes_fully_completed' not in set(variables_to_export)):
        raise ValueError(' export csv needs the episodes fully completed param on variables')
    csv_outfile = os.path.join(root_path, exp_batch, 'result.csv')
    with open(csv_outfile, 'w') as f:
        f.write('experiment,environment')
        for variable in variables_to_export:
            f.write((',%s' % variable))
        f.write('\n')
    for exp in experiments:
        if os.path.isdir(os.path.join(root_path, exp_batch, exp)):
            experiments_logs = os.listdir(os.path.join(root_path, exp_batch, exp))
            for log in experiments_logs:
                if (('drive' in log) and ('_csv' in log)):
                    csv_file_path = os.path.join(root_path, exp_batch, exp, log, 'control_output.csv')
                    if (not os.path.exists(csv_file_path)):
                        continue
                    control_csv = read_summary_csv(csv_file_path)
                    if (control_csv is None):
                        continue
                    print(control_csv)
                    with open(csv_outfile, 'a') as f:
                        f.write(('%s,%s' % (exp, log.split('_')[1])))
                        print(' var', variable)
                        print(control_csv[variable])
                        position_of_max_success = np.argmax(control_csv['episodes_fully_completed'])
                        for variable in variables_to_export:
                            f.write((',%f' % control_csv[variable][position_of_max_success]))
                        f.write('\n')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in  ... :
        if:
            for  ...  in  ... :
                if:
                    with:
                         ...  = np.argmax

idx = 95:------------------- similar code ------------------ index = 123, score = 5.0 
def accuracy(outputs, labels):
    '\n    Compute the accuracy, given the outputs and labels for all images.\n\n    Args:\n        outputs: (np.ndarray) dimension batch_size x 6 - log softmax output of the model\n        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1, 2, 3, 4, 5]\n\n    Returns: (float) accuracy in [0,1]\n    '
    outputs = np.argmax(outputs, axis=1)
    return (np.sum((outputs == labels)) / float(labels.size))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 96:------------------- similar code ------------------ index = 124, score = 5.0 
def preds_for_cell_content_multi(test_df, probs, group_by=['cell_content']):
    test_df = test_df.copy()
    probs_df = pd.DataFrame(probs, index=test_df.index)
    test_df = pd.concat([test_df, probs_df], axis=1)
    grouped_preds = np.argmax(test_df.groupby(group_by)[probs_df.columns].sum().values, axis=1)
    grouped_counts = test_df.groupby(group_by)['label'].count()
    results = pd.DataFrame({'true': test_df.groupby(group_by)['label'].agg((lambda x: x.value_counts().index[0])), 'pred': grouped_preds, 'counts': grouped_counts})
    return results

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 97:------------------- similar code ------------------ index = 125, score = 5.0 
def gatherData(self, pop, species):
    fitness = [ind.fitness for ind in pop]
    peakfit = [ind.fitMax for ind in pop]
    nodes = np.asarray([np.shape(ind.node)[1] for ind in pop])
    conns = np.asarray([ind.nConn for ind in pop])
    if (len(self.x_scale) is 0):
        self.x_scale = np.append(self.x_scale, len(pop))
    else:
        self.x_scale = np.append(self.x_scale, (self.x_scale[(- 1)] + len(pop)))
    self.elite.append(pop[np.argmax(fitness)])
    if (len(self.best) is 0):
        self.best = copy.deepcopy(self.elite)
    elif (self.elite[(- 1)].fitness > self.best[(- 1)].fitness):
        self.best = np.append(self.best, copy.deepcopy(self.elite[(- 1)]))
        self.newBest = True
    else:
        self.best = np.append(self.best, copy.deepcopy(self.best[(- 1)]))
        self.newBest = False
    self.node_med = np.append(self.node_med, np.median(nodes))
    self.conn_med = np.append(self.conn_med, np.median(conns))
    self.fit_med = np.append(self.fit_med, np.median(fitness))
    self.fit_max = np.append(self.fit_max, self.elite[(- 1)].fitness)
    self.fit_top = np.append(self.fit_top, self.best[(- 1)].fitness)
    self.fit_peak = np.append(self.fit_peak, self.best[(- 1)].fitMax)
    if (len(self.objVals) == 0):
        self.objVals = np.c_[(fitness, peakfit, conns)]
    else:
        self.objVals = np.c_[(self.objVals, np.c_[(fitness, peakfit, conns)])]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ... . ... ( ... [np.argmax])

idx = 98:------------------- similar code ------------------ index = 126, score = 5.0 
def to_int_preds(y):
    return [int(np.argmax(y_i)) for y_i in y.cpu()]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    return [ ... (np.argmax)]

idx = 99:------------------- similar code ------------------ index = 32, score = 5.0 
def step(self, action):
    index = np.argmax(action).astype(int)
    if self._strict:
        reference = np.zeros_like(action)
        reference[index] = 1
        assert np.allclose(reference, action), action
    return self._env.step(index)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .argmax
    if:
        assert np,  ... 

