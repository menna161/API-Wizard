------------------------- example 1 ------------------------ 
def ordered_indices(self):
    return np.arange(len(self.dataset))

------------------------- example 2 ------------------------ 
def get_sub_slice(indices, sub_indices):
    '\n    Safe indexer with nested slices.\n\n    Parameters\n    ----------\n    indices: ndarray or slice\n    sub_indices: ndarray or slice\n\n    Returns\n    -------\n    result: np.array(indices[sub_indices])\n    '
    if (indices is None):
        if isinstance(sub_indices, slice):
            return np.arange(sub_indices.start, sub_indices.stop)
        else:
// your code ...
    elif isinstance(indices, slice):
        return np.arange((indices.start + sub_indices.start), (indices.start + sub_indices.stop))
    else:
        return indices[sub_indices]

------------------------- example 3 ------------------------ 
def ordered_indices(self):
    'Ordered indices for batching.'
// your code ...
    return np.arange(len(self))

------------------------- example 4 ------------------------ 
def get_hole_card_2_idx_LUT(self):
    return np.expand_dims(np.arange(self.rules.N_CARDS_IN_DECK), axis=1)

------------------------- example 5 ------------------------ 
def ordered_indices(self):
    'Return an ordered list of indices. Batches will be constructed based\n        on this order.'
    return np.arange(len(self))

examples  ||  representativeness  ||  number of lines  || number of comments   ||  relevancy  
example1  ||          3           ||        2         ||         0        ||        0.5         
example2  ||          2           ||        10         ||         1        ||        0.2         
example3  ||          4           ||        3         ||         1        ||        0.3333333333333333         
example4  ||          2           ||        2         ||         0        ||        0.5         
example5  ||          3           ||        3         ||         0        ||        0.3333333333333333         

avg       ||          0.3977272727272727           ||        4.0         ||         0.4        ||         37.33333333333333        

idx = 0:------------------- similar code ------------------ index = 177, score = 6.0 
def ordered_indices(self):
    return np.arange(len(self.dataset))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.arange

idx = 1:------------------- similar code ------------------ index = 162, score = 6.0 
def get_sub_slice(indices, sub_indices):
    '\n    Safe indexer with nested slices.\n\n    Parameters\n    ----------\n    indices: ndarray or slice\n    sub_indices: ndarray or slice\n\n    Returns\n    -------\n    result: np.array(indices[sub_indices])\n    '
    if (indices is None):
        if isinstance(sub_indices, slice):
            return np.arange(sub_indices.start, sub_indices.stop)
        else:
            return sub_indices
    elif isinstance(indices, slice):
        return np.arange((indices.start + sub_indices.start), (indices.start + sub_indices.stop))
    else:
        return indices[sub_indices]

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        if:
            return np.arange

idx = 2:------------------- similar code ------------------ index = 78, score = 6.0 
def ordered_indices(self):
    'Ordered indices for batching.'
    if (self._ordered_indices is None):
        self._ordered_indices = OrderedDict([(key, dataset.ordered_indices()) for (key, dataset) in self.datasets.items()])
    return np.arange(len(self))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.arange

idx = 3:------------------- similar code ------------------ index = 165, score = 6.0 
def get_hole_card_2_idx_LUT(self):
    return np.expand_dims(np.arange(self.rules.N_CARDS_IN_DECK), axis=1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np. ... ( ... .arange,)

idx = 4:------------------- similar code ------------------ index = 363, score = 6.0 
def ordered_indices(self):
    'Return an ordered list of indices. Batches will be constructed based\n        on this order.'
    return np.arange(len(self))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.arange

idx = 5:------------------- similar code ------------------ index = 297, score = 6.0 
def trun_scorer(treated_run):
    start = treated_run.initial_state.val
    run_vals = np.array([s.val for s in treated_run.states])
    assert np.allclose((start + np.arange(10)), run_vals)
    return 0.5

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    assert np. ... (( ...  +  ... .arange),  ... )

idx = 6:------------------- similar code ------------------ index = 175, score = 6.0 
def test_setting_from_masked_column():
    'Test issue in #2997'
    mask_b = np.array([True, True, False, False])
    for select in (mask_b, slice(0, 2)):
        t = Table(masked=True)
        t['a'] = Column([1, 2, 3, 4])
        t['b'] = MaskedColumn([11, 22, 33, 44], mask=mask_b)
        t['c'] = MaskedColumn([111, 222, 333, 444], mask=[True, False, True, False])
        t['b'][select] = t['c'][select]
        assert (t['b'][1] == t[1]['b'])
        assert (t['b'][0] is np.ma.masked)
        assert (t['b'][1] == 222)
        assert (t['b'][2] == 33)
        assert (t['b'][3] == 44)
        assert np.all((t['b'].mask == t.mask['b']))
        mask_before_add = t.mask.copy()
        t['d'] = np.arange(len(t))
        assert np.all((t.mask['b'] == mask_before_add['b']))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    for  ...  in:
        assert np
 =  ... .arange

idx = 7:------------------- similar code ------------------ index = 57, score = 6.0 
def cruns_scorer(untreated_runs):
    start_vals = sorted([run.initial_state.val for run in untreated_runs])
    assert np.allclose(np.arange(len(untreated_runs)), start_vals)
    for run in untreated_runs:
        assert np.allclose(np.array([s.val for s in run.states]), run.initial_state.val)
    n = len(untreated_runs)
    propensities = np.zeros((n,))
    propensities[:(n // 2)] = 1.0
    return propensities

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    assert np. ... ( ... .arange,  ... )

idx = 8:------------------- similar code ------------------ index = 139, score = 6.0 
def ordered_indices(self):
    return np.arange(self.num_items)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.arange

idx = 9:------------------- similar code ------------------ index = 319, score = 6.0 
def ordered_indices(self):
    'Return an ordered list of indices. Batches will be constructed based\n        on this order.'
    return np.arange(len(self))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.arange

idx = 10:------------------- similar code ------------------ index = 87, score = 6.0 
def truns_scorer(treated_runs):
    start_vals = sorted([run.initial_state.val for run in treated_runs])
    assert np.allclose(np.arange(len(treated_runs)), start_vals)
    for run in treated_runs:
        assert np.allclose(np.array([s.val for s in run.states]), (run.initial_state.val + np.arange(10)))
    n = len(treated_runs)
    propensities = np.zeros((n,))
    propensities[:(n // 2)] = 1.0
    return propensities

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    assert np. ... ( ... .arange,  ... )

idx = 11:------------------- similar code ------------------ index = 106, score = 6.0 
def ordered_indices(self):
    "\n        Ordered indices for batching. Here we call the underlying\n        dataset's ordered_indices() so that we get the same random ordering\n        as we would have from using the underlying dataset directly.\n        "
    if (self._ordered_indices is None):
        self._ordered_indices = OrderedDict([(key, dataset.ordered_indices()) for (key, dataset) in self.datasets.items()])
    return np.arange(len(self))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.arange

idx = 12:------------------- similar code ------------------ index = 96, score = 6.0 
def test_extract_array_1d_trim():
    'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    '
    assert np.all((extract_array(np.arange(4), (2,), (0,), mode='trim') == np.array([0])))
    for i in [1, 2, 3]:
        assert np.all((extract_array(np.arange(4), (2,), (i,), mode='trim') == np.array([(i - 1), i])))
    assert np.all((extract_array(np.arange(4.0), (2,), (4,), mode='trim') == np.array([3])))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    assert np. ... (( ... ( ... .arange,,,) ==))

idx = 13:------------------- similar code ------------------ index = 95, score = 6.0 
def indices_other_than(size, indices):
    '\n    size: size of the array you want to get elements from\n    example:\n    >>> indices_other_than(8, [1, 2, 3])\n    [0, 4, 5, 6, 7]\n    '
    return np.setxor1d(indices, np.arange(size))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return np. ... ( ... ,  ... .arange)

idx = 14:------------------- similar code ------------------ index = 271, score = 6.0 
def ordered_indices(self):
    return np.arange(self.num_items)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.arange

idx = 15:------------------- similar code ------------------ index = 64, score = 6.0 
def test_extract_array_1d_even():
    'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    '
    assert np.all((extract_array(np.arange(4), (2,), (0,), fill_value=(- 99)) == np.array([(- 99), 0])))
    for i in [1, 2, 3]:
        assert np.all((extract_array(np.arange(4), (2,), (i,)) == np.array([(i - 1), i])))
    assert np.all((extract_array(np.arange(4.0), (2,), (4,), fill_value=np.inf) == np.array([3, np.inf])))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    assert np. ... (( ... ( ... .arange,,,) ==))

idx = 16:------------------- similar code ------------------ index = 24, score = 6.0 
def ordered_indices(self):
    'Ordered indices for batching.'
    if (self._ordered_indices is None):
        self._ordered_indices = OrderedDict([(key, dataset.ordered_indices()) for (key, dataset) in self.datasets.items()])
    return np.arange(len(self))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.arange

idx = 17:------------------- similar code ------------------ index = 416, score = 6.0 
def test_mutable_operations(T1):
    '\n    Operations like adding or deleting a row should removing grouping,\n    but adding or removing or renaming a column should retain grouping.\n    '
    for masked in (False, True):
        t1 = Table(T1, masked=masked)
        tg = t1.group_by('a')
        tg.add_row((0, 'a', 3.0, 4))
        assert np.all((tg.groups.indices == np.array([0, len(tg)])))
        assert (tg.groups.keys is None)
        tg = t1.group_by('a')
        tg.remove_row(4)
        assert np.all((tg.groups.indices == np.array([0, len(tg)])))
        assert (tg.groups.keys is None)
        tg = t1.group_by('a')
        indices = tg.groups.indices.copy()
        tg.add_column(Column(name='e', data=np.arange(len(tg))))
        assert np.all((tg.groups.indices == indices))
        assert np.all((tg['e'].groups.indices == indices))
        assert np.all((tg['e'].groups.keys == tg.groups.keys))
        tg = t1.group_by('a')
        tg.remove_column('b')
        assert np.all((tg.groups.indices == indices))
        assert (tg.groups.keys.dtype.names == ('a',))
        assert np.all((tg['a'].groups.indices == indices))
        tg = t1.group_by('a')
        tg.remove_column('a')
        assert np.all((tg.groups.indices == indices))
        assert (tg.groups.keys.dtype.names == ('a',))
        assert np.all((tg['b'].groups.indices == indices))
        tg = t1.group_by('a')
        tg.rename_column('a', 'aa')
        assert np.all((tg.groups.indices == indices))
        assert (tg.groups.keys.dtype.names == ('a',))
        assert np.all((tg['aa'].groups.indices == indices))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in (False, True):
        assert np
         ... . ... ( ... (,  ... = ... .arange))

idx = 18:------------------- similar code ------------------ index = 212, score = 6.0 
def ordered_indices(self):
    return np.arange(len(self.dataset))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.arange

idx = 19:------------------- similar code ------------------ index = 193, score = 6.0 
def get_idx_2_hole_card_LUT(self):
    return np.expand_dims(np.arange(self.rules.N_CARDS_IN_DECK), axis=1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np. ... ( ... .arange,)

idx = 20:------------------- similar code ------------------ index = 201, score = 6.0 
def compute_ap(precision, recall):
    if np.any(np.isnan(recall)):
        return np.nan
    ap = 0
    for t in np.arange(0, 1.1, 0.1):
        try:
            selected_p = precision[(recall >= t)]
        except:
            ForkedPdb().set_trace()
        if (selected_p.size == 0):
            p = 0
        else:
            p = np.max(selected_p)
        ap += (p / 11.0)
    return ap

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        return np
    for  ...  in  ... .arange:
idx = 21:------------------- similar code ------------------ index = 16, score = 6.0 
def ordered_indices(self):
    'Return an ordered list of indices. Batches will be constructed based\n        on this order.'
    return np.arange(len(self))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.arange

idx = 22:------------------- similar code ------------------ index = 6, score = 6.0 
def test_state_sampler():
    'Test the state sampler function.'

    def initial_state_covariates(run):
        'Outcome is the initial state of the run.'
        return run.initial_state.val

    def random_initial_state(rng):
        'Initial state is 0 for all rollouts.'
        if (rng.uniform() < 0.5):
            return ToyState(val=0)
        return ToyState(val=1)
    exp = build_experiment(state_sampler=random_initial_state, covariate_builder=initial_state_covariates)
    dataset0 = exp.run(num_samples=100, seed=1234)
    dataset1 = exp.run(num_samples=100, seed=1234)
    assert np.allclose(dataset0.covariates, dataset1.covariates)
    exp = build_experiment(state_sampler=correlated_state_sampler, covariate_builder=initial_state_covariates)
    dataset = exp.run(num_samples=100, seed=1234)
    assert np.allclose(np.expand_dims(np.arange(100), axis=1), dataset.covariates)

    @parameter(name='init_val', default=2)
    def parameterized_sampler(init_val):
        return ToyState(val=init_val)
    exp = build_experiment(state_sampler=parameterized_sampler, covariate_builder=initial_state_covariates)
    dataset = exp.run(num_samples=10)
    assert np.allclose(dataset.covariates, 2)
    dataset = exp.run(num_samples=10, init_val=3.14)
    assert np.allclose(dataset.covariates, 3.14)
    dataset = exp.run(num_samples=10, init_val=1234)
    assert np.allclose(dataset.covariates, 1234)
    with pytest.raises(ValueError):
        exp.run(num_samples=10, random_val=123)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    assert np
    assert  ... . ... ( ... . ... ( ... .arange,),)


idx = 23:------------------- similar code ------------------ index = 403, score = 6.0 
def test_extract_array_1d():
    'In 1d, shape can be int instead of tuple'
    assert np.all((extract_array(np.arange(4), 3, ((- 1),), fill_value=(- 99)) == np.array([(- 99), (- 99), 0])))
    assert np.all((extract_array(np.arange(4), 3, (- 1), fill_value=(- 99)) == np.array([(- 99), (- 99), 0])))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    assert np. ... (( ... ( ... .arange,  ... ,,) ==))

idx = 24:------------------- similar code ------------------ index = 184, score = 6.0 
def _draw_resample_index(n, seed):
    'Compute vector of randomly drawn indices with replacement.\n\n    Draw indices with replacement from the discrete uniform distribution\n    on {0,...,n-1}. We control the randomness by setting the seed to *seed*.\n    If *seed* = -1 we return all indices {0,...,n-1} for debugging.\n\n    Args:\n        n (int): Upper bound for indices and number of indices to draw\n        seed (int): Random number seed.\n\n    Returns:\n        indices (np.array): Resample indices.\n\n    '
    if (seed == (- 1)):
        return np.arange(n)
    np.random.seed(seed)
    indices = np.random.randint(0, n, n)
    return indices

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        return np.arange

idx = 25:------------------- similar code ------------------ index = 103, score = 6.0 
def gen_new_list(self):
    np.random.seed(0)
    all_size = (self.total_size * self.world_size)
    indices = np.arange(len(self.dataset))
    np.random.shuffle(indices)
    indices = indices[:all_size]
    num_repeat = (((all_size - 1) // indices.shape[0]) + 1)
    indices = np.tile(indices, num_repeat)
    indices = indices[:all_size]
    np.random.shuffle(indices)
    beg = (self.total_size * self.rank)
    indices = indices[beg:(beg + self.total_size)]
    assert (len(indices) == self.total_size)
    return indices

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 26:------------------- similar code ------------------ index = 350, score = 6.0 
def prepare_datasets(sig_h5_file, bkd_h5_file, dataset_name='dataset', n_sig=(- 1), n_bkd=(- 1), test_frac=0.1, val_frac=0.0, n_folds=1, shuffle=True, shuffle_seed=1, balance=True):
    'Prepare datasets for network training.\n\n    Combine signal and background images; k-fold into training, validation,\n    test sets. Save to files.\n\n    Args:\n        sig_h5_file, bkd_h5_file: location of h5 files containing signal,\n                                  background images.\n        dataset_name: base filename to use for saving datasets.\n        n_sig, n_bkd: number of signal, background images to load.\n        test_frac: proportion of images to save for testing.\n        val_frac: proportion of images to save for validation. Leave at zero\n                  unless using ROC AUC scoring.\n        n_folds: number of k-folds.\n        auxvars: list of auxvar field names to load.\n        shuffle: if True shuffle images before k-folding.\n        shuffle_seed: seed for shuffling.\n    Returns:\n        file_dict: dict containing list of filenames containing train, test\n                   datasets.\n    TODO: add support for multiple classes.\n    '
    (sig_images, sig_auxvars) = load_images(sig_h5_file, n_sig)
    (bkd_images, bkd_auxvars) = load_images(bkd_h5_file, n_bkd)
    if balance:
        n = min(len(sig_images), len(bkd_images))
        sig_images = sig_images[:n]
        sig_auxvars = sig_auxvars[:n]
        bkd_images = bkd_images[:n]
        bkd_auxvars = bkd_auxvars[:n]
    n_sig = len(sig_images)
    n_bkd = len(bkd_images)
    print('collected {0} signal and {1} background images'.format(n_sig, n_bkd))
    n_images = (n_sig + n_bkd)
    images = np.concatenate((sig_images, bkd_images))
    images = images.reshape((- 1), (images.shape[1] * images.shape[2]))
    auxvars = np.concatenate((sig_auxvars, bkd_auxvars))
    classes = np.concatenate([np.repeat([[1, 0]], n_sig, axis=0), np.repeat([[0, 1]], n_bkd, axis=0)])
    if (test_frac >= 1):
        out_file = (dataset_name + '_test.h5')
        with h5py.File(out_file, 'w') as h5file:
            h5file.create_dataset('X_test', data=images)
            h5file.create_dataset('Y_test', data=classes)
            h5file.create_dataset('auxvars_test', data=auxvars)
        return {'test': out_file}
    if (test_frac <= 0):
        rs = np.random.RandomState(shuffle_seed)
        train = np.arange(len(images))
        rs.shuffle(train)
        out_file = (dataset_name + '_train.h5')
        with h5py.File(out_file, 'w') as h5file:
            n_val = int((val_frac * len(train)))
            h5file.create_dataset('X_val', data=images[train][:n_val])
            h5file.create_dataset('Y_val', data=classes[train][:n_val])
            h5file.create_dataset('auxvars_val', data=auxvars[train][:n_val])
            h5file.create_dataset('X_train', data=images[train][n_val:])
            h5file.create_dataset('Y_train', data=classes[train][n_val:])
            h5file.create_dataset('auxvars_train', data=auxvars[train][n_val:])
        return {'train': out_file}
    rs = cross_validation.ShuffleSplit(n_images, n_iter=1, test_size=test_frac, random_state=shuffle_seed)
    for (trn, tst) in rs:
        (train, test) = (trn, tst)
    out_file = (dataset_name + '_test.h5')
    with h5py.File(out_file, 'w') as h5file:
        h5file.create_dataset('X_test', data=images[test])
        h5file.create_dataset('Y_test', data=classes[test])
        h5file.create_dataset('auxvars_test', data=auxvars[test])
    file_dict = {'test': out_file}
    if (n_folds > 1):
        kf = cross_validation.KFold(len(train), n_folds, shuffle=True, random_state=shuffle_seed)
        i = 0
        kf_files = []
        for (ktrain, ktest) in kf:
            np.random.shuffle(ktrain)
            out_file = (dataset_name + '_train_kf{0}.h5'.format(i))
            with h5py.File(out_file, 'w') as h5file:
                h5file.create_dataset('X_test', data=images[train][ktest])
                h5file.create_dataset('Y_test', data=classes[train][ktest])
                h5file.create_dataset('auxvars_test', data=auxvars[train][ktest])
                n_val = int((val_frac * len(ktrain)))
                h5file.create_dataset('X_val', data=images[train][ktrain][:n_val])
                h5file.create_dataset('Y_val', data=classes[train][ktrain][:n_val])
                h5file.create_dataset('auxvars_val', data=auxvars[train][ktrain][:n_val])
                h5file.create_dataset('X_train', data=images[train][ktrain][n_val:])
                h5file.create_dataset('Y_train', data=classes[train][ktrain][n_val:])
                h5file.create_dataset('auxvars_train', data=auxvars[train][ktrain][n_val:])
            kf_files.append(out_file)
            i += 1
        file_dict['train'] = kf_files
    else:
        rs = np.random.RandomState(shuffle_seed)
        rs.shuffle(train)
        out_file = (dataset_name + '_train.h5')
        with h5py.File(out_file, 'w') as h5file:
            n_val = int((val_frac * len(train)))
            h5file.create_dataset('X_val', data=images[train][:n_val])
            h5file.create_dataset('Y_val', data=classes[train][:n_val])
            h5file.create_dataset('auxvars_val', data=auxvars[train][:n_val])
            h5file.create_dataset('X_train', data=images[train][n_val:])
            h5file.create_dataset('Y_train', data=classes[train][n_val:])
            h5file.create_dataset('auxvars_train', data=auxvars[train][n_val:])
        file_dict['train'] = out_file
    return file_dict

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.arange

idx = 27:------------------- similar code ------------------ index = 352, score = 6.0 
def test_scalar_parameters_1d_array_input(self):
    '\n        Scalar parameters should broadcast with an array input to result in an\n        array output of the same shape as the input.\n        '
    t = TModel_1_1(1, 10)
    y = t((np.arange(5) * 100))
    assert isinstance(y, np.ndarray)
    assert (np.shape(y) == (5,))
    assert np.all((y == [11, 111, 211, 311, 411]))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  =  ... ((np.arange *  ... ))

idx = 28:------------------- similar code ------------------ index = 163, score = 6.0 
def __getitem__(self, idx):
    pt_idxs = np.arange(0, self.num_points)
    np.random.shuffle(pt_idxs)
    current_points = torch.from_numpy(self.points[(idx, pt_idxs)].copy()).float()
    current_labels = torch.from_numpy(self.labels[(idx, pt_idxs)].copy()).long()
    return (current_points, current_labels)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 29:------------------- similar code ------------------ index = 356, score = 6.0 
@pytest.mark.skipif('not HAS_SCIPY')
def test_fit_with_bound_constraints_estimate_jacobian():
    '\n    Regression test for https://github.com/astropy/astropy/issues/2400\n\n    Checks that bounds constraints are obeyed on a custom model that does not\n    define fit_deriv (and thus its Jacobian must be estimated for non-linear\n    fitting).\n    '

    class MyModel(Fittable1DModel):
        a = Parameter(default=1)
        b = Parameter(default=2)

        @staticmethod
        def evaluate(x, a, b):
            return ((a * x) + b)
    m_real = MyModel(a=1.5, b=(- 3))
    x = np.arange(100)
    y = m_real(x)
    m = MyModel()
    f = fitting.LevMarLSQFitter()
    fitted_1 = f(m, x, y)
    assert np.allclose(fitted_1.a, 1.5)
    assert np.allclose(fitted_1.b, (- 3))
    m2 = MyModel()
    m2.a.bounds = ((- 2), 2)
    f2 = fitting.LevMarLSQFitter()
    fitted_2 = f2(m2, x, y)
    assert np.allclose(fitted_1.a, 1.5)
    assert np.allclose(fitted_1.b, (- 3))
    assert np.any((f2.fit_info['fjac'] != 0))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 30:------------------- similar code ------------------ index = 79, score = 6.0 
def sample(corpus, N):
    shuffle_indices = np.random.permutation(np.arange(len(corpus)))
    return np.array(corpus)[shuffle_indices][:N]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... (np.arange)

idx = 31:------------------- similar code ------------------ index = 358, score = 6.0 
def test_nondefault_nbins(self):
    '\n        Testing correct generation of hist_bins with a non-default nbins\n        '
    for nbins in [128, 256, 512]:
        bd = (np.arange(1025) - 0.5)
        xd = np.linspace(0, 1, len(bd))
        xs = np.linspace(0, 1, (nbins + 1))
        bins = np.interp(xs, xd, bd)
        np.testing.assert_array_equal(self.d[0].hist_bins('FSC-H', nbins=nbins, scale='linear'), bins)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
         ...  = (np.arange -  ... )

idx = 32:------------------- similar code ------------------ index = 164, score = 6.0 
def cubic_lattice(N):
    array = np.arange(N)
    (xs, ys, zs) = np.meshgrid(array, array, array)
    return np.vstack((xs.flatten(), ys.flatten(), zs.flatten())).T

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 33:------------------- similar code ------------------ index = 256, score = 6.0 
def triangulation_indices(n_keypoints):
    N = n_triangulated(n_keypoints)
    indices = np.arange(0, n_keypoints)
    np.random.shuffle(indices)
    return indices[:N]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 34:------------------- similar code ------------------ index = 73, score = 6.0 
def hot_one_vector(y, max):
    import numpy as np
    labels_hot_vector = np.zeros((y.shape[0], max), dtype=np.int32)
    labels_hot_vector[(np.arange(y.shape[0]), y)] = 1
    return labels_hot_vector

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ... [(np.arange,  ... )]

idx = 35:------------------- similar code ------------------ index = 167, score = 6.0 
def main():
    np.random.seed(314)
    batch_size = 3
    sequence_length = 4
    num_vocabs = 10
    num_hidden = 5
    model_fn = MyLSTM(num_hidden, batch_size, sequence_length)
    (labels, lengths) = sequence_utils.gen_random_sequence(batch_size, sequence_length, num_vocabs)
    xs = []
    for l in lengths:
        xs.append(np.random.rand(l, num_hidden).astype(dtype=np.float32))
    h = np.zeros((batch_size, num_hidden), dtype=np.float32)
    c = np.zeros((batch_size, num_hidden), dtype=np.float32)
    mask = (np.expand_dims(np.arange(sequence_length), 0) < np.expand_dims(lengths, 1)).astype(np.float32)
    args = [xs, h, c, mask]
    testtools.generate_testcase(model_fn, args)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = ( ... . ... (np.arange,  ... ) <)

idx = 36:------------------- similar code ------------------ index = 70, score = 6.0 
def reset(self):
    ' reset order of h5 files '
    self.file_idxs = np.arange(0, len(self.h5_files))
    if self.shuffle:
        np.random.shuffle(self.file_idxs)
    self.current_data = None
    self.current_label = None
    self.current_file_idx = 0
    self.batch_idx = 0

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = np.arange

idx = 37:------------------- similar code ------------------ index = 368, score = 6.0 
def test_1d(self):
    'Test 1D array.'
    data = np.arange(2)
    expected = np.array([0, 0, 0.5, 0.5])
    result = block_replicate(data, 2)
    assert np.all((result == expected))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 38:------------------- similar code ------------------ index = 253, score = 6.0 
def test_smooth_lsf(pop_and_params):
    (pop, params) = pop_and_params
    _reset_default_params(pop, params)
    tmax = 1.0
    wave_lsf = np.arange(4000, 7000.0, 10)
    x = ((wave_lsf - 5500) / 1500.0)
    sigma_lsf = (50 * ((1.0 + (0.4 * x)) + (0.6 * (x ** 2))))
    (w, spec) = pop.get_spectrum(tage=tmax)
    pop.params['smooth_lsf'] = True
    assert (pop.params.dirtiness == 2)
    pop.set_lsf(wave_lsf, sigma_lsf)
    (w, smspec) = pop.get_spectrum(tage=tmax)
    hi = (w > 7100)
    sm = ((w < 7000) & (w > 3000))
    assert np.allclose(((spec[hi] / smspec[hi]) - 1.0), 0.0)
    assert (not np.allclose(((spec[sm] / smspec[sm]) - 1.0), 0.0))
    pop.set_lsf(wave_lsf, (sigma_lsf * 2))
    assert (pop.params.dirtiness == 2)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 39:------------------- similar code ------------------ index = 171, score = 6.0 
def ordered_indices(self):
    'Return an ordered list of indices. Batches will be constructed based\n        on this order.'
    if self.shuffle:
        order = [np.random.permutation(len(self))]
    else:
        order = [np.arange(len(self))]
    order.append(self.sizes)
    return np.lexsort(order)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:    else:
         ...  = [np.arange]

idx = 40:------------------- similar code ------------------ index = 251, score = 6.0 
def test_broadcast_one_not_writable(self):
    val = (2458000 + np.arange(3))
    val2 = np.arange(1)
    t = Time(val=val, val2=val2, format='jd', scale='tai')
    t_b = Time(val=(val + (0 * val2)), val2=((0 * val) + val2), format='jd', scale='tai')
    t_i = Time(val=57990, val2=0.3, format='jd', scale='tai')
    t_b[1] = t_i
    t[1] = t_i
    assert (t_b[1] == t[1]), 'writing worked'
    assert (t_b[0] == t[0]), "broadcasting didn't cause problems"
    assert np.all((t_b == t)), 'behaved as expected'

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = ( ...  + np.arange)

idx = 41:------------------- similar code ------------------ index = 373, score = 6.0 
def test_extract_Array_float():
    'integer is at bin center'
    for a in np.arange(2.51, 3.49, 0.1):
        assert np.all((extract_array(np.arange(5), 3, a) == np.array([2, 3, 4])))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in np.arange:
idx = 42:------------------- similar code ------------------ index = 63, score = 6.0 
def test_1d(self):
    'Test 1D array.'
    data = np.arange(4)
    expected = np.array([1, 5])
    result = block_reduce(data, 2)
    assert np.all((result == expected))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 43:------------------- similar code ------------------ index = 379, score = 6.0 
def test_arithmetic_add_with_array():
    ccd = CCDData(np.ones((3, 3)), unit='')
    res = ccd.add(np.arange(3))
    np.testing.assert_array_equal(res.data, ([[1, 2, 3]] * 3))
    ccd = CCDData(np.ones((3, 3)), unit='adu')
    with pytest.raises(ValueError):
        ccd.add(np.arange(3))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... (np.arange)

idx = 44:------------------- similar code ------------------ index = 382, score = 6.0 
def ordered_indices(self):
    'Return an ordered list of indices. Batches will be constructed based\n        on this order.'
    if self.shuffle:
        order = [np.random.permutation(len(self))]
    else:
        order = [np.arange(len(self))]
    order.append(self.sizes)
    return np.lexsort(order)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:    else:
         ...  = [np.arange]

idx = 45:------------------- similar code ------------------ index = 59, score = 6.0 
def reset(self):
    self.idxs = np.arange(0, len(self.datapath))
    if self.shuffle:
        np.random.shuffle(self.idxs)
    self.num_batches = (((len(self.datapath) + self.batch_size) - 1) // self.batch_size)
    self.batch_idx = 0

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = np.arange

idx = 46:------------------- similar code ------------------ index = 386, score = 6.0 
def test_mixin_functionality(self, mixin_cols):
    col = mixin_cols['m']
    cls_name = type(col).__name__
    len_col = len(col)
    idx = np.arange(len_col)
    t1 = table.QTable([idx, col], names=['idx', 'm1'])
    t2 = table.QTable([idx, col], names=['idx', 'm2'])
    t1 = t1[[0, 1, 3]]
    t2 = t2[[0, 2, 3]]
    out = table.join(t1, t2, join_type='inner')
    assert (len(out) == 2)
    assert (out['m2'].__class__ is col.__class__)
    assert np.all((out['idx'] == [0, 3]))
    if (cls_name == 'SkyCoord'):
        assert skycoord_equal(out['m1'], col[[0, 3]])
        assert skycoord_equal(out['m2'], col[[0, 3]])
    else:
        assert np.all((out['m1'] == col[[0, 3]]))
        assert np.all((out['m2'] == col[[0, 3]]))
    if (cls_name == 'Time'):
        out = table.join(t1, t2, join_type='left')
        assert (len(out) == 3)
        assert np.all((out['idx'] == [0, 1, 3]))
        assert np.all((out['m1'] == t1['m1']))
        assert np.all((out['m2'] == t2['m2']))
        assert np.all((out['m1'].mask == [False, False, False]))
        assert np.all((out['m2'].mask == [False, True, False]))
        out = table.join(t1, t2, join_type='right')
        assert (len(out) == 3)
        assert np.all((out['idx'] == [0, 2, 3]))
        assert np.all((out['m1'] == t1['m1']))
        assert np.all((out['m2'] == t2['m2']))
        assert np.all((out['m1'].mask == [False, True, False]))
        assert np.all((out['m2'].mask == [False, False, False]))
        out = table.join(t1, t2, join_type='outer')
        assert (len(out) == 4)
        assert np.all((out['idx'] == [0, 1, 2, 3]))
        assert np.all((out['m1'] == col))
        assert np.all((out['m2'] == col))
        assert np.all((out['m1'].mask == [False, False, True, False]))
        assert np.all((out['m2'].mask == [False, True, False, False]))
    else:
        for join_type in ('outer', 'left', 'right'):
            with pytest.raises(NotImplementedError) as err:
                table.join(t1, t2, join_type='outer')
            assert (('join requires masking' in str(err.value)) or ('join unavailable' in str(err.value)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 47:------------------- similar code ------------------ index = 84, score = 6.0 
def ordered_indices(self):
    'Return an ordered list of indices. Batches will be constructed based\n        on this order.'
    if self.shuffle:
        order = [np.random.permutation(len(self))]
    else:
        order = [np.arange(len(self))]
    order.append(self.sizes)
    return np.lexsort(order)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:    else:
         ...  = [np.arange]

idx = 48:------------------- similar code ------------------ index = 264, score = 6.0 
def batch_iter(data, batch_size, num_epochs, shuffle=True):
    data = np.array(data)
    data_size = len(data)
    num_batches_per_epoch = (int(((data_size - 1) / batch_size)) + 1)
    for epoch in range(num_epochs):
        if shuffle:
            np.random.seed(2017)
            shuffle_indices = np.random.permutation(np.arange(data_size))
            shuffled_data = data[shuffle_indices]
        else:
            shuffled_data = data
        for batch_num in range(num_batches_per_epoch):
            start_index = (batch_num * batch_size)
            end_index = min(((batch_num + 1) * batch_size), data_size)
            (yield shuffled_data[start_index:end_index])

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
        if  ... :
             ...  =  ... . ... (np.arange)

idx = 49:------------------- similar code ------------------ index = 265, score = 6.0 
def fn(test_name):
    gb = onnx_script.GraphBuilder(test_name)
    if (cell_type == 'LSTM'):
        wr = 8
        perm = [0, 2, 1, 3, 4, 6, 5, 7]
        num_direction = 1
        direction = 'forward'
    elif (cell_type == 'BiLSTM'):
        wr = 16
        perm = (np.tile([0, 2, 1, 3], 4) + (np.repeat(np.arange(4), 4) * 4))
        num_direction = 2
        direction = 'bidirectional'
    elif (cell_type == 'GRU'):
        wr = 6
        perm = [1, 0, 2, 4, 3, 5]
        num_direction = 1
        direction = 'forward'
    elif (cell_type == 'BiGRU'):
        wr = 12
        perm = [1, 0, 2, 4, 3, 5, 7, 6, 8, 10, 9, 11]
        num_direction = 2
        direction = 'bidirectional'
    else:
        raise RuntimeError(('Unknown cell_type: %s' % cell_type))
    embed_size = num_hidden
    np.random.seed(42)
    if ((batch_size == 3) and (sequence_length == 4)):
        labels = np.array([[1, 2, 3, 7], [4, 5, 0, 0], [6, 0, 0, 0]])
        lengths = np.array([4, 2, 1])
        targets = np.array([1, 0, 1])
    else:
        (labels, lengths) = _gen_random_sequence(batch_size, sequence_length, num_vocabs)
        targets = np.random.randint(2, size=batch_size)
    labels = labels.astype(np.int32)
    embed = param_initializer(size=(num_vocabs, embed_size)).astype(np.float32)
    weight = param_initializer(size=(embed_size, (num_hidden * wr))).astype(np.float32)
    bias = param_initializer(size=((num_hidden * wr),)).astype(np.float32)
    linear_w = param_initializer(size=((num_direction * num_hidden), 2)).astype(np.float32)
    linear_b = param_initializer(size=(2,)).astype(np.float32)
    x = F.embed_id(labels, embed)
    state = np.zeros((num_direction, len(labels), num_hidden)).astype(np.float32)
    xs = F.transpose_sequence([v[:l] for (v, l) in zip(x, lengths)])
    ch_weight = np.split(weight, wr, axis=1)
    ch_weight = [ch_weight[i] for i in perm]
    ch_bias = np.split(bias, wr, axis=0)
    ch_bias = [ch_bias[i] for i in perm]
    if (cell_type == 'LSTM'):
        (h, _, rnn_outputs) = F.n_step_lstm(1, 0.0, state, state, [ch_weight], [ch_bias], xs)
    elif (cell_type == 'BiLSTM'):
        (h, _, rnn_outputs) = F.n_step_bilstm(1, 0.0, state, state, [ch_weight[:8], ch_weight[8:]], [ch_bias[:8], ch_bias[8:]], xs)
    elif (cell_type == 'GRU'):
        (h, rnn_outputs) = F.n_step_gru(1, 0.0, state, [ch_weight], [ch_bias], xs)
    elif (cell_type == 'BiGRU'):
        (h, rnn_outputs) = F.n_step_bigru(1, 0.0, state, [ch_weight[:6], ch_weight[6:]], [ch_bias[:6], ch_bias[6:]], xs)
    shape = (len(labels), (num_hidden * num_direction))
    h = F.reshape(h, shape)
    rnn_outputs = F.pad_sequence(rnn_outputs)
    rnn_outputs = F.reshape(rnn_outputs, ((- 1), len(labels), num_direction, num_hidden))
    rnn_outputs = F.transpose(rnn_outputs, axes=[0, 2, 1, 3])
    result = F.linear(h, np.transpose(linear_w), linear_b)
    loss = F.softmax_cross_entropy(result, targets)
    (weight_w, weight_r) = np.split(weight, 2, axis=1)
    labels_v = gb.input('labels', labels)
    lengths_v = gb.input('lengths', lengths)
    targets_v = gb.input('targets', targets)
    embed_v = gb.param('embed', embed)
    weight_w_v = gb.param('weight_w', np.reshape(np.transpose(weight_w), (num_direction, (- 1), embed_size)))
    weight_r_v = gb.param('weight_r', np.reshape(np.transpose(weight_r), (num_direction, (- 1), num_hidden)))
    bias_v = gb.param('bias', np.reshape(bias, (num_direction, (- 1))))
    linear_w_v = gb.param('linear_w', linear_w)
    linear_b_v = gb.param('linear_b', linear_b)
    x = gb.Gather([embed_v, labels_v])
    x = gb.Transpose([x], perm=[1, 0, 2])
    if (cell_type in ['LSTM', 'BiLSTM']):
        (rnn_outputs_v, h) = gb.LSTM([x, weight_w_v, weight_r_v, bias_v, lengths_v], outputs=['rnn_outputs', 'last_state'], hidden_size=num_hidden, direction=direction)
    elif (cell_type in ['GRU', 'BiGRU']):
        (rnn_outputs_v, h) = gb.GRU([x, weight_w_v, weight_r_v, bias_v, lengths_v], outputs=['rnn_outputs', 'last_state'], hidden_size=num_hidden, direction=direction)
    shape_v = gb.const(shape)
    h = gb.Reshape([h, shape_v])
    result_v = gb.Gemm([h, linear_w_v, linear_b_v])
    loss_v = gb.ChainerSoftmaxCrossEntropy([result_v, targets_v])
    if (not output_loss_only):
        gb.output(rnn_outputs_v, rnn_outputs.array)
        gb.output(result_v, result.array)
    gb.output(loss_v, loss.array)
    gb.gen_test()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:    elif:
         ...  = ( + ( ... . ... (np.arange,  ... ) *  ... ))

idx = 50:------------------- similar code ------------------ index = 282, score = 6.0 
def test_group_mixins():
    '\n    Test grouping a table with mixin columns\n    '
    idx = np.arange(4)
    x = np.array([3.0, 1.0, 2.0, 1.0])
    q = (x * u.m)
    lon = coordinates.Longitude((x * u.deg))
    lat = coordinates.Latitude((x * u.deg))
    tm = (time.Time(2000, format='jyear') + time.TimeDelta((x * 1e-10), format='sec'))
    sc = coordinates.SkyCoord(ra=lon, dec=lat)
    aw = table_helpers.ArrayWrapper(x)
    nd = np.array([(3, 'c'), (1, 'a'), (2, 'b'), (1, 'a')], dtype='<i4,|S1').view(NdarrayMixin)
    qt = QTable([idx, x, q, lon, lat, tm, sc, aw, nd], names=['idx', 'x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd'])
    mixin_keys = ['x', 'q', 'lon', 'lat', 'tm', 'sc', 'aw', 'nd']
    for key in mixin_keys:
        qtg = qt.group_by(key)
        assert np.all((qtg['idx'] == [1, 3, 2, 0]))
        for name in ['x', 'q', 'lon', 'lat', 'tm', 'aw', 'nd']:
            assert np.all((qt[name][[1, 3]] == qtg.groups[0][name]))
            assert np.all((qt[name][[2]] == qtg.groups[1][name]))
            assert np.all((qt[name][[0]] == qtg.groups[2][name]))
    uqt = unique(qt, keys=mixin_keys)
    assert (len(uqt) == 3)
    assert np.all((uqt['idx'] == [1, 2, 0]))
    assert np.all((uqt['x'] == [1.0, 2.0, 3.0]))
    idxg = qt['idx'].group_by(qt[mixin_keys])
    assert np.all((idxg == [1, 3, 2, 0]))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 51:------------------- similar code ------------------ index = 128, score = 6.0 
def train(base_lr, batch_sz, gpu_no, model_name, power_s):
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_no
    root_path = os.path.dirname(os.path.realpath(__file__))
    log_path = create_dir(os.path.join(root_path, 'log'))
    save_path = create_dir(os.path.join(root_path, 'weights'))
    acc_count = 0
    while (acc_count < 100):
        if os.path.exists(os.path.join(log_path, ('log_test_%02d.txt' % acc_count))):
            acc_count += 1
        else:
            break
    assert (acc_count < 100)
    log_train_fname = ('log_train_%02d.txt' % acc_count)
    log_test_fname = ('log_test_%02d.txt' % acc_count)
    n_class = 100
    batch_sz = batch_sz
    batch_test = 100
    max_epoch = 42500
    lr = base_lr
    momentum = 0.9
    is_training = tf.placeholder('bool')
    images = tf.placeholder(tf.float32, (None, 32, 32, 3))
    labels = tf.placeholder(tf.int32, None)
    vgg = VGG()
    vgg.build(images, n_class, is_training, model_name, power_s)
    fit_loss = loss2(vgg.score, labels, n_class, 'c_entropy')
    loss_op = fit_loss
    reg_loss_list = tf.losses.get_regularization_losses()
    if (len(reg_loss_list) != 0):
        reg_loss = tf.add_n(reg_loss_list)
        loss_op += reg_loss
    thom_loss_list = tf.get_collection('thomson_loss')
    if (len(thom_loss_list) != 0):
        thom_loss = tf.add_n(thom_loss_list)
        loss_op += thom_loss
    thom_final_list = tf.get_collection('thomson_final')
    if (len(thom_final_list) != 0):
        thom_final = tf.add_n(thom_final_list)
        loss_op += thom_final
    lr_ = tf.placeholder('float')
    update_op = tf.train.MomentumOptimizer(lr_, 0.9).minimize(loss_op)
    predc = vgg.pred
    acc_op = tf.reduce_mean(tf.to_float(tf.equal(labels, tf.to_int32(vgg.pred))))
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        tf.summary.scalar('fit loss', fit_loss)
        if (len(reg_loss_list) != 0):
            tf.summary.scalar('reg loss', reg_loss)
        if (len(thom_loss_list) != 0):
            tf.summary.scalar('thomson loss', thom_loss)
        if (len(thom_final_list) != 0):
            tf.summary.scalar('thomson final loss', thom_final)
        tf.summary.scalar('learning rate', lr)
        tf.summary.scalar('accuracy', acc_op)
        merged = tf.summary.merge_all()
        train_writer = tf.summary.FileWriter((root_path + '/tf_log'), sess.graph)
        print('====================')
        print(('Log will be saved to: ' + log_path))
        with open(os.path.join(log_path, log_train_fname), 'w'):
            pass
        with open(os.path.join(log_path, log_test_fname), 'w'):
            pass
        with open(os.path.join(log_path, log_train_fname), 'a') as train_acc_file:
            train_acc_file.write(('model_name: %s, power_s: %s\n' % (model_name, power_s)))
        with open(os.path.join(log_path, log_test_fname), 'a') as test_acc_file:
            test_acc_file.write(('model_name: %s, power_s: %s\n' % (model_name, power_s)))
        for i in range(max_epoch):
            t = (i % 390)
            if (t == 0):
                idx = np.arange(0, 50000)
                np.random.shuffle(idx)
                train_data['data'] = train_data['data'][idx]
                train_data['fine_labels'] = np.reshape(train_data['fine_labels'], [50000])
                train_data['fine_labels'] = train_data['fine_labels'][idx]
            (tr_images, tr_labels) = ip.load_train(train_data, batch_sz, t)
            if (i == 20000):
                lr *= 0.1
            elif (i == 30000):
                lr *= 0.1
            elif (i == 37500):
                lr *= 0.1
            if (len(thom_loss_list) != 0):
                (summary, fit, reg, thom, thomf, acc, _) = sess.run([merged, fit_loss, reg_loss, thom_loss, thom_final, acc_op, update_op], {lr_: lr, is_training: True, images: tr_images, labels: tr_labels})
                if (((i % 100) == 0) and (i != 0)):
                    print(('====iter_%d: fit=%.4f, reg=%.4f, thom=%.4f, thomf=%.4f, acc=%.4f' % (i, fit, reg, thom, thomf, acc)))
                    with open(os.path.join(log_path, log_train_fname), 'a') as train_acc_file:
                        train_acc_file.write(('====iter_%d: fit=%.4f, reg=%.4f, thom=%.4f, thomf=%.4f, acc=%.4f\n' % (i, fit, reg, thom, thomf, acc)))
                train_writer.add_summary(summary, i)
            else:
                (summary, fit, reg, acc, _) = sess.run([merged, fit_loss, reg_loss, acc_op, update_op], {lr_: lr, is_training: True, images: tr_images, labels: tr_labels})
                if (((i % 100) == 0) and (i != 0)):
                    print(('====iter_%d: fit=%.4f, reg=%.4f, acc=%.4f' % (i, fit, reg, acc)))
                    with open(os.path.join(log_path, log_train_fname), 'a') as train_acc_file:
                        train_acc_file.write(('====iter_%d: fit=%.4f, reg=%.4f, acc=%.4f\n' % (i, fit, reg, acc)))
                train_writer.add_summary(summary, i)
            if (((i % 500) == 0) and (i != 0)):
                n_test = 10000
                acc = 0.0
                for j in range(int((n_test / batch_test))):
                    (te_images, te_labels) = ip.load_test(test_data, batch_test, j)
                    acc = (acc + sess.run(acc_op, {is_training: False, images: te_images, labels: te_labels}))
                acc = ((acc * batch_test) / float(n_test))
                print(('++++iter_%d: test acc=%.4f' % (i, acc)))
                with open(os.path.join(log_path, log_test_fname), 'a') as test_acc_file:
                    test_acc_file.write(('++++iter_%d: test acc=%.4f\n' % (i, acc)))
            if (((i % 10000) == 0) and (i != 0)):
                tf.train.Saver().save(sess, os.path.join(save_path, str(i)))
        tf.train.Saver().save(sess, os.path.join(save_path, str(i)))
        n_test = 10000
        acc = 0.0
        for j in range(int((n_test / batch_test))):
            (te_images, te_labels) = ip.load_test(test_data, batch_test, j)
            acc = (acc + sess.run(acc_op, {is_training: False, images: te_images, labels: te_labels}))
        acc = ((acc * batch_test) / float(n_test))
        print(('++++iter_%d: test acc=%.4f' % (i, acc)))
        with open(os.path.join(log_path, log_test_fname), 'a') as test_acc_file:
            test_acc_file.write(('++++iter_%d: test acc=%.4f\n' % (i, acc)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        for  ...  in:
            if:
                 ...  = np.arange

idx = 52:------------------- similar code ------------------ index = 126, score = 6.0 
def train_batch(self, epochs=1, lr=0.001, seq=1):
    count = (self.settings.num_dataset + 1)
    checkdir = 'checkpoint'
    try:
        os.mkdir(checkdir)
    except FileExistsError:
        print('Folder exists: ', checkdir)
    is_model_compiled = False
    indexes = np.arange(1, count)
    np.random.shuffle(indexes)
    for i in indexes:
        filename = self.settings.dataset
        filename += ('.densemapnet.weights.%d-%d.h5' % (seq, i))
        filepath = os.path.join(checkdir, filename)
        checkpoint = ModelCheckpoint(filepath=filepath, save_weights_only=True, verbose=1, save_best_only=False)
        callbacks = [checkpoint]
        self.load_train_data(i)
        if (self.network is None):
            self.network = DenseMapNet(settings=self.settings)
            self.model = self.network.build_model()
        if (not is_model_compiled):
            print('Using loss=mae on final conv layer')
            self.model.compile(loss=_loss_mae_disparity, optimizer=Adam(lr=lr))
            is_model_compiled = True
        if self.settings.model_weights:
            if self.settings.notrain:
                self.predict_disparity()
                return
        x = [self.train_lx, self.train_rx]
        self.model.fit(x, self.train_dx, epochs=epochs, batch_size=self.settings.batch_size, shuffle=True, callbacks=callbacks)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 53:------------------- similar code ------------------ index = 124, score = 6.0 
@staticmethod
def random_choice(gallery, num):
    "Random select some elements from the gallery.\n\n        It seems that Pytorch's implementation is slower than numpy so we use\n        numpy to randperm the indices.\n        "
    assert (len(gallery) >= num)
    if isinstance(gallery, list):
        gallery = np.array(gallery)
    cands = np.arange(len(gallery))
    np.random.shuffle(cands)
    rand_inds = cands[:num]
    if (not isinstance(gallery, np.ndarray)):
        rand_inds = torch.from_numpy(rand_inds).long().to(gallery.device)
    return gallery[rand_inds]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 54:------------------- similar code ------------------ index = 136, score = 6.0 
def test_column_with_and_without_units(self):
    'Ensure a Column without a unit is treated as an array [#3648]'
    a = np.arange(50000.0, 50010.0)
    ta = Time(a, format='mjd')
    c1 = Column(np.arange(50000.0, 50010.0), name='mjd')
    tc1 = Time(c1, format='mjd')
    assert np.all((ta == tc1))
    c2 = Column(np.arange(50000.0, 50010.0), name='mjd', unit='day')
    tc2 = Time(c2, format='mjd')
    assert np.all((ta == tc2))
    c3 = Column(np.arange(50000.0, 50010.0), name='mjd', unit='m')
    with pytest.raises(u.UnitsError):
        Time(c3, format='mjd')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 55:------------------- similar code ------------------ index = 307, score = 6.0 
def _shuffle_update_order(self, n):
    self._update_order = np.arange(n)
    np.random.shuffle(self._update_order)
    self._update_order = self._update_order.tolist()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = np.arange

idx = 56:------------------- similar code ------------------ index = 308, score = 6.0 
def icecore_diffuse(d18O, b, time, T, P, depth, depth_horizons, dz, drho):
    '\n        DOCSTRING: Function \'icecore_diffuse\'\n        DESCRIPTION: accounts for diffusion and compaction in the firn.\n\n        Inputs:\n            d18O: ice core isotope ratio, output from sensor Model (permil)\n            b   : average accumulation rate at site (m/year)\n            time: calendar years of record\n            T   : average temperature at site (K)\n            P   : average sea level pressure at site (atm)\n            depth: total depth of core\n            depth_horizons: accumulation by year in ice core--depth horizons (moving downwards in core) in meters\n            dz: step in depth (default = min(depth_horizons)/10.) \n            drho: step in density (default=0.5 kg/m^3)\n\n        Diffusion is computed using a convolution (Gaussian smoothing).\n\n        Functionality: Calculates diffusion length as a function of density\n        in firn, and given vectors of time-depth and density-depth \n        Also expects Pressure in atm, T in K, rho and rho_ice in \n        kg/m^3, but defaults on these, so only the first three arguments\n        must be entered.\n\n        "Time" is really "age" (increasing down core) and is given in years.\n        z is depth in meters\n        rho should be in kg/m^3\n        the vectors rho, time, and z should all correponding with one another\n    '
    import numpy as np
    import scipy
    from scipy import integrate
    import matplotlib.pyplot as plt
    R = 8.314478
    m = 0.01802
    rho_s = 300.0
    rho_d = 822.0
    rho_i = 920.0
    z = (np.arange(0, depth, dz) + dz)
    (rho, zieq, t) = densification(T, b, rho_s, z)
    rho = rho[0:len(z)]
    time_d = np.cumsum((((dz / b) * rho) / rho_i))
    ts = (((time_d * 365.25) * 24) * 3600)
    drho = np.diff(rho)
    dtdrho = (np.diff(ts) / np.diff(rho))
    D = diffusivity(rho, T, P, rho_d, b)
    D = D[0:(- 1)]
    rho = rho[0:(- 1)]
    diffs = (np.diff(z) / np.diff(time_d))
    diffs = diffs[0:(- 1)]
    solidice = np.where((rho >= (rho_d - 5.0)))
    diffusion = np.where((rho < (rho_d - 5.0)))
    sigma_sqrd_dummy = ((((2 * np.power(rho, 2)) * dtdrho) * D) * drho)
    sigma_sqrd = integrate.cumtrapz(sigma_sqrd_dummy)
    rho = rho[0:(- 1)]
    sigma = np.zeros((len(rho) + 1))
    sigma[diffusion] = np.sqrt(((1 / np.power(rho, 2)) * sigma_sqrd))
    sigma[solidice] = sigma[diffusion][(- 1)]
    sigma = sigma[0:(- 1)]
    del18 = np.flipud(d18O)
    depth_horizons = depth_horizons
    years_rev = np.flipud(time)
    z = np.reshape(z, len(z))
    del18 = np.reshape(del18, len(del18))
    iso_interp = np.interp(z, depth_horizons, del18)
    time_interp = np.interp(z, depth_horizons, years_rev)
    diffused_final = np.zeros(len(iso_interp))
    zp = np.arange((- 100), 100, dz)
    if (len(zp) >= (0.5 * len(z))):
        print('Warning: convolution kernal length (zp) is approaching that of half the length of timeseries. Kernal being clipped.')
        bound = ((0.2 * len(z)) * dz)
        zp = np.arange((- bound), bound, dz)
    sigma_dummy = np.tile(0.08, len(sigma))
    for i in range(len(sigma)):
        part1 = (1.0 / (sigma[i] * np.sqrt((2.0 * np.pi))))
        part2 = scipy.exp(((- (zp ** 2)) / (2 * (sigma[i] ** 2))))
        G = (part1 * part2)
        rm = np.mean(iso_interp)
        cdel = (iso_interp - rm)
        diffused = (np.convolve(G, cdel, mode='same') * dz)
        diffused = (diffused + rm)
        diffused_final[i] = diffused[i]
    diffused_timeseries = diffused_final[0:(- 3)]
    final_iso = np.interp(depth_horizons, z[0:(- 3)], diffused_timeseries)
    ice_diffused = final_iso
    return (z, sigma, D, time_d, diffs, ice_diffused, rho, zieq)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = (np.arange +  ... )

idx = 57:------------------- similar code ------------------ index = 311, score = 6.0 
def __iter__(self):
    idx = np.arange(self.n_sample)
    if ((self.n_sample % self.seq_len) != 0):
        idx = self._pad_ind(idx)
    idx = np.reshape(idx, ((- 1), self.seq_len))
    np.random.shuffle(idx)
    idx = np.reshape(idx, (- 1))
    return iter(idx.astype(int))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 58:------------------- similar code ------------------ index = 312, score = 6.0 
def test_get_possible_range_idxs_leduc(self):
    for n in range(2, 9):
        env_bldr = get_leduc_env_bldr()
        for c in range(env_bldr.rules.N_CARDS_IN_DECK):
            board_2d = env_bldr.lut_holder.get_2d_cards(np.array([c], dtype=np.int32))
            result = PokerRange.get_possible_range_idxs(rules=env_bldr.rules, lut_holder=env_bldr.lut_holder, board_2d=board_2d)
            should_be = np.delete(np.arange(env_bldr.rules.RANGE_SIZE, dtype=np.int32), c)
            assert np.array_equal(a1=result, a2=should_be)
        board_2d = np.array([Poker.CARD_NOT_DEALT_TOKEN_2D], dtype=np.int8)
        result = PokerRange.get_possible_range_idxs(rules=env_bldr.rules, lut_holder=env_bldr.lut_holder, board_2d=board_2d)
        should_be = np.arange(env_bldr.rules.RANGE_SIZE, dtype=np.int32)
        assert np.array_equal(a1=result, a2=should_be)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
        for  ...  in:
             ...  =  ... . ... (np.arange,  ... )

idx = 59:------------------- similar code ------------------ index = 313, score = 6.0 
def _fill(_node):
    if (_node.p_id_acting_next == p_id):
        if (self._iter_counter > self.delay):
            current_weight = np.sum(np.arange((self.delay + 1), (self._iter_counter + 1)))
            new_weight = ((self._iter_counter - self.delay) + 1)
            m_old = (current_weight / (current_weight + new_weight))
            m_new = (new_weight / (current_weight + new_weight))
            _node.data['avg_strat'] = ((m_old * _node.data['avg_strat']) + (m_new * _node.strategy))
            assert np.allclose(np.sum(_node.data['avg_strat'], axis=1), 1, atol=0.0001)
        elif (self._iter_counter == self.delay):
            _node.data['avg_strat'] = np.copy(_node.strategy)
            assert np.allclose(np.sum(_node.data['avg_strat'], axis=1), 1, atol=0.0001)
    for c in _node.children:
        _fill(c)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:
        if:
             ...  =  ... . ... (np.arange)

idx = 60:------------------- similar code ------------------ index = 287, score = 6.0 
def test_single_big(self, table_types):
    'Sort a big-ish table with a non-trivial sort order'
    x = np.arange(10000)
    y = np.sin(x)
    t = table_types.Table([x, y], names=('x', 'y'))
    t.sort('y')
    idx = np.argsort(y)
    assert np.all((t['x'] == x[idx]))
    assert np.all((t['y'] == y[idx]))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 61:------------------- similar code ------------------ index = 116, score = 6.0 
def shuffle_data(data, labels):
    ' Shuffle data and labels.\n        Input:\n          data: B,N,... numpy array\n          label: B,... numpy array\n        Return:\n          shuffled data, label and shuffle indices\n    '
    idx = np.arange(len(labels))
    np.random.shuffle(idx)
    return (data[(idx, ...)], labels[idx], idx)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 62:------------------- similar code ------------------ index = 146, score = 6.0 
def gen_new_list(self):
    np.random.seed(0)
    all_size = (self.total_size * self.world_size)
    indices = np.arange(len(self.dataset))
    indices = indices[:all_size]
    num_repeat = (((all_size - 1) // indices.shape[0]) + 1)
    indices = np.tile(indices, num_repeat)
    indices = indices[:all_size]
    np.random.shuffle(indices)
    beg = (self.total_size * self.rank)
    indices = indices[beg:(beg + self.total_size)]
    assert (len(indices) == self.total_size)
    return indices

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 63:------------------- similar code ------------------ index = 326, score = 6.0 
def choose_nonoverlap(size, ratio):
    N = int((size * ratio))
    indices_ = np.arange(0, size)
    np.random.shuffle(indices_)
    indices = indices_[:N]
    return indices

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 64:------------------- similar code ------------------ index = 148, score = 6.0 
def ordered_indices(self):
    'Return an ordered list of indices. Batches will be constructed based\n        on this order.'
    if self.shuffle:
        order = [np.random.permutation(len(self))]
    else:
        order = [np.arange(len(self))]
    order.append(self.sizes)
    return np.lexsort(order)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:    else:
         ...  = [np.arange]

idx = 65:------------------- similar code ------------------ index = 329, score = 6.0 
if (__name__ == '__main__'):
    import numpy as np
    np.random.seed(314)
    batch_size = 3
    num_hidden = 5
    sequence_length = 4
    model = LinkInFor(num_hidden)
    x = np.random.rand(batch_size, sequence_length, num_hidden).astype(np.float32)
    h = np.random.rand(batch_size, num_hidden).astype(np.float32)
    args = [x, h, np.arange(sequence_length)]
    dprint(model(*args))
    ch2o.generate_testcase(model, args)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
     ...  = [ ... ,  ... , np.arange]

idx = 66:------------------- similar code ------------------ index = 102, score = 6.0 
def test_scalar_parameters_1d_array_input(self):
    '\n        The dimension of the input should match the number of models unless\n        model_set_axis=False is given, in which case the input is copied across\n        all models.\n        '
    t = TModel_1_1([1, 2], [10, 20], n_models=2)
    with pytest.raises(ValueError):
        y = t((np.arange(5) * 100))
    y1 = t([100, 200])
    assert (np.shape(y1) == (2,))
    assert np.all((y1 == [111, 222]))
    y2 = t([100, 200], model_set_axis=False)
    assert (np.shape(y2) == (2, 2))
    assert np.all((y2 == [[111, 211], [122, 222]]))
    y3 = t([100, 200, 300], model_set_axis=False)
    assert (np.shape(y3) == (2, 3))
    assert np.all((y3 == [[111, 211, 311], [122, 222, 322]]))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    with:
         ...  =  ... ((np.arange *  ... ))

idx = 67:------------------- similar code ------------------ index = 151, score = 6.0 
def main():
    np.random.seed(314)
    batch_size = 3
    num_hidden = 5
    sequence_length = 4
    model = LinkInFor(num_hidden)
    x = np.random.rand(batch_size, sequence_length, num_hidden).astype(np.float32)
    h = np.random.rand(batch_size, num_hidden).astype(np.float32)
    args = [x, h, np.arange(sequence_length)]
    testtools.generate_testcase(model, args)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = [ ... ,  ... , np.arange]

idx = 68:------------------- similar code ------------------ index = 152, score = 6.0 
def load_cifar100(split, path=None):
    if (path is None):
        cache_path = os.path.join(os.path.expanduser('~'), '.capslayer')
        path = get_file('cifar-100-python', cache_dir=cache_path, file_hash=md5sum, origin=URL, untar=True)
    split = split.lower()
    if (split == 'test'):
        fpath = os.path.join(path, 'test')
        (images, labels) = load_batch(fpath, label_key='fine_labels')
    else:
        fpath = os.path.join(path, 'train')
        (images, labels) = load_batch(fpath, label_key='fine_labels')
        idx = np.arange(len(images))
        np.random.seed(201808)
        np.random.shuffle(idx)
        labels = np.reshape(labels, ((- 1),))
        images = (images[idx[:45000]] if (split == 'train') else images[idx[45000:]])
        labels = (labels[idx[:45000]] if (split == 'train') else labels[idx[45000:]])
    images = np.reshape(images.transpose(0, 2, 3, 1), ((- 1), 3072)).astype(np.float32)
    labels = np.reshape(labels, ((- 1),)).astype(np.int32)
    return zip(images, labels)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:    else:
         ...  = np.arange

idx = 69:------------------- similar code ------------------ index = 156, score = 6.0 
def ordered_indices(self):
    'Return an ordered list of indices. Batches will be constructed based\n        on this order.'
    if self.shuffle:
        order = [np.random.permutation(len(self))]
    else:
        order = [np.arange(len(self))]
    order.append(self.sizes)
    return np.lexsort(order)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:    else:
         ...  = [np.arange]

idx = 70:------------------- similar code ------------------ index = 157, score = 6.0 
def solve_lambda_x(A, y, lambs_exponent=np.arange((- 10), 5, 0.01)):
    '\n    Minimize the Lagrangian L(x,lambda) = || A*x - y||**2 + lambda*||x||**2 based on the idea of Tikhonov Regularization.\n    This program is used to roughly estimate the parameters x and the Lagrange multiplier lambda. \n    The L-curve method is applied to get the proper Lagrangian multiplier.\n\n    Usage:\n    estimate_lamb,estimate_x,log10_residual_norm,log10_solution_norm,curvature = solve_lambda_x(A,y) \n\n    Inputs:\n    A -> [float 2d array] Design matrix\n    y -> [float array] Measurements\n\n    Parameters:\n    lambs_exponent -> [optional, float 3d/4d array, default = np.arange(-10,5,0.01)] Exponent for lambda with base of 10\n    \n    Outputs:\n    estimate_lamb -> [float] Lagrange multiplier\n    estimate_x -> [float array] Estimated parameters\n    log10_residual_norm -> [float array] log10(||A*x-y||) with lambda taking 10**lambs_exponent\n    log10_solution_norm -> [float array] log10(||x||) with lambda taking 10**lambs_exponent\n    curvature -> [float array] curvature of the L-curve, where the ordinate of the curve is log10_solution_norm and the abscissa is log10_residual_norm.\n\n    For more information, please refer to \n    (1) [NumPy/SciPy Recipes for Data Science: Regularized Least Squares Optimization](https://www.researchgate.net/publication/274138835_NumPy_SciPy_Recipes_for_Data_Science_Regularized_Least_Squares_Optimization)\n    (2) [Choosing the Regularization Parameter](http://www2.compute.dtu.dk/~pcha/DIP/chap5.pdf)\n    '
    np.seterr(divide='ignore', invalid='ignore')
    m = A.shape[1]
    (log10_residual_norm, log10_solution_norm) = ([], [])
    lambs = np.float_power(10, lambs_exponent)
    for lamb in lambs:
        x = np.dot(inv((np.dot(A.T, A) + (lamb * np.eye(m)))), np.dot(A.T, y))
        residual_norm = norm((np.dot(A, x) - y))
        solution_norm = norm(x)
        log10_residual_norm.append(np.log10(residual_norm))
        log10_solution_norm.append(np.log10(solution_norm))
    log10_residual_norm = np.array(log10_residual_norm)
    log10_solution_norm = np.array(log10_solution_norm)
    g1 = np.gradient(log10_solution_norm, log10_residual_norm)
    g1[np.isnan(g1)] = (- np.inf)
    g2 = np.gradient(g1, log10_residual_norm)
    g2[np.isnan(g2)] = np.inf
    curvature = (np.abs(g2) / ((1 + (g1 ** 2)) ** 1.5))
    curvature[np.isnan(curvature)] = 0
    curvature[np.isinf(curvature)] = 0
    index_curvature_max = np.argmax(curvature)
    estimate_lamb = lambs[index_curvature_max]
    estimate_x = np.dot(inv((np.dot(A.T, A) + (estimate_lamb * np.eye(m)))), np.dot(A.T, y))
    return (estimate_lamb, estimate_x, log10_residual_norm, log10_solution_norm, curvature)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ,  ... ,  ... =np.arange):
idx = 71:------------------- similar code ------------------ index = 387, score = 6.0 
def test_insert_table_row(self, table_types):
    '\n        Light testing of Table.insert_row() method.  The deep testing is done via\n        the add_row() tests which calls insert_row(index=len(self), ...), so\n        here just test that the added index parameter is handled correctly.\n        '
    self._setup(table_types)
    row = (10, 40.0, 'x', [10, 20])
    for index in range((- 3), 4):
        indices = np.insert(np.arange(3), index, 3)
        t = table_types.Table([self.a, self.b, self.c, self.d])
        t2 = t.copy()
        t.add_row(row)
        t2.insert_row(index, row)
        for name in t.colnames:
            if (t[name].dtype.kind == 'f'):
                assert np.allclose(t[name][indices], t2[name])
            else:
                assert np.all((t[name][indices] == t2[name]))
    for index in ((- 4), 4):
        t = table_types.Table([self.a, self.b, self.c, self.d])
        with pytest.raises(IndexError):
            t.insert_row(index, row)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ...  =  ... . ... (np.arange,  ... ,  ... )

idx = 72:------------------- similar code ------------------ index = 37, score = 6.0 
def convert_files_to_npz(input_folder, out_folder, out_prefix):
    files = glob.glob((input_folder + '*LEFT_RGB*.tif'))
    num = len(files)
    print('Number of images = ', num)
    if (num == 0):
        print('No matching files found', file=stderr)
        return
    train_fraction = TRAIN_FRACTION
    num_train = int((train_fraction * num))
    max_per_train = MAX_IMAGES_PER_TRAIN_FILE
    print('Number of training images = ', num_train)
    print('Number of validation images = ', (num - num_train))
    count = 0
    num_files = 0
    disparities = []
    lefts = []
    rights = []
    left_categories = []
    left_agls = []
    indices = np.arange(num)
    np.random.seed(0)
    np.random.shuffle(indices)
    files = [files[i] for i in indices]
    for i in tqdm(range(num)):
        left_name = os.path.basename(files[i])
        start = left_name.find('LEFT_RGB')
        right_name = ((input_folder + left_name[0:start]) + 'RIGHT_RGB.tif')
        left_agl_name = ((input_folder + left_name[0:start]) + 'LEFT_AGL.tif')
        disparity_name = ((input_folder + left_name[0:start]) + 'LEFT_DSP.tif')
        left_cls_name = ((input_folder + left_name[0:start]) + 'LEFT_CLS.tif')
        left_name = (input_folder + left_name)
        left = np.array(tifffile.imread(left_name))
        right = np.array(tifffile.imread(right_name))
        left_cls = np.array(tifffile.imread(left_cls_name))
        disparity = np.array(tifffile.imread(disparity_name))
        left_agl = np.array(tifffile.imread(left_agl_name))
        left_labels = las_to_sequential_labels(left_cls)
        lefts.append(left)
        rights.append(right)
        disparities.append(disparity)
        left_categories.append(left_labels)
        left_agls.append(left_agl)
        count = (count + 1)
        if (((count >= max_per_train) and (i < num_train)) or (i == (num_train - 1))):
            num_files = (num_files + 1)
            print(' ')
            print('Counts for train file ', num_files)
            cats = np.asarray(left_categories)
            max_category = cats.max()
            for j in range(max_category):
                print(j, ': ', len(cats[(cats == j)]))
            print('Writing files...')
            print(' ')
            out_path = Path(out_folder)
            if (not out_path.exists()):
                out_path.mkdir()
            disparity_name = join(out_path, (((out_prefix + '.train.disparity.') + '{:1d}'.format(num_files)) + '.npz'))
            left_name = join(out_path, (((out_prefix + '.train.left.') + '{:1d}'.format(num_files)) + '.npz'))
            right_name = join(out_path, (((out_prefix + '.train.right.') + '{:1d}'.format(num_files)) + '.npz'))
            left_cat_name = join(out_path, (((out_prefix + '.train.left_label.') + '{:1d}'.format(num_files)) + '.npz'))
            left_agl_name = join(out_path, (((out_prefix + '.train.left_agl.') + '{:1d}'.format(num_files)) + '.npz'))
            np.savez_compressed(disparity_name, disparities)
            np.savez_compressed(left_name, lefts)
            np.savez_compressed(right_name, rights)
            np.savez_compressed(left_cat_name, left_categories)
            np.savez_compressed(left_agl_name, left_agls)
            count = 0
            disparities = []
            lefts = []
            rights = []
            left_categories = []
            left_agls = []
    print(' ')
    print('Counts for validation file')
    cats = np.asarray(left_categories)
    max_category = cats.max()
    for j in range(max_category):
        print(j, ': ', len(cats[(cats == j)]))
    print('Writing files...')
    print(' ')
    out_path = Path(out_folder)
    if (not out_path.exists()):
        out_path.mkdir()
    print('Writing validation files')
    print('Number of validation samples = ', len(disparities))
    disparity_name = join(out_path, (out_prefix + '.test.disparity.npz'))
    left_name = join(out_path, (out_prefix + '.test.left.npz'))
    right_name = join(out_path, (out_prefix + '.test.right.npz'))
    left_cat_name = join(out_path, (out_prefix + '.test.left_label.npz'))
    left_agl_name = join(out_path, (out_prefix + '.test.left_agl.npz'))
    np.savez_compressed(disparity_name, disparities)
    np.savez_compressed(left_name, lefts)
    np.savez_compressed(right_name, rights)
    np.savez_compressed(left_cat_name, left_categories)
    np.savez_compressed(left_agl_name, left_agls)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 73:------------------- similar code ------------------ index = 445, score = 6.0 
def get_sampler(labels, n=None, n_valid=None):
    (indices,) = np.where(reduce(__or__, [(labels == i) for i in np.arange(n_labels)]))
    np.random.shuffle(indices)
    indices_valid = np.hstack([list(filter((lambda idx: (labels[idx] == i)), indices))[:n_valid] for i in range(n_labels)])
    indices_train = np.hstack([list(filter((lambda idx: (labels[idx] == i)), indices))[n_valid:(n_valid + n)] for i in range(n_labels)])
    indices_unlabelled = np.hstack([list(filter((lambda idx: (labels[idx] == i)), indices))[:] for i in range(n_labels)])
    indices_train = torch.from_numpy(indices_train)
    indices_valid = torch.from_numpy(indices_valid)
    indices_unlabelled = torch.from_numpy(indices_unlabelled)
    sampler_train = SubsetRandomSampler(indices_train)
    sampler_valid = SubsetRandomSampler(indices_valid)
    sampler_unlabelled = SubsetRandomSampler(indices_unlabelled)
    return (sampler_train, sampler_valid, sampler_unlabelled)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... ( ... ( ... , [ for  ...  in np.arange]))

idx = 74:------------------- similar code ------------------ index = 187, score = 6.0 
def test_gridded_filters():
    allfilters = np.array(_get_all_filters())
    w = np.array([f.wave_effective for f in allfilters])
    fnames = np.array([f.name for f in allfilters])
    good = (w < 100000.0)
    obs = {}
    obs['filters'] = allfilters[good][0:40]
    spec = np.random.uniform(0, 1.0, 5996)
    wave = np.exp(np.linspace(np.log(90), np.log(1000000.0), len(spec)))
    m_default = observate.getSED(wave, spec, obs['filters'])
    (wlo, whi, dlo) = ([], [], [])
    for f in obs['filters']:
        dlnlam = (np.gradient(f.wavelength) / f.wavelength)
        wlo.append(f.wavelength.min())
        dlo.append(dlnlam.min())
        whi.append(f.wavelength.max())
    wmin = np.min(wlo)
    wmax = np.max(whi)
    dlnlam = np.min(dlo)
    obs['filters'] = observate.load_filters(fnames[good][0:40], dlnlam=dlnlam, wmin=wmin)
    lnlam = np.exp(np.arange(np.log(wmin), np.log(wmax), dlnlam))
    lnspec = np.interp(lnlam, wave, spec)
    m_grid = observate.getSED(lnlam, lnspec, obs['filters'], gridded=True)
    assert np.allclose(m_grid, m_default, atol=0.05)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... (np.arange)

idx = 75:------------------- similar code ------------------ index = 188, score = 6.0 
if (__name__ == '__main__'):
    import numpy as np
    np.random.seed(314)
    batch_size = 3
    sequence_length = 4
    num_vocabs = 10
    num_hidden = 5
    model_fn = (lambda : MyLSTM(num_hidden, batch_size, sequence_length))
    (labels, lengths) = sequence_utils.gen_random_sequence(batch_size, sequence_length, num_vocabs)
    xs = []
    for l in lengths:
        xs.append(np.random.rand(l, num_hidden).astype(dtype=np.float32))
    h = np.zeros((batch_size, num_hidden), dtype=np.float32)
    c = np.zeros((batch_size, num_hidden), dtype=np.float32)
    mask = (np.expand_dims(np.arange(sequence_length), 0) < np.expand_dims(lengths, 1)).astype(np.float32)
    args = [xs, h, c, mask]
    ch2o.generate_testcase(model_fn, args)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
     ...  = ( ... . ... (np.arange,  ... ) <)

idx = 76:------------------- similar code ------------------ index = 418, score = 6.0 
def test_1d_mean(self):
    'Test 1D array with func=np.mean.'
    data = np.arange(4)
    block_size = 2.0
    expected = (block_reduce(data, block_size, func=np.sum) / block_size)
    result_mean = block_reduce(data, block_size, func=np.mean)
    assert np.all((result_mean == expected))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 77:------------------- similar code ------------------ index = 444, score = 6.0 
def test_float(self):
    'test floating point quantization'
    formats = [(2, 2), (2, 3), (3, 2)]
    for (exp, man) in formats:
        for d in ['cpu', 'cuda']:
            for r in ['stochastic', 'nearest']:
                a_max = ((2 ** (2 ** (exp - 1))) * (1 - (2 ** ((- man) - 1))))
                a_min = (2 ** ((- (2 ** (exp - 1))) + 1))
                max_exp = int(((2 ** exp) / 2))
                min_exp = (- (max_exp - 2))
                mantissa_step = (2 ** (- man))
                min_mantissa = mantissa_step
                max_mantissa = (2 - mantissa_step)
                a_min = ((2 ** min_exp) * min_mantissa)
                a_max = ((2 ** max_exp) * max_mantissa)
                expected_vals = []
                log(f'With {exp} exponent bits, our exponent goes from {min_exp} to {max_exp}')
                log(f'With {man} mantissa bits, our mantissa goes from {min_mantissa} (denormalized) to {max_mantissa}')
                log(f'With {man} mantissa bits and {exp} exponent bits, we can go from {a_min} to {a_max}')
                representable_normalized = []
                for sign in [1, (- 1)]:
                    for e in range(0, (2 ** exp)):
                        for m in range(0, (2 ** man)):
                            if (e == 0):
                                val = (sign * (((2 ** (e + min_exp)) * m) * (2 ** (- man))))
                                log(f'{(0 if (sign == 1) else 1)} {e:0{exp}b} {m:0{man}b} = {sign} * 2^{(e + min_exp)} * {(m * (2 ** (- man)))} 	= {val} (denormalized)')
                            else:
                                val = (sign * ((2 ** ((e + min_exp) - 1)) * (1 + (m * (2 ** (- man))))))
                                log(f'{(0 if (sign == 1) else 1)} {e:0{exp}b} {m:0{man}b} = {sign} * 2^{((e + min_exp) - 1)} * {(1 + (m * (2 ** (- man))))} 	= {val}')
                            if (val not in expected_vals):
                                expected_vals.append(val)
                expected_vals.sort()
                import numpy as np
                quant_vals = []
                for i in np.arange((- 30), 30, 0.01):
                    a = torch.Tensor([i]).to(device=d)
                    quant_a = float_quantize(a, exp=exp, man=man, rounding=r)
                    if (quant_a[0] not in quant_vals):
                        quant_vals.append(quant_a[0].item())
                log('Values representable in QPytorch')
                log(quant_vals)
                self.assertEqual(quant_vals, expected_vals)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for in  ... :
        for  ...  in:
            for  ...  in:
                for  ...  in np.arange:
idx = 78:------------------- similar code ------------------ index = 189, score = 6.0 
def gen_new_list(self):
    np.random.seed(0)
    all_size = (self.total_size * self.world_size)
    indices = np.arange(len(self.dataset))
    np.random.shuffle(indices)
    indices = indices[:all_size]
    num_repeat = (((all_size - 1) // indices.shape[0]) + 1)
    indices = np.tile(indices, num_repeat)
    indices = indices[:all_size]
    np.random.shuffle(indices)
    beg = (self.total_size * self.rank)
    indices = indices[beg:(beg + self.total_size)]
    assert (len(indices) == self.total_size)
    return indices

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 79:------------------- similar code ------------------ index = 191, score = 6.0 
def ordered_indices(self):
    '\n        Return an ordered list of indices. Batches will be constructed based\n        on this order.\n        '
    if self.shuffle:
        return np.random.permutation(len(self))
    else:
        order = [np.arange(len(self))]
        order.append(self.sizes)
        return np.lexsort(order)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:    else:
         ...  = [np.arange]

idx = 80:------------------- similar code ------------------ index = 224, score = 6.0 
def run(self, total_steps):
    ' Runs PPO\n\n        Args:\n            total_steps (int): total number of environment steps to run for\n        '
    N = self.num_workers
    T = self.worker_steps
    E = self.opt_epochs
    A = self.venv.action_space.n
    while (self.taken_steps < total_steps):
        progress = (self.taken_steps / total_steps)
        (obs, rewards, masks, actions, steps) = self.interact()
        ob_shape = obs.size()[2:]
        ep_reward = self.test()
        self.reward_histr.append(ep_reward)
        self.steps_histr.append(self.taken_steps)
        group_size = (len(self.steps_histr) // self.plot_points)
        if (self.plot_reward and ((len(self.steps_histr) % (self.plot_points * 10)) == 0) and (group_size >= 10)):
            (x_means, _, y_means, y_stds) = mean_std_groups(np.array(self.steps_histr), np.array(self.reward_histr), group_size)
            fig = plt.figure()
            fig.set_size_inches(8, 6)
            plt.ticklabel_format(axis='x', style='sci', scilimits=((- 2), 6))
            plt.errorbar(x_means, y_means, yerr=y_stds, ecolor='xkcd:blue', fmt='xkcd:black', capsize=5, elinewidth=1.5, mew=1.5, linewidth=1.5)
            plt.title('Training progress')
            plt.xlabel('Total steps')
            plt.ylabel('Episode reward')
            plt.savefig(self.plot_path, dpi=200)
            plt.clf()
            plt.close()
            plot_timer = 0
        obs_ = obs.view(((((T + 1) * N),) + ob_shape))
        obs_ = Variable(obs_)
        (_, values) = self.policy(obs_)
        values = values.view((T + 1), N, 1)
        (advantages, returns) = gae(rewards, masks, values, self.gamma, self.lambd)
        self.policy_old.load_state_dict(self.policy.state_dict())
        for e in range(E):
            self.policy.zero_grad()
            MB = (steps // self.minibatch_steps)
            b_obs = Variable(obs[:T].view(((steps,) + ob_shape)))
            b_rewards = Variable(rewards.view(steps, 1))
            b_masks = Variable(masks.view(steps, 1))
            b_actions = Variable(actions.view(steps, 1))
            b_advantages = Variable(advantages.view(steps, 1))
            b_returns = Variable(returns.view(steps, 1))
            b_inds = np.arange(steps)
            np.random.shuffle(b_inds)
            for start in range(0, steps, self.minibatch_steps):
                mb_inds = b_inds[start:(start + self.minibatch_steps)]
                mb_inds = cuda_if(torch.from_numpy(mb_inds).long(), self.cuda)
                (mb_obs, mb_rewards, mb_masks, mb_actions, mb_advantages, mb_returns) = [arr[mb_inds] for arr in [b_obs, b_rewards, b_masks, b_actions, b_advantages, b_returns]]
                (mb_pis, mb_vs) = self.policy(mb_obs)
                (mb_pi_olds, mb_v_olds) = self.policy_old(mb_obs)
                (mb_pi_olds, mb_v_olds) = (mb_pi_olds.detach(), mb_v_olds.detach())
                losses = self.objective(self.clip_func(progress), mb_pis, mb_vs, mb_pi_olds, mb_v_olds, mb_actions, mb_advantages, mb_returns)
                (policy_loss, value_loss, entropy_loss) = losses
                loss = ((policy_loss + (value_loss * self.value_coef)) + (entropy_loss * self.entropy_coef))
                set_lr(self.optimizer, self.lr_func(progress))
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm(self.policy.parameters(), self.max_grad_norm)
                self.optimizer.step()
        self.taken_steps += steps
        print(self.taken_steps)
        torch.save({'policy': self.policy.state_dict()}, (('./save/PPO_' + self.env_name) + '.pt'))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    while:
        for  ...  in:
             ...  = np.arange

idx = 81:------------------- similar code ------------------ index = 9, score = 6.0 
def _add_strategy_to_average(self, p_id):

    def _fill(_node):
        if (_node.p_id_acting_next == p_id):
            if (self._iter_counter > self.delay):
                current_weight = np.sum(np.arange((self.delay + 1), (self._iter_counter + 1)))
                new_weight = ((self._iter_counter - self.delay) + 1)
                m_old = (current_weight / (current_weight + new_weight))
                m_new = (new_weight / (current_weight + new_weight))
                _node.data['avg_strat'] = ((m_old * _node.data['avg_strat']) + (m_new * _node.strategy))
                assert np.allclose(np.sum(_node.data['avg_strat'], axis=1), 1, atol=0.0001)
            elif (self._iter_counter == self.delay):
                _node.data['avg_strat'] = np.copy(_node.strategy)
                assert np.allclose(np.sum(_node.data['avg_strat'], axis=1), 1, atol=0.0001)
        for c in _node.children:
            _fill(c)
    for t_idx in range(len(self._trees)):
        _fill(self._trees[t_idx].root)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():

    def  ... ( ... ):
        if:
            if:
                 ...  =  ... . ... (np.arange)

idx = 82:------------------- similar code ------------------ index = 390, score = 6.0 
def train_model(self):
    checkdir = 'checkpoint'
    try:
        os.mkdir(checkdir)
    except FileExistsError:
        print('Folder exists: ', checkdir)
    lr = (self.args.lr + self.args.decay)
    for i in range(1, (self.args.n_epochs + 1)):
        lr = (lr - self.args.decay)
        indexes = np.arange(1, (self.args.n_trains + 1))
        np.random.shuffle(indexes)
        is_compiled = False
        for j in indexes:
            self.load_train_data(j)
            filename = ('us3d.icnet.weights.%d-%d.h5' % (i, j))
            filepath = os.path.join(checkdir, filename)
            checkpoint = ModelCheckpoint(filepath=filepath, save_weights_only=True, verbose=1, save_best_only=False)
            predict_callback = LambdaCallback(on_epoch_end=(lambda epoch, logs: self.compute_accuracy()))
            callbacks = [checkpoint, predict_callback]
            height = self.train_images[0].shape[0]
            width = self.train_images[0].shape[1]
            bands = self.train_images[0].shape[2]
            myloss = tversky_loss
            if (self.model is None):
                self.model = build_icnet(height, width, bands, self.n_classes, weights_path=self.args.checkpoint, train=True)
            if (not is_compiled):
                self.model.compile(optimizer=Adam(lr=lr), loss=myloss, loss_weights=[1.0, 0.4, 0.16])
                is_compiled = True
            self.model.fit(self.train_images, [self.Y1, self.Y2, self.Y3], epochs=1, batch_size=self.args.batch_size, shuffle=True, callbacks=callbacks)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
         ...  = np.arange

idx = 83:------------------- similar code ------------------ index = 426, score = 6.0 
def test_mul_div(self):
    for dt in (self.dt, self.dt_array):
        dt2 = ((dt + dt) + dt)
        dt3 = (3.0 * dt)
        assert allclose_jd(dt2.jd, dt3.jd)
        dt4 = (dt3 / 3.0)
        assert allclose_jd(dt4.jd, dt.jd)
    dt5 = (self.dt * np.arange(3))
    assert (dt5[0].jd == 0.0)
    assert (dt5[(- 1)].jd == (self.dt + self.dt).jd)
    dt6 = (self.dt * [0, 1, 2])
    assert np.all((dt6.jd == dt5.jd))
    with pytest.raises(OperandTypeError):
        (self.dt * self.t)
    with pytest.raises(TypeError):
        (self.dt * object())

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = ( * np.arange)

idx = 84:------------------- similar code ------------------ index = 23, score = 6.0 
def gen_rnn_sentiment_test(cell_type, num_vocabs=10, num_hidden=5, batch_size=3, sequence_length=4, output_loss_only=False, param_initializer=np.random.random):

    def fn(test_name):
        gb = onnx_script.GraphBuilder(test_name)
        if (cell_type == 'LSTM'):
            wr = 8
            perm = [0, 2, 1, 3, 4, 6, 5, 7]
            num_direction = 1
            direction = 'forward'
        elif (cell_type == 'BiLSTM'):
            wr = 16
            perm = (np.tile([0, 2, 1, 3], 4) + (np.repeat(np.arange(4), 4) * 4))
            num_direction = 2
            direction = 'bidirectional'
        elif (cell_type == 'GRU'):
            wr = 6
            perm = [1, 0, 2, 4, 3, 5]
            num_direction = 1
            direction = 'forward'
        elif (cell_type == 'BiGRU'):
            wr = 12
            perm = [1, 0, 2, 4, 3, 5, 7, 6, 8, 10, 9, 11]
            num_direction = 2
            direction = 'bidirectional'
        else:
            raise RuntimeError(('Unknown cell_type: %s' % cell_type))
        embed_size = num_hidden
        np.random.seed(42)
        if ((batch_size == 3) and (sequence_length == 4)):
            labels = np.array([[1, 2, 3, 7], [4, 5, 0, 0], [6, 0, 0, 0]])
            lengths = np.array([4, 2, 1])
            targets = np.array([1, 0, 1])
        else:
            (labels, lengths) = _gen_random_sequence(batch_size, sequence_length, num_vocabs)
            targets = np.random.randint(2, size=batch_size)
        labels = labels.astype(np.int32)
        embed = param_initializer(size=(num_vocabs, embed_size)).astype(np.float32)
        weight = param_initializer(size=(embed_size, (num_hidden * wr))).astype(np.float32)
        bias = param_initializer(size=((num_hidden * wr),)).astype(np.float32)
        linear_w = param_initializer(size=((num_direction * num_hidden), 2)).astype(np.float32)
        linear_b = param_initializer(size=(2,)).astype(np.float32)
        x = F.embed_id(labels, embed)
        state = np.zeros((num_direction, len(labels), num_hidden)).astype(np.float32)
        xs = F.transpose_sequence([v[:l] for (v, l) in zip(x, lengths)])
        ch_weight = np.split(weight, wr, axis=1)
        ch_weight = [ch_weight[i] for i in perm]
        ch_bias = np.split(bias, wr, axis=0)
        ch_bias = [ch_bias[i] for i in perm]
        if (cell_type == 'LSTM'):
            (h, _, rnn_outputs) = F.n_step_lstm(1, 0.0, state, state, [ch_weight], [ch_bias], xs)
        elif (cell_type == 'BiLSTM'):
            (h, _, rnn_outputs) = F.n_step_bilstm(1, 0.0, state, state, [ch_weight[:8], ch_weight[8:]], [ch_bias[:8], ch_bias[8:]], xs)
        elif (cell_type == 'GRU'):
            (h, rnn_outputs) = F.n_step_gru(1, 0.0, state, [ch_weight], [ch_bias], xs)
        elif (cell_type == 'BiGRU'):
            (h, rnn_outputs) = F.n_step_bigru(1, 0.0, state, [ch_weight[:6], ch_weight[6:]], [ch_bias[:6], ch_bias[6:]], xs)
        shape = (len(labels), (num_hidden * num_direction))
        h = F.reshape(h, shape)
        rnn_outputs = F.pad_sequence(rnn_outputs)
        rnn_outputs = F.reshape(rnn_outputs, ((- 1), len(labels), num_direction, num_hidden))
        rnn_outputs = F.transpose(rnn_outputs, axes=[0, 2, 1, 3])
        result = F.linear(h, np.transpose(linear_w), linear_b)
        loss = F.softmax_cross_entropy(result, targets)
        (weight_w, weight_r) = np.split(weight, 2, axis=1)
        labels_v = gb.input('labels', labels)
        lengths_v = gb.input('lengths', lengths)
        targets_v = gb.input('targets', targets)
        embed_v = gb.param('embed', embed)
        weight_w_v = gb.param('weight_w', np.reshape(np.transpose(weight_w), (num_direction, (- 1), embed_size)))
        weight_r_v = gb.param('weight_r', np.reshape(np.transpose(weight_r), (num_direction, (- 1), num_hidden)))
        bias_v = gb.param('bias', np.reshape(bias, (num_direction, (- 1))))
        linear_w_v = gb.param('linear_w', linear_w)
        linear_b_v = gb.param('linear_b', linear_b)
        x = gb.Gather([embed_v, labels_v])
        x = gb.Transpose([x], perm=[1, 0, 2])
        if (cell_type in ['LSTM', 'BiLSTM']):
            (rnn_outputs_v, h) = gb.LSTM([x, weight_w_v, weight_r_v, bias_v, lengths_v], outputs=['rnn_outputs', 'last_state'], hidden_size=num_hidden, direction=direction)
        elif (cell_type in ['GRU', 'BiGRU']):
            (rnn_outputs_v, h) = gb.GRU([x, weight_w_v, weight_r_v, bias_v, lengths_v], outputs=['rnn_outputs', 'last_state'], hidden_size=num_hidden, direction=direction)
        shape_v = gb.const(shape)
        h = gb.Reshape([h, shape_v])
        result_v = gb.Gemm([h, linear_w_v, linear_b_v])
        loss_v = gb.ChainerSoftmaxCrossEntropy([result_v, targets_v])
        if (not output_loss_only):
            gb.output(rnn_outputs_v, rnn_outputs.array)
            gb.output(result_v, result.array)
        gb.output(loss_v, loss.array)
        gb.gen_test()
    return fn

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():

    def  ... ( ... ):
        if:        elif:
             ...  = ( + ( ... . ... (np.arange,  ... ) *  ... ))

idx = 85:------------------- similar code ------------------ index = 221, score = 6.0 
def _payout_pots(self):
    self._assign_hand_ranks_to_all_players()
    if (self.N_SEATS == 2):
        if (self.seats[0].hand_rank > self.seats[1].hand_rank):
            self.seats[0].award(self.main_pot)
        elif (self.seats[0].hand_rank < self.seats[1].hand_rank):
            self.seats[1].award(self.main_pot)
        else:
            self.seats[0].award((self.main_pot / 2))
            self.seats[1].award((self.main_pot / 2))
        self.main_pot = 0
    else:
        pots = np.array(([self.main_pot] + self.side_pots))
        pot_ranks = np.arange(start=(- 1), stop=len(self.side_pots))
        pot_and_pot_ranks = np.array((pots, pot_ranks)).T
        for e in pot_and_pot_ranks:
            pot = e[0]
            rank = e[1]
            eligible_players = [p for p in self.seats if ((p.side_pot_rank >= rank) and (not p.folded_this_episode))]
            num_eligible = len(eligible_players)
            if (num_eligible > 0):
                winner_list = self._get_winner_list(players_to_consider=eligible_players)
                num_winners = int(len(winner_list))
                chips_per_winner = int((pot / num_winners))
                num_non_div_chips = (int(pot) % num_winners)
                for p in winner_list:
                    p.award(chips_per_winner)
                shuffled_winner_idxs = np.arange(num_winners)
                np.random.shuffle(shuffled_winner_idxs)
                for p_idx in shuffled_winner_idxs[:num_non_div_chips]:
                    self.seats[p_idx].award(1)
        self.side_pots = ([0] * self.N_SEATS)
        self.main_pot = 0

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:    else:
         ...  = np.arange

idx = 86:------------------- similar code ------------------ index = 220, score = 6.0 
def GroundTruthVisualise(data, dataset, original=True):
    from matplotlib.pyplot import imshow, show, colorbar, set_cmap, clim
    import matplotlib.pyplot as plt
    import numpy as np
    labels = []
    if (dataset == 'Indian_pines'):
        if original:
            labels = ['Unlabelled', 'Corn-notil', 'Corn-mintill', 'Corn', 'Grass-pasture', 'Grass-trees', 'Hay-windrowed', 'Soybean-notil', 'Soybean-mintil', 'Soybean-clean', 'Woods', 'BGTD']
        else:
            labels = []
    elif (dataset == 'Salinas'):
        labels = ['Unlabelled', 'Brocoli green weeds 1', 'Brocoli green weeds 2', 'Fallow', 'Fallow rough plow', 'Fallow smooth', 'Stubble', 'Celery', 'Grapes untrained', 'Soil vinyard develop', 'Corn senesced green weeds', 'Lettuce romaine 4wk', 'Lettuce romaine 5wk', 'Lettuce romaine 6wk', 'Lettuce romaine 7wk', 'Vinyard untrained', 'Vunyard vertical trellis']
    elif (dataset == 'KSC'):
        labels = ['Unlabelled', 'Scrub', 'Williw swamp', 'SP hammock', 'Slash pine', 'Oak/Broadleaf', 'Hardwood', 'Swamp', 'Gramminoid marsh', 'Spartina marsh', 'Cattail marsh', 'Salt marsh', 'Mud flats', 'Water']

    def discrete_matshow(data):
        data = data.astype(np.int64)
        cmap = plt.get_cmap('tab20', ((np.max(data) - np.min(data)) + 1))
        mat = plt.matshow(data, cmap=cmap, vmin=(np.min(data) - 0.5), vmax=(np.max(data) + 0.5))
        cax = plt.colorbar(mat, ticks=np.arange(np.min(data), (np.max(data) + 1)))
        cax.ax.set_yticklabels(labels)
    imshow(data)
    discrete_matshow(data)
    show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    def  ... ( ... ):
         ...  =  ... . ... ( ... ,  ... =np.arange)

idx = 87:------------------- similar code ------------------ index = 20, score = 6.0 
def load_data_subset(data_aug, batch_size, workers, dataset, data_target_dir, labels_per_class=100, valid_labels_per_class=500):
    import numpy as np
    from functools import reduce
    from operator import __or__
    from torch.utils.data.sampler import SubsetRandomSampler
    if (dataset == 'cifar10'):
        mean = [(x / 255) for x in [125.3, 123.0, 113.9]]
        std = [(x / 255) for x in [63.0, 62.1, 66.7]]
    elif (dataset == 'cifar100'):
        mean = [(x / 255) for x in [129.3, 124.1, 112.4]]
        std = [(x / 255) for x in [68.2, 65.4, 70.4]]
    elif (dataset == 'svhn'):
        mean = [(x / 255) for x in [127.5, 127.5, 127.5]]
        std = [(x / 255) for x in [127.5, 127.5, 127.5]]
    elif (dataset == 'tiny-imagenet-200'):
        mean = [(x / 255) for x in [127.5, 127.5, 127.5]]
        std = [(x / 255) for x in [127.5, 127.5, 127.5]]
    elif (dataset == 'mnist'):
        pass
    else:
        assert False, 'Unknow dataset : {}'.format(dataset)
    if (data_aug == 1):
        print('data aug')
        if (dataset == 'svhn'):
            train_transform = transforms.Compose([transforms.RandomCrop(32, padding=2), transforms.ToTensor(), transforms.Normalize(mean, std)])
            test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])
        elif (dataset == 'mnist'):
            hw_size = 24
            train_transform = transforms.Compose([transforms.RandomCrop(hw_size), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
            test_transform = transforms.Compose([transforms.CenterCrop(hw_size), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        elif (dataset == 'tiny-imagenet-200'):
            train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(64, padding=4), transforms.ToTensor(), transforms.Normalize(mean, std)])
            test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])
        else:
            train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=2), transforms.ToTensor(), transforms.Normalize(mean, std)])
            test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])
    else:
        print('no data aug')
        if (dataset == 'mnist'):
            hw_size = 28
            train_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
            test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        else:
            train_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])
            test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])
    if (dataset == 'cifar10'):
        train_data = datasets.CIFAR10(data_target_dir, train=True, transform=train_transform, download=True)
        test_data = datasets.CIFAR10(data_target_dir, train=False, transform=test_transform, download=True)
        num_classes = 10
    elif (dataset == 'cifar100'):
        train_data = datasets.CIFAR100(data_target_dir, train=True, transform=train_transform, download=True)
        test_data = datasets.CIFAR100(data_target_dir, train=False, transform=test_transform, download=True)
        num_classes = 100
    elif (dataset == 'svhn'):
        train_data = datasets.SVHN(data_target_dir, split='train', transform=train_transform, download=True)
        test_data = datasets.SVHN(data_target_dir, split='test', transform=test_transform, download=True)
        num_classes = 10
    elif (dataset == 'mnist'):
        train_data = datasets.MNIST(data_target_dir, train=True, transform=train_transform, download=True)
        test_data = datasets.MNIST(data_target_dir, train=False, transform=test_transform, download=True)
        num_classes = 10
    elif (dataset == 'stl10'):
        train_data = datasets.STL10(data_target_dir, split='train', transform=train_transform, download=True)
        test_data = datasets.STL10(data_target_dir, split='test', transform=test_transform, download=True)
        num_classes = 10
    elif (dataset == 'tiny-imagenet-200'):
        train_root = os.path.join(data_target_dir, 'train')
        validation_root = os.path.join(data_target_dir, 'val/images')
        train_data = datasets.ImageFolder(train_root, transform=train_transform)
        test_data = datasets.ImageFolder(validation_root, transform=test_transform)
        num_classes = 200
    elif (dataset == 'imagenet'):
        assert False, 'Do not finish imagenet code'
    else:
        assert False, 'Do not support dataset : {}'.format(dataset)
    n_labels = num_classes

    def get_sampler(labels, n=None, n_valid=None):
        (indices,) = np.where(reduce(__or__, [(labels == i) for i in np.arange(n_labels)]))
        np.random.shuffle(indices)
        indices_valid = np.hstack([list(filter((lambda idx: (labels[idx] == i)), indices))[:n_valid] for i in range(n_labels)])
        indices_train = np.hstack([list(filter((lambda idx: (labels[idx] == i)), indices))[n_valid:(n_valid + n)] for i in range(n_labels)])
        indices_unlabelled = np.hstack([list(filter((lambda idx: (labels[idx] == i)), indices))[:] for i in range(n_labels)])
        indices_train = torch.from_numpy(indices_train)
        indices_valid = torch.from_numpy(indices_valid)
        indices_unlabelled = torch.from_numpy(indices_unlabelled)
        sampler_train = SubsetRandomSampler(indices_train)
        sampler_valid = SubsetRandomSampler(indices_valid)
        sampler_unlabelled = SubsetRandomSampler(indices_unlabelled)
        return (sampler_train, sampler_valid, sampler_unlabelled)
    if (dataset == 'svhn'):
        (train_sampler, valid_sampler, unlabelled_sampler) = get_sampler(train_data.labels, labels_per_class, valid_labels_per_class)
    elif (dataset == 'mnist'):
        (train_sampler, valid_sampler, unlabelled_sampler) = get_sampler(train_data.targets.numpy(), labels_per_class, valid_labels_per_class)
    elif (dataset == 'tiny-imagenet-200'):
        pass
    else:
        (train_sampler, valid_sampler, unlabelled_sampler) = get_sampler(train_data.targets, labels_per_class, valid_labels_per_class)
    if (dataset == 'tiny-imagenet-200'):
        labelled = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)
        validation = None
        unlabelled = None
        test = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True)
    else:
        labelled = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, shuffle=False, num_workers=workers, pin_memory=True)
        validation = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, shuffle=False, num_workers=workers, pin_memory=True)
        unlabelled = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=unlabelled_sampler, shuffle=False, num_workers=workers, pin_memory=True)
        test = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True)
    return (labelled, validation, unlabelled, test, num_classes)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    def  ... ():
 =  ... . ... ( ... ( ... , [ for  ...  in np.arange]))

idx = 88:------------------- similar code ------------------ index = 19, score = 6.0 
def test_scalar_parameters_1d_array_input(self):
    '\n        Scalar parameters should broadcast with an array input to result in an\n        array output of the same shape as the input.\n        '
    t = TModel_1_2(1, 10, 1000)
    (y, z) = t((np.arange(5) * 100))
    assert isinstance(y, np.ndarray)
    assert isinstance(z, np.ndarray)
    assert (np.shape(y) == np.shape(z) == (5,))
    assert np.all((y == [11, 111, 211, 311, 411]))
    assert np.all((z == (y + 1000)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 =  ... ((np.arange *  ... ))

idx = 89:------------------- similar code ------------------ index = 17, score = 6.0 
def optimization_function(network, train_samples, test_samples, val_samples, current_space, year, mode, metric):
    "\n    Train the model 'folds' times with the specific parameters (current space) that are chosen by hyper_opt algorithm\n    and given the performance of the model on the test and validation data, writes on the log file the best epoch,\n    the performance of each epoch ('+', '-' increasing-decreasing) with respect to validation loss and\n    the results (correlations) of the model on the validation and test data.\n    :param network: The compiled network ready to be trained.\n    :param train_samples: A dict the will be fed on the network at the training process.\n    :param test_samples: A dict the will be fed on the network at the testing process.\n    :param val_samples: A dict the will be fed on the network at the validation process.\n    :param current_space: A dict with the specific parameters that will be used at the training of the model.\n    :param year: A year that we are testing.\n    :param mode: Depending on your choice : ['Single Task', 'Multi Task-1', 'Multi Task-5'].\n    :param metric: The metric for which the model will be trained. It is needed only on 'Single Task' mode.\n    :return:\n    "
    trial_start = time.time()
    LOGGER.info(((((('\n' + ('=' * 115)) + '\n') + MSG_TEMPLATE.format(TRIAL_NO, HYPER_OPT_CONFIG['trials'], str(current_space['n_hidden_layers']), str(current_space['hidden_units_size']), current_space['batch_size'], current_space['dropout_rate'], current_space['word_dropout_rate'], current_space['attention_mechanism'], current_space['learning_rate'], year, metric, mode)) + '\n') + ('=' * 115)))
    statistics = {method: {} for method in ['validation', 'test']}
    fold_loss = []
    for fold_no in range(HYPER_OPT_CONFIG['folds']):
        LOGGER.info('\n----- Fold: {0}/{1} -----\n'.format((fold_no + 1), HYPER_OPT_CONFIG['folds']))
        indices = np.arange(len(list(train_samples['x'])))
        if (HYPER_OPT_CONFIG['folds'] != 1):
            np.random.seed(fold_no)
            np.random.shuffle(indices)
        early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)
        with tempfile.NamedTemporaryFile(delete=True) as w_fd:
            weights_file = w_fd.name
            model_checkpoint = ModelCheckpoint(filepath=weights_file, monitor='val_loss', mode='auto', verbose=1, save_best_only=True, save_weights_only=True)
            fit_history = network.fit(x=train_samples['x'], y=train_samples['y'], epochs=HYPER_OPT_CONFIG['epochs'], validation_data=(val_samples['x'], val_samples['y']), callbacks=[early_stopping, model_checkpoint], verbose=2)
        best_epoch = (np.argmin(fit_history.history['val_loss']) + 1)
        n_epochs = len(fit_history.history['val_loss'])
        val_loss_per_epoch = ('- ' + ' '.join((('-' if (fit_history.history['val_loss'][i] < np.min(fit_history.history['val_loss'][:i])) else '+') for i in range(1, len(fit_history.history['val_loss'])))))
        LOGGER.info('\nBest epoch: {}/{}'.format(best_epoch, n_epochs))
        LOGGER.info('Val loss per epoch: {}\n'.format(val_loss_per_epoch))
        LOGGER.info('\n----- Validation Results -----')
        val_report_statistics = calculate_performance(network=network, true_samples=val_samples['x'], true_targets=val_samples['y'], ordered_ids=val_samples['ordered_ids'], empty_ids=[], mode=mode, human_metric=metric)
        if ((mode == 'Multi Task-1') or (mode == 'Multi Task-5')):
            for q in ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']:
                statistics['validation'][q] = val_report_statistics[q]
        else:
            statistics['validation'][metric] = val_report_statistics[metric]
        LOGGER.info('\n----- Test Results ------------')
        test_report_statistics = calculate_performance(network=network, true_samples=test_samples['x'], true_targets=test_samples['y'], ordered_ids=test_samples['ordered_ids'], empty_ids=test_samples['empty_ids'], mode=mode, human_metric=metric)
        if ((mode == 'Multi Task-1') or (mode == 'Multi Task-5')):
            for q in ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']:
                statistics['test'][q] = test_report_statistics[q]
        else:
            statistics['test'][metric] = test_report_statistics[metric]
        fold_loss.append((1 - val_report_statistics[metric]['Spearman']))
    LOGGER.info('Trial training took {0} sec\n'.format(time.strftime('%H:%M:%S', time.gmtime((time.time() - trial_start)))))
    current_space['trial_no'] = TRIAL_NO
    return {'loss': np.average(fold_loss), 'status': STATUS_OK, 'trial_no': TRIAL_NO, 'results': {'configuration': current_space, 'time': (time.time() - trial_start), 'statistics': statistics}}

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ...  = np.arange

idx = 90:------------------- similar code ------------------ index = 15, score = 6.0 
@pytest.mark.skipif('not HAS_SCIPY')
def test_joint_fitter(self):
    g1 = models.Gaussian1D(10, 14.9, stddev=0.3)
    g2 = models.Gaussian1D(10, 13, stddev=0.4)
    jf = fitting.JointFitter([g1, g2], {g1: ['amplitude'], g2: ['amplitude']}, [9.8])
    x = np.arange(10, 20, 0.1)
    y1 = g1(x)
    y2 = g2(x)
    n = np.random.randn(100)
    ny1 = (y1 + (2 * n))
    ny2 = (y2 + (2 * n))
    jf(x, ny1, x, ny2)
    p1 = [14.9, 0.3]
    p2 = [13, 0.4]
    A = 9.8
    p = np.r_[(A, p1, p2)]

    def compmodel(A, p, x):
        return (A * np.exp((((- 0.5) / (p[1] ** 2)) * ((x - p[0]) ** 2))))

    def errf(p, x1, y1, x2, y2):
        return np.ravel(np.r_[((compmodel(p[0], p[1:3], x1) - y1), (compmodel(p[0], p[3:], x2) - y2))])
    (fitparams, _) = optimize.leastsq(errf, p, args=(x, ny1, x, ny2))
    assert_allclose(jf.fitparams, fitparams, rtol=(10 ** (- 5)))
    assert_allclose(g1.amplitude.value, g2.amplitude.value)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 91:------------------- similar code ------------------ index = 414, score = 6.0 
def test_arithmetic_subtract_with_array():
    ccd = CCDData(np.ones((3, 3)), unit='')
    res = ccd.subtract(np.arange(3))
    np.testing.assert_array_equal(res.data, ([[1, 0, (- 1)]] * 3))
    ccd = CCDData(np.ones((3, 3)), unit='adu')
    with pytest.raises(ValueError):
        ccd.subtract(np.arange(3))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... (np.arange)

idx = 92:------------------- similar code ------------------ index = 241, score = 6.0 
def test_attribute_single(self):
    '\n        Testing correct reporting of hist_bins for a single channel.\n        '
    np.testing.assert_array_equal(self.d[0].hist_bins('FSC-H', scale='linear'), (np.arange(1025) - 0.5))
    np.testing.assert_array_equal(self.d[1].hist_bins('FITC-A', scale='linear'), (np.arange(1025) - 0.5))
    np.testing.assert_array_equal(self.d[2].hist_bins('SSC', scale='linear'), (np.arange(1025) - 0.5))
    np.testing.assert_array_equal(self.d[3].hist_bins('GFP-A', scale='linear'), (np.arange(262145) - 0.5))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ... . ... (, (np.arange -  ... ))

idx = 93:------------------- similar code ------------------ index = 240, score = 6.0 
def test_reverse_big(self, table_types):
    x = np.arange(10000)
    y = (x + 1)
    t = table_types.Table([x, y], names=('x', 'y'))
    t.reverse()
    assert np.all((t['x'] == x[::(- 1)]))
    assert np.all((t['y'] == y[::(- 1)]))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 94:------------------- similar code ------------------ index = 51, score = 6.0 
def ordered_indices(self):
    'Return an ordered list of indices. Batches will be constructed based\n        on this order.'
    if self.shuffle:
        order = [np.random.permutation(len(self))]
    else:
        order = [np.arange(len(self))]
    order.append(self.sizes)
    return np.lexsort(order)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:    else:
         ...  = [np.arange]

idx = 95:------------------- similar code ------------------ index = 397, score = 6.0 
def shuffle_data(data, labels):
    ' Shuffle data and labels.\n        Input:\n          data: B,N,... numpy array\n          label: B,... numpy array\n        Return:\n          shuffled data, label and shuffle indices\n    '
    idx = np.arange(len(labels))
    np.random.shuffle(idx)
    return (data[(idx, ...)], labels[idx], idx)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 96:------------------- similar code ------------------ index = 452, score = 6.0 
def image_coordinates(image_shape):
    '\n    Returns:\n        If image_shape is (m+1, n+1), returns np.ndarray of shape\n        (2, m+1 * n+1) in the form\n        [[x0, y0], [x1, y0], [x2, y0], ..., [xn, y0],\n         [x0, y1], [x1, y1], [x2, y1], ..., [xn, y1],\n         ...,\n         [x0, ym], [x1, ym], [x2, ym], ..., [xn, ym]]\n    '
    (height, width) = image_shape[0:2]
    (xs, ys) = np.meshgrid(np.arange(width), np.arange(height))
    return np.column_stack((xs.flatten(), ys.flatten()))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 =  ... . ... (np.arange,)

idx = 97:------------------- similar code ------------------ index = 398, score = 6.0 
def test_1d_conserve_sum(self):
    'Test 1D array with conserve_sum=False.'
    data = np.arange(2)
    block_size = 2.0
    expected = (block_replicate(data, block_size) * block_size)
    result = block_replicate(data, block_size, conserve_sum=False)
    assert np.all((result == expected))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 98:------------------- similar code ------------------ index = 375, score = 6.0 
def ordered_indices(self):
    if self.batch_by_size:
        order = [np.arange(len(self)), self.sizes]
        return np.lexsort(order)
    else:
        return np.arange(len(self))

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ( ... ):
    if:
         ...  = [ ... .arange,]
    else:
        return np

idx = 99:------------------- similar code ------------------ index = 104, score = 6.0 
def ordered_indices(self):
    if self.batch_by_size:
        order = [np.arange(len(self)), self.sizes]
        return np.lexsort(order)
    else:
        return np.arange(len(self))

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ( ... ):
    if:
         ...  = [ ... .arange,]
    else:
        return np

