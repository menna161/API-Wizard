------------------------- example 1 ------------------------ 
def get_book_crossing(min_positive_score=7, min_interactions_per_book=5, user_indicators=False, item_indicators=False, cold_start_users=False, cold_start_items=False):
    "\n    Dataset from http://www2.informatik.uni-freiburg.de/~cziegler/BX/\n\n    Improving Recommendation Lists Through Topic Diversification,\n    Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, Georg Lausen; Proceedings of the 14th International World\n    Wide Web Conference (WWW '05), May 10-14, 2005, Chiba, Japan. To appear.\n\n    :param min_positive_score:\n    :return:\n    "
// your code ...

    with open(paths['BX-Book-Ratings.csv'], 'rb') as ratings_file:
        ratings_lines = ratings_file.readlines()
        for line in ratings_lines:
            split_line = line.decode('iso-8859-1').replace('\r', '').replace('\n', '').split('";"')
            split_line = [snip.replace('"', '') for snip in split_line]
            if (split_line[0] == 'User-ID'):
                continue
            user_id = int(split_line[0])
            isin = split_line[1]
            rating = int(split_line[2])
            rating = (1 if (rating >= min_positive_score) else 0)
            if (rating == 0):
// your code ...

    book_metadata_raw = {}
    with open(paths['BX-Books.csv'], 'rb') as books_file:
        books_lines = books_file.readlines()
        for line in books_lines:
            split_line = line.decode('iso-8859-1').replace('\r', '').replace('\n', '').split('";"')
            split_line = [snip.replace('"', '') for snip in split_line]
// your code ...

            title = split_line[1]
            author = split_line[2]
            year = split_line[3]
            publisher = split_line[4]
            cover_url = split_line[5]
            book_metadata_raw[isbn] = (title, author, year, publisher, cover_url)
    user_metadata_raw = {}
    with open(paths['BX-Users.csv'], 'rb') as users_file:
        users_lines = users_file.readlines()
        for line in users_lines:
            split_line = line.decode('iso-8859-1').replace('\r', '').replace('\n', '').replace('NULL', '"NULL"').split('";"')
// your code ...

            location = split_line[1]
            try:
// your code ...

    isbn_counter = {}
    for (_, isbn, _) in ratings:
        isbn_counter[isbn] = (isbn_counter.get(isbn, 0) + 1)
    ratings = [r for r in ratings if (isbn_counter[r[1]] >= min_interactions_per_book)]
    user_to_row_map = {}
// your code ...

    interactions_raw = [(user_to_row_map[user], isbn_to_column_map[isbn], score) for (user, isbn, score) in ratings]
    (r, c, v) = zip(*interactions_raw)
// your code ...

    for row in range((max(row_to_user_map) + 1)):
// your code ...

    for col in range((max(column_to_isbn_map) + 1)):
// your code ...

    book_transformers = [('title_pipeline', Pipeline([('title_extractor', TupleExtractor(0)), ('title_vectorizer', CountVectorizer(min_df=2))])), ('author_pipeline', Pipeline([('author_extractor', TupleExtractor(1)), ('author_vectorizer', PipelineLabelBinarizer(sparse_output=True))])), ('year_pipeline', Pipeline([('year_extractor', TupleExtractor(2)), ('year_vectorizer', PipelineLabelBinarizer(sparse_output=True))])), ('publisher_pipeline', Pipeline([('publisher_extractor', TupleExtractor(3)), ('publisher_vectorizer', PipelineLabelBinarizer(sparse_output=True))]))]
    if item_indicators:
// your code ...

    book_features = book_pipeline.fit_transform(book_metadata)
    book_titles = [book[0] for book in book_metadata]
    user_transformers = [('location_pipeline', Pipeline([('location_extractor', TupleExtractor(0)), ('location_vectorizer', CountVectorizer(min_df=2))])), ('age_pipeline', Pipeline([('age_extractor', TupleExtractor(1)), ('age_vectorizer', PipelineLabelBinarizer(sparse_output=True))]))]
    if user_indicators:
// your code ...


examples  ||  representativeness  ||  number of lines  || number of comments 
example1  ||          2           ||        59         ||         11        

avg       ||          2.0           ||        59.0         ||         11.0        

idx = 0:------------------- similar code ------------------ index = 4, score = 1.0 
def get_book_crossing(min_positive_score=7, min_interactions_per_book=5, user_indicators=False, item_indicators=False, cold_start_users=False, cold_start_items=False):
    "\n    Dataset from http://www2.informatik.uni-freiburg.de/~cziegler/BX/\n\n    Improving Recommendation Lists Through Topic Diversification,\n    Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, Georg Lausen; Proceedings of the 14th International World\n    Wide Web Conference (WWW '05), May 10-14, 2005, Chiba, Japan. To appear.\n\n    :param min_positive_score:\n    :return:\n    "
    if (cold_start_items and cold_start_users):
        raise ValueError("get_book_crossing() can't return both cold_start_users and cold_start_items. Set one to False.")
    paths = _download_and_unpack_zip(url='http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip', local_path='/tmp/tensorrec/book-crossing', skip_if_not_empty=True)
    ratings = []
    with open(paths['BX-Book-Ratings.csv'], 'rb') as ratings_file:
        ratings_lines = ratings_file.readlines()
        for line in ratings_lines:
            split_line = line.decode('iso-8859-1').replace('\r', '').replace('\n', '').split('";"')
            split_line = [snip.replace('"', '') for snip in split_line]
            if (split_line[0] == 'User-ID'):
                continue
            user_id = int(split_line[0])
            isin = split_line[1]
            rating = int(split_line[2])
            rating = (1 if (rating >= min_positive_score) else 0)
            if (rating == 0):
                continue
            ratings.append((user_id, isin, rating))
    book_metadata_raw = {}
    with open(paths['BX-Books.csv'], 'rb') as books_file:
        books_lines = books_file.readlines()
        for line in books_lines:
            split_line = line.decode('iso-8859-1').replace('\r', '').replace('\n', '').split('";"')
            split_line = [snip.replace('"', '') for snip in split_line]
            if (split_line[0] == 'ISBN'):
                continue
            isbn = split_line[0]
            title = split_line[1]
            author = split_line[2]
            year = split_line[3]
            publisher = split_line[4]
            cover_url = split_line[5]
            book_metadata_raw[isbn] = (title, author, year, publisher, cover_url)
    user_metadata_raw = {}
    with open(paths['BX-Users.csv'], 'rb') as users_file:
        users_lines = users_file.readlines()
        for line in users_lines:
            split_line = line.decode('iso-8859-1').replace('\r', '').replace('\n', '').replace('NULL', '"NULL"').split('";"')
            split_line = [snip.replace('"', '') for snip in split_line]
            if (split_line[0] == 'User-ID'):
                continue
            try:
                user_id = int(split_line[0])
            except ValueError:
                continue
            location = split_line[1]
            try:
                age = split_line[2]
            except IndexError:
                continue
            user_metadata_raw[user_id] = (location, age)
    isbn_counter = {}
    for (_, isbn, _) in ratings:
        isbn_counter[isbn] = (isbn_counter.get(isbn, 0) + 1)
    ratings = [r for r in ratings if (isbn_counter[r[1]] >= min_interactions_per_book)]
    user_to_row_map = {}
    isbn_to_column_map = {}
    for (user, isbn, _) in ratings:
        if (user not in user_to_row_map):
            user_to_row_map[user] = len(user_to_row_map)
        if (isbn not in isbn_to_column_map):
            isbn_to_column_map[isbn] = len(isbn_to_column_map)
    row_to_user_map = {row: user for (user, row) in user_to_row_map.items()}
    column_to_isbn_map = {col: isbn for (isbn, col) in isbn_to_column_map.items()}
    interactions_raw = [(user_to_row_map[user], isbn_to_column_map[isbn], score) for (user, isbn, score) in ratings]
    (r, c, v) = zip(*interactions_raw)
    interactions = sp.coo_matrix((v, (r, c)), dtype=np.float64)
    user_metadata = []
    for row in range((max(row_to_user_map) + 1)):
        user = row_to_user_map[row]
        user_metadata.append(user_metadata_raw[user])
    book_metadata = []
    for col in range((max(column_to_isbn_map) + 1)):
        isbn = column_to_isbn_map[col]
        try:
            book_metadata.append(book_metadata_raw[isbn])
        except KeyError:
            book_metadata.append(('', '', '', '', ''))
    book_transformers = [('title_pipeline', Pipeline([('title_extractor', TupleExtractor(0)), ('title_vectorizer', CountVectorizer(min_df=2))])), ('author_pipeline', Pipeline([('author_extractor', TupleExtractor(1)), ('author_vectorizer', PipelineLabelBinarizer(sparse_output=True))])), ('year_pipeline', Pipeline([('year_extractor', TupleExtractor(2)), ('year_vectorizer', PipelineLabelBinarizer(sparse_output=True))])), ('publisher_pipeline', Pipeline([('publisher_extractor', TupleExtractor(3)), ('publisher_vectorizer', PipelineLabelBinarizer(sparse_output=True))]))]
    if item_indicators:
        book_transformers.append(('indicator', IndicatorFeature()))
    book_pipeline = FeatureUnion(book_transformers)
    book_features = book_pipeline.fit_transform(book_metadata)
    book_titles = [book[0] for book in book_metadata]
    user_transformers = [('location_pipeline', Pipeline([('location_extractor', TupleExtractor(0)), ('location_vectorizer', CountVectorizer(min_df=2))])), ('age_pipeline', Pipeline([('age_extractor', TupleExtractor(1)), ('age_vectorizer', PipelineLabelBinarizer(sparse_output=True))]))]
    if user_indicators:
        user_transformers.append(('indicator', IndicatorFeature()))
    user_pipeline = FeatureUnion(user_transformers)
    user_features = user_pipeline.fit_transform(user_metadata)
    if cold_start_users:
        (train_interactions, test_interactions) = _split_interactions_cold_start_users(interactions)
    elif cold_start_items:
        (train_interactions, test_interactions) = _split_interactions_cold_start_items(interactions)
    else:
        (train_interactions, test_interactions) = _split_interactions_warm_start(interactions)
    return (train_interactions, test_interactions, user_features, book_features, book_titles)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .fit_transform

idx = 1:------------------- similar code ------------------ index = 3, score = 1.0 
def main(data_dir, log_dir, source='xl-1542M-k40', n_train=500000, n_valid=10000, n_jobs=None, verbose=False):
    (train_texts, train_labels) = load_split(data_dir, source, 'train', n=n_train)
    (valid_texts, valid_labels) = load_split(data_dir, source, 'valid', n=n_valid)
    (test_texts, test_labels) = load_split(data_dir, source, 'test')
    vect = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_features=(2 ** 21))
    train_features = vect.fit_transform(train_texts)
    valid_features = vect.transform(valid_texts)
    test_features = vect.transform(test_texts)
    model = LogisticRegression(solver='liblinear')
    params = {'C': [(1 / 64), (1 / 32), (1 / 16), (1 / 8), (1 / 4), (1 / 2), 1, 2, 4, 8, 16, 32, 64]}
    split = PredefinedSplit((([(- 1)] * n_train) + ([0] * n_valid)))
    search = GridSearchCV(model, params, cv=split, n_jobs=n_jobs, verbose=verbose, refit=False)
    search.fit(sparse.vstack([train_features, valid_features]), (train_labels + valid_labels))
    model = model.set_params(**search.best_params_)
    model.fit(train_features, train_labels)
    valid_accuracy = (model.score(valid_features, valid_labels) * 100.0)
    test_accuracy = (model.score(test_features, test_labels) * 100.0)
    data = {'source': source, 'n_train': n_train, 'valid_accuracy': valid_accuracy, 'test_accuracy': test_accuracy}
    print(data)
    json.dump(data, open(os.path.join(log_dir, f'{source}.json'), 'w'))

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .fit_transform

idx = 2:------------------- similar code ------------------ index = 2, score = 1.0 
@timing
def fit_data(self):
    if self.saved_dots:
        self._result = np.loadtxt(self.saved_dots)
    else:
        if (self._mode == 'pca'):
            self._model = PCA(n_components=self._dim, random_state=opt.seed)
        if (self._mode == 'tsne'):
            self._model = TSNE(n_components=self._dim, perplexity=15, random_state=opt.seed)
        if (self.reduce is None):
            self._result = self._model.fit_transform(self._data)
        else:
            fraction = int(((self._data.shape[0] * self.reduce) / 100))
            self._model.fit(self._data[:fraction])
            self._result = self._model.transform(self._data)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
    if:    else:
        if:
 =  ... .fit_transform

idx = 3:------------------- similar code ------------------ index = 1, score = 1.0 
def generate_vocab(qa_pairs, instance_id):
    questions = [qa_pair['question'] for qa_pair in qa_pairs]
    answers = [qa_pair['answer'] for qa_pair in qa_pairs]
    questions_cleaned = list(map(clean_str, list(questions)))
    answers_cleaned = list(map(clean_str, answers))
    count_vect_q = CountVectorizer(stop_words='english')
    counts_q = count_vect_q.fit_transform(questions_cleaned)
    count_vect_a = CountVectorizer(stop_words='english')
    counts_a = count_vect_a.fit_transform(answers_cleaned)
    q_vocab = (set(count_vect_q.get_feature_names()) & terms_dict[instance_id]['context_terms'])
    a_vocab = (set(count_vect_a.get_feature_names()) & terms_dict[instance_id]['resp_terms'])
    q_vocab_dict = {}
    a_vocab_dict = {}
    for term in q_vocab:
        q_vocab_dict[term] = 1
    for term in a_vocab:
        a_vocab_dict[term] = 1
    return (q_vocab_dict, a_vocab_dict)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .fit_transform

idx = 4:------------------- similar code ------------------ index = 0, score = 1.0 
def visualize(model, idx2word, args):
    test_x = np.diag(np.ones(len(idx2word)))
    embed_layer = model.net.layers[0]
    embedding = ((test_x @ embed_layer.params['w']) + embed_layer.params['b'])
    labels = list(idx2word.values())
    embedding_reduced = TSNE().fit_transform(embedding)
    plt.figure(figsize=(12, 6))
    words = ['he', 'she', 'man', 'woman', 'is', 'are', 'you', 'i']
    for word in words:
        i = labels.index(word)
        (x, y) = embedding_reduced[i]
        plt.scatter(x, y)
        plt.annotate(labels[i], (x, y))
    if (not os.path.isdir(args.output_dir)):
        os.makedirs(args.output_dir)
    output_path = os.path.join(args.output_dir, 'tsne-sample.jpg')
    plt.savefig(output_path)
    plt.close()
    print(f'visualization result: {output_path}')

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... ().fit_transform

