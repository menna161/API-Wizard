------------------------- example 1 ------------------------ 
def raise_if_form_exists(url, session):
    '\n    This function raises a UserWarning if the link has forms\n    '
    user_warning = ('Navigate to {0}, '.format(url) + 'login and follow instructions. It is likely that you have to perform some one-time registration steps before acessing this data.')
    resp = session.get(url)
    soup = BeautifulSoup(resp.content, 'lxml')
    if (len(soup.select('form')) > 0):
// your code ...


------------------------- example 2 ------------------------ 
def test_body(self):
    'Test the HTML response.\n\n        We use BeautifulSoup to parse the response, and check for some\n        elements that should be there.\n\n        '
// your code ...

    soup = BeautifulSoup(res.text, 'html.parser')
    self.assertEqual(soup.title.string, 'Dataset http://localhost/.html')
    self.assertEqual(soup.form['action'], 'http://localhost/.html')
// your code ...


------------------------- example 3 ------------------------ 
def request_single_patent(self, patent, url=False):
    'Calls request function to retreive google patent data and parses returned html using BeautifulSoup\n\n\n        Returns: \n            - Status of scrape   <- String\n            - Html of patent     <- BS4 object\n\n        Inputs:\n            - patent (str)  : if    url == False then patent is patent number\n                              elif  url == True  then patent is google patent url\n            - url    (bool) : determines whether patent is treated as patent number \n                                or google patent url\n\n        '
    try:
        if (not url):
            url = 'https://patents.google.com/patent/{0}'.format(patent)
        else:
            url = patent
        print(url)
        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        webpage = urlopen(req).read()
        soup = BeautifulSoup(webpage, features='lxml')
        return ('Success', soup, url)
    except HTTPError as e:
        print('Patent: {0}, Error Status Code : {1}'.format(patent, e.code))
        return (e.code, '', url)

------------------------- example 4 ------------------------ 
def soup_login(session, url, username, password, username_field='username', password_field='password'):
    resp = session.get(url)
    soup = BeautifulSoup(resp.content, 'lxml')
    login_form = soup.select('form')[0]

    def get_to_url(current_url, to_url):
        split_current = urlsplit(current_url)
// your code ...

        comb = [(val2 if (val1 == '') else val1) for (val1, val2) in zip(split_to, split_current)]
        return urlunsplit(comb)
    to_url = get_to_url(resp.url, login_form.get('action'))
// your code ...


examples  ||  representativeness  ||  number of lines  || number of comments 
example1  ||          3           ||        7         ||         1        
example2  ||          3           ||        7         ||         2        
example3  ||          2           ||        15         ||         0        
example4  ||          3           ||        10         ||         2        

avg       ||          2.75           ||        9.75         ||         1.25        

idx = 0:------------------- similar code ------------------ index = 12, score = 2.0 
def _parse(source, beautifulsoup, makeelement, **bsargs):
    if (beautifulsoup is None):
        beautifulsoup = BeautifulSoup
    if hasattr(beautifulsoup, 'HTML_ENTITIES'):
        if ('convertEntities' not in bsargs):
            bsargs['convertEntities'] = 'html'
    if hasattr(beautifulsoup, 'DEFAULT_BUILDER_FEATURES'):
        if ('features' not in bsargs):
            bsargs['features'] = 'html.parser'
    tree = beautifulsoup(source, **bsargs)
    root = _convert_tree(tree, makeelement)
    if ((len(root) == 1) and (root[0].tag == 'html')):
        return root[0]
    root.tag = 'html'
    return root

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ():
    if:
         ...  = BeautifulSoup

idx = 1:------------------- similar code ------------------ index = 8, score = 2.0 
def _build_doc(self):
    from bs4 import BeautifulSoup
    return BeautifulSoup(self._setup_build_doc(), features='html5lib', from_encoding=self.encoding)

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ( ... ):
    from  ...  import BeautifulSoup

idx = 2:------------------- similar code ------------------ index = 13, score = 1.0 
def raise_if_form_exists(url, session):
    '\n    This function raises a UserWarning if the link has forms\n    '
    user_warning = ('Navigate to {0}, '.format(url) + 'login and follow instructions. It is likely that you have to perform some one-time registration steps before acessing this data.')
    resp = session.get(url)
    soup = BeautifulSoup(resp.content, 'lxml')
    if (len(soup.select('form')) > 0):
        raise UserWarning(user_warning)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = BeautifulSoup

idx = 3:------------------- similar code ------------------ index = 11, score = 1.0 
def get_dce_daily(date=None, type='future', retries=0):
    "\n        获取大连商品交易所日交易数据\n    Parameters\n    ------\n        date: 日期 format：YYYY-MM-DD 或 YYYYMMDD 或 datetime.date对象 为空时为当天\n        type: 数据类型, 为'future'期货 或 'option'期权二者之一\n        retries: int, 当前重试次数，达到3次则获取数据失败\n    Return\n    -------\n        DataFrame\n            大商所日交易数据(DataFrame):\n                symbol        合约代码\n                date          日期\n                open          开盘价\n                high          最高价\n                low           最低价\n                close         收盘价\n                volume        成交量\n                open_interest   持仓量\n                turnover       成交额\n                settle        结算价\n                pre_settle    前结算价\n                variety       合约类别\n        或 \n        DataFrame\n           郑商所每日期权交易数据\n                symbol        合约代码\n                date          日期\n                open          开盘价\n                high          最高价\n                low           最低价\n                close         收盘价\n                pre_settle      前结算价\n                settle         结算价\n                delta          对冲值  \n                volume         成交量\n                open_interest     持仓量\n                oi_change       持仓变化\n                turnover        成交额\n                implied_volatility 隐含波动率\n                exercise_volume   行权量\n                variety        合约类别\n        或 None(给定日期没有交易数据)\n    "
    day = (ct.convert_date(date) if (date is not None) else datetime.date.today())
    if (retries > 3):
        print('maximum retires for DCE market data: ', day.strftime('%Y%m%d'))
        return
    if (type == 'future'):
        url = ((ct.DCE_DAILY_URL + '?') + urlencode({'currDate': day.strftime('%Y%m%d'), 'year': day.strftime('%Y'), 'month': str((int(day.strftime('%m')) - 1)), 'day': day.strftime('%d')}))
        listed_columns = ct.DCE_COLUMNS
        output_columns = ct.OUTPUT_COLUMNS
    elif (type == 'option'):
        url = ((ct.DCE_DAILY_URL + '?') + urlencode({'currDate': day.strftime('%Y%m%d'), 'year': day.strftime('%Y'), 'month': str((int(day.strftime('%m')) - 1)), 'day': day.strftime('%d'), 'dayQuotes.trade_type': '1'}))
        listed_columns = ct.DCE_OPTION_COLUMNS
        output_columns = ct.OPTION_OUTPUT_COLUMNS
    else:
        print((('invalid type :' + type) + ', should be one of "future" or "option"'))
        return
    try:
        response = urlopen(Request(url, method='POST', headers=ct.DCE_HEADERS)).read().decode('utf8')
    except IncompleteRead as reason:
        return get_dce_daily(day, retries=(retries + 1))
    except HTTPError as reason:
        if (reason.code == 504):
            return get_dce_daily(day, retries=(retries + 1))
        elif (reason.code != 404):
            print(ct.DCE_DAILY_URL, reason)
        return
    if (u'错误：您所请求的网址（URL）无法获取' in response):
        return get_dce_daily(day, retries=(retries + 1))
    elif (u'暂无数据' in response):
        return
    data = BeautifulSoup(response, 'html.parser').find_all('tr')
    if (len(data) == 0):
        return
    dict_data = list()
    implied_data = list()
    for idata in data[1:]:
        if ((u'小计' in idata.text) or (u'总计' in idata.text)):
            continue
        x = idata.find_all('td')
        if (type == 'future'):
            row_dict = {'variety': ct.DCE_MAP[x[0].text.strip()]}
            row_dict['symbol'] = (row_dict['variety'] + x[1].text.strip())
            for (i, field) in enumerate(listed_columns):
                field_content = x[(i + 2)].text.strip()
                if ('-' in field_content):
                    row_dict[field] = 0
                elif (field in ['volume', 'open_interest']):
                    row_dict[field] = int(field_content.replace(',', ''))
                else:
                    row_dict[field] = float(field_content.replace(',', ''))
            dict_data.append(row_dict)
        elif (len(x) == 16):
            m = ct.FUTURE_SYMBOL_PATTERN.match(x[1].text.strip())
            if (not m):
                continue
            row_dict = {'symbol': x[1].text.strip(), 'variety': m.group(1).upper(), 'contract_id': m.group(0)}
            for (i, field) in enumerate(listed_columns):
                field_content = x[(i + 2)].text.strip()
                if ('-' in field_content):
                    row_dict[field] = 0
                elif (field in ['volume', 'open_interest']):
                    row_dict[field] = int(field_content.replace(',', ''))
                else:
                    row_dict[field] = float(field_content.replace(',', ''))
            dict_data.append(row_dict)
        elif (len(x) == 2):
            implied_data.append({'contract_id': x[0].text.strip(), 'implied_volatility': float(x[1].text.strip())})
    df = pd.DataFrame(dict_data)
    df['date'] = day.strftime('%Y%m%d')
    if (type == 'future'):
        return df[output_columns]
    else:
        return pd.merge(df, pd.DataFrame(implied_data), on='contract_id', how='left', indicator=False)[output_columns]

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = BeautifulSoup

idx = 4:------------------- similar code ------------------ index = 10, score = 1.0 
def test_body(self):
    'Test the HTML response.\n\n        We use BeautifulSoup to parse the response, and check for some\n        elements that should be there.\n\n        '
    res = self.app.get('/.html')
    soup = BeautifulSoup(res.text, 'html.parser')
    self.assertEqual(soup.title.string, 'Dataset http://localhost/.html')
    self.assertEqual(soup.form['action'], 'http://localhost/.html')
    self.assertEqual(soup.form['method'], 'POST')
    ids = [var.id for var in walk(VerySimpleSequence)]
    for (h2, id_) in zip(soup.find_all('h2'), ids):
        self.assertEqual(h2.string, id_)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = BeautifulSoup

idx = 5:------------------- similar code ------------------ index = 9, score = 1.0 
def request_single_patent(self, patent, url=False):
    'Calls request function to retreive google patent data and parses returned html using BeautifulSoup\n\n\n        Returns: \n            - Status of scrape   <- String\n            - Html of patent     <- BS4 object\n\n        Inputs:\n            - patent (str)  : if    url == False then patent is patent number\n                              elif  url == True  then patent is google patent url\n            - url    (bool) : determines whether patent is treated as patent number \n                                or google patent url\n\n        '
    try:
        if (not url):
            url = 'https://patents.google.com/patent/{0}'.format(patent)
        else:
            url = patent
        print(url)
        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        webpage = urlopen(req).read()
        soup = BeautifulSoup(webpage, features='lxml')
        return ('Success', soup, url)
    except HTTPError as e:
        print('Patent: {0}, Error Status Code : {1}'.format(patent, e.code))
        return (e.code, '', url)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    try:
         ...  = BeautifulSoup

idx = 6:------------------- similar code ------------------ index = 7, score = 1.0 
def soup_login(session, url, username, password, username_field='username', password_field='password'):
    resp = session.get(url)
    soup = BeautifulSoup(resp.content, 'lxml')
    login_form = soup.select('form')[0]

    def get_to_url(current_url, to_url):
        split_current = urlsplit(current_url)
        split_to = urlsplit(to_url)
        comb = [(val2 if (val1 == '') else val1) for (val1, val2) in zip(split_to, split_current)]
        return urlunsplit(comb)
    to_url = get_to_url(resp.url, login_form.get('action'))
    session.headers['Referer'] = resp.url
    payload = {}
    if (username_field is not None):
        if (len(login_form.findAll('input', {'name': username_field})) > 0):
            payload.update({username_field: username})
    if (password_field is not None):
        if (len(login_form.findAll('input', {'name': password_field})) > 0):
            payload.update({password_field: password})
        else:
            raise Exception('Navigate to {0}. If you are unable to login, you must either wait or use authentication from another service.'.format(url))
    for input in login_form.findAll('input'):
        if ((input.get('name') not in payload) and (input.get('name') is not None)):
            payload.update({input.get('name'): input.get('value')})
    submit_type = 'submit'
    submit_names = [input.get('name') for input in login_form.findAll('input', {'type': submit_type})]
    for input in login_form.findAll('input', {'type': submit_type}):
        if (('submit' in submit_names) and (input.get('name').lower() != 'submit')):
            payload.pop(input.get('name'), None)
    return session.post(to_url, data=payload)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = BeautifulSoup

idx = 7:------------------- similar code ------------------ index = 4, score = 1.0 
def recipeToJSON(browser, recipeID):
    html = browser.page_source
    soup = BeautifulSoup(html, 'html.parser')
    recipe = {}
    recipe['id'] = recipeID
    recipe['language'] = soup.select_one('html').attrs['lang']
    recipe['title'] = soup.select_one('.recipe-card__title').text
    recipe['rating_count'] = re.sub('\\D', '', soup.select_one('.core-rating__label').text, flags=re.IGNORECASE)
    recipe['rating_score'] = soup.select_one('.core-rating__counter').text
    recipe['tm-versions'] = [v.text.replace('\n', '').strip().lower() for v in soup.select('.recipe-card__tm-version core-badge')]
    recipe.update({l.text: l.next_sibling.strip() for l in soup.select('core-feature-icons label span')})
    recipe['ingredients'] = [re.sub(' +', ' ', li.text).replace('\n', '').strip() for li in soup.select('#ingredients li')]
    recipe['nutritions'] = {}
    for item in list(zip(soup.select('.nutritions dl')[0].find_all('dt'), soup.select('.nutritions dl')[0].find_all('dd'))):
        (dt, dl) = item
        recipe['nutritions'].update({dt.string.replace('\n', '').strip().lower(): re.sub('\\s{2,}', ' ', dl.string.replace('\n', '').strip().lower())})
    recipe['steps'] = [re.sub(' +', ' ', li.text).replace('\n', '').strip() for li in soup.select('#preparation-steps li')]
    recipe['tags'] = [a.text.replace('#', '').replace('\n', '').strip().lower() for a in soup.select('.core-tags-wrapper__tags-container a')]
    return recipe

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = BeautifulSoup

idx = 8:------------------- similar code ------------------ index = 2, score = 1.0 
def request_single_patent(self, patent, url=False):
    'Calls request function to retreive google patent data and parses returned html using BeautifulSoup\n\n\n        Returns: \n            - Status of scrape   <- String\n            - Html of patent     <- BS4 object\n\n        Inputs:\n            - patent (str)  : if    url == False then patent is patent number\n                              elif  url == True  then patent is google patent url\n            - url    (bool) : determines whether patent is treated as patent number \n                                or google patent url\n\n        '
    try:
        if (not url):
            url = 'https://patents.google.com/patent/{0}'.format(patent)
        else:
            url = patent
        print(url)
        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        webpage = urlopen(req).read()
        soup = BeautifulSoup(webpage, features='lxml')
        return ('Success', soup, url)
    except HTTPError as e:
        print('Patent: {0}, Error Status Code : {1}'.format(patent, e.code))
        return (e.code, '', url)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    try:
         ...  = BeautifulSoup

