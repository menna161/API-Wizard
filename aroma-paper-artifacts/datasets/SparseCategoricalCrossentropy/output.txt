examples  ||  representativeness  ||  number of lines  || number of comments   ||  relevancy  

avg       ||          0           ||        0         ||         0        ||         0        

idx = 0:------------------- similar code ------------------ index = 2, score = 1.0 
if (__name__ == '__main__'):
    physical_devices = tf.config.experimental.list_physical_devices('GPU')
    assert (len(physical_devices) > 0), 'Not enough GPU hardware devices available'
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    batch_size = 500
    tf.random.set_seed(10101)
    np.random.seed(10101)
    ((train_x, train_y), (test_x, test_y)) = k.datasets.cifar10.load_data()
    train_x = ((train_x - 127.5) / 127.5).astype('float32')
    test_x = ((test_x - 127.5) / 127.5).astype('float32')
    softmax_model: k.Model = k.Sequential([kl.Input(shape=(32, 32, 3)), kl.Conv2D(64, kernel_size=(3, 3), padding='SAME'), kl.BatchNormalization(), kl.ReLU(6), kl.MaxPooling2D((2, 2)), kl.Conv2D(128, kernel_size=(3, 3), padding='SAME'), kl.BatchNormalization(), kl.ReLU(6), kl.MaxPooling2D((2, 2)), kl.Conv2D(256, kernel_size=(3, 3), padding='SAME'), kl.BatchNormalization(), kl.ReLU(6), kl.MaxPooling2D((2, 2)), kl.Conv2D(256, kernel_size=(3, 3), padding='SAME'), kl.BatchNormalization(), kl.ReLU(6), kl.Conv2D(128, kernel_size=(3, 3), padding='SAME'), kl.BatchNormalization(), kl.ReLU(6), kl.GlobalMaxPooling2D(), kl.Dense(128), kl.Lambda((lambda x: tf.nn.l2_normalize(x, 1)), name='emmbeding'), kl.Dense(10, use_bias=False, kernel_constraint=k.constraints.unit_norm())])
    ams_model = k.models.clone_model(softmax_model)
    circle_model = k.models.clone_model(softmax_model)
    proxy_model = k.models.clone_model(softmax_model)
    softmax_model.compile(loss=k.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=k.optimizers.Adam(), metrics=[k.metrics.SparseCategoricalAccuracy('acc')])
    ams_model.compile(loss=SparseAmsoftmaxLoss(batch_size=batch_size), optimizer=k.optimizers.Adam(), metrics=[k.metrics.SparseCategoricalAccuracy('acc')])
    circle_model.compile(loss=SparseCircleLoss(batch_size=batch_size), optimizer=k.optimizers.Adam(), metrics=[k.metrics.SparseCategoricalAccuracy('acc')])
    proxy_model.compile(loss=ProxyAnchorLoss(batch_size=batch_size), optimizer=k.optimizers.Adam(), metrics=[k.metrics.CategoricalAccuracy('acc')])
    if (not tf.io.gfile.exists('softmax_loss.h5')):
        softmax_history = softmax_model.fit(x=train_x, y=train_y, batch_size=batch_size, epochs=20, validation_data=(test_x, test_y))
        softmax_model.save('softmax_loss.h5')
        plt.plot(softmax_history.epoch, softmax_history.history['val_acc'], label='softmax')
    else:
        softmax_model.load_weights('softmax_loss.h5')
    if (not tf.io.gfile.exists('ams_loss.h5')):
        ams_history = ams_model.fit(x=train_x, y=train_y, batch_size=batch_size, epochs=20, validation_data=(test_x, test_y))
        ams_model.save('ams_loss.h5')
        plt.plot(ams_history.epoch, ams_history.history['val_acc'], label='am-softmax')
    else:
        ams_model.load_weights('ams_loss.h5')
    if (not tf.io.gfile.exists('circle_loss.h5')):
        circle_history = circle_model.fit(x=train_x, y=train_y, batch_size=batch_size, epochs=20, validation_data=(test_x, test_y))
        circle_model.save('circle_loss.h5')
        plt.plot(circle_history.epoch, circle_history.history['val_acc'], label='circle loss')
    else:
        circle_model.load_weights('circle_loss.h5')
    if (not tf.io.gfile.exists('proxy_loss.h5')):
        proxy_history = proxy_model.fit(x=train_x, y=tf.keras.utils.to_categorical(train_y, 10), batch_size=batch_size, epochs=20, validation_data=(test_x, tf.keras.utils.to_categorical(test_y, 10)))
        proxy_model.save('proxy_loss.h5')
        plt.plot(proxy_history.epoch, proxy_history.history['val_acc'], label='proxy loss')
        plt.legend(loc='upper left')
        plt.title('Validation Accuracy')
        plt.tight_layout()
        plt.savefig('benchmark.png', transparent=True, bbox_inches='tight', pad_inches=0)
        plt.show()
    else:
        proxy_model.load_weights('proxy_loss.h5')
    print('Softmax evaluate:')
    softmax_model.evaluate(test_x, test_y, batch_size=batch_size)
    print('Am Softmax evaluate:')
    ams_model.evaluate(test_x, test_y, batch_size=batch_size)
    print('Circle Loss evaluate:')
    circle_model.evaluate(test_x, test_y, batch_size=batch_size)
    print('Proxy Loss evaluate:')
    proxy_model.evaluate(test_x, tf.keras.utils.to_categorical(test_y, 10), batch_size=batch_size)

------------------- similar code (pruned) ------------------ score = 0.2 
if:
     ... . ... ( ... = ... .SparseCategoricalCrossentropy,,)

idx = 1:------------------- similar code ------------------ index = 1, score = 1.0 
def run_network(data, input_size, output_size, problem_type, net_kw, run_kw, num_workers=8, pin_memory=True, validate=True, val_patience=np.inf, test=False, ensemble=False, numepochs=100, wt_init=None, bias_init=None, verbose=True):
    "\n    ARGS:\n        data:\n            6-ary tuple (xtr,ytr, xva,yva, xte,yte) from get_data_mlp(), OR\n            Dict with keys 'train', 'val', 'test' from get_data_cnn()\n        input_size, output_size, net_kw : See Net()\n        run_kw:\n            lr: Initial learning rate\n            gamma: Learning rate decay coefficient\n            milestones: When to step decay learning rate, e.g. 0.5 will decay lr halfway through training\n            weight_decay: Default 0\n            batch_size: Default 256\n        num_workers, pin_memory: Only required if using Pytorch data loaders\n            Generally, set num_workers equal to number of threads (e.g. my Macbook pro has 4 cores x 2 = 8 threads)\n        validate: Whether to do validation at the end of every epoch.\n        val_patience: If best val acc doesn't increase for this many epochs, then stop training. Set as np.inf to never stop training (until numepochs)\n        test: True - Test at end, False - don't\n        ensemble: If True, return feedforward soft outputs to be later used for ensembling\n        numepochs: Self explanatory\n        wt_init, bias_init: Respective pytorch functions\n        verbose: Print messages\n    \n    RETURNS:\n        net: Complete net\n        recs: Dictionary with a key for each stat collected and corresponding value for all values of the stat\n    "
    net = Net(input_size=input_size, output_size=output_size, problem_type=problem_type, **net_kw)
    net.build(tuple((([None] + input_size[1:]) + [input_size[0]])))
    '\n    for i in range(len(net.mlp)):\n        if wt_init is not None:\n            wt_init(net.mlp[i].weight.data)\n        if bias_init is not None:\n            bias_init(net.mlp[i].bias.data)\n    '
    lr = (run_kw['lr'] if ('lr' in run_kw) else run_kws_defaults['lr'])
    gamma = (run_kw['gamma'] if ('gamma' in run_kw) else run_kws_defaults['gamma'])
    milestones = (run_kw['milestones'] if ('milestones' in run_kw) else run_kws_defaults['milestones'])
    weight_decay = (run_kw['weight_decay'] if ('weight_decay' in run_kw) else run_kws_defaults['weight_decay'])
    batch_size = (run_kw['batch_size'] if ('batch_size' in run_kw) else run_kws_defaults['batch_size'])
    if (not isinstance(batch_size, int)):
        batch_size = batch_size.item()
    if (problem_type == 'classification'):
        lossfunc = tf.keras.losses.SparseCategoricalCrossentropy()
    elif (problem_type == 'regression'):
        lossfunc = tf.keras.losses.MeanSquaredError()
    opt = tf.keras.optimizers.Adam(learning_rate=lr, decay=weight_decay)
    net.compile(optimizer=opt, loss=lossfunc, metrics=['accuracy'])
    trainable_count = np.sum([K.count_params(w) for w in net.trainable_weights])

    def multi_step_lr(epoch):
        LR_START = lr
        GAMMA = gamma
        NUMEPOCHS = numepochs
        step = 0
        for milestone in milestones:
            if (epoch >= int((milestone * NUMEPOCHS))):
                step += 1
        return (LR_START * (GAMMA ** step))
    lr_callback = tf.keras.callbacks.LearningRateScheduler(multi_step_lr, verbose=verbose)
    (xtr, ytr, xva, yva, xte, yte) = data
    recs = {}
    total_t = 0
    best_val_acc = (- np.inf)
    best_val_loss = np.inf

    class TimeHistory(tf.keras.callbacks.Callback):

        def on_train_begin(self, logs={}):
            self.times = []

        def on_epoch_begin(self, batch, logs={}):
            self.epoch_time_start = time.time()

        def on_epoch_end(self, batch, logs={}):
            self.times.append((time.time() - self.epoch_time_start))
    th_callback = TimeHistory()
    if (validate is True):
        if (problem_type == 'classification'):
            es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=val_patience, verbose=verbose, restore_best_weights=True)
        elif (problem_type == 'regression'):
            es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=val_patience, verbose=verbose, restore_best_weights=True)
        history = net.fit(x=xtr, y=ytr, verbose=verbose, validation_data=(xva, yva), batch_size=batch_size, epochs=numepochs, shuffle=True, use_multiprocessing=False, callbacks=[lr_callback, th_callback, es_callback])
        recs['val_accs'] = (np.array(history.history['val_accuracy']) * 100)
        recs['val_losses'] = history.history['val_loss']
    else:
        history = net.fit(x=xtr, y=ytr, verbose=verbose, batch_size=batch_size, epochs=numepochs, shuffle=True, use_multiprocessing=False, callbacks=[lr_callback, th_callback])
    recs['train_accs'] = (np.array(history.history['accuracy']) * 100)
    recs['train_losses'] = history.history['loss']
    total_t += np.sum(th_callback.times)
    if (validate is True):
        if (problem_type == 'classification'):
            print('\nBest validation accuracy = {0}% obtained in epoch {1}'.format(np.max(recs['val_accs']), (np.argmax(recs['val_accs']) + 1)))
        elif (problem_type == 'regression'):
            print('\nBest validation loss = {0} obtained in epoch {1}'.format(np.min(recs['val_losses']), (np.argmin(recs['val_losses']) + 1)))
    if (test is True):
        ret = net.evaluate(x=xte, y=yte, verbose=verbose, batch_size=batch_size, workers=1, use_multiprocessing=False)
        recs['test_acc'] = (ret[1] * 100)
        recs['test_loss'] = ret[0]
        print('Test accuracy = {0}%, Loss = {1}\n'.format(np.round(recs['test_acc'], 2), np.round(recs['test_loss'], 3)))
    recs['t_epoch'] = ((total_t / (numepochs - 1)) if (numepochs > 1) else total_t)
    print('Avg time taken per epoch = {0}'.format(recs['t_epoch']))
    return (net, recs)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    if:
         ...  =  ... .SparseCategoricalCrossentropy()

idx = 2:------------------- similar code ------------------ index = 0, score = 1.0 
def build_loss(self):
    if (self.VOC_SIZE[self.LABEL] == 2):
        return tf.keras.losses.BinaryCrossentropy()
    else:
        return tf.keras.losses.SparseCategoricalCrossentropy()

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
    if:    else:
        return  ... .SparseCategoricalCrossentropy()

