------------------------- example 1 ------------------------ 
def read_total_dos(self) -> pd.DataFrame:
    start_to_read: int = Doscar.number_of_header_lines
    df: pd.DataFrame = pd.read_csv(self.filename, skiprows=start_to_read, nrows=self.number_of_data_points, delim_whitespace=True, names=['energy', 'up', 'down', 'int_up', 'int_down'], index_col=False)
    self.energy: np.ndarray = df.energy.values
    df.drop('energy', axis=1)
    self.tdos = df

------------------------- example 2 ------------------------ 
def check_if_drop(self):
    current_progress = LearnedGroupConv.global_progress
// your code ...

    for i in range((self.condense_factor - 1)):
// your code ...

    else:
        stage = (self.condense_factor - 1)
    if (not self.reach_stage(stage)):
        self.stage = stage
        delta = (self.in_channels // your code ...
 self.condense_factor)
        print(delta)
    if (delta > 0):
        self.drop(delta)
    return

------------------------- example 3 ------------------------ 
def creator_plot(watch: pd.DataFrame, number: int):
    df = watch['channel'].value_counts()
    df.drop(['unknown'], inplace=True)
    df = df.head(number).sort_values()
// your code ...


------------------------- example 4 ------------------------ 
def load_dataset():
    'Load the LaLonde dataset.'
// your code ...

    lalonde = lalonde.drop('re78', axis=1)
    return lalonde.rename(columns={'treat': 'treatment'})

------------------------- example 5 ------------------------ 
def build_diagnoses_table(data_dir, out_dir, conn):
    print('Build diagnoses_table')
    left = pandas.read_csv(os.path.join(data_dir, 'DIAGNOSES_ICD.csv'), dtype=str)
    right = pandas.read_csv(os.path.join(data_dir, 'D_ICD_DIAGNOSES.csv'), dtype=str)
    left = left.drop(columns=['ROW_ID', 'SEQ_NUM'])
    right = right.drop(columns=['ROW_ID'])
    out = pandas.merge(left, right, on='ICD9_CODE')
// your code ...

    out.to_sql('DIAGNOSES', conn, if_exists='replace', index=False)

examples  ||  representativeness  ||  number of lines  || number of comments 
example1  ||          3           ||        6         ||         0        
example2  ||          2           ||        14         ||         3        
example3  ||          2           ||        5         ||         1        
example4  ||          4           ||        5         ||         1        
example5  ||          4           ||        9         ||         1        

avg       ||          3.0           ||        7.8         ||         1.2        

idx = 0:------------------- similar code ------------------ index = 38, score = 2.0 
def drop(self):
    if (self.with_database and self.db_exists()):
        self.drop_database()
    if self.role_exists():
        self.drop_user()

------------------- similar code (pruned) ------------------ score = 0.6666666666666666 
def drop( ... ):
idx = 1:------------------- similar code ------------------ index = 32, score = 2.0 
def read_total_dos(self) -> pd.DataFrame:
    start_to_read: int = Doscar.number_of_header_lines
    df: pd.DataFrame = pd.read_csv(self.filename, skiprows=start_to_read, nrows=self.number_of_data_points, delim_whitespace=True, names=['energy', 'up', 'down', 'int_up', 'int_down'], index_col=False)
    self.energy: np.ndarray = df.energy.values
    df.drop('energy', axis=1)
    self.tdos = df

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ( ... ) ->:
     ... .drop

idx = 2:------------------- similar code ------------------ index = 41, score = 2.0 
def test_to_mongo_with_if_exists_replace_calls_drop(mocker):
    collection_name = 'ACollection'
    mock_db = mocker.patch('pymongo.database.Database')
    pdm.to_mongo(pd.DataFrame(), collection_name, mock_db, if_exists='replace')
    mock_db[collection_name].drop.assert_called_once()

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ( ... ):
     ... .drop

idx = 3:------------------- similar code ------------------ index = 1, score = 2.0 
def test_gams():
    'Ensure calling default simulator has results reflective of the GAMS implementaiton'
    gams_file = (os.path.realpath(__file__).rsplit('/', 1)[0] + '/gamsBaseline.csv')
    gams = pd.read_csv(gams_file).replace([np.inf, (- np.inf)], np.nan).fillna(0)
    gams.columns = ['Variable', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']
    gams.drop('Variable', axis=1, inplace=True)
    config = wn.dice.Config(numPeriods=10, ifopt=0)
    initial_state = wn.dice.State()
    run = wn.dice.simulate(initial_state, config, stochastic=False)
    sim_dict = {var: [getattr(state, var) for state in run.states[1:]] for var in initial_state.variables}
    sim = pd.DataFrame.from_dict(sim_dict, orient='index', columns=gams.columns).replace([np.inf, (- np.inf)], np.nan).fillna(0)
    np.testing.assert_allclose(sim, gams, rtol=0.0001, atol=0.05, equal_nan=False)

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ():
     ... .drop

idx = 4:------------------- similar code ------------------ index = 13, score = 2.0 
def check_if_drop(self):
    current_progress = LearnedGroupConv.global_progress
    delta = 0
    for i in range((self.condense_factor - 1)):
        if ((current_progress * 2) < ((i + 1) / (self.condense_factor - 1))):
            stage = i
            break
    else:
        stage = (self.condense_factor - 1)
    if (not self.reach_stage(stage)):
        self.stage = stage
        delta = (self.in_channels // self.condense_factor)
        print(delta)
    if (delta > 0):
        self.drop(delta)
    return

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ( ... ):
    if:
         ... .drop
    return

idx = 5:------------------- similar code ------------------ index = 49, score = 2.0 
def creator_plot(watch: pd.DataFrame, number: int):
    df = watch['channel'].value_counts()
    df.drop(['unknown'], inplace=True)
    df = df.head(number).sort_values()
    plot = df.plot(kind='barh', color=COLOR, figsize=[6.4, (number * 0.28)])
    plot.set_xlabel('videos watched')
    plot.set_title(f'Top {number} creators')
    return plot

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ():
     ... .drop

idx = 6:------------------- similar code ------------------ index = 3, score = 2.0 
def _handle_exists_collection(name: str, exists: Optional[str], db: Database) -> None:
    "\n    Handles the `if_exists` argument of `to_mongo`.\n\n    Parameters\n    ----------\n    if_exists: str\n        Can be one of 'fail', 'replace', 'append'\n            - fail: A ValueError is raised\n            - replace: Collection is deleted before inserting new documents\n            - append: Documents are appended to existing collection\n    "
    if (exists == 'fail'):
        if _collection_exists(db, name):
            raise ValueError(f"Collection '{name}' already exists.")
        return
    if (exists == 'replace'):
        if _collection_exists(db, name):
            db[name].drop()
        return
    if (exists == 'append'):
        return
    raise ValueError(f"'{exists}' is not valid for if_exists")

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... () -> None:
    if:
        if:
             ... .drop()
        return

idx = 7:------------------- similar code ------------------ index = 54, score = 2.0 
def delete(self):
    if (self.physical_resource_id == 'could-not-create'):
        self.success('user was never created')
    try:
        self.connect()
        self.drop()
    except Exception as e:
        return self.fail(str(e))
    finally:
        self.close()

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ( ... ):
    try:
         ... .drop()

idx = 8:------------------- similar code ------------------ index = 43, score = 2.0 
def dominance_level(self):
    gen_dom = self.predict_general_dominance()
    condition_dom = self.predict_conditional_dominance()
    comp_dom = self.predict_complete_dominance()
    gen_dom.rename(columns={'Dominating': 'Generally Dominating'}, inplace=True)
    condition_dom.drop('Conditional Dominance', inplace=True, axis=1)
    condition_dom.rename(columns={'Dominating': 'Conditionally Dominating'}, inplace=True)
    comp_dom.rename(columns={'Dominating': 'Completelly Dominating'}, inplace=True)
    return pd.merge(pd.merge(left=gen_dom, right=condition_dom[['Predictors', 'Conditionally Dominating']], how='left'), comp_dom, how='left').fillna('')

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ( ... ):
     ... .drop

idx = 9:------------------- similar code ------------------ index = 31, score = 1.0 
def load_dataset():
    'Load the LaLonde dataset.'
    dir_path = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(dir_path, 'lalonde.csv')
    lalonde = pd.read_csv(data_path, index_col=0)
    lalonde = lalonde.drop('re78', axis=1)
    return lalonde.rename(columns={'treat': 'treatment'})

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .drop

idx = 10:------------------- similar code ------------------ index = 30, score = 1.0 
def build_diagnoses_table(data_dir, out_dir, conn):
    print('Build diagnoses_table')
    left = pandas.read_csv(os.path.join(data_dir, 'DIAGNOSES_ICD.csv'), dtype=str)
    right = pandas.read_csv(os.path.join(data_dir, 'D_ICD_DIAGNOSES.csv'), dtype=str)
    left = left.drop(columns=['ROW_ID', 'SEQ_NUM'])
    right = right.drop(columns=['ROW_ID'])
    out = pandas.merge(left, right, on='ICD9_CODE')
    out = out.sort_values(by='HADM_ID')
    print('-- write table')
    out.to_csv(os.path.join(out_dir, 'DIAGNOSES.csv'), sep=',', index=False)
    print('-- write sql')
    out.to_sql('DIAGNOSES', conn, if_exists='replace', index=False)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .drop

idx = 11:------------------- similar code ------------------ index = 29, score = 1.0 
def _transform_df(self, df):
    df = df.copy(True)
    if self.distinguish_ablation:
        df['label'] = 2
        df.loc[(df.ablation, 'label')] = 1
        df.loc[(df.sota, 'label')] = 0
    else:
        df['label'] = 1
        df.loc[(df.sota, 'label')] = 0
        df.loc[(df.ablation, 'label')] = 0
    if self.sigmoid:
        if self.irrelevant_as_class:
            df['irrelevant'] = (~ (df['sota'] | df['ablation']))
        if (not self.distinguish_ablation):
            df['sota'] = (df['sota'] | df['ablation'])
            df = df.drop(columns=['ablation'])
    else:
        df['class'] = df['label']
    drop_columns = []
    if (not self.caption):
        drop_columns.append('caption')
    if (not self.first_column):
        drop_columns.append('col0')
    if (not self.first_row):
        drop_columns.append('row0')
    if (not self.referencing_sections):
        drop_columns.append('sections')
    df = df.drop(columns=drop_columns)
    return df

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    if:
        if:
             ...  =  ... .drop

idx = 12:------------------- similar code ------------------ index = 8, score = 1.0 
def build_prescriptions_table(data_dir, out_dir, conn):
    print('Build prescriptions_table')
    data = pandas.read_csv(os.path.join(data_dir, 'PRESCRIPTIONS.csv'), dtype=str)
    data = data.drop(columns=['ROW_ID', 'GSN', 'DRUG_NAME_POE', 'DRUG_NAME_GENERIC', 'NDC', 'PROD_STRENGTH', 'FORM_VAL_DISP', 'FORM_UNIT_DISP', 'STARTDATE', 'ENDDATE'])
    data = data.dropna(subset=['DOSE_VAL_RX', 'DOSE_UNIT_RX'])
    data['DRUG_DOSE'] = data[['DOSE_VAL_RX', 'DOSE_UNIT_RX']].apply((lambda x: ''.join(x)), axis=1)
    data = data.drop(columns=['DOSE_VAL_RX', 'DOSE_UNIT_RX'])
    print('-- write table')
    data.to_csv(os.path.join(out_dir, 'PRESCRIPTIONS.csv'), sep=',', index=False)
    print('-- write sql')
    data.to_sql('PRESCRIPTIONS', conn, if_exists='replace', index=False)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .drop

idx = 13:------------------- similar code ------------------ index = 35, score = 1.0 
def forward(self, x):
    out = F.relu(self.bn1(x))
    shortcut = (self.shortcut(out) if hasattr(self, 'shortcut') else x)
    out = self.conv1(out)
    if (self.drop is not None):
        out = self.drop(out)
    out = self.conv2(F.relu(self.bn2(out)))
    out += shortcut
    return out

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    if ( ... .drop is not None):
idx = 14:------------------- similar code ------------------ index = 26, score = 1.0 
def main():
    '\n\tMain function\n\n\tMain function that joins the bitcoin and dark web data\n\n\t'
    appName = 'DarkCoinJoin'
    master = config.spark['MASTERIP']
    spark = SparkSession.builder.appName(appName).config('spark.cassandra.connection.host', config.cassandra['HOSTS']).config('spark.cassandra.connection.port', config.cassandra['PORT']).config('spark.cassandra.output.consistency.level', 'ONE').config('spark.kryoserializer.buffer.max', '2047m').config('spark.driver.port', config.cassandra['DRIVERPORT']).config('spark.network.timeout', '10000000').master(master).getOrCreate()
    sqlContext = SQLContext(spark)
    df_bitcoin = spark.read.format('org.apache.spark.sql.cassandra').options(table=config.cassandra['BITCOIN'], keyspace=config.cassandra['KEYSPACE']).load()
    df_bitcoin.registerTempTable('bitcoin')
    df_marketplace = spark.read.format('org.apache.spark.sql.cassandra').options(table=config.cassandra['MARKETPLACE'], keyspace=config.cassandra['KEYSPACE']).load()
    df_marketplace.registerTempTable('marketplace')
    substring = udf((lambda x: x[0:510]), StringType())
    substring2 = udf((lambda x: x[0:99]), StringType())
    df_marketplace = df_marketplace.withColumn('description', substring('description')).withColumn('ship_to', substring2('ship_to')).withColumn('ship_from', substring2('ship_from')).withColumn('category', substring2('category')).withColumn('image_url', substring('image_url')).withColumn('product_name', substring('product_name'))
    date_incr = udf((lambda date, num_days: (date + relativedelta(days=num_days))), TimestampType())
    result = df_bitcoin.join(df_marketplace, (((df_bitcoin.recv_amount == df_marketplace.price) & (df_bitcoin.time > df_marketplace.ad_date)) & (df_bitcoin.time < date_incr(df_marketplace.ad_date, lit(10)))), 'inner').drop(df_bitcoin.recv_amount)
    write_postgres(result)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .drop

idx = 15:------------------- similar code ------------------ index = 7, score = 1.0 
def forward(self, input_):
    ' \n        (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n        '
    return self.drop(self.ff2(gelu(self.ff1(input_))))

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    return  ... .drop

idx = 16:------------------- similar code ------------------ index = 11, score = 1.0 
def _get_dataframe_datatypes(dataframe: pd.DataFrame, partitions=[], use_parts=False) -> dict:
    ' Gets the dtypes of the columns in the dataframe\n\n    Args:\n        dataframe (pd.DataFrame): Dataframe that has been published\n        partitions (list, Optional): List of partition columns\n        use_parts (bool, Optional): bool to determine if only partition datatypes\n            should be returned\n\n    Returns:\n        Dictionary of the column names as keys and dtypes as values,\n            if use_parts is true then only limited to partition columns\n    '
    types = dict()
    if use_parts:
        columns = partitions
    else:
        columns = dataframe.drop(labels=partitions, axis='columns').columns
    for col in columns:
        type_string = dataframe[col].dtype.name
        types[col] = type_string
    return types

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... () ->  ... :
    if  ... :    else:
         ...  =  ... .drop

idx = 17:------------------- similar code ------------------ index = 12, score = 1.0 
def clean_stock_perf(perf):
    perf = perf[(perf.assetCategory == 'STK')]
    perf = perf.drop(['acctAlias', 'assetCategory', 'expiry', 'multiplier', 'putCall', 'strike', 'securityID', 'securityIDType', 'underlyingSymbol', 'underlyingConid'], axis='columns')
    return perf.set_index('symbol')

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  =  ... .drop

idx = 18:------------------- similar code ------------------ index = 21, score = 1.0 
def load_data(self):
    'Load, preprocess and class-balance the credit data.'
    rng = np.random.RandomState(self.seed)
    data = pd.read_csv(self.datapath, index_col=0)
    data.dropna(inplace=True)
    features = data.drop('SeriousDlqin2yrs', axis=1)
    features = preprocessing.scale(features)
    features = np.append(features, np.ones((features.shape[0], 1)), axis=1)
    outcomes = np.array(data['SeriousDlqin2yrs'])
    default_indices = np.where((outcomes == 1))[0]
    other_indices = np.where((outcomes == 0))[0][:10000]
    indices = np.concatenate((default_indices, other_indices))
    features_balanced = features[indices]
    outcomes_balanced = outcomes[indices]
    shape = features_balanced.shape
    shuffled = rng.permutation(len(indices))
    return (features_balanced[shuffled], outcomes_balanced[shuffled])

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  =  ... .drop

idx = 19:------------------- similar code ------------------ index = 20, score = 1.0 
def forward(self, input_):
    ' \n        HT+X(1-T)\n        '
    hh = torch.relu(self.ff1(input_))
    tt = torch.sigmoid(self.ff2(input_))
    return self.drop(((hh * tt) + (input_ * (1 - tt))))

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    return  ... .drop

idx = 20:------------------- similar code ------------------ index = 19, score = 1.0 
def get_marginal_loan_repaid_probs(data_dir='data'):
    'Return object that specifies distn p(Y|X).'
    (all_cdfs, performance, totals) = get_FICO_data(data_dir)
    all_cdfs = all_cdfs.drop(all_cdfs.index[(- 1)])
    performance = performance.drop(performance.index[(- 1)])
    cdfs = all_cdfs[['White', 'Black']]
    repay_B = performance['White']
    repay_A = performance['Black']
    scores = cdfs.index
    group_ratio = np.array((totals['Black'], totals['White']))
    group_size_ratio = (group_ratio / group_ratio.sum())
    (p_A, p_B) = group_size_ratio
    repay_marginal = ((repay_A * p_A) + (repay_B * p_B))
    loan_repaid_probs = _loan_repaid_probs_factory(repay_marginal, scores)
    loan_repaid_probs = [loan_repaid_probs, loan_repaid_probs]
    return loan_repaid_probs

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .drop

idx = 21:------------------- similar code ------------------ index = 9, score = 1.0 
def create_assignment_dataframe(b, reviewer_map, paper_id_map, pool_group='a'):
    '\n    Get the assignment array, generate assignment dataframe\n    '
    assignments = []
    for i in range(len(b)):
        assignments.append([paper_id_map[i], [reviewer_map[b_] for b_ in np.nonzero(b[i])[0]]])
    assignments_df = pd.DataFrame(assignments, columns=['PaperID', 'UserIDs'])
    n_reviewers = len(assignments_df.UserIDs.iloc[0])
    for c in range(n_reviewers):
        assignments_df['UserID_{}_{}'.format(pool_group, (c + 1))] = assignments_df.UserIDs.map((lambda x: x[c]))
    return assignments_df.drop('UserIDs', axis=1)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    return  ... .drop

idx = 22:------------------- similar code ------------------ index = 16, score = 1.0 
def forward(self, x):
    identity = x
    x = self.conv1(F.relu(x))
    x = self.conv2(F.relu(x))
    x = self.conv3(F.relu(x))
    return (self.drop(x) + identity)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    return ( ... .drop +  ... )

idx = 23:------------------- similar code ------------------ index = 34, score = 1.0 
def project_graph(G, to_crs=None):
    '\n    https://github.com/gboeing/osmnx/blob/v0.9/osmnx/projection.py#L126\n    Project a graph from lat-long to the UTM zone appropriate for its geographic\n    location.\n    Parameters\n    ----------\n    G : networkx multidigraph\n        the networkx graph to be projected\n    to_crs : dict\n        if not None, just project to this CRS instead of to UTM\n    Returns\n    -------\n    networkx multidigraph\n    '
    G_proj = G.copy()
    start_time = time.time()
    (nodes, data) = zip(*G_proj.nodes(data=True))
    gdf_nodes = gpd.GeoDataFrame(list(data), index=nodes)
    gdf_nodes.crs = G_proj.graph['crs']
    gdf_nodes.gdf_name = '{}_nodes'.format(G_proj.name)
    gdf_nodes['lon'] = gdf_nodes['x']
    gdf_nodes['lat'] = gdf_nodes['y']
    gdf_nodes['geometry'] = gdf_nodes.apply((lambda row: Point(row['x'], row['y'])), axis=1)
    gdf_nodes_utm = project_gdf(gdf_nodes, to_crs=to_crs)
    edges_with_geom = []
    for (u, v, key, data) in G_proj.edges(keys=True, data=True):
        if ('geometry' in data):
            edges_with_geom.append({'u': u, 'v': v, 'key': key, 'geometry': data['geometry']})
    if (len(edges_with_geom) > 0):
        gdf_edges = gpd.GeoDataFrame(edges_with_geom)
        gdf_edges.crs = G_proj.graph['crs']
        gdf_edges.gdf_name = '{}_edges'.format(G_proj.name)
        gdf_edges_utm = project_gdf(gdf_edges, to_crs=to_crs)
    start_time = time.time()
    gdf_nodes_utm['x'] = gdf_nodes_utm['geometry'].map((lambda point: point.x))
    gdf_nodes_utm['y'] = gdf_nodes_utm['geometry'].map((lambda point: point.y))
    gdf_nodes_utm = gdf_nodes_utm.drop('geometry', axis=1)
    start_time = time.time()
    edges = list(G_proj.edges(keys=True, data=True))
    graph_name = G_proj.graph['name']
    G_proj.clear()
    G_proj.add_nodes_from(gdf_nodes_utm.index)
    attributes = gdf_nodes_utm.to_dict()
    for label in gdf_nodes_utm.columns:
        nx.set_node_attributes(G_proj, name=label, values=attributes[label])
    for (u, v, key, attributes) in edges:
        if ('geometry' in attributes):
            row = gdf_edges_utm[(((gdf_edges_utm['u'] == u) & (gdf_edges_utm['v'] == v)) & (gdf_edges_utm['key'] == key))]
            attributes['geometry'] = row['geometry'].iloc[0]
        G_proj.add_edge(u, v, **attributes)
    G_proj.graph['crs'] = gdf_nodes_utm.crs
    G_proj.graph['name'] = '{}_UTM'.format(graph_name)
    if ('streets_per_node' in G.graph):
        G_proj.graph['streets_per_node'] = G.graph['streets_per_node']
    return G_proj

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .drop

idx = 24:------------------- similar code ------------------ index = 6, score = 1.0 
def build_procedures_table(data_dir, out_dir, conn):
    print('Build procedures_table')
    left = pandas.read_csv(os.path.join(data_dir, 'PROCEDURES_ICD.csv'), dtype=str)
    right = pandas.read_csv(os.path.join(data_dir, 'D_ICD_PROCEDURES.csv'), dtype=str)
    left = left.drop(columns=['ROW_ID', 'SEQ_NUM'])
    right = right.drop(columns=['ROW_ID'])
    out = pandas.merge(left, right, on='ICD9_CODE')
    out = out.sort_values(by='HADM_ID')
    print('-- write table')
    out.to_csv(os.path.join(out_dir, 'PROCEDURES.csv'), sep=',', index=False)
    print('-- write sql')
    out.to_sql('PROCEDURES', conn, if_exists='replace', index=False)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .drop

idx = 25:------------------- similar code ------------------ index = 5, score = 1.0 
def _sample_unevaluated_unique_spoints(self, sample_size: int, max_num_retries: int=100) -> List[dict]:
    self._verify_sample_size(sample_size)
    self._verify_max_num_retries(max_num_retries)
    if (len(self.evaluated_spoints) > 0):
        evaluated_spoints_df = pd.DataFrame(self.evaluated_spoints)
        evaluated_spoints_df = evaluated_spoints_df[self.space.variable_names]
        evaluated_spoints_df = evaluated_spoints_df.drop_duplicates()
        num_unique_evaluated_spoints = len(evaluated_spoints_df)
    else:
        num_unique_evaluated_spoints = 0
    max_sample_size = min(sample_size, (self.space.size - num_unique_evaluated_spoints))
    sampled_spoints = []
    for i in range(max_num_retries):
        sampled_spoints += self._sample_random_spoints(sample_size=sample_size)
        sampled_spoints_df = pd.DataFrame(sampled_spoints)
        sampled_spoints_df = sampled_spoints_df.drop_duplicates()
        if (num_unique_evaluated_spoints > 0):
            sampled_spoints_df = sampled_spoints_df.merge(right=evaluated_spoints_df, on=self.space.variable_names, how='left', indicator=True)
            indicator_left_only = sampled_spoints_df['_merge'].eq('left_only')
            sampled_spoints_df = sampled_spoints_df[indicator_left_only]
            sampled_spoints_df = sampled_spoints_df.drop(columns='_merge')
        sampled_spoints = sampled_spoints_df.to_dict('records')
        if (len(sampled_spoints) >= max_sample_size):
            break
    sampled_spoints = sampled_spoints[:max_sample_size]
    if (len(sampled_spoints) == 0):
        raise RuntimeError(f'''could not sample any new spoints - search_space is fully explored or random sampling was unfortunate.
search_space.size = {self.space.size}
num evaluated spoints = {len(self.evaluated_spoints)}
num unevaluated spoints = {(self.space.size - len(self.evaluated_spoints))}''')
    random.shuffle(sampled_spoints)
    return sampled_spoints

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... () ->:
    for  ...  in:
        if:
             ...  =  ... .drop

idx = 26:------------------- similar code ------------------ index = 39, score = 1.0 
def get_data_args(data_dir='data'):
    'Return objects that specify p(A), p(X|A), p(Y|X,A).'
    (all_cdfs, performance, totals) = get_FICO_data(data_dir)
    all_cdfs = all_cdfs.drop(all_cdfs.index[(- 1)])
    performance = performance.drop(performance.index[(- 1)])
    cdfs = all_cdfs[['White', 'Black']]
    cdf_B = cdfs['White'].values
    cdf_A = cdfs['Black'].values
    repay_B = performance['White']
    repay_A = performance['Black']
    scores = cdfs.index
    scores_list = scores.tolist()
    loan_repaid_probs = [_loan_repaid_probs_factory(repay_A, scores), _loan_repaid_probs_factory(repay_B, scores)]
    inv_cdfs = get_inv_cdf_fns(cdfs)
    pi_A = _get_pmf(cdf_A)
    pi_B = _get_pmf(cdf_B)
    pis = np.vstack([pi_A, pi_B])
    group_ratio = np.array((totals['Black'], totals['White']))
    group_size_ratio = (group_ratio / group_ratio.sum())
    rate_indices = (list(reversed((1 - cdf_A))), list(reversed((1 - cdf_B))))
    return (inv_cdfs, loan_repaid_probs, pis, group_size_ratio, scores_list, rate_indices)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .drop

idx = 27:------------------- similar code ------------------ index = 40, score = 1.0 
def clean(x):
    return x.drop('Total', axis=1)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
    return  ... .drop

idx = 28:------------------- similar code ------------------ index = 4, score = 1.0 
def forward(self, inputs):
    net = inputs.mean(dim=1)
    eval_feas = F.normalize(net, p=2, dim=1)
    net = self.embeding(net)
    net = self.embeding_bn(net)
    net = F.normalize(net, p=2, dim=1)
    net = self.drop(net)
    return (net, eval_feas)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .drop

idx = 29:------------------- similar code ------------------ index = 44, score = 1.0 
def feather_clean(in_directory):
    ' Utility function to clean feather files'
    Path.is_dir(in_directory)
    all_files = os.listdir(in_directory)
    for item in all_files:
        if item.endswith('.feather'):
            option_df = pd.read_feather((in_directory / item))
            idx = (option_df['strike'] == 5)
            option_df = option_df.drop(option_df.index[idx])
            option_df.to_feather(str((in_directory / item)))

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
    for  ...  in  ... :
        if:
             ...  =  ... .drop

idx = 30:------------------- similar code ------------------ index = 15, score = 1.0 
@property
def performance_summary(self):
    'Get simulation performance'
    performance = pf.timeseries.perf_stats(self.returns)
    perf_index = list(performance.index)
    performance = performance['perf_stats']
    (performance['StartDate'], performance['EndDate']) = list(self.simulation_parameters.sim_dates_live[[0, (- 1)]].strftime('%b %d, %Y'))
    (performance['Leverage'], performance['ZScore'], performance['Avg_Days']) = [self.leverage.mean(), self.simulation_parameters.zscore, self.days_2_expiry.mean()]
    performance = performance.reindex((['StartDate', 'EndDate', 'Leverage', 'ZScore', 'Avg_Days'] + perf_index))
    performance = performance.append(self.greeks.mean())
    performance = performance.rename(self.strategy_name)
    performance = performance.to_frame()
    performance = performance.drop(['active_underlying_price_1545'], axis=0)
    return performance

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  =  ... .drop

idx = 31:------------------- similar code ------------------ index = 45, score = 1.0 
def read_atomic_dos_as_df(self, atom_number: int) -> pd.DataFrame:
    assert (atom_number > (0 & atom_number) <= self.number_of_atoms)
    start_to_read = (Doscar.number_of_header_lines + (atom_number * (self.number_of_data_points + 1)))
    df = pd.read_csv(self.filename, skiprows=start_to_read, nrows=self.number_of_data_points, delim_whitespace=True, names=pdos_column_names(lmax=self.lmax, ispin=self.ispin), index_col=False)
    return df.drop('energy', axis=1)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... () ->:
    return  ... .drop

idx = 32:------------------- similar code ------------------ index = 47, score = 1.0 
def build_lab_table(data_dir, out_dir, conn):
    print('Build lab_table')
    cnt = 0
    show_progress(cnt, 4)
    left = pandas.read_csv(os.path.join(data_dir, 'LABEVENTS.csv'), dtype=str)
    cnt += 1
    show_progress(cnt, 4)
    right = pandas.read_csv(os.path.join(data_dir, 'D_LABITEMS.csv'), dtype=str)
    cnt += 1
    show_progress(cnt, 4)
    left = left.dropna(subset=['HADM_ID', 'VALUE', 'VALUEUOM'])
    left = left.drop(columns=['ROW_ID', 'VALUENUM'])
    left['VALUE_UNIT'] = left[['VALUE', 'VALUEUOM']].apply((lambda x: ''.join(x)), axis=1)
    left = left.drop(columns=['VALUE', 'VALUEUOM'])
    right = right.drop(columns=['ROW_ID', 'LOINC_CODE'])
    cnt += 1
    show_progress(cnt, 4)
    out = pandas.merge(left, right, on='ITEMID')
    cnt += 1
    show_progress(cnt, 4)
    print()
    print('-- write table')
    out.to_csv(os.path.join(out_dir, 'LAB.csv'), sep=',', index=False)
    print('-- write sql')
    out.to_sql('LAB', conn, if_exists='replace', index=False)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .drop
    print()

idx = 33:------------------- similar code ------------------ index = 48, score = 1.0 
def rollup_statements(statements, key='mtm_ytd'):

    def clean(x):
        return x.drop('Total', axis=1)
    result = clean(getattr(statements[0], key))
    for stmt in statements[1:]:
        result = result.add(clean(getattr(stmt, key)), fill_value=0)
    result = result.fillna(0)
    result['Total'] = result.sum(1)
    return result

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():

    def  ... ( ... ):
        return  ... .drop

idx = 34:------------------- similar code ------------------ index = 2, score = 1.0 
def forward(self, input_, mask=None):
    '\n        Transformer\n        '
    hd = self.attentionMH(input_, mask)
    hd = self.norm1((input_ + self.drop(hd)))
    hd = self.norm2((hd + self.layer_ff(hd)))
    return self.drop(hd)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... . ... (( ...  +  ... .drop))

idx = 35:------------------- similar code ------------------ index = 51, score = 1.0 
@staticmethod
def _get_user_data(user: pd.DataFrame) -> Dict[(int, Dict[(str, Union[(int, str)])])]:
    '\n        Create dictionary of user stats (e.g., age, occupation, gender)\n        to use as inputs into other functions.\n        '
    tmp_user = user.drop(['zip_code'], axis=1)
    tmp_user.index = tmp_user.user_id
    tmp_user = tmp_user.drop(['user_id'], axis=1)
    return tmp_user.to_dict(orient='index')

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... () ->:
     ...  =  ... .drop

idx = 36:------------------- similar code ------------------ index = 52, score = 1.0 
def perf_stats(returns: pd.Series, **kwargs):
    ' Wrapper function for pf.timeseries.performance'
    performance = pf.timeseries.perf_stats(returns, **kwargs)
    perf_index = list(performance.index)
    (performance['StartDate'], performance['EndDate']) = list(returns.index[[0, (- 1)]].strftime('%b %d, %Y'))
    performance = performance.reindex((['StartDate', 'EndDate'] + perf_index))
    performance = performance.rename(returns.name)
    performance = performance.drop('common_sense_ratio', axis=0)
    return performance

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .drop

