------------------------- example 1 ------------------------ 
def build_torch_optimizer(model, opt):
    'Builds the PyTorch optimizer.\n\n    We use the default parameters for Adam that are suggested by\n    the original paper https://arxiv.org/pdf/1412.6980.pdf\n    These values are also used by other established implementations,\n    e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n    https://keras.io/optimizers/\n    Recently there are slightly different values used in the paper\n    "Attention is all you need"\n    https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98\n    was used there however, beta2=0.999 is still arguably the more\n    established value, so we use that here as well\n\n    Args:\n      model: The model to optimize.\n      opt. The dictionary of options.\n\n    Returns:\n      A ``torch.optim.Optimizer`` instance.\n    '
    params = [p for p in model.parameters() if p.requires_grad]
    betas = [opt.adam_beta1, opt.adam_beta2]
    if (opt.optim == 'sgd'):
        optimizer = optim.SGD(params, lr=opt.learning_rate)
    elif (opt.optim == 'adagrad'):
        optimizer = optim.Adagrad(params, lr=opt.learning_rate, initial_accumulator_value=opt.adagrad_accumulator_init)
    elif (opt.optim == 'adadelta'):
// your code ...
    elif (opt.optim == 'adam'):
        optimizer = optim.Adam(params, lr=opt.learning_rate, betas=betas, eps=1e-09)
    elif (opt.optim == 'sparseadam'):
// your code ...
    if (opt.model_dtype == 'fp16'):
        import apex
        if (opt.optim != 'fusedadam'):
            loss_scale = ('dynamic' if (opt.loss_scale == 0) else opt.loss_scale)
            (model, optimizer) = apex.amp.initialize([model, model.generator], optimizer, opt_level=opt.apex_opt_level, loss_scale=loss_scale, keep_batchnorm_fp32=None)
        else:
// your code ...
    return optimizer

------------------------- example 2 ------------------------ 
def load_optimizer(self):
    if (self.arg.optimizer == 'SGD'):
        self.optimizer = optim.SGD(params=self.model.parameters(), lr=self.arg.base_lr, momentum=0.9, nesterov=self.arg.nesterov, weight_decay=self.arg.weight_decay)
    elif (self.arg.optimizer == 'Adam'):
        self.optimizer = optim.Adam(params=self.model.parameters(), lr=self.arg.base_lr, weight_decay=self.arg.weight_decay)

------------------------- example 3 ------------------------ 
def create_optimizer(model, new_lr):
    if (args.optimizer == 'sgd'):
        optimizer = optim.SGD(model.parameters(), lr=new_lr, momentum=0.9, dampening=0.9, weight_decay=args.wd)
    elif (args.optimizer == 'adam'):
        optimizer = optim.Adam(model.parameters(), lr=new_lr, weight_decay=args.wd)
    else:
        raise Exception('Not supported optimizer: {0}'.format(args.optimizer))
    return optimizer

------------------------- example 4 ------------------------ 
if (__name__ == '__main__'):
    opt = parse_opts()
// your code ...
    (model, parameters) = generate_model(opt)
    print(model)
// your code ...
    if (not opt.no_train):
        assert (opt.train_crop in ['random', 'corner', 'center'])
// your code ...
        optimizer = optim.SGD(parameters, lr=opt.learning_rate, momentum=opt.momentum, dampening=dampening, weight_decay=opt.weight_decay, nesterov=opt.nesterov)
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=opt.lr_patience)
    if (not opt.no_val):
// your code ...
    if opt.resume_path:
        print('loading checkpoint {}'.format(opt.resume_path))
// your code ...
        model.load_state_dict(checkpoint['state_dict'])
        if (not opt.no_train):
// your code ...
    print('run')
    for i in range(opt.begin_epoch, (opt.n_epochs + 1)):
        if (not opt.no_train):
            train_epoch(i, train_loader, model, criterion, optimizer, opt, train_logger, train_batch_logger)
        if (not opt.no_val):
// your code ...
    if opt.test:
        spatial_transform = Compose([Scale(int((opt.sample_size / opt.scale_in_test))), CornerCrop(opt.sample_size, opt.crop_position_in_test), ToTensor(opt.norm_value), norm_method])
// your code ...
        test.test(test_loader, model, opt, test_data.class_names)

------------------------- example 5 ------------------------ 
def create_optimizer(optimizer_config, model, master_params=None):
    'Creates optimizer and schedule from configuration\n\n    Parameters\n    ----------\n    optimizer_config : dict\n        Dictionary containing the configuration options for the optimizer.\n    model : Model\n        The network model.\n\n    Returns\n    -------\n    optimizer : Optimizer\n        The optimizer.\n    scheduler : LRScheduler\n        The learning rate scheduler.\n    '
    if (optimizer_config['classifier_lr'] != (- 1)):
        net_params = []
// your code ...
        for (k, v) in model.named_parameters():
// your code ...
    elif master_params:
// your code ...
    else:
        params = model.parameters()
    if (optimizer_config['type'] == 'SGD'):
        optimizer = optim.SGD(params, lr=optimizer_config['learning_rate'], momentum=optimizer_config['momentum'], weight_decay=optimizer_config['weight_decay'], nesterov=optimizer_config['nesterov'])
    elif (optimizer_config['type'] == 'Adam'):
        optimizer = optim.Adam(params, lr=optimizer_config['learning_rate'], weight_decay=optimizer_config['weight_decay'])
    elif (optimizer_config['type'] == 'AdamW'):
        optimizer = AdamW(params, lr=optimizer_config['learning_rate'], weight_decay=optimizer_config['weight_decay'])
    elif (optimizer_config['type'] == 'RmsProp'):
// your code ...
    if (optimizer_config['schedule']['type'] == 'step'):
        scheduler = lr_scheduler.StepLR(optimizer, **optimizer_config['schedule']['params'])
    elif (optimizer_config['schedule']['type'] == 'multistep'):
        scheduler = lr_scheduler.MultiStepLR(optimizer, **optimizer_config['schedule']['params'])
    elif (optimizer_config['schedule']['type'] == 'exponential'):
// your code ...
    elif (optimizer_config['schedule']['type'] == 'clr'):
        scheduler = CyclicLR(optimizer, **optimizer_config['schedule']['params'])
    elif (optimizer_config['schedule']['type'] == 'constant'):
// your code ...

examples  ||  representativeness  ||  number of lines  || number of comments   ||  relevancy  
example1  ||          5           ||        19         ||         3        ||        0.0         
example2  ||          7           ||        5         ||         0        ||        0.0         
example3  ||          5           ||        8         ||         0        ||        0.0         
example4  ||          5           ||        21         ||         8        ||        0.0         
example5  ||          5           ||        23         ||         6        ||        0.0         

avg       ||          5.346534653465347           ||        15.2         ||         3.4        ||         0.0        

idx = 0:------------------- similar code ------------------ index = 55, score = 6.0 
def make_optimizer(args, my_model):
    trainable = filter((lambda x: x.requires_grad), my_model.parameters())
    if (args.optimizer == 'SGD'):
        optimizer_function = optim.SGD
        kwargs = {'momentum': args.momentum}
    elif (args.optimizer == 'ADAM'):
        optimizer_function = optim.Adam
        kwargs = {'betas': (args.beta1, args.beta2), 'eps': args.epsilon}
    elif (args.optimizer == 'RMSprop'):
        optimizer_function = optim.RMSprop
        kwargs = {'eps': args.epsilon}
    kwargs['lr'] = args.lr
    kwargs['weight_decay'] = args.weight_decay
    return optimizer_function(trainable, **kwargs)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
         ...  = optim.SGD

idx = 1:------------------- similar code ------------------ index = 68, score = 6.0 
def create_optimizer(args, optim_params):
    if (args.optimizer == 'sgd'):
        return optim.SGD(optim_params, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    elif (args.optimizer == 'adagrad'):
        return optim.Adagrad(optim_params, args.lr, weight_decay=args.weight_decay)
    elif (args.optimizer == 'adam'):
        return optim.Adam(optim_params, args.lr, betas=(args.beta1, args.beta2), weight_decay=args.weight_decay)
    elif (args.optimizer == 'amsgrad'):
        return optim.Adam(optim_params, args.lr, betas=(args.beta1, args.beta2), weight_decay=args.weight_decay, amsgrad=True)
    elif (args.optimizer == 'adabound'):
        from adabound import AdaBound
        return AdaBound(optim_params, args.lr, betas=(args.beta1, args.beta2), final_lr=args.final_lr, gamma=args.gamma, weight_decay=args.weight_decay)
    else:
        assert (args.optimizer == 'amsbound')
        from adabound import AdaBound
        return AdaBound(optim_params, args.lr, betas=(args.beta1, args.beta2), final_lr=args.final_lr, gamma=args.gamma, weight_decay=args.weight_decay, amsbound=True)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        return optim.SGD

idx = 2:------------------- similar code ------------------ index = 43, score = 5.0 
def build_torch_optimizer(model, opt):
    'Builds the PyTorch optimizer.\n\n    We use the default parameters for Adam that are suggested by\n    the original paper https://arxiv.org/pdf/1412.6980.pdf\n    These values are also used by other established implementations,\n    e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n    https://keras.io/optimizers/\n    Recently there are slightly different values used in the paper\n    "Attention is all you need"\n    https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98\n    was used there however, beta2=0.999 is still arguably the more\n    established value, so we use that here as well\n\n    Args:\n      model: The model to optimize.\n      opt. The dictionary of options.\n\n    Returns:\n      A ``torch.optim.Optimizer`` instance.\n    '
    params = [p for p in model.parameters() if p.requires_grad]
    betas = [opt.adam_beta1, opt.adam_beta2]
    if (opt.optim == 'sgd'):
        optimizer = optim.SGD(params, lr=opt.learning_rate)
    elif (opt.optim == 'adagrad'):
        optimizer = optim.Adagrad(params, lr=opt.learning_rate, initial_accumulator_value=opt.adagrad_accumulator_init)
    elif (opt.optim == 'adadelta'):
        optimizer = optim.Adadelta(params, lr=opt.learning_rate)
    elif (opt.optim == 'adafactor'):
        optimizer = AdaFactor(params, non_constant_decay=True, enable_factorization=True, weight_decay=0)
    elif (opt.optim == 'adam'):
        optimizer = optim.Adam(params, lr=opt.learning_rate, betas=betas, eps=1e-09)
    elif (opt.optim == 'sparseadam'):
        dense = []
        sparse = []
        for (name, param) in model.named_parameters():
            if (not param.requires_grad):
                continue
            if ('embed' in name):
                sparse.append(param)
            else:
                dense.append(param)
        optimizer = MultipleOptimizer([optim.Adam(dense, lr=opt.learning_rate, betas=betas, eps=1e-08), optim.SparseAdam(sparse, lr=opt.learning_rate, betas=betas, eps=1e-08)])
    elif (opt.optim == 'fusedadam'):
        optimizer = FusedAdam(params, lr=opt.learning_rate, betas=betas)
    else:
        raise ValueError(('Invalid optimizer type: ' + opt.optim))
    if (opt.model_dtype == 'fp16'):
        import apex
        if (opt.optim != 'fusedadam'):
            loss_scale = ('dynamic' if (opt.loss_scale == 0) else opt.loss_scale)
            (model, optimizer) = apex.amp.initialize([model, model.generator], optimizer, opt_level=opt.apex_opt_level, loss_scale=loss_scale, keep_batchnorm_fp32=None)
        else:
            static_loss_scale = opt.loss_scale
            dynamic_loss_scale = (opt.loss_scale == 0)
            optimizer = apex.optimizers.FP16_Optimizer(optimizer, static_loss_scale=static_loss_scale, dynamic_loss_scale=dynamic_loss_scale)
    return optimizer

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = optim.SGD

idx = 3:------------------- similar code ------------------ index = 65, score = 5.0 
def stoch_AID(params: List[Tensor], hparams: List[Tensor], outer_loss: Callable[([List[Tensor], List[Tensor]], Tensor)], K: int, J_inner: int=1, J_outer: int=1, fp_map: Union[(Callable[([List[Tensor], List[Tensor]], List[Tensor])], None)]=None, inner_loss: Union[(Callable[([List[Tensor], List[Tensor]], Tensor)], None)]=None, linsys_start: Union[(List[Tensor], None)]=None, stoch_outer: bool=False, stoch_inner: bool=False, optim_build: Union[(Callable[(..., Tuple[(Optimizer, Any)])], None)]=None, opt_params: dict=None, set_grad: bool=True, verbose: bool=True):
    '\n    Computes the hypergradient by solving the linear system by applying K steps of the optimizer output of the optim_build function,\n    this should be a torch.optim.Optimizer. optim_build (optionally) returns also a scheduler whose step()\n    method is called after every iteration of the optimizer.\n\n    Args:\n        params: the output of the inner solver procedure.\n        hparams: the outer variables (or hyperparameters), each element needs requires_grad=True\n        outer_loss: computes the outer objective taking parameters and hyperparameters as inputs\n        K: the number of iteration of the LS solver which is given as output of optim_build\n        J_inner: the minibatch size used to compute the jacobian w.r.t. hparams of fp_map\n        J_outer: the minibatch size used to compute the gradient w.r.t. params and hparams  of outer_loss\n        fp_map: the fixed point map which defines the inner problem, used if inner_loss is None\n        inner_loss: the loss of the inner problem, used if fp_map is None\n        linsys_start: starting point of the linear system, set to the 0 vector if None\n        stoch_outer: set to True if outer_loss is stochastic, otherwise set to False\n        stoch_inner: set to True if fp_map or inner_loss is stochastic, otherwise False\n        optim_build: function used to obtain the linear system optimizer\n        opt_params: parameters of he linear system optimizer (input of optim_build)\n        set_grad: if True set t.grad to the hypergradient for every t in hparams\n        verbose: print the distance between two consecutive iterates for the linear system.\n    Returns:\n        the list of hypergradients for each element in hparams\n    '
    assert (stoch_inner or (J_inner == 1))
    assert (stoch_outer or (J_outer == 1))
    params = [w.detach().clone().requires_grad_(True) for w in params]
    if (fp_map is not None):
        w_update_f = fp_map

        def v_update(v, jtv, g):
            return ((v - jtv) - g)
    elif (inner_loss is not None):

        def w_update_f(params, hparams):
            return torch.autograd.grad((- inner_loss(params, hparams)), params, create_graph=True)

        def v_update(v, jtv, g):
            return ((- jtv) - g)
    else:
        raise NotImplementedError('Either fp_map or inner loss should be not None')
    o_loss = outer_loss(params, hparams)
    (grad_outer_w, grad_outer_hparams) = get_outer_gradients(o_loss, params, hparams, retain_graph=False)
    if stoch_outer:
        for _ in range((J_outer - 1)):
            o_loss = outer_loss(params, hparams)
            (grad_outer_w_1, grad_outer_hparams_1) = get_outer_gradients(o_loss, params, hparams, retain_graph=False)
            for (g, g1) in zip(grad_outer_w, grad_outer_w_1):
                g += g1
            for (g, g1) in zip(grad_outer_hparams, grad_outer_hparams_1):
                g += g1
        for g in grad_outer_w:
            g /= J_outer
        for g in grad_outer_hparams:
            g /= J_outer
    if stoch_inner:

        def w_updated():
            return w_update_f(params, hparams)
    else:
        w_new = w_update_f(params, hparams)

        def w_updated():
            return w_new

    def compute_and_set_grads(vs):
        Jfp_mapTv = torch_grad(w_updated(), params, grad_outputs=vs, retain_graph=(not stoch_inner))
        for (v, jtv, g) in zip(vs, Jfp_mapTv, grad_outer_w):
            v.grad = torch.zeros_like(v)
            v.grad += v_update(v, jtv, g)
    if (linsys_start is not None):
        vparams = [l.detach().clone() for l in linsys_start]
    else:
        vparams = [gw.detach().clone() for gw in grad_outer_w]
    if (optim_build is None):
        optim = torch.optim.SGD(vparams, lr=1.0)
        scheduler = None
    elif (opt_params is None):
        (optim, scheduler) = optim_build(vparams)
    else:
        (optim, scheduler) = optim_build(vparams, **opt_params)
    for i in range(K):
        vparams_prev = [v.detach().clone() for v in vparams]
        optim.zero_grad()
        compute_and_set_grads(vparams)
        optim.step()
        if scheduler:
            scheduler.step()
        if (verbose and ((K < 5) or (((i % (K // 5)) == 0) or (i == (K - 1))))):
            print(f'k={i}: linsys, ||v - v_prev|| = {[torch.norm((v - v_prev)).item() for (v, v_prev) in zip(vparams, vparams_prev)]}')
    if any([(torch.isnan(torch.norm(v)) or torch.isinf(torch.norm(v))) for v in vparams]):
        raise ValueError("Hypergradient's linear system diverged!")
    grads_indirect = [torch.zeros_like(g) for g in hparams]
    for i in range(J_inner):
        retain_graph = ((not stoch_inner) and (i < (J_inner - 1)))
        djac_wrt_lambda = torch_grad(w_updated(), hparams, grad_outputs=vparams, retain_graph=retain_graph, allow_unused=True)
        for (g, g1) in zip(grads_indirect, djac_wrt_lambda):
            if (g1 is not None):
                g += (g1 / J_inner)
    grads = [(g + v) for (g, v) in zip(grad_outer_hparams, grads_indirect)]
    if set_grad:
        update_tensor_grads(hparams, grads)
    return (grads, vparams)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
        optim =  ... .SGD

idx = 4:------------------- similar code ------------------ index = 73, score = 5.0 
def load_optimizer(self):
    if (self.arg.optimizer == 'SGD'):
        self.optimizer = optim.SGD(params=self.model.parameters(), lr=self.arg.base_lr, momentum=0.9, nesterov=self.arg.nesterov, weight_decay=self.arg.weight_decay)
    elif (self.arg.optimizer == 'Adam'):
        self.optimizer = optim.Adam(params=self.model.parameters(), lr=self.arg.base_lr, weight_decay=self.arg.weight_decay)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:
 = optim.SGD

idx = 5:------------------- similar code ------------------ index = 77, score = 5.0 
def build_torch_optimizer(model, opt):
    'Builds the PyTorch optimizer.\n    Input:\n        model: The model to optimize.\n        opt: The dictionary of options.\n    Output:\n        A ``torch.optim.Optimizer`` instance.\n    '
    params = list(filter((lambda p: p.requires_grad), model.parameters()))
    betas = [0.9, 0.999]
    if (opt.optim == 'sgd'):
        optimizer = optim.SGD(params, lr=opt.learning_rate)
    elif (opt.optim == 'adagrad'):
        optimizer = optim.Adagrad(params, lr=opt.learning_rate, initial_accumulator_value=opt.adagrad_accumulator_init)
    elif (opt.optim == 'adadelta'):
        optimizer = optim.Adadelta(params, lr=opt.learning_rate)
    elif (opt.optim == 'adam'):
        optimizer = optim.Adam(params, lr=opt.learning_rate, betas=betas, eps=1e-09)
    elif (opt.optim == 'fusedadam'):
        import apex
        optimizer = apex.optimizers.FusedAdam(params, lr=opt.learning_rate, betas=betas)
    else:
        raise ValueError(('Invalid optimizer type: ' + opt.optim))
    return {'optim': optimizer, 'para': params}

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = optim.SGD

idx = 6:------------------- similar code ------------------ index = 15, score = 5.0 
def create_optimizer(model, new_lr):
    if (args.optimizer == 'sgd'):
        optimizer = optim.SGD(model.parameters(), lr=new_lr, momentum=0.9, dampening=0.9, weight_decay=args.wd)
    elif (args.optimizer == 'adam'):
        optimizer = optim.Adam(model.parameters(), lr=new_lr, weight_decay=args.wd)
    else:
        raise Exception('Not supported optimizer: {0}'.format(args.optimizer))
    return optimizer

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = optim.SGD

idx = 7:------------------- similar code ------------------ index = 79, score = 5.0 
def create_optimizer(model, new_lr):
    if (args.optimizer == 'sgd'):
        optimizer = optim.SGD(model.parameters(), lr=new_lr, momentum=0.9, dampening=0.9, weight_decay=args.wd)
    elif (args.optimizer == 'adam'):
        optimizer = optim.Adam(model.parameters(), lr=new_lr, weight_decay=args.wd)
    else:
        raise Exception('Not supported optimizer: {0}'.format(args.optimizer))
    return optimizer

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = optim.SGD

idx = 8:------------------- similar code ------------------ index = 14, score = 5.0 
def load_optimizer(self):
    if (self.arg.optimizer == 'SGD'):
        self.optimizer = optim.SGD(params=self.model.parameters(), lr=self.arg.base_lr, momentum=0.9, nesterov=self.arg.nesterov, weight_decay=self.arg.weight_decay)
    elif (self.arg.optimizer == 'Adam'):
        self.optimizer = optim.Adam(params=self.model.parameters(), lr=self.arg.base_lr, weight_decay=self.arg.weight_decay)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:
 = optim.SGD

idx = 9:------------------- similar code ------------------ index = 72, score = 5.0 
def load_optimizer(self):
    if (self.arg.optimizer == 'SGD'):
        self.optimizer = optim.SGD(params=self.model.parameters(), lr=self.arg.base_lr, momentum=0.9, nesterov=self.arg.nesterov, weight_decay=self.arg.weight_decay)
    elif (self.arg.optimizer == 'Adam'):
        self.optimizer = optim.Adam(params=self.model.parameters(), lr=self.arg.base_lr, weight_decay=self.arg.weight_decay)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:
 = optim.SGD

idx = 10:------------------- similar code ------------------ index = 12, score = 5.0 
if (__name__ == '__main__'):
    opt = parse_opts()
    if (opt.root_path != ''):
        opt.video_path = os.path.join(opt.root_path, opt.video_path)
        opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)
        opt.result_path = os.path.join(opt.root_path, opt.result_path)
        if opt.resume_path:
            opt.resume_path = os.path.join(opt.root_path, opt.resume_path)
        if opt.pretrain_path:
            opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)
    opt.scales = [opt.initial_scale]
    for i in range(1, opt.n_scales):
        opt.scales.append((opt.scales[(- 1)] * opt.scale_step))
    opt.arch = '{}-{}'.format(opt.model, opt.model_depth)
    opt.mean = get_mean(opt.norm_value, dataset=opt.mean_dataset)
    opt.std = get_std(opt.norm_value)
    print(opt)
    torch.manual_seed(opt.manual_seed)
    (model, parameters) = generate_model(opt)
    print(model)
    criterion = nn.CrossEntropyLoss()
    if (not opt.no_cuda):
        criterion = criterion.cuda()
    if (opt.no_mean_norm and (not opt.std_norm)):
        norm_method = Normalize([0, 0, 0], [1, 1, 1])
    elif (not opt.std_norm):
        norm_method = Normalize(opt.mean, [1, 1, 1])
    else:
        norm_method = Normalize(opt.mean, opt.std)
    if (not opt.no_train):
        assert (opt.train_crop in ['random', 'corner', 'center'])
        if (opt.train_crop == 'random'):
            crop_method = MultiScaleRandomCrop(opt.scales, opt.sample_size)
        elif (opt.train_crop == 'corner'):
            crop_method = MultiScaleCornerCrop(opt.scales, opt.sample_size)
        elif (opt.train_crop == 'center'):
            crop_method = MultiScaleCornerCrop(opt.scales, opt.sample_size, crop_positions=['c'])
        spatial_transform = Compose([crop_method, RandomHorizontalFlip(), ToTensor(opt.norm_value), norm_method])
        temporal_transform = TemporalRandomCrop(opt.sample_duration)
        target_transform = ClassLabel()
        training_data = get_training_set(opt, spatial_transform, temporal_transform, target_transform)
        train_loader = torch.utils.data.DataLoader(training_data, batch_size=opt.batch_size, shuffle=True, num_workers=opt.n_threads, pin_memory=True)
        train_logger = Logger(os.path.join(opt.result_path, 'train.log'), ['epoch', 'loss', 'acc', 'lr'])
        train_batch_logger = Logger(os.path.join(opt.result_path, 'train_batch.log'), ['epoch', 'batch', 'iter', 'loss', 'acc', 'lr'])
        if opt.nesterov:
            dampening = 0
        else:
            dampening = opt.dampening
        optimizer = optim.SGD(parameters, lr=opt.learning_rate, momentum=opt.momentum, dampening=dampening, weight_decay=opt.weight_decay, nesterov=opt.nesterov)
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=opt.lr_patience)
    if (not opt.no_val):
        spatial_transform = Compose([Scale(opt.sample_size), CenterCrop(opt.sample_size), ToTensor(opt.norm_value), norm_method])
        temporal_transform = LoopPadding(opt.sample_duration)
        target_transform = ClassLabel()
        validation_data = get_validation_set(opt, spatial_transform, temporal_transform, target_transform)
        val_loader = torch.utils.data.DataLoader(validation_data, batch_size=16, shuffle=False, num_workers=opt.n_threads, pin_memory=True)
        val_logger = Logger(os.path.join(opt.result_path, 'val.log'), ['epoch', 'loss', 'acc'])
    if opt.resume_path:
        print('loading checkpoint {}'.format(opt.resume_path))
        checkpoint = torch.load(opt.resume_path)
        assert (opt.arch == checkpoint['arch'])
        opt.begin_epoch = checkpoint['epoch']
        model.load_state_dict(checkpoint['state_dict'])
        if (not opt.no_train):
            optimizer.load_state_dict(checkpoint['optimizer'])
    print('run')
    for i in range(opt.begin_epoch, (opt.n_epochs + 1)):
        if (not opt.no_train):
            train_epoch(i, train_loader, model, criterion, optimizer, opt, train_logger, train_batch_logger)
        if (not opt.no_val):
            validation_loss = val_epoch(i, val_loader, model, criterion, opt, val_logger)
        if ((not opt.no_train) and (not opt.no_val)):
            scheduler.step(validation_loss)
    if opt.test:
        spatial_transform = Compose([Scale(int((opt.sample_size / opt.scale_in_test))), CornerCrop(opt.sample_size, opt.crop_position_in_test), ToTensor(opt.norm_value), norm_method])
        temporal_transform = LoopPadding(opt.sample_duration)
        target_transform = VideoID()
        test_data = get_test_set(opt, spatial_transform, temporal_transform, target_transform)
        test_loader = torch.utils.data.DataLoader(test_data, batch_size=opt.batch_size, shuffle=False, num_workers=opt.n_threads, pin_memory=True)
        test.test(test_loader, model, opt, test_data.class_names)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
    if:
         ...  = optim.SGD

idx = 11:------------------- similar code ------------------ index = 20, score = 5.0 
def create_optimizer(optimizer_config, model, master_params=None):
    'Creates optimizer and schedule from configuration\n\n    Parameters\n    ----------\n    optimizer_config : dict\n        Dictionary containing the configuration options for the optimizer.\n    model : Model\n        The network model.\n\n    Returns\n    -------\n    optimizer : Optimizer\n        The optimizer.\n    scheduler : LRScheduler\n        The learning rate scheduler.\n    '
    if (optimizer_config['classifier_lr'] != (- 1)):
        net_params = []
        classifier_params = []
        for (k, v) in model.named_parameters():
            if (not v.requires_grad):
                continue
            if (k.find('encoder') != (- 1)):
                net_params.append(v)
            else:
                classifier_params.append(v)
        params = [{'params': net_params}, {'params': classifier_params, 'lr': optimizer_config['classifier_lr']}]
    elif master_params:
        params = master_params
    else:
        params = model.parameters()
    if (optimizer_config['type'] == 'SGD'):
        optimizer = optim.SGD(params, lr=optimizer_config['learning_rate'], momentum=optimizer_config['momentum'], weight_decay=optimizer_config['weight_decay'], nesterov=optimizer_config['nesterov'])
    elif (optimizer_config['type'] == 'Adam'):
        optimizer = optim.Adam(params, lr=optimizer_config['learning_rate'], weight_decay=optimizer_config['weight_decay'])
    elif (optimizer_config['type'] == 'AdamW'):
        optimizer = AdamW(params, lr=optimizer_config['learning_rate'], weight_decay=optimizer_config['weight_decay'])
    elif (optimizer_config['type'] == 'RmsProp'):
        optimizer = optim.Adam(params, lr=optimizer_config['learning_rate'], weight_decay=optimizer_config['weight_decay'])
    else:
        raise KeyError('unrecognized optimizer {}'.format(optimizer_config['type']))
    if (optimizer_config['schedule']['type'] == 'step'):
        scheduler = lr_scheduler.StepLR(optimizer, **optimizer_config['schedule']['params'])
    elif (optimizer_config['schedule']['type'] == 'multistep'):
        scheduler = lr_scheduler.MultiStepLR(optimizer, **optimizer_config['schedule']['params'])
    elif (optimizer_config['schedule']['type'] == 'exponential'):
        scheduler = lr_scheduler.ExponentialLR(optimizer, **optimizer_config['schedule']['params'])
    elif (optimizer_config['schedule']['type'] == 'poly'):
        scheduler = PolyLR(optimizer, **optimizer_config['schedule']['params'])
    elif (optimizer_config['schedule']['type'] == 'clr'):
        scheduler = CyclicLR(optimizer, **optimizer_config['schedule']['params'])
    elif (optimizer_config['schedule']['type'] == 'constant'):
        scheduler = lr_scheduler.LambdaLR(optimizer, (lambda epoch: 1.0))
    elif (optimizer_config['schedule']['type'] == 'linear'):

        def linear_lr(it):
            return ((it * optimizer_config['schedule']['params']['alpha']) + optimizer_config['schedule']['params']['beta'])
        scheduler = lr_scheduler.LambdaLR(optimizer, linear_lr)
    return (optimizer, scheduler)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = optim.SGD

idx = 12:------------------- similar code ------------------ index = 10, score = 5.0 
def main():
    maybe_install_wordnet()
    datasets = ((data.cifar.names + data.imagenet.names) + data.custom.names)
    parser = argparse.ArgumentParser(description='PyTorch CIFAR Training')
    parser.add_argument('--batch-size', default=512, type=int, help='Batch size used for training')
    parser.add_argument('--epochs', '-e', default=200, type=int, help='By default, lr schedule is scaled accordingly')
    parser.add_argument('--dataset', default='CIFAR10', choices=datasets)
    parser.add_argument('--arch', default='ResNet18', choices=list(models.get_model_choices()))
    parser.add_argument('--lr', default=0.1, type=float, help='learning rate')
    parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')
    parser.add_argument('--path-resume', default='', help='Overrides checkpoint path generation')
    parser.add_argument('--name', default='', help='Name of experiment. Used for checkpoint filename')
    parser.add_argument('--pretrained', action='store_true', help='Download pretrained model. Not all models support this.')
    parser.add_argument('--eval', help='eval only', action='store_true')
    parser.add_argument('--dataset-test', choices=datasets, help='If not set, automatically set to train dataset')
    parser.add_argument('--disable-test-eval', help='Allows you to run model inference on a test dataset  different from train dataset. Use an anlayzer to define a metric.', action='store_true')
    parser.add_argument('--loss', choices=loss.names, default=['CrossEntropyLoss'], nargs='+')
    parser.add_argument('--metric', choices=metrics.names, default='top1')
    parser.add_argument('--analysis', choices=analysis.names, help='Run analysis after each epoch')
    data.custom.add_arguments(parser)
    T.add_arguments(parser)
    loss.add_arguments(parser)
    analysis.add_arguments(parser)
    args = parser.parse_args()
    loss.set_default_values(args)
    device = ('cuda' if torch.cuda.is_available() else 'cpu')
    best_acc = 0
    start_epoch = 0
    print('==> Preparing data..')
    dataset_train = getattr(data, args.dataset)
    dataset_test = getattr(data, (args.dataset_test or args.dataset))
    transform_train = dataset_train.transform_train()
    transform_test = dataset_test.transform_val()
    dataset_train_kwargs = generate_kwargs(args, dataset_train, name=f'Dataset {dataset_train.__class__.__name__}', globals=locals())
    dataset_test_kwargs = generate_kwargs(args, dataset_test, name=f'Dataset {dataset_test.__class__.__name__}', globals=locals())
    trainset = dataset_train(**dataset_train_kwargs, root='./data', train=True, download=True, transform=transform_train)
    testset = dataset_test(**dataset_test_kwargs, root='./data', train=False, download=True, transform=transform_test)
    assert ((trainset.classes == testset.classes) or args.disable_test_eval), (trainset.classes, testset.classes)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=2)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)
    Colors.cyan(f'Training with dataset {args.dataset} and {len(trainset.classes)} classes')
    Colors.cyan(f'Testing with dataset {(args.dataset_test or args.dataset)} and {len(testset.classes)} classes')
    print('==> Building model..')
    model = getattr(models, args.arch)
    if args.pretrained:
        print('==> Loading pretrained model..')
        model = make_kwarg_optional(model, dataset=args.dataset)
        net = model(pretrained=True, num_classes=len(trainset.classes))
    else:
        net = model(num_classes=len(trainset.classes))
    net = net.to(device)
    if (device == 'cuda'):
        net = torch.nn.DataParallel(net)
        cudnn.benchmark = True
    checkpoint_fname = generate_checkpoint_fname(**vars(args))
    checkpoint_path = './checkpoint/{}.pth'.format(checkpoint_fname)
    print(f'==> Checkpoints will be saved to: {checkpoint_path}')
    resume_path = (args.path_resume or checkpoint_path)
    if args.resume:
        print('==> Resuming from checkpoint..')
        assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'
        if (not os.path.exists(resume_path)):
            print('==> No checkpoint found. Skipping...')
        else:
            checkpoint = torch.load(resume_path, map_location=torch.device(device))
            if ('net' in checkpoint):
                load_state_dict(net, checkpoint['net'])
                best_acc = checkpoint['acc']
                start_epoch = checkpoint['epoch']
                Colors.cyan(f'==> Checkpoint found for epoch {start_epoch} with accuracy {best_acc} at {resume_path}')
            else:
                load_state_dict(net, checkpoint)
                Colors.cyan(f'==> Checkpoint found at {resume_path}')
    tree = Tree.create_from_args(args, classes=trainset.classes)
    criterion = None
    for _loss in args.loss:
        if ((criterion is None) and (not hasattr(nn, _loss))):
            criterion = nn.CrossEntropyLoss()
        class_criterion = getattr(loss, _loss)
        loss_kwargs = generate_kwargs(args, class_criterion, name=f'Loss {args.loss}', globals=locals())
        criterion = class_criterion(**loss_kwargs)
    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=0.0005)
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(((3 / 7.0) * args.epochs)), int(((5 / 7.0) * args.epochs))])
    class_analysis = getattr(analysis, (args.analysis or 'Noop'))
    analyzer_kwargs = generate_kwargs(args, class_analysis, name=f'Analyzer {args.analysis}', globals=locals())
    analyzer = class_analysis(**analyzer_kwargs)
    metric = getattr(metrics, args.metric)()

    @analyzer.train_function
    def train(epoch):
        if hasattr(criterion, 'set_epoch'):
            criterion.set_epoch(epoch, args.epochs)
        print(('\nEpoch: %d / LR: %.04f' % (epoch, scheduler.get_last_lr()[0])))
        net.train()
        train_loss = 0
        metric.clear()
        for (batch_idx, (inputs, targets)) in enumerate(trainloader):
            (inputs, targets) = (inputs.to(device), targets.to(device))
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            metric.forward(outputs, targets)
            transform = trainset.transform_val_inverse().to(device)
            stat = analyzer.update_batch(outputs, targets, transform(inputs))
            progress_bar(batch_idx, len(trainloader), ('Loss: %.3f | Acc: %.3f%% (%d/%d) %s' % ((train_loss / (batch_idx + 1)), (100.0 * metric.report()), metric.correct, metric.total, (f'| {analyzer.name}: {stat}' if stat else ''))))
        scheduler.step()

    @analyzer.test_function
    def test(epoch, checkpoint=True):
        nonlocal best_acc
        net.eval()
        test_loss = 0
        metric.clear()
        with torch.no_grad():
            for (batch_idx, (inputs, targets)) in enumerate(testloader):
                (inputs, targets) = (inputs.to(device), targets.to(device))
                outputs = net(inputs)
                if (not args.disable_test_eval):
                    loss = criterion(outputs, targets)
                    test_loss += loss.item()
                    metric.forward(outputs, targets)
                transform = testset.transform_val_inverse().to(device)
                stat = analyzer.update_batch(outputs, targets, transform(inputs))
                progress_bar(batch_idx, len(testloader), ('Loss: %.3f | Acc: %.3f%% (%d/%d) %s' % ((test_loss / (batch_idx + 1)), (100.0 * metric.report()), metric.correct, metric.total, (f'| {analyzer.name}: {stat}' if stat else ''))))
        acc = (100.0 * metric.report())
        print('Accuracy: {}, {}/{} | Best Accurracy: {}'.format(acc, metric.correct, metric.total, best_acc))
        if ((acc > best_acc) and checkpoint):
            Colors.green(f'Saving to {checkpoint_fname} ({acc})..')
            state = {'net': net.state_dict(), 'acc': acc, 'epoch': epoch}
            os.makedirs('checkpoint', exist_ok=True)
            torch.save(state, f'./checkpoint/{checkpoint_fname}.pth')
            best_acc = acc
    if (args.disable_test_eval and ((not args.analysis) or (args.analysis == 'Noop'))):
        Colors.red(' * Warning: `disable_test_eval` is used but no custom metric `--analysis` is supplied. I suggest supplying an analysis to perform  custom loss and accuracy calculation.')
    if args.eval:
        if ((not args.resume) and (not args.pretrained)):
            Colors.red(' * Warning: Model is not loaded from checkpoint. Use --resume or --pretrained (if supported)')
        with analyzer.epoch_context(0):
            test(0, checkpoint=False)
    else:
        for epoch in range(start_epoch, args.epochs):
            with analyzer.epoch_context(epoch):
                train(epoch)
                test(epoch)
    print(f'Best accuracy: {best_acc} // Checkpoint name: {checkpoint_fname}')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = optim.SGD

idx = 13:------------------- similar code ------------------ index = 62, score = 5.0 
def get_opt(model, args):
    if (args.opt == 'adam'):
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif (args.opt == 'sgd'):
        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.epoch, gamma=args.gamma)
    return (optimizer, scheduler)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:    elif:
         ...  = optim.SGD

idx = 14:------------------- similar code ------------------ index = 18, score = 5.0 
def CreateModel(args):
    if (args.model == 'DeepLab'):
        model = Deeplab(num_classes=args.num_classes, init_weights=args.init_weights, restore_from=args.restore_from, phase=args.set)
        if (args.set == 'train'):
            optimizer = optim.SGD(model.optim_parameters(args), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)
            optimizer.zero_grad()
            return (model, optimizer)
        else:
            return model
    if (args.model == 'VGG'):
        model = VGG16_FCN8s(num_classes=19, init_weights=args.init_weights, restore_from=args.restore_from)
        if (args.set == 'train'):
            optimizer = optim.Adam([{'params': model.get_parameters(bias=False)}, {'params': model.get_parameters(bias=True), 'lr': (args.learning_rate * 2)}], lr=args.learning_rate, betas=(0.9, 0.99))
            optimizer.zero_grad()
            return (model, optimizer)
        else:
            return model

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:
        if:
             ...  = optim.SGD

idx = 15:------------------- similar code ------------------ index = 30, score = 5.0 
def train_and_eval(tag, dataroot, test_ratio=0.0, cv_fold=0, reporter=None, metric='last', save_path=None, only_eval=False, local_rank=(- 1), evaluation_interval=5):
    total_batch = C.get()['batch']
    if (local_rank >= 0):
        dist.init_process_group(backend='nccl', init_method='env://', world_size=int(os.environ['WORLD_SIZE']))
        device = torch.device('cuda', local_rank)
        torch.cuda.set_device(device)
        C.get()['lr'] *= dist.get_world_size()
        logger.info(f"local batch={C.get()['batch']} world_size={dist.get_world_size()} ----> total batch={(C.get()['batch'] * dist.get_world_size())}")
        total_batch = (C.get()['batch'] * dist.get_world_size())
    is_master = ((local_rank < 0) or (dist.get_rank() == 0))
    if is_master:
        add_filehandler(logger, ('master' + '.log'))
    if (not reporter):
        reporter = (lambda **kwargs: 0)
    max_epoch = C.get()['epoch']
    (trainsampler, trainloader, validloader, testloader_) = get_dataloaders(C.get()['dataset'], C.get()['batch'], dataroot, test_ratio, split_idx=cv_fold, multinode=(local_rank >= 0))
    model = get_model(C.get()['model'], num_class(C.get()['dataset']), local_rank=local_rank)
    model_ema = get_model(C.get()['model'], num_class(C.get()['dataset']), local_rank=(- 1))
    model_ema.eval()
    criterion_ce = criterion = CrossEntropyLabelSmooth(num_class(C.get()['dataset']), C.get().conf.get('lb_smooth', 0))
    if (C.get().conf.get('mixup', 0.0) > 0.0):
        criterion = CrossEntropyMixUpLabelSmooth(num_class(C.get()['dataset']), C.get().conf.get('lb_smooth', 0))
    if (C.get()['optimizer']['type'] == 'sgd'):
        optimizer = optim.SGD(model.parameters(), lr=C.get()['lr'], momentum=C.get()['optimizer'].get('momentum', 0.9), weight_decay=0.0, nesterov=C.get()['optimizer'].get('nesterov', True))
    elif (C.get()['optimizer']['type'] == 'rmsprop'):
        optimizer = RMSpropTF(model.parameters(), lr=C.get()['lr'], weight_decay=0.0, alpha=0.9, momentum=0.9, eps=0.001)
    else:
        raise ValueError(('invalid optimizer type=%s' % C.get()['optimizer']['type']))
    lr_scheduler_type = C.get()['lr_schedule'].get('type', 'cosine')
    if (lr_scheduler_type == 'cosine'):
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=C.get()['epoch'], eta_min=0.0)
    elif (lr_scheduler_type == 'resnet'):
        scheduler = adjust_learning_rate_resnet(optimizer)
    elif (lr_scheduler_type == 'efficientnet'):
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=(lambda x: (0.97 ** int(((x + C.get()['lr_schedule']['warmup']['epoch']) / 2.4)))))
    else:
        raise ValueError(('invalid lr_schduler=%s' % lr_scheduler_type))
    if (C.get()['lr_schedule'].get('warmup', None) and (C.get()['lr_schedule']['warmup']['epoch'] > 0)):
        scheduler = GradualWarmupScheduler(optimizer, multiplier=C.get()['lr_schedule']['warmup']['multiplier'], total_epoch=C.get()['lr_schedule']['warmup']['epoch'], after_scheduler=scheduler)
    if ((not tag) or (not is_master)):
        from FastAutoAugment.metrics import SummaryWriterDummy as SummaryWriter
        logger.warning('tag not provided, no tensorboard log.')
    else:
        from tensorboardX import SummaryWriter
    writers = [SummaryWriter(log_dir=('./logs/%s/%s' % (tag, x))) for x in ['train', 'valid', 'test']]
    if ((C.get()['optimizer']['ema'] > 0.0) and is_master):
        ema = EMA(C.get()['optimizer']['ema'])
    else:
        ema = None
    result = OrderedDict()
    epoch_start = 1
    if (save_path != 'test.pth'):
        if (save_path and (not os.path.exists(save_path))):
            import torch.utils.model_zoo as model_zoo
            data = model_zoo.load_url('https://download.pytorch.org/models/resnet50-19c8e357.pth', model_dir=os.path.join(os.getcwd(), 'FastAutoAugment/models'))
            if (C.get()['dataset'] == 'cifar10'):
                data.pop('fc.weight')
                data.pop('fc.bias')
                model_dict = model.state_dict()
                model_dict.update(data)
                model.load_state_dict(model_dict)
                torch.save(model_dict, save_path)
        logger.info(('%s file found. loading...' % save_path))
        data = torch.load(save_path)
        key = ('model' if ('model' in data) else 'state_dict')
        if ('epoch' not in data):
            model.load_state_dict(data)
        else:
            logger.info(('checkpoint epoch@%d' % data['epoch']))
            if (not isinstance(model, (DataParallel, DistributedDataParallel))):
                model.load_state_dict({k.replace('module.', ''): v for (k, v) in data[key].items()})
            else:
                model.load_state_dict({(k if ('module.' in k) else ('module.' + k)): v for (k, v) in data[key].items()})
            logger.info('optimizer.load_state_dict+')
            optimizer.load_state_dict(data['optimizer'])
            if (data['epoch'] < C.get()['epoch']):
                epoch_start = data['epoch']
            else:
                only_eval = True
            if (ema is not None):
                ema.shadow = (data.get('ema', {}) if isinstance(data.get('ema', {}), dict) else data['ema'].state_dict())
        del data
    if (local_rank >= 0):
        for (name, x) in model.state_dict().items():
            dist.broadcast(x, 0)
        logger.info(f'multinode init. local_rank={dist.get_rank()} is_master={is_master}')
        torch.cuda.synchronize()
    tqdm_disabled = (bool(os.environ.get('TASK_NAME', '')) and (local_rank != 0))
    if only_eval:
        logger.info('evaluation only+')
        model.eval()
        rs = dict()
        rs['train'] = run_epoch(model, trainloader, criterion, None, desc_default='train', epoch=0, writer=writers[0], is_master=is_master)
        with torch.no_grad():
            rs['valid'] = run_epoch(model, validloader, criterion, None, desc_default='valid', epoch=0, writer=writers[1], is_master=is_master)
            rs['test'] = run_epoch(model, testloader_, criterion, None, desc_default='*test', epoch=0, writer=writers[2], is_master=is_master)
            if ((ema is not None) and (len(ema) > 0)):
                model_ema.load_state_dict({k.replace('module.', ''): v for (k, v) in ema.state_dict().items()})
                rs['valid'] = run_epoch(model_ema, validloader, criterion_ce, None, desc_default='valid(EMA)', epoch=0, writer=writers[1], verbose=is_master, tqdm_disabled=tqdm_disabled)
                rs['test'] = run_epoch(model_ema, testloader_, criterion_ce, None, desc_default='*test(EMA)', epoch=0, writer=writers[2], verbose=is_master, tqdm_disabled=tqdm_disabled)
        for (key, setname) in itertools.product(['loss', 'top1', 'top5'], ['train', 'valid', 'test']):
            if (setname not in rs):
                continue
            result[('%s_%s' % (key, setname))] = rs[setname][key]
        result['epoch'] = 0
        return result
    best_top1 = 0
    for epoch in range(epoch_start, (max_epoch + 1)):
        if (local_rank >= 0):
            trainsampler.set_epoch(epoch)
        model.train()
        rs = dict()
        rs['train'] = run_epoch(model, trainloader, criterion, optimizer, desc_default='train', epoch=epoch, writer=writers[0], verbose=(is_master and (local_rank <= 0)), scheduler=scheduler, ema=ema, wd=C.get()['optimizer']['decay'], tqdm_disabled=tqdm_disabled)
        model.eval()
        if math.isnan(rs['train']['loss']):
            raise Exception('train loss is NaN.')
        if ((ema is not None) and (C.get()['optimizer']['ema_interval'] > 0) and ((epoch % C.get()['optimizer']['ema_interval']) == 0)):
            logger.info(f'ema synced+ rank={dist.get_rank()}')
            if (ema is not None):
                model.load_state_dict(ema.state_dict())
            for (name, x) in model.state_dict().items():
                dist.broadcast(x, 0)
            torch.cuda.synchronize()
            logger.info(f'ema synced- rank={dist.get_rank()}')
        if (is_master and (((epoch % evaluation_interval) == 0) or (epoch == max_epoch))):
            with torch.no_grad():
                rs['valid'] = run_epoch(model, validloader, criterion_ce, None, desc_default='valid', epoch=epoch, writer=writers[1], verbose=is_master, tqdm_disabled=tqdm_disabled)
                rs['test'] = run_epoch(model, testloader_, criterion_ce, None, desc_default='*test', epoch=epoch, writer=writers[2], verbose=is_master, tqdm_disabled=tqdm_disabled)
                if (ema is not None):
                    model_ema.load_state_dict({k.replace('module.', ''): v for (k, v) in ema.state_dict().items()})
                    rs['valid'] = run_epoch(model_ema, validloader, criterion_ce, None, desc_default='valid(EMA)', epoch=epoch, writer=writers[1], verbose=is_master, tqdm_disabled=tqdm_disabled)
                    rs['test'] = run_epoch(model_ema, testloader_, criterion_ce, None, desc_default='*test(EMA)', epoch=epoch, writer=writers[2], verbose=is_master, tqdm_disabled=tqdm_disabled)
            logger.info(f"epoch={epoch} [train] loss={rs['train']['loss']:.4f} top1={rs['train']['top1']:.4f} [valid] loss={rs['valid']['loss']:.4f} top1={rs['valid']['top1']:.4f} [test] loss={rs['test']['loss']:.4f} top1={rs['test']['top1']:.4f} ")
            if ((metric == 'last') or (rs[metric]['top1'] > best_top1)):
                if (metric != 'last'):
                    best_top1 = rs[metric]['top1']
                for (key, setname) in itertools.product(['loss', 'top1', 'top5'], ['train', 'valid', 'test']):
                    result[('%s_%s' % (key, setname))] = rs[setname][key]
                result['epoch'] = epoch
                writers[1].add_scalar('valid_top1/best', rs['valid']['top1'], epoch)
                writers[2].add_scalar('test_top1/best', rs['test']['top1'], epoch)
                reporter(loss_valid=rs['valid']['loss'], top1_valid=rs['valid']['top1'], loss_test=rs['test']['loss'], top1_test=rs['test']['top1'])
                if (is_master and save_path):
                    logger.info(('save model@%d to %s, err=%.4f' % (epoch, save_path, (1 - best_top1))))
                    torch.save({'epoch': epoch, 'log': {'train': rs['train'].get_dict(), 'valid': rs['valid'].get_dict(), 'test': rs['test'].get_dict()}, 'optimizer': optimizer.state_dict(), 'model': model.state_dict(), 'ema': (ema.state_dict() if (ema is not None) else None)}, save_path)
    del model
    result['top1_test'] = best_top1
    return result

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = optim.SGD

idx = 16:------------------- similar code ------------------ index = 89, score = 5.0 
def create_optimizer(model, new_lr):
    if (args.optimizer == 'sgd'):
        optimizer = optim.SGD(model.parameters(), lr=new_lr, momentum=0.9, dampening=0.9, weight_decay=args.wd)
    elif (args.optimizer == 'adam'):
        optimizer = optim.Adam(model.parameters(), lr=new_lr, weight_decay=args.wd)
    else:
        raise Exception('Not supported optimizer: {0}'.format(args.optimizer))
    return optimizer

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = optim.SGD

idx = 17:------------------- similar code ------------------ index = 54, score = 5.0 
def main():
    args = parser.parse_args()
    json_path = os.path.join(args.model_dir, 'params.json')
    assert os.path.isfile(json_path), 'No json configuration file found at {}'.format(json_path)
    params = utils.Params(json_path)
    random.seed(230)
    torch.manual_seed(230)
    np.random.seed(230)
    torch.cuda.manual_seed(230)
    warnings.filterwarnings('ignore')
    utils.set_logger(os.path.join(args.model_dir, 'train.log'))
    logging.info('Loading the datasets...')
    if (params.subset_percent < 1.0):
        train_dl = data_loader.fetch_subset_dataloader('train', params)
    else:
        train_dl = data_loader.fetch_dataloader('train', params)
    dev_dl = data_loader.fetch_dataloader('dev', params)
    logging.info('- done.')
    '\n    Load student and teacher model\n    '
    if ('distill' in params.model_version):
        if (params.model_version == 'cnn_distill'):
            print('Student model: {}'.format(params.model_version))
            model = net.Net(params).cuda()
        elif (params.model_version == 'shufflenet_v2_distill'):
            print('Student model: {}'.format(params.model_version))
            model = shufflenet.shufflenetv2(class_num=args.num_class).cuda()
        elif (params.model_version == 'mobilenet_v2_distill'):
            print('Student model: {}'.format(params.model_version))
            model = mobilenet.mobilenetv2(class_num=args.num_class).cuda()
        elif (params.model_version == 'resnet18_distill'):
            print('Student model: {}'.format(params.model_version))
            model = resnet.ResNet18(num_classes=args.num_class).cuda()
        elif (params.model_version == 'resnet50_distill'):
            print('Student model: {}'.format(params.model_version))
            model = resnet.ResNet50(num_classes=args.num_class).cuda()
        elif (params.model_version == 'alexnet_distill'):
            print('Student model: {}'.format(params.model_version))
            model = alexnet.alexnet(num_classes=args.num_class).cuda()
        elif (params.model_version == 'vgg19_distill'):
            print('Student model: {}'.format(params.model_version))
            model = models.vgg19_bn(num_classes=args.num_class).cuda()
        elif (params.model_version == 'googlenet_distill'):
            print('Student model: {}'.format(params.model_version))
            model = googlenet.GoogleNet(num_class=args.num_class).cuda()
        elif (params.model_version == 'resnext29_distill'):
            print('Student model: {}'.format(params.model_version))
            model = resnext.CifarResNeXt(cardinality=8, depth=29, num_classes=args.num_class).cuda()
        elif (params.model_version == 'densenet121_distill'):
            print('Student model: {}'.format(params.model_version))
            model = densenet.densenet121(num_class=args.num_class).cuda()
        if (params.model_version == 'cnn_distill'):
            optimizer = optim.Adam(model.parameters(), lr=(params.learning_rate * (params.batch_size / 128)))
        else:
            optimizer = optim.SGD(model.parameters(), lr=(params.learning_rate * (params.batch_size / 128)), momentum=0.9, weight_decay=0.0005)
        iter_per_epoch = len(train_dl)
        warmup_scheduler = utils.WarmUpLR(optimizer, (iter_per_epoch * args.warm))
        if args.self_training:
            print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>self training>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')
            loss_fn_kd = loss_kd_self
        else:
            loss_fn_kd = loss_kd
        ' \n            Specify the pre-trained teacher models for knowledge distillation\n            Checkpoints can be obtained by regular training or downloading our pretrained models\n            For model which is pretrained in multi-GPU, use "nn.DaraParallel" to correctly load the model weights.\n        '
        if (params.teacher == 'resnet18'):
            print('Teacher model: {}'.format(params.teacher))
            teacher_model = resnet.ResNet18(num_classes=args.num_class)
            teacher_checkpoint = 'experiments/pretrained_teacher_models/base_resnet18/best.pth.tar'
            if args.pt_teacher:
                teacher_checkpoint = 'experiments/pretrained_teacher_models/base_resnet18/0.pth.tar'
            teacher_model = teacher_model.cuda()
        elif (params.teacher == 'alexnet'):
            print('Teacher model: {}'.format(params.teacher))
            teacher_model = alexnet.alexnet(num_classes=args.num_class)
            teacher_checkpoint = 'experiments/pretrained_teacher_models/base_alexnet/best.pth.tar'
            teacher_model = teacher_model.cuda()
        elif (params.teacher == 'googlenet'):
            print('Teacher model: {}'.format(params.teacher))
            teacher_model = googlenet.GoogleNet(num_class=args.num_class)
            teacher_checkpoint = 'experiments/pretrained_teacher_models/base_googlenet/best.pth.tar'
            teacher_model = teacher_model.cuda()
        elif (params.teacher == 'vgg19'):
            print('Teacher model: {}'.format(params.teacher))
            teacher_model = models.vgg19_bn(num_classes=args.num_class)
            teacher_checkpoint = 'experiments/pretrained_teacher_models/base_vgg19/best.pth.tar'
            teacher_model = teacher_model.cuda()
        elif (params.teacher == 'resnet50'):
            print('Teacher model: {}'.format(params.teacher))
            teacher_model = resnet.ResNet50(num_classes=args.num_class).cuda()
            teacher_checkpoint = 'experiments/pretrained_teacher_models/base_resnet50/best.pth.tar'
            if args.pt_teacher:
                teacher_checkpoint = 'experiments/pretrained_teacher_models/base_resnet50/50.pth.tar'
        elif (params.teacher == 'resnet101'):
            print('Teacher model: {}'.format(params.teacher))
            teacher_model = resnet.ResNet101(num_classes=args.num_class).cuda()
            teacher_checkpoint = 'experiments/pretrained_teacher_models/base_resnet101/best.pth.tar'
            teacher_model = teacher_model.cuda()
        elif (params.teacher == 'densenet121'):
            print('Teacher model: {}'.format(params.teacher))
            teacher_model = densenet.densenet121(num_class=args.num_class).cuda()
            teacher_checkpoint = 'experiments/pretrained_teacher_models/base_densenet121/best.pth.tar'
        elif (params.teacher == 'resnext29'):
            print('Teacher model: {}'.format(params.teacher))
            teacher_model = resnext.CifarResNeXt(cardinality=8, depth=29, num_classes=args.num_class).cuda()
            teacher_checkpoint = 'experiments/pretrained_teacher_models/base_resnext29/best.pth.tar'
            if args.pt_teacher:
                teacher_checkpoint = 'experiments/pretrained_teacher_models/base_resnext29/50.pth.tar'
                teacher_model = nn.DataParallel(teacher_model).cuda()
        elif (params.teacher == 'mobilenet_v2'):
            print('Teacher model: {}'.format(params.teacher))
            teacher_model = mobilenet.mobilenetv2(class_num=args.num_class).cuda()
            teacher_checkpoint = 'experiments/pretrained_teacher_models/base_mobilenet_v2/best.pth.tar'
        elif (params.teacher == 'shufflenet_v2'):
            print('Teacher model: {}'.format(params.teacher))
            teacher_model = shufflenet.shufflenetv2(class_num=args.num_class).cuda()
            teacher_checkpoint = 'experiments/pretrained_teacher_models/base_shufflenet_v2/best.pth.tar'
        utils.load_checkpoint(teacher_checkpoint, teacher_model)
        logging.info('Starting training for {} epoch(s)'.format(params.num_epochs))
        train_and_evaluate_kd(model, teacher_model, train_dl, dev_dl, optimizer, loss_fn_kd, warmup_scheduler, params, args, args.restore_file)
    else:
        print('Train base model')
        if (params.model_version == 'cnn'):
            model = net.Net(params).cuda()
        elif (params.model_version == 'mobilenet_v2'):
            print('model: {}'.format(params.model_version))
            model = mobilenet.mobilenetv2(class_num=args.num_class).cuda()
        elif (params.model_version == 'shufflenet_v2'):
            print('model: {}'.format(params.model_version))
            model = shufflenet.shufflenetv2(class_num=args.num_class).cuda()
        elif (params.model_version == 'alexnet'):
            print('model: {}'.format(params.model_version))
            model = alexnet.alexnet(num_classes=args.num_class).cuda()
        elif (params.model_version == 'vgg19'):
            print('model: {}'.format(params.model_version))
            model = models.vgg19_bn(num_classes=args.num_class).cuda()
        elif (params.model_version == 'googlenet'):
            print('model: {}'.format(params.model_version))
            model = googlenet.GoogleNet(num_class=args.num_class).cuda()
        elif (params.model_version == 'densenet121'):
            print('model: {}'.format(params.model_version))
            model = densenet.densenet121(num_class=args.num_class).cuda()
        elif (params.model_version == 'resnet18'):
            model = resnet.ResNet18(num_classes=args.num_class).cuda()
        elif (params.model_version == 'resnet50'):
            model = resnet.ResNet50(num_classes=args.num_class).cuda()
        elif (params.model_version == 'resnet101'):
            model = resnet.ResNet101(num_classes=args.num_class).cuda()
        elif (params.model_version == 'resnet152'):
            model = resnet.ResNet152(num_classes=args.num_class).cuda()
        elif (params.model_version == 'resnext29'):
            model = resnext.CifarResNeXt(cardinality=8, depth=29, num_classes=args.num_class).cuda()
        if args.regularization:
            print('>>>>>>>>>>>>>>>>>>>>>>>>Loss of Regularization>>>>>>>>>>>>>>>>>>>>>>>>')
            loss_fn = loss_kd_regularization
        elif args.label_smoothing:
            print('>>>>>>>>>>>>>>>>>>>>>>>>Label Smoothing>>>>>>>>>>>>>>>>>>>>>>>>')
            loss_fn = loss_label_smoothing
        else:
            print('>>>>>>>>>>>>>>>>>>>>>>>>Normal Training>>>>>>>>>>>>>>>>>>>>>>>>')
            loss_fn = nn.CrossEntropyLoss()
            if args.double_training:
                print('>>>>>>>>>>>>>>>>>>>>>>>>Double Training>>>>>>>>>>>>>>>>>>>>>>>>')
                checkpoint = (('experiments/pretrained_teacher_models/base_' + str(params.model_version)) + '/best.pth.tar')
                utils.load_checkpoint(checkpoint, model)
        if (params.model_version == 'cnn'):
            optimizer = optim.Adam(model.parameters(), lr=(params.learning_rate * (params.batch_size / 128)))
        else:
            optimizer = optim.SGD(model.parameters(), lr=(params.learning_rate * (params.batch_size / 128)), momentum=0.9, weight_decay=0.0005)
        iter_per_epoch = len(train_dl)
        warmup_scheduler = utils.WarmUpLR(optimizer, (iter_per_epoch * args.warm))
        logging.info('Starting training for {} epoch(s)'.format(params.num_epochs))
        train_and_evaluate(model, train_dl, dev_dl, optimizer, loss_fn, params, args.model_dir, warmup_scheduler, args, args.restore_file)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
        if:        else:
             ...  = optim.SGD

idx = 18:------------------- similar code ------------------ index = 52, score = 5.0 
def train(args):
    cfg = exp_cfg[args.cfg]
    dataset = GTDBDetection(args, args.training_data, split='train', transform=SSDAugmentation(cfg['min_dim'], mean=MEANS))
    if args.visdom:
        import visdom
        viz = visdom.Visdom()
    gpu_id = 0
    if args.cuda:
        gpu_id = helpers.get_freer_gpu()
        logging.debug(('Using GPU with id ' + str(gpu_id)))
        torch.cuda.set_device(gpu_id)
    ssd_net = build_ssd(args, 'train', cfg, gpu_id, cfg['min_dim'], cfg['num_classes'])
    logging.debug(ssd_net)
    ct = 0
    for child in ssd_net.vgg.children():
        if (ct >= args.layers_to_freeze):
            break
        child.requires_grad = False
        ct += 1
    if args.resume:
        logging.debug('Resuming training, loading {}...'.format(args.resume))
        ssd_net.load_state_dict(torch.load(args.resume))
    else:
        vgg_weights = torch.load(('base_weights/' + args.basenet))
        logging.debug('Loading base network...')
        ssd_net.vgg.load_state_dict(vgg_weights)
    step_index = 0
    if (not args.resume):
        logging.debug('Initializing weights...')
        ssd_net.extras.apply(weights_init)
        ssd_net.loc.apply(weights_init)
        ssd_net.conf.apply(weights_init)
        for val in cfg['lr_steps']:
            if (args.start_iter > val):
                step_index = (step_index + 1)
        torch.save(ssd_net.state_dict(), os.path.join(('weights_' + args.exp_name), ((('initial_' + str(args.model_type)) + args.dataset) + '.pth')))
    optimizer = optim.SGD(ssd_net.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    adjust_learning_rate(args, optimizer, args.gamma, step_index)
    criterion = MultiBoxLoss(args, cfg, args.pos_thresh, 0, 3)
    if args.cuda:
        ssd_net = torch.nn.DataParallel(ssd_net)
        cudnn.benchmark = True
    ssd_net.train()
    loc_loss = 0
    conf_loss = 0
    min_total_loss = float('inf')
    epoch = 0
    logging.debug('Loading the dataset...')
    epoch_size = (len(dataset) // args.batch_size)
    logging.debug(('Training SSD on:' + dataset.name))
    logging.debug('Using the specified args:')
    logging.debug(args)
    if args.visdom:
        vis_title = args.exp_name
        vis_legend = ['Location Loss', 'Confidence Loss', 'Total Loss']
        iter_plot = create_vis_plot('Iteration', 'Loss', viz, ('Training ' + vis_title), vis_legend)
        epoch_plot = create_vis_plot('Epoch', 'Loss', viz, ('Training ' + vis_title), vis_legend)
    data_loader = data.DataLoader(dataset, args.batch_size, num_workers=args.num_workers, shuffle=True, collate_fn=detection_collate, pin_memory=True)
    logging.debug(('Training set size is ' + str(len(dataset))))
    batch_iterator = iter(data_loader)
    for iteration in range(args.start_iter, cfg['max_iter']):
        ssd_net.train()
        t0 = time.time()
        if (iteration in cfg['lr_steps']):
            step_index += 1
            adjust_learning_rate(args, optimizer, args.gamma, step_index)
        try:
            (images, targets, _) = next(batch_iterator)
        except StopIteration:
            batch_iterator = iter(data_loader)
            (images, targets, _) = next(batch_iterator)
        if args.cuda:
            images = images.cuda()
            targets = [ann.cuda() for ann in targets]
        else:
            images = Variable(images)
            targets = [Variable(ann, volatile=True) for ann in targets]
        out = ssd_net(images)
        optimizer.zero_grad()
        (loss_l, loss_c) = criterion(out, targets)
        loss = ((args.alpha * loss_l) + loss_c)
        loss.backward()
        optimizer.step()
        loc_loss += loss_l.item()
        conf_loss += loss_c.item()
        t1 = time.time()
        if ((iteration % 10) == 0):
            logging.debug(('timer: %.4f sec.' % (t1 - t0)))
            logging.debug((('iter ' + repr(iteration)) + (' || Loss: %.4f ||' % loss.item())))
        if args.visdom:
            update_vis_plot(iteration, loss_l.item(), viz, loss_c.item(), iter_plot, epoch_plot, 'append')
        if ((iteration != 0) and ((iteration % 1000) == 0)):
            logging.debug(('Saving state, iter:' + str(iteration)))
            torch.save(ssd_net.state_dict(), os.path.join(('weights_' + args.exp_name), (((('ssd' + str(args.model_type)) + args.dataset) + repr(iteration)) + '.pth')))
        if ((iteration != 0) and ((iteration % epoch_size) == 0)):
            epoch += 1
            torch.save(ssd_net.state_dict(), os.path.join(('weights_' + args.exp_name), (((('epoch_ssd' + str(args.model_type)) + args.dataset) + repr(epoch)) + '.pth')))
            train_loss = (loc_loss + conf_loss)
            update_vis_plot(epoch, loc_loss, viz, conf_loss, epoch_plot, None, 'append', epoch_size)
            if (args.validation_data != ''):
                validation_loss = validate(args, ssd_net, criterion, cfg)
                if (epoch == 1):
                    validation_plot = create_validation_plot(epoch, validation_loss, 'Epoch', 'Loss', viz, ('Validating ' + vis_title), ['Validation'])
                else:
                    update_validation_plot(epoch, validation_loss, viz, validation_plot, 'append')
                if (validation_loss < min_total_loss):
                    min_total_loss = validation_loss
                    torch.save(ssd_net.state_dict(), os.path.join(('weights_' + args.exp_name), (((('best_ssd' + str(args.model_type)) + args.dataset) + repr(iteration)) + '.pth')))
            loc_loss = 0
            conf_loss = 0
    torch.save(ssd_net.state_dict(), (((args.exp_name + '') + args.dataset) + '.pth'))
    logging.debug((((('Final weights are saved at ' + args.exp_name) + '') + args.dataset) + '.pth'))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = optim.SGD

idx = 19:------------------- similar code ------------------ index = 34, score = 5.0 
def create_optimizer(model, new_lr):
    if (args.optimizer == 'sgd'):
        optimizer = optim.SGD(model.parameters(), lr=new_lr, momentum=0.9, dampening=0.9, weight_decay=args.wd)
    elif (args.optimizer == 'adam'):
        optimizer = optim.Adam(model.parameters(), lr=new_lr, weight_decay=args.wd)
    else:
        raise Exception('Not supported optimizer: {0}'.format(args.optimizer))
    return optimizer

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = optim.SGD

idx = 20:------------------- similar code ------------------ index = 94, score = 5.0 
def main():
    args = parse_args()
    if (args.name is None):
        args.name = ('%s_WideResNet%s-%s' % (args.dataset, args.depth, args.width))
        if args.cutout:
            args.name += '_wCutout'
        if args.auto_augment:
            args.name += '_wAutoAugment'
    if (not os.path.exists(('models/%s' % args.name))):
        os.makedirs(('models/%s' % args.name))
    print('Config -----')
    for arg in vars(args):
        print(('%s: %s' % (arg, getattr(args, arg))))
    print('------------')
    with open(('models/%s/args.txt' % args.name), 'w') as f:
        for arg in vars(args):
            print(('%s: %s' % (arg, getattr(args, arg))), file=f)
    joblib.dump(args, ('models/%s/args.pkl' % args.name))
    criterion = nn.CrossEntropyLoss().cuda()
    cudnn.benchmark = True
    if (args.dataset == 'cifar10'):
        transform_train = [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip()]
        if args.auto_augment:
            transform_train.append(AutoAugment())
        if args.cutout:
            transform_train.append(Cutout())
        transform_train.extend([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.201))])
        transform_train = transforms.Compose(transform_train)
        transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])
        train_set = datasets.CIFAR10(root='~/data', train=True, download=True, transform=transform_train)
        train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=8)
        test_set = datasets.CIFAR10(root='~/data', train=False, download=True, transform=transform_test)
        test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=8)
        num_classes = 10
    elif (args.dataset == 'cifar100'):
        transform_train = [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip()]
        if args.auto_augment:
            transform_train.append(AutoAugment())
        if args.cutout:
            transform_train.append(Cutout())
        transform_train.extend([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.201))])
        transform_train = transforms.Compose(transform_train)
        transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.201))])
        train_set = datasets.CIFAR100(root='~/data', train=True, download=True, transform=transform_train)
        train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=8)
        test_set = datasets.CIFAR100(root='~/data', train=False, download=True, transform=transform_test)
        test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=8)
        num_classes = 100
    model = WideResNet(args.depth, args.width, num_classes=num_classes)
    model = model.cuda()
    optimizer = optim.SGD(filter((lambda p: p.requires_grad), model.parameters()), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[int(e) for e in args.milestones.split(',')], gamma=args.gamma)
    log = pd.DataFrame(index=[], columns=['epoch', 'lr', 'loss', 'acc', 'val_loss', 'val_acc'])
    best_acc = 0
    for epoch in range(args.epochs):
        print(('Epoch [%d/%d]' % ((epoch + 1), args.epochs)))
        scheduler.step()
        train_log = train(args, train_loader, model, criterion, optimizer, epoch)
        val_log = validate(args, test_loader, model, criterion)
        print(('loss %.4f - acc %.4f - val_loss %.4f - val_acc %.4f' % (train_log['loss'], train_log['acc'], val_log['loss'], val_log['acc'])))
        tmp = pd.Series([epoch, scheduler.get_lr()[0], train_log['loss'], train_log['acc'], val_log['loss'], val_log['acc']], index=['epoch', 'lr', 'loss', 'acc', 'val_loss', 'val_acc'])
        log = log.append(tmp, ignore_index=True)
        log.to_csv(('models/%s/log.csv' % args.name), index=False)
        if (val_log['acc'] > best_acc):
            torch.save(model.state_dict(), ('models/%s/model.pth' % args.name))
            best_acc = val_log['acc']
            print('=> saved best model')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = optim.SGD

idx = 21:------------------- similar code ------------------ index = 95, score = 5.0 
def train():
    args = parseArgs()
    (cfg, cfg_file_path) = getCfgByDatasetAndBackbone(datasetname=args.datasetname, backbonename=args.backbonename)
    checkDir(cfg.TRAIN_BACKUPDIR)
    logger_handle = Logger(cfg.TRAIN_LOGFILE)
    use_cuda = torch.cuda.is_available()
    is_multi_gpus = cfg.IS_MULTI_GPUS
    if is_multi_gpus:
        assert use_cuda
    if (args.datasetname == 'coco'):
        dataset = COCODataset(rootdir=cfg.DATASET_ROOT_DIR, image_size_dict=cfg.IMAGESIZE_DICT, max_num_gt_boxes=cfg.MAX_NUM_GT_BOXES, use_color_jitter=cfg.USE_COLOR_JITTER, img_norm_info=cfg.IMAGE_NORMALIZE_INFO, use_caffe_pretrained_model=cfg.USE_CAFFE_PRETRAINED_MODEL, mode='TRAIN', datasettype='train2017')
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.BATCHSIZE, sampler=NearestRatioRandomSampler(dataset.img_ratios, cfg.BATCHSIZE), num_workers=cfg.NUM_WORKERS, collate_fn=COCODataset.paddingCollateFn, pin_memory=cfg.PIN_MEMORY)
    else:
        raise ValueError(('Unsupport datasetname <%s> now...' % args.datasetname))
    if (args.backbonename.find('resnet') != (- 1)):
        model = FasterRCNNResNets(mode='TRAIN', cfg=cfg, logger_handle=logger_handle)
    else:
        raise ValueError(('Unsupport backbonename <%s> now...' % args.backbonename))
    start_epoch = 1
    end_epoch = cfg.MAX_EPOCHS
    if use_cuda:
        model = model.cuda()
    learning_rate_idx = 0
    if cfg.IS_USE_WARMUP:
        learning_rate = (cfg.LEARNING_RATES[learning_rate_idx] / 3)
    else:
        learning_rate = cfg.LEARNING_RATES[learning_rate_idx]
    optimizer = optim.SGD(filter((lambda p: p.requires_grad), model.parameters()), lr=learning_rate, momentum=cfg.MOMENTUM, weight_decay=cfg.WEIGHT_DECAY)
    if args.checkpointspath:
        checkpoints = loadCheckpoints(args.checkpointspath, logger_handle)
        model.load_state_dict(checkpoints['model'])
        optimizer.load_state_dict(checkpoints['optimizer'])
        start_epoch = (checkpoints['epoch'] + 1)
        for epoch in range(1, start_epoch):
            if (epoch in cfg.LR_ADJUST_EPOCHS):
                learning_rate_idx += 1
    if is_multi_gpus:
        model = nn.DataParallel(model)
    logger_handle.info(('Dataset used: %s, Number of images: %s' % (args.datasetname, len(dataset))))
    logger_handle.info(('Backbone used: %s' % args.backbonename))
    logger_handle.info(('Checkpoints used: %s' % args.checkpointspath))
    logger_handle.info(('Config file used: %s' % cfg_file_path))
    FloatTensor = (torch.cuda.FloatTensor if use_cuda else torch.FloatTensor)
    for epoch in range(start_epoch, (end_epoch + 1)):
        if is_multi_gpus:
            model.module.setTrain()
        else:
            model.setTrain()
        if (epoch in cfg.LR_ADJUST_EPOCHS):
            learning_rate_idx += 1
            adjustLearningRate(optimizer=optimizer, target_lr=cfg.LEARNING_RATES[learning_rate_idx], logger_handle=logger_handle)
        logger_handle.info(('Start epoch %s, learning rate is %s...' % (epoch, cfg.LEARNING_RATES[learning_rate_idx])))
        for (batch_idx, samples) in enumerate(dataloader):
            if ((epoch == 1) and cfg.IS_USE_WARMUP and (batch_idx <= cfg.NUM_WARMUP_STEPS)):
                assert (learning_rate_idx == 0), 'BUGS may exist...'
                target_lr = (cfg.LEARNING_RATES[learning_rate_idx] / 3)
                target_lr += (((cfg.LEARNING_RATES[learning_rate_idx] - (cfg.LEARNING_RATES[learning_rate_idx] / 3)) * batch_idx) / cfg.NUM_WARMUP_STEPS)
                adjustLearningRate(optimizer=optimizer, target_lr=target_lr)
            optimizer.zero_grad()
            (img_ids, imgs, gt_boxes, img_info, num_gt_boxes) = samples
            output = model(x=imgs.type(FloatTensor), gt_boxes=gt_boxes.type(FloatTensor), img_info=img_info.type(FloatTensor), num_gt_boxes=num_gt_boxes.type(FloatTensor))
            (rois, cls_probs, bbox_preds, rpn_cls_loss, rpn_reg_loss, loss_cls, loss_reg) = output
            loss = (((rpn_cls_loss.mean() + rpn_reg_loss.mean()) + loss_cls.mean()) + loss_reg.mean())
            logger_handle.info(('[EPOCH]: %s/%s, [BTACH]: %s/%s, [LEARNING_RATE]: %s, [DATASET]: %s \n\t [LOSS]: rpn_cls_loss %.4f, rpn_reg_loss %.4f, loss_cls %.4f, loss_reg %.4f, total %.4f' % (epoch, end_epoch, (batch_idx + 1), len(dataloader), cfg.LEARNING_RATES[learning_rate_idx], args.datasetname, rpn_cls_loss.mean().item(), rpn_reg_loss.mean().item(), loss_cls.mean().item(), loss_reg.mean().item(), loss.item())))
            loss.backward()
            optimizer.step()
        if (((epoch % cfg.SAVE_INTERVAL) == 0) or (epoch == end_epoch)):
            state_dict = {'epoch': epoch, 'model': (model.module.state_dict() if is_multi_gpus else model.state_dict()), 'optimizer': optimizer.state_dict()}
            savepath = os.path.join(cfg.TRAIN_BACKUPDIR, ('epoch_%s.pth' % epoch))
            saveCheckpoints(state_dict, savepath, logger_handle)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = optim.SGD

idx = 22:------------------- similar code ------------------ index = 36, score = 5.0 
def benchmark_training(model, opts):
    'Benchmarks training phase.\n\n    :param obj model: A model to benchmark\n    :param dict opts: A dictionary of parameters.\n    :rtype: tuple:\n    :return: A tuple of (model_name, list of batch times)\n    '

    def _reduce_tensor(tensor):
        reduced = tensor.clone()
        dist.all_reduce(reduced, op=dist.reduce_op.SUM)
        reduced /= opts['world_size']
        return reduced
    if (opts['phase'] != 'training'):
        raise ("Phase in benchmark_training func is '%s'" % opts['phase'])
    opts['distributed'] = (opts['world_size'] > 1)
    opts['with_cuda'] = (opts['device'] == 'gpu')
    opts['fp16'] = (opts['dtype'] == 'float16')
    opts['loss_scale'] = 1
    if (opts['fp16'] and (not opts['with_cuda'])):
        raise ValueError('Configuration error: FP16 can only be used with GPUs')
    if opts['with_cuda']:
        torch.cuda.set_device(opts['local_rank'])
        cudnn.benchmark = opts['cudnn_benchmark']
        cudnn.fastest = opts['cudnn_fastest']
    if opts['distributed']:
        dist.init_process_group(backend=opts['dist_backend'], init_method='env://')
    if opts['with_cuda']:
        model = model.cuda()
        if (opts['dtype'] == 'float16'):
            model = network_to_half(model)
    if opts['distributed']:
        model = DDP(model, shared_param=True)
    if opts['fp16']:
        (model_params, master_params) = prep_param_lists(model)
    else:
        master_params = list(model.parameters())
    criterion = nn.CrossEntropyLoss()
    if opts['with_cuda']:
        criterion = criterion.cuda()
    optimizer = optim.SGD(master_params, lr=0.01, momentum=0.9, weight_decay=0.0001)
    data_loader = DatasetFactory.get_data_loader(opts, opts['__input_shape'], opts['__num_classes'])
    is_warmup = (opts['num_warmup_batches'] > 0)
    done = (opts['num_warmup_batches'] == 0)
    num_iterations_done = 0
    model.train()
    batch_times = np.zeros(opts['num_batches'])
    end_time = timeit.default_timer()
    while (not done):
        prefetcher = DataPrefetcher(data_loader, opts)
        (batch_data, batch_labels) = prefetcher.next()
        while (batch_data is not None):
            data_var = torch.autograd.Variable(batch_data)
            labels_var = torch.autograd.Variable(batch_labels)
            output = model(data_var)
            loss = criterion(output, labels_var)
            loss = (loss * opts['loss_scale'])
            if opts['fp16']:
                model.zero_grad()
                loss.backward()
                model_grads_to_master_grads(model_params, master_params)
                if (opts['loss_scale'] != 1):
                    for param in master_params:
                        param.grad.data = (param.grad.data / opts['loss_scale'])
                optimizer.step()
                master_params_to_model_params(model_params, master_params)
            else:
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            if opts['with_cuda']:
                torch.cuda.synchronize()
            num_iterations_done += 1
            cur_time = timeit.default_timer()
            (batch_data, batch_labels) = prefetcher.next()
            if is_warmup:
                if (num_iterations_done >= opts['num_warmup_batches']):
                    is_warmup = False
                    num_iterations_done = 0
            else:
                if (opts['num_batches'] != 0):
                    batch_times[(num_iterations_done - 1)] = (cur_time - end_time)
                if (num_iterations_done >= opts['num_batches']):
                    done = True
                    break
            end_time = cur_time
    return (opts['__name'], batch_times)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = optim.SGD

idx = 23:------------------- similar code ------------------ index = 38, score = 5.0 
def init_optimizers(self, optim_params):
    optimizer = optim.SGD(optim_params)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=self.scheduler_params['step_size'], gamma=self.scheduler_params['gamma'])
    return (optimizer, scheduler)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = optim.SGD

idx = 24:------------------- similar code ------------------ index = 17, score = 5.0 
def create_optimizer(model, new_lr):
    if (args.optimizer == 'sgd'):
        optimizer = optim.SGD(model.parameters(), lr=new_lr, momentum=0.9, dampening=0.9, weight_decay=args.wd)
    elif (args.optimizer == 'adam'):
        optimizer = optim.Adam(model.parameters(), lr=new_lr, weight_decay=args.wd)
    else:
        raise Exception('Not supported optimizer: {0}'.format(args.optimizer))
    return optimizer

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = optim.SGD

idx = 25:------------------- similar code ------------------ index = 98, score = 5.0 
def get_optimizer(self, task: model_args.Task, finetune: bool=False) -> Callable[([Any], torch.optim.Optimizer)]:
    self.check_initialized()
    learning_rate: float = 0.0
    l2_coefficient: float = 0.0
    if (task == model_args.Task.PLAN_PREDICTOR):
        learning_rate = self._plan_prediction_learning_rate
        l2_coefficient = self._plan_prediction_l2_coefficient
    elif (task == model_args.Task.ACTION_GENERATOR):
        if finetune:
            learning_rate = self._finetune_learning_rate
            l2_coefficient = self._finetune_l2_coefficient
        else:
            learning_rate = self._action_generation_learning_rate
            l2_coefficient = self._action_generation_l2_coefficient
    if (self._optimizer_type == OptimizerType.ADAM):
        logging.info(((('Adam with lr = ' + str(learning_rate)) + ', weight decay = ') + str(l2_coefficient)))
        return (lambda params: torch.optim.Adam(params, lr=learning_rate, weight_decay=l2_coefficient))
    elif (self._optimizer_type == OptimizerType.ADAGRAD):
        logging.info(((('Adagrad with lr = ' + str(learning_rate)) + ', weight decay = ') + str(l2_coefficient)))
        return (lambda params: torch.optim.Adagrad(params, lr=learning_rate, weight_decay=l2_coefficient))
    elif (self._optimizer_type == OptimizerType.RMSPROP):
        logging.info(((('RMSProp with lr = ' + str(learning_rate)) + ', weight decay = ') + str(l2_coefficient)))
        return (lambda params: torch.optim.RMSprop(params, lr=learning_rate, weight_decay=l2_coefficient))
    elif (self._optimizer_type == OptimizerType.SGD):
        logging.info(((('SGD with lr = ' + str(learning_rate)) + ', weight decay = ') + str(l2_coefficient)))
        return (lambda params: torch.optim.SGD(params, lr=learning_rate, weight_decay=l2_coefficient))

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... () ->:
    if:    elif ( ==  ... .SGD):
        return (lambda  ... :  ... .optim)

idx = 26:------------------- similar code ------------------ index = 100, score = 4.0 
def __init__(self, params, load_path=None, dist_model=False):
    self.model = backbone.__dict__[params['backbone_arch']](**params['backbone_param'])
    utils.init_weights(self.model, init_type='xavier')
    if (load_path is not None):
        utils.load_weights(load_path, self.model)
    self.model.cuda()
    if dist_model:
        self.model = utils.DistModule(self.model)
        self.world_size = dist.get_world_size()
    else:
        self.model = backbone.FixModule(self.model)
        self.world_size = 1
    if (params['optim'] == 'SGD'):
        self.optim = torch.optim.SGD(self.model.parameters(), lr=params['lr'], momentum=0.9, weight_decay=params['weight_decay'])
    elif (params['optim'] == 'Adam'):
        self.optim = torch.optim.Adam(self.model.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))
    else:
        raise Exception('No such optimizer: {}'.format(params['optim']))
    cudnn.benchmark = True

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ... .optim =  ... .SGD

idx = 27:------------------- similar code ------------------ index = 84, score = 4.0 
def str_to_optim_cls(optim_string):
    if (optim_string.lower() == 'sgd'):
        return torch.optim.SGD
    elif (optim_string.lower() == 'adam'):

        def fn(parameters, lr):
            return torch.optim.Adam(parameters, lr=lr)
        return fn
    elif (optim_string.lower() == 'rms'):

        def fn(parameters, lr):
            return torch.optim.RMSprop(parameters, lr=lr)
        return fn
    elif (optim_string.lower() == 'sgdmom'):

        def fn(parameters, lr):
            return torch.optim.SGD(parameters, lr=lr, momentum=0.9, nesterov=True)
        return fn
    else:
        raise ValueError(optim_string)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ( ... ):
    if:
        return  ... .optim.SGD

idx = 28:------------------- similar code ------------------ index = 16, score = 3.0 
if (__name__ == '__main__'):
    args = parse_args()
    print('Called with args:')
    print(args)
    args.imdb_name = 'visdrone_2017_trainval'
    args.imdbval_name = 'visdrone_2017_test'
    args.set_cfgs = ['FPN_ANCHOR_SCALES', '[16, 32, 64, 128, 256]', 'FPN_FEAT_STRIDES', '[4, 8, 16, 32, 64]', 'ANCHOR_RATIOS', '[0.5, 1, 2]', 'MAX_NUM_GT_BOXES', '60']
    args.cfg_file = ('cfgs/{}_ls.yml'.format(args.net) if args.lscale else 'cfgs/{}.yml'.format(args.net))
    if (args.cfg_file is not None):
        cfg_from_file(args.cfg_file)
    if (args.set_cfgs is not None):
        cfg_from_list(args.set_cfgs)
    print('Using config:')
    pprint.pprint(cfg)
    logging.info(cfg)
    np.random.seed(cfg.RNG_SEED)
    if (torch.cuda.is_available() and (not args.cuda)):
        print('WARNING: You have a CUDA device, so you should probably run with --cuda')
    cfg.TRAIN.USE_FLIPPED = False
    cfg.USE_GPU_NMS = args.cuda
    (imdb, roidb, ratio_list, ratio_index) = combined_roidb(args.imdb_name)
    train_size = len(roidb)
    print('{:d} roidb entries'.format(len(roidb)))
    output_dir = ((((args.save_dir + '/') + args.net) + '/') + args.dataset)
    if (not os.path.exists(output_dir)):
        os.makedirs(output_dir)
    sampler_batch = sampler(train_size, args.batch_size)
    dataset = roibatchLoader(roidb, ratio_list, ratio_index, args.batch_size, imdb.num_classes, training=True)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, sampler=sampler_batch, num_workers=args.num_workers)
    im_data = torch.FloatTensor(1)
    im_info = torch.FloatTensor(1)
    num_boxes = torch.LongTensor(1)
    gt_boxes = torch.FloatTensor(1)
    if args.cuda:
        im_data = im_data.cuda()
        im_info = im_info.cuda()
        num_boxes = num_boxes.cuda()
        gt_boxes = gt_boxes.cuda()
    im_data = Variable(im_data)
    im_info = Variable(im_info)
    num_boxes = Variable(num_boxes)
    gt_boxes = Variable(gt_boxes)
    if args.cuda:
        cfg.CUDA = True
    if (args.net == 'res101'):
        FPN = resnet(imdb.classes, 101, pretrained=True, class_agnostic=args.class_agnostic)
    elif (args.net == 'res50'):
        FPN = resnet(imdb.classes, 50, pretrained=True, class_agnostic=args.class_agnostic)
    elif (args.net == 'res152'):
        FPN = resnet(imdb.classes, 152, pretrained=True, class_agnostic=args.class_agnostic)
    else:
        print('network is not defined')
        pdb.set_trace()
    FPN.create_architecture()
    lr = cfg.TRAIN.LEARNING_RATE
    lr = args.lr
    params = []
    for (key, value) in dict(FPN.named_parameters()).items():
        if value.requires_grad:
            if ('bias' in key):
                params += [{'params': [value], 'lr': (lr * (cfg.TRAIN.DOUBLE_BIAS + 1)), 'weight_decay': ((cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY) or 0)}]
            else:
                params += [{'params': [value], 'lr': lr, 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]
    if (args.optimizer == 'adam'):
        lr = (lr * 0.1)
        optimizer = torch.optim.Adam(params)
    elif (args.optimizer == 'sgd'):
        optimizer = torch.optim.SGD(params, momentum=cfg.TRAIN.MOMENTUM)
    load_name = os.path.join('models', 'baseline', 'fpn_{}_{}_{}.pth'.format(args.checksession, '1', '10161'))
    print(('loading checkpoint %s' % load_name))
    checkpoint = torch.load(load_name)
    for key in checkpoint['model'].keys():
        if any(((name in key) for name in ['RCNN_angle_score', 'RCNN_altitude_score', 'RCNN_weather_score', 'RCNN_angle', 'RCNN_weather', 'RCNN_altitude'])):
            del checkpoint['model'][key]
    model_dict = FPN.state_dict()
    model_dict.update(checkpoint['model'])
    FPN.load_state_dict(model_dict)
    print(('loaded checkpoint %s' % load_name))
    if args.mGPUs:
        FPN = nn.DataParallel(FPN)
    if args.cuda:
        FPN.cuda()
    iters_per_epoch = int((train_size / args.batch_size))
    for epoch in range(args.start_epoch, args.max_epochs):
        FPN.train()
        loss_temp = 0
        start = time.time()
        if ((epoch % (args.lr_decay_step + 1)) == 0):
            adjust_learning_rate(optimizer, args.lr_decay_gamma)
            lr *= args.lr_decay_gamma
        data_iter = iter(dataloader)
        for step in range(iters_per_epoch):
            data = data_iter.next()
            im_data.data.resize_(data[0].size()).copy_(data[0])
            im_info.data.resize_(data[1].size()).copy_(data[1])
            gt_boxes.data.resize_(data[2].size()).copy_(data[2])
            num_boxes.data.resize_(data[3].size()).copy_(data[3])
            FPN.zero_grad()
            (_, _, _, rpn_loss_cls, rpn_loss_box, RCNN_loss_cls, RCNN_loss_bbox, roi_labels) = FPN(im_data, im_info, gt_boxes, num_boxes)
            loss = (((rpn_loss_cls.mean() + rpn_loss_box.mean()) + RCNN_loss_cls.mean()) + RCNN_loss_bbox.mean())
            loss_temp += loss.item()
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if ((step % args.disp_interval) == 0):
                end = time.time()
                if (step > 0):
                    loss_temp /= args.disp_interval
                if args.mGPUs:
                    loss_rpn_cls = rpn_loss_cls.mean().item()
                    loss_rpn_box = rpn_loss_box.mean().item()
                    loss_rcnn_cls = RCNN_loss_cls.mean().item()
                    loss_rcnn_box = RCNN_loss_bbox.mean().item()
                    fg_cnt = torch.sum(roi_labels.data.ne(0))
                    bg_cnt = (roi_labels.data.numel() - fg_cnt)
                else:
                    loss_rpn_cls = rpn_loss_cls.item()
                    loss_rpn_box = rpn_loss_box.item()
                    loss_rcnn_cls = RCNN_loss_cls.item()
                    loss_rcnn_box = RCNN_loss_bbox.item()
                    fg_cnt = torch.sum(roi_labels.data.ne(0))
                    bg_cnt = (roi_labels.data.numel() - fg_cnt)
                print(('[session %d][epoch %2d][iter %4d/%4d] loss: %.4f, lr: %.2e' % (args.session, epoch, step, iters_per_epoch, loss_temp, lr)))
                print(('\t\t\tfg/bg=(%d/%d), time cost: %f' % (fg_cnt, bg_cnt, (end - start))))
                print(('\t\t\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f' % (loss_rpn_cls, loss_rpn_box, loss_rcnn_cls, loss_rcnn_box)))
                loss_temp = 0
                start = time.time()
            if ((step % args.save_iters) == 0):
                save_name = os.path.join(args.save_dir, 'fpn_{}_{}_{}_adv.pth'.format(args.session, epoch, step))
                save_checkpoint({'session': args.session, 'epoch': epoch, 'model': (FPN.module.state_dict() if args.mGPUs else FPN.state_dict()), 'optimizer': optimizer.state_dict(), 'pooling_mode': cfg.POOLING_MODE, 'class_agnostic': args.class_agnostic}, save_name)
                print('save model: {}'.format(save_name))
        end = time.time()
        print((end - start))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
    if:    elif:
         ...  =  ... .optim.SGD

idx = 29:------------------- similar code ------------------ index = 7, score = 3.0 
def train():
    device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))
    model = fastscnn.FastSCNN(numClasses, True)
    numParams = sum((torch.numel(p) for p in model.parameters()))
    print(f'Total paramers: {numParams}')
    model = model.to(device)
    weightsInit(model)
    (mean, std) = getMeanStd()
    criterion = diceloss.DiceLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=baseLr, momentum=0.9, weight_decay=0.0001)
    (trainDataLoader, valDataLoader) = prepareDataset(dataRoot, trainList, valList, mean, std)
    maxIter = (globalEpoch * len(trainDataLoader))
    for epoch in range(1, globalEpoch):
        subTrain(model, optimizer, criterion, trainDataLoader, epoch, maxIter, device)
        subVal(model, criterion, valDataLoader, device)
        if ((epoch % 20) == 0):
            filename = ((('save/' + 'train_') + str(epoch)) + '.pth')
            torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, filename)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 30:------------------- similar code ------------------ index = 41, score = 3.0 
def main():
    global args, best_prec1
    args = parser.parse_args()
    model_weight = torch.load(FLAGS['pretrain_model'])
    model = MobileNetV2(model_weight, num_classes=FLAGS['class_num'])
    model.eval()
    summary(model, torch.zeros((1, 3, 224, 224)))
    model = model.cuda()
    cudnn.benchmark = True
    model_teacher = OriMobileNetV2(num_classes=FLAGS['class_num'], width_mult=1.0)
    model_teacher.load_state_dict(model_weight)
    model_teacher.cuda()
    criterion = nn.CrossEntropyLoss().cuda()
    optimizer = torch.optim.SGD(filter((lambda p: p.requires_grad), model.parameters()), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    data_transforms = {'train': transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]), 'val': transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}
    data_dir = FLAGS['data_base']
    print('| Preparing model...')
    dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}
    train_loader = torch.utils.data.DataLoader(dsets['train'], batch_size=FLAGS['batch_size'], shuffle=True, num_workers=8, pin_memory=True)
    val_loader = torch.utils.data.DataLoader(dsets['val'], batch_size=(4 * FLAGS['batch_size']), shuffle=False, num_workers=8, pin_memory=True)
    print('data_loader_success!')
    validate(val_loader, model, criterion)
    for epoch in range(args.start_epoch, args.epochs):
        train(train_loader, model_teacher, model, criterion, optimizer, epoch)
        prec1 = validate(val_loader, model, criterion)
        best_prec1 = max(prec1, best_prec1)
        folder_path = 'checkpoint/fine_tune'
        if (not os.path.exists(folder_path)):
            os.makedirs(folder_path)
        torch.save(model.state_dict(), (folder_path + '/model.pth'))
        print(('best acc is %.3f' % best_prec1))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 31:------------------- similar code ------------------ index = 40, score = 3.0 
def main():
    global args, best_prec1
    args = parser.parse_args()
    model_weight = torch.load('/opt/luojh/pretrained_models/ResNet50_ImageNet.pth')
    model = resnet50(model_weight, num_classes=args.class_num)
    model.eval()
    summary(model, torch.zeros((1, 3, 224, 224)))
    model = torch.nn.DataParallel(model.cuda(), device_ids=range(torch.cuda.device_count()))
    cudnn.benchmark = True
    criterion = nn.CrossEntropyLoss().cuda()
    optimizer = torch.optim.SGD(filter((lambda p: p.requires_grad), model.parameters()), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    data_transforms = {'train': transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]), 'val': transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}
    data_dir = args.data_base
    print('| Preparing model...')
    dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}
    train_loader = torch.utils.data.DataLoader(dsets['train'], batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True)
    val_loader = torch.utils.data.DataLoader(dsets['val'], batch_size=args.batch_size, shuffle=False, num_workers=8, pin_memory=True)
    print('data_loader_success!')
    for epoch in range(args.start_epoch, args.epochs):
        train(train_loader, model, criterion, optimizer, epoch)
        prec1 = validate(val_loader, model, criterion)
        best_prec1 = max(prec1, best_prec1)
        folder_path = 'checkpoint/fine_tune'
        if (not os.path.exists(folder_path)):
            os.makedirs(folder_path)
        torch.save(model.state_dict(), (folder_path + '/model.pth'))
        print(('best acc is %.3f' % best_prec1))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 32:------------------- similar code ------------------ index = 39, score = 3.0 
if (__name__ == '__main__'):
    args = parse_args()
    print('Called with args:')
    print(args)
    if (args.dataset == 'visdrone'):
        args.imdb_name = 'visdrone_2017_trainval'
        args.imdbval_name = 'visdrone_2017_test'
        args.set_cfgs = ['FPN_ANCHOR_SCALES', '[16, 32, 64, 128, 256]', 'FPN_FEAT_STRIDES', '[4, 8, 16, 32, 64]', 'ANCHOR_RATIOS', '[0.5, 1, 2]', 'MAX_NUM_GT_BOXES', '60']
    elif (args.dataset == 'uav'):
        args.imdb_name = 'uav_2017_trainval'
        args.imdbval_name = 'uav_2017_test'
        args.set_cfgs = ['FPN_ANCHOR_SCALES', '[16, 32, 64, 128, 256]', 'FPN_FEAT_STRIDES', '[4, 8, 16, 32, 64]', 'ANCHOR_RATIOS', '[0.5, 1, 2]', 'MAX_NUM_GT_BOXES', '107']
    elif (args.dataset == 'coco'):
        args.imdb_name = 'coco_2014_train+coco_2014_valminusminival'
        args.imdbval_name = 'coco_2014_minival'
        args.set_cfgs = ['FPN_ANCHOR_SCALES', '[32, 64, 128, 256, 512]', 'FPN_FEAT_STRIDES', '[4, 8, 16, 32, 64]', 'ANCHOR_RATIOS', '[0.5, 1, 2]', 'MAX_NUM_GT_BOXES', '20']
    args.cfg_file = ('cfgs/{}_ls.yml'.format(args.net) if args.large_scale else 'cfgs/{}.yml'.format(args.net))
    if (args.cfg_file is not None):
        cfg_from_file(args.cfg_file)
    if (args.set_cfgs is not None):
        cfg_from_list(args.set_cfgs)
    print('Using config:')
    pprint.pprint(cfg)
    np.random.seed(cfg.RNG_SEED)
    if (torch.cuda.is_available() and (not args.cuda)):
        print('WARNING: You have a CUDA device, so you should probably run with --cuda')
    cfg.TRAIN.USE_FLIPPED = False
    cfg.USE_GPU_NMS = args.cuda
    'Dataloader for the training'
    (imdb_train, roidb_train, ratio_list_train, ratio_index_train) = combined_roidb(args.imdb_name)
    train_size = len(roidb_train)
    print('{:d} roidb entries'.format(len(roidb_train)))
    sampler_batch = sampler(train_size, args.batch_size)
    dataset_train = roibatchLoader(roidb_train, ratio_list_train, ratio_index_train, args.batch_size, imdb_train.num_classes, training=True)
    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=args.batch_size, sampler=sampler_batch, num_workers=args.num_workers)
    'Dataloader for the validation/testing'
    (imdb_val, roidb_val, ratio_list_val, ratio_index_val) = combined_roidb(args.imdbval_name)
    val_size = len(roidb_val)
    print('{:d} roidb entries'.format(len(roidb_train)))
    sampler_batch = sampler(val_size, args.batch_size)
    dataset_val = roibatchLoader(roidb_val, ratio_list_val, ratio_index_val, args.batch_size, imdb_val.num_classes, training=True)
    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=args.batch_size, sampler=sampler_batch, num_workers=args.num_workers)
    im_data = torch.FloatTensor(1)
    im_info = torch.FloatTensor(1)
    meta_data = torch.FloatTensor(1)
    num_boxes = torch.LongTensor(1)
    gt_boxes = torch.FloatTensor(1)
    if args.cuda:
        im_data = im_data.cuda()
        im_info = im_info.cuda()
        meta_data = meta_data.cuda()
        num_boxes = num_boxes.cuda()
        gt_boxes = gt_boxes.cuda()
    im_data = Variable(im_data)
    im_info = Variable(im_info)
    meta_data = Variable(meta_data)
    num_boxes = Variable(num_boxes)
    gt_boxes = Variable(gt_boxes)
    if args.cuda:
        cfg.CUDA = True
    if (args.net == 'res101'):
        FPN = resnet(imdb_train.classes, 101, pretrained=True, class_agnostic=args.class_agnostic)
    elif (args.net == 'res50'):
        FPN = resnet(imdb_train.classes, 50, pretrained=True, class_agnostic=args.class_agnostic)
    elif (args.net == 'res152'):
        FPN = resnet(imdb_train.classes, 152, pretrained=True, class_agnostic=args.class_agnostic)
    else:
        print('network is not defined')
        pdb.set_trace()
    FPN.create_architecture()
    lr = cfg.TRAIN.LEARNING_RATE
    lr = args.lr
    params_util = []
    params_adv = []
    params_aux = []
    params_keys_util = []
    params_keys_adv = []
    params_keys_aux = []
    for (key, value) in dict(FPN.named_parameters()).items():
        if (value.requires_grad and (not any(((name in key) for name in ['RCNN_weather', 'RCNN_altitude', 'RCNN_angle', 'RCNN_weather_score', 'RCNN_altitude_score', 'RCNN_angle_score'])))):
            params_keys_util.append(key)
            if ('bias' in key):
                params_util += [{'params': [value], 'lr': (lr * (cfg.TRAIN.DOUBLE_BIAS + 1)), 'weight_decay': ((cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY) or 0)}]
            else:
                params_util += [{'params': [value], 'lr': lr, 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]
        if (value.requires_grad and any(((name in key) for name in ['RCNN_weather', 'RCNN_altitude', 'RCNN_angle', 'RCNN_weather_score', 'RCNN_altitude_score', 'RCNN_angle_score']))):
            params_keys_aux.append(key)
            if ('bias' in key):
                params_aux += [{'params': [value], 'lr': (lr * (cfg.TRAIN.DOUBLE_BIAS + 1)), 'weight_decay': ((cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY) or 0)}]
            else:
                params_aux += [{'params': [value], 'lr': lr, 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]
        if (value.requires_grad and any(((name in key) for name in ['RCNN_base']))):
            params_keys_adv.append(key)
            if ('bias' in key):
                params_adv += [{'params': [value], 'lr': ((lr * 0.1) * (cfg.TRAIN.DOUBLE_BIAS + 1)), 'weight_decay': ((cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY) or 0)}]
            else:
                params_adv += [{'params': [value], 'lr': (lr * 0.1), 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]
    if (args.optimizer == 'adam'):
        lr = (lr * 0.1)
        optimizer = torch.optim.Adam(params_util)
    elif (args.optimizer == 'sgd'):
        optimizer = torch.optim.SGD(params_util, momentum=cfg.TRAIN.MOMENTUM)
    aux_optimizer = torch.optim.Adam(params_aux)
    isRestarting = (lambda bool: ('Restart' if bool else 'NoRestart'))
    model_dir = os.path.join(args.model_dir, 'altitude={}_angle={}_weather={}'.format(str(args.gamma_altitude), str(args.gamma_angle), str(args.gamma_weather)), isRestarting(args.use_restarting))
    summary_dir = os.path.join(args.summary_dir, 'altitude={}_angle={}_weather={}'.format(str(args.gamma_altitude), str(args.gamma_angle), str(args.gamma_weather)), isRestarting(args.use_restarting))
    if args.resume:
        load_name = os.path.join(model_dir, 'fpn_{}_{}_{}_adv.pth'.format(args.checksession, args.checkepoch, args.checkpoint))
        print(('loading checkpoint %s' % load_name))
        checkpoint = torch.load(load_name)
        args.session = checkpoint['session']
        args.start_epoch = checkpoint['epoch']
        model_dict = FPN.state_dict()
        model_dict.update(checkpoint['model'])
        FPN.load_state_dict(model_dict)
        optimizer.load_state_dict(checkpoint['optimizer'])
        for state in optimizer.state.values():
            for (k, v) in state.items():
                if isinstance(v, torch.Tensor):
                    state[k] = v.cuda()
        aux_optimizer.load_state_dict(checkpoint['aux_optimizer'])
        for state in aux_optimizer.state.values():
            for (k, v) in state.items():
                if isinstance(v, torch.Tensor):
                    state[k] = v.cuda()
        print(('loaded checkpoint %s' % load_name))
    else:
        load_name = os.path.join(os.path.join(args.model_dir, 'baseline'), 'fpn_{}_{}_{}.pth'.format(args.checksession, args.checkepoch, args.checkpoint))
        print(('loading checkpoint %s' % load_name))
        checkpoint = torch.load(load_name)
        args.session = checkpoint['session']
        args.start_epoch = checkpoint['epoch']
        model_dict = FPN.state_dict()
        model_dict.update(checkpoint['model'])
        FPN.load_state_dict(model_dict)
        print(('loaded checkpoint %s' % load_name))
    if args.mGPUs:
        FPN = nn.DataParallel(FPN)
    if args.cuda:
        FPN.cuda()
    iters_per_epoch = int((train_size / args.batch_size))
    if args.use_tfboard:
        from tensorboardX import SummaryWriter
        logger = SummaryWriter('logs')
    niter = args.niter
    FPN.train()
    data_iter_train = iter(dataloader_train)
    data_iter_val = iter(dataloader_val)
    if (not os.path.exists(model_dir)):
        os.makedirs(model_dir)
    if (not os.path.exists(summary_dir)):
        os.makedirs(summary_dir)
    train_summary_file = open(os.path.join(summary_dir, 'train_summary.txt'), 'w', 0)
    val_summary_file = open(os.path.join(summary_dir, 'val_summary.txt'), 'w', 0)
    while (niter < (iters_per_epoch * ((args.max_epochs - args.start_epoch) + 1))):
        FPN.zero_grad()
        start = time.time()
        if ((niter % ((args.lr_decay_step + 1) * iters_per_epoch)) == 0):
            adjust_learning_rate(optimizer, args.lr_decay_gamma)
            lr *= args.lr_decay_gamma
        if ((niter == 0) or (args.use_restarting and ((niter % args.restarting_iters) == 0))):
            '\n            Restarting\n            '
            rcnn_altitude = FPN.module.RCNN_altitude
            rcnn_angle = FPN.module.RCNN_angle
            rcnn_weather = FPN.module.RCNN_weather
            rcnn_altitude_score = FPN.module.RCNN_altitude_score
            rcnn_angle_score = FPN.module.RCNN_angle_score
            rcnn_weather_score = FPN.module.RCNN_weather_score
            FPN.module.normal_init(rcnn_weather_score, 0, 0.001, cfg.TRAIN.TRUNCATED)
            FPN.module.normal_init(rcnn_angle_score, 0, 0.001, cfg.TRAIN.TRUNCATED)
            FPN.module.normal_init(rcnn_altitude_score, 0, 0.001, cfg.TRAIN.TRUNCATED)
            FPN.module.weights_init(rcnn_altitude, 0, 0.01, cfg.TRAIN.TRUNCATED)
            FPN.module.weights_init(rcnn_angle, 0, 0.01, cfg.TRAIN.TRUNCATED)
            FPN.module.weights_init(rcnn_weather, 0, 0.01, cfg.TRAIN.TRUNCATED)
            for _ in itertools.repeat(None, args.retraining_steps):
                loss_adv_temp = 0
                loss_aux_temp = 0
                acc_altitude_temp = 0
                acc_angle_temp = 0
                acc_weather_temp = 0
                aux_optimizer.zero_grad()
                for _ in itertools.repeat(None, args.n_minibatches):
                    try:
                        data = next(data_iter_train)
                    except StopIteration:
                        data_iter_train = iter(dataloader_train)
                        data = next(data_iter_train)
                    im_data.data.resize_(data[0].size()).copy_(data[0])
                    im_info.data.resize_(data[1].size()).copy_(data[1])
                    meta_data.data.resize_(data[2].size()).copy_(data[2])
                    gt_boxes.data.resize_(data[3].size()).copy_(data[3])
                    num_boxes.data.resize_(data[4].size()).copy_(data[4])
                    try:
                        (RCNN_loss_altitude, RCNN_loss_altitude_adv, RCNN_acc_altitude, RCNN_loss_angle, RCNN_loss_angle_adv, RCNN_acc_angle, RCNN_loss_weather, RCNN_loss_weather_adv, RCNN_acc_weather) = FPN(im_data, im_info, meta_data, gt_boxes, num_boxes, run_partial=True)
                    except Exception:
                        sys.exc_clear()
                        print(im_data)
                        print(im_info)
                        print(meta_data)
                        print(gt_boxes)
                        print('Catching TypeError: forward() takes at least 6 arguments (2 given)')
                        continue
                    loss_altitude = RCNN_loss_altitude.mean()
                    loss_altitude_adv = RCNN_loss_altitude_adv.mean()
                    acc_altitude = RCNN_acc_altitude.mean()
                    loss_angle = RCNN_loss_angle.mean()
                    loss_angle_adv = RCNN_loss_angle_adv.mean()
                    acc_angle = RCNN_acc_angle.mean()
                    loss_weather = RCNN_loss_weather.mean()
                    loss_weather_adv = RCNN_loss_weather_adv.mean()
                    acc_weather = RCNN_acc_weather.mean()
                    acc_altitude = (acc_altitude / args.n_minibatches)
                    loss_altitude_adv = (loss_altitude_adv / args.n_minibatches)
                    loss_altitude_aux = (loss_altitude / args.n_minibatches)
                    acc_angle = (acc_angle / args.n_minibatches)
                    loss_angle_adv = (loss_angle_adv / args.n_minibatches)
                    loss_angle_aux = (loss_angle / args.n_minibatches)
                    acc_weather = (acc_weather / args.n_minibatches)
                    loss_weather_adv = (loss_weather_adv / args.n_minibatches)
                    loss_weather_aux = (loss_weather / args.n_minibatches)
                    loss_adv = ((loss_altitude_adv + loss_angle_adv) + loss_weather_adv)
                    loss_aux = ((loss_altitude_aux + loss_angle_aux) + loss_weather_aux)
                    loss_adv_temp += loss_adv.item()
                    loss_aux_temp += loss_aux.item()
                    acc_altitude_temp += acc_altitude.item()
                    acc_angle_temp += acc_angle.item()
                    acc_weather_temp += acc_weather.item()
                    loss_aux.backward(retain_graph=False)
                aux_optimizer.step()
                if (niter == 0):
                    print(('Initialization (Auxiliary): [session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_aux_temp, loss_adv_temp, lr)))
                else:
                    print(('Restarting (Auxiliary): [session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_aux_temp, loss_adv_temp, lr)))
        if args.use_adversarial_loss:
            loss_temp = 0
            loss_util_temp = 0
            loss_adv_temp = 0
            loss_aux_temp = 0
            optimizer.zero_grad()
            for _ in itertools.repeat(None, args.n_minibatches):
                try:
                    data = next(data_iter_train)
                except StopIteration:
                    data_iter_train = iter(dataloader_train)
                    data = next(data_iter_train)
                im_data.data.resize_(data[0].size()).copy_(data[0])
                im_info.data.resize_(data[1].size()).copy_(data[1])
                meta_data.data.resize_(data[2].size()).copy_(data[2])
                gt_boxes.data.resize_(data[3].size()).copy_(data[3])
                num_boxes.data.resize_(data[4].size()).copy_(data[4])
                try:
                    (rois, cls_prob, bbox_pred, rpn_loss_cls, rpn_loss_box, RCNN_loss_cls, RCNN_loss_bbox, RCNN_loss_altitude, RCNN_loss_altitude_adv, RCNN_acc_altitude, RCNN_loss_angle, RCNN_loss_angle_adv, RCNN_acc_angle, RCNN_loss_weather, RCNN_loss_weather_adv, RCNN_acc_weather, rois_label) = FPN(im_data, im_info, meta_data, gt_boxes, num_boxes, run_partial=False)
                except Exception:
                    sys.exc_clear()
                    print(im_data)
                    print(im_info)
                    print(meta_data)
                    print(gt_boxes)
                    print('Catching TypeError: forward() takes at least 6 arguments (2 given)')
                    continue
                loss_rpn = (rpn_loss_cls.mean() + rpn_loss_box.mean())
                loss_rcnn = (RCNN_loss_cls.mean() + RCNN_loss_bbox.mean())
                loss_altitude = RCNN_loss_altitude.mean()
                loss_altitude_adv = RCNN_loss_altitude_adv.mean()
                loss_angle = RCNN_loss_angle.mean()
                loss_angle_adv = RCNN_loss_angle_adv.mean()
                loss_weather = RCNN_loss_weather.mean()
                loss_weather_adv = RCNN_loss_weather_adv.mean()
                loss_util = ((loss_rpn + loss_rcnn) / args.n_minibatches)
                loss_altitude_adv = (loss_altitude_adv / args.n_minibatches)
                loss_altitude_aux = (loss_altitude / args.n_minibatches)
                loss_angle_adv = (loss_angle_adv / args.n_minibatches)
                loss_angle_aux = (loss_angle / args.n_minibatches)
                loss_weather_adv = (loss_weather_adv / args.n_minibatches)
                loss_weather_aux = (loss_weather / args.n_minibatches)
                loss_adv = ((loss_altitude_adv + loss_angle_adv) + loss_weather_adv)
                loss_aux = ((loss_altitude_aux + loss_angle_aux) + loss_weather_aux)
                loss = (((loss_util + (args.gamma_altitude * loss_altitude_adv)) + (args.gamma_angle * loss_angle_adv)) + (args.gamma_weather * loss_weather_adv))
                loss_util_temp += loss_util.item()
                loss_adv_temp += loss_adv.item()
                loss_aux_temp += loss_aux.item()
                loss_temp += loss.item()
                loss.backward(retain_graph=False)
            optimizer.step()
            print(('Alternating Training (Utility + Adversarial): [session %d][epoch %2d][iter %4d/%4d] utility loss: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, utility+adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, loss_util_temp, loss_aux_temp, loss_adv_temp, loss_temp, lr)))
        if args.monitor_discriminator:
            while True:
                loss_adv_temp = 0
                loss_aux_temp = 0
                acc_altitude_temp = 0
                acc_angle_temp = 0
                acc_weather_temp = 0
                aux_optimizer.zero_grad()
                for _ in itertools.repeat(None, args.n_minibatches):
                    try:
                        data = next(data_iter_train)
                    except StopIteration:
                        data_iter_train = iter(dataloader_train)
                        data = next(data_iter_train)
                    im_data.data.resize_(data[0].size()).copy_(data[0])
                    im_info.data.resize_(data[1].size()).copy_(data[1])
                    meta_data.data.resize_(data[2].size()).copy_(data[2])
                    gt_boxes.data.resize_(data[3].size()).copy_(data[3])
                    num_boxes.data.resize_(data[4].size()).copy_(data[4])
                    try:
                        (RCNN_loss_altitude, RCNN_loss_altitude_adv, RCNN_acc_altitude, RCNN_loss_angle, RCNN_loss_angle_adv, RCNN_acc_angle, RCNN_loss_weather, RCNN_loss_weather_adv, RCNN_acc_weather) = FPN(im_data, im_info, meta_data, gt_boxes, num_boxes, run_partial=True)
                    except Exception:
                        sys.exc_clear()
                        print(im_data)
                        print(im_info)
                        print(meta_data)
                        print(gt_boxes)
                        print('Catching TypeError: forward() takes at least 6 arguments (2 given)')
                        continue
                    loss_altitude = RCNN_loss_altitude.mean()
                    loss_altitude_adv = RCNN_loss_altitude_adv.mean()
                    acc_altitude = RCNN_acc_altitude.mean()
                    loss_angle = RCNN_loss_angle.mean()
                    loss_angle_adv = RCNN_loss_angle_adv.mean()
                    acc_angle = RCNN_acc_angle.mean()
                    loss_weather = RCNN_loss_weather.mean()
                    loss_weather_adv = RCNN_loss_weather_adv.mean()
                    acc_weather = RCNN_acc_weather.mean()
                    acc_altitude = (acc_altitude / args.n_minibatches)
                    loss_altitude_adv = (loss_altitude_adv / args.n_minibatches)
                    loss_altitude_aux = (loss_altitude / args.n_minibatches)
                    acc_angle = (acc_angle / args.n_minibatches)
                    loss_angle_adv = (loss_angle_adv / args.n_minibatches)
                    loss_angle_aux = (loss_angle / args.n_minibatches)
                    acc_weather = (acc_weather / args.n_minibatches)
                    loss_weather_adv = (loss_weather_adv / args.n_minibatches)
                    loss_weather_aux = (loss_weather / args.n_minibatches)
                    loss_adv = ((loss_altitude_adv + loss_angle_adv) + loss_weather_adv)
                    loss_aux = ((loss_altitude_aux + loss_angle_aux) + loss_weather_aux)
                    loss_adv_temp += loss_adv.item()
                    loss_aux_temp += loss_aux.item()
                    acc_altitude_temp += acc_altitude.item()
                    acc_angle_temp += acc_angle.item()
                    acc_weather_temp += acc_weather.item()
                    loss_aux.backward(retain_graph=False)
                aux_optimizer.step()
                print(('Alternating Training (Auxiliary): [session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_aux_temp, loss_adv_temp, lr)))
                if ((acc_angle_temp > args.angle_thresh) and (acc_altitude_temp > args.altitude_thresh) and (acc_weather_temp > args.weather_thresh)):
                    break
        if args.eval_display:
            if ((niter % args.disp_interval) == 0):
                end = time.time()
                'Training evaluation'
                loss_temp = 0
                loss_util_temp = 0
                loss_adv_temp = 0
                loss_aux_temp = 0
                acc_altitude_temp = 0
                acc_angle_temp = 0
                acc_weather_temp = 0
                loss_rpn_cls_temp = 0
                loss_rpn_box_temp = 0
                loss_rcnn_cls_temp = 0
                loss_rcnn_box_temp = 0
                fg_cnt = 0
                bg_cnt = 0
                with torch.no_grad():
                    for _ in itertools.repeat(None, args.n_minibatches_eval):
                        try:
                            data = next(data_iter_train)
                        except StopIteration:
                            data_iter_train = iter(dataloader_train)
                            data = next(data_iter_train)
                        im_data.data.resize_(data[0].size()).copy_(data[0])
                        im_info.data.resize_(data[1].size()).copy_(data[1])
                        meta_data.data.resize_(data[2].size()).copy_(data[2])
                        gt_boxes.data.resize_(data[3].size()).copy_(data[3])
                        num_boxes.data.resize_(data[4].size()).copy_(data[4])
                        try:
                            (rois, cls_prob, bbox_pred, rpn_loss_cls, rpn_loss_box, RCNN_loss_cls, RCNN_loss_bbox, RCNN_loss_altitude, RCNN_loss_altitude_adv, RCNN_acc_altitude, RCNN_loss_angle, RCNN_loss_angle_adv, RCNN_acc_angle, RCNN_loss_weather, RCNN_loss_weather_adv, RCNN_acc_weather, rois_label) = FPN(im_data, im_info, meta_data, gt_boxes, num_boxes, run_partial=False)
                        except Exception:
                            sys.exc_clear()
                            print(im_data)
                            print(im_info)
                            print(meta_data)
                            print(gt_boxes)
                            print('Catching TypeError: forward() takes at least 6 arguments (2 given)')
                            continue
                        loss_rpn = (rpn_loss_cls.mean() + rpn_loss_box.mean())
                        loss_rcnn = (RCNN_loss_cls.mean() + RCNN_loss_bbox.mean())
                        loss_altitude = RCNN_loss_altitude.mean()
                        loss_altitude_adv = RCNN_loss_altitude_adv.mean()
                        acc_altitude = RCNN_acc_altitude.mean()
                        loss_angle = RCNN_loss_angle.mean()
                        loss_angle_adv = RCNN_loss_angle_adv.mean()
                        acc_angle = RCNN_acc_angle.mean()
                        loss_weather = RCNN_loss_weather.mean()
                        loss_weather_adv = RCNN_loss_weather_adv.mean()
                        acc_weather = RCNN_acc_weather.mean()
                        rpn_loss_cls = rpn_loss_cls.mean()
                        rpn_loss_box = rpn_loss_box.mean()
                        RCNN_loss_cls = RCNN_loss_cls.mean()
                        RCNN_loss_bbox = RCNN_loss_bbox.mean()
                        loss_rpn_cls = (rpn_loss_cls / args.n_minibatches_eval)
                        loss_rpn_box = (rpn_loss_box / args.n_minibatches_eval)
                        loss_rcnn_cls = (RCNN_loss_cls / args.n_minibatches_eval)
                        loss_rcnn_box = (RCNN_loss_bbox / args.n_minibatches_eval)
                        loss_util = ((loss_rpn + loss_rcnn) / args.n_minibatches_eval)
                        loss_altitude_adv = (loss_altitude_adv / args.n_minibatches_eval)
                        loss_altitude_aux = (loss_altitude / args.n_minibatches_eval)
                        acc_altitude = (acc_altitude / args.n_minibatches_eval)
                        loss_angle_adv = (loss_angle_adv / args.n_minibatches_eval)
                        loss_angle_aux = (loss_angle / args.n_minibatches_eval)
                        acc_angle = (acc_angle / args.n_minibatches_eval)
                        loss_weather_adv = (loss_weather_adv / args.n_minibatches_eval)
                        loss_weather_aux = (loss_weather / args.n_minibatches_eval)
                        acc_weather = (acc_weather / args.n_minibatches_eval)
                        loss = (((loss_util + (args.gamma_altitude * loss_altitude_adv)) + (args.gamma_angle * loss_angle_adv)) + (args.gamma_weather * loss_weather_adv))
                        loss_adv = ((loss_altitude_adv + loss_angle_adv) + loss_weather_adv)
                        loss_aux = ((loss_altitude_aux + loss_angle_aux) + loss_weather_aux)
                        loss_util_temp += loss_util.item()
                        loss_adv_temp += loss_adv.item()
                        loss_aux_temp += loss_aux.item()
                        loss_temp += loss.item()
                        acc_altitude_temp += acc_altitude.item()
                        acc_angle_temp += acc_angle.item()
                        acc_weather_temp += acc_weather.item()
                        loss_rpn_cls_temp += loss_rpn_cls
                        loss_rpn_box_temp += loss_rpn_box
                        loss_rcnn_cls_temp += loss_rcnn_cls
                        loss_rcnn_box_temp += loss_rcnn_box
                        fg_cnt += torch.sum(rois_label.data.ne(0))
                        bg_cnt += (rois_label.data.numel() - fg_cnt)
                print(('**********DISPLAY TRAINING**********: [session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, utility loss: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, utility+adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_util_temp, loss_aux_temp, loss_adv_temp, loss_temp, lr)))
                print(('\t\t\tfg/bg=(%d/%d), time cost: %f' % (fg_cnt, bg_cnt, (end - start))))
                print(('\t\t\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f' % (loss_rpn_cls_temp, loss_rpn_box_temp, loss_rcnn_cls_temp, loss_rcnn_box_temp)))
                if args.use_tfboard:
                    info = {'training_altitude_acc': acc_altitude_temp, 'training_angle_acc': acc_angle_temp, 'training_weather_acc': acc_weather_temp, 'train_loss': loss_temp, 'train_loss_util': loss_util_temp, 'train_loss_aux': loss_aux_temp, 'train_loss_adv': loss_adv_temp, 'train_loss_rpn_cls': loss_rpn_cls_temp, 'train_loss_rpn_box': loss_rpn_box_temp, 'train_loss_rcnn_cls': loss_rcnn_cls_temp, 'train_loss_rcnn_box': loss_rcnn_box_temp}
                    logger.add_scalars('logs_altitude={}_angle={}_weather=_{}/losses_train'.format(args.gamma_altitude, args.gamma_angle, args.gamma_weather, isRestarting(args.use_restarting)), info, niter)
                train_summary_file.write(('[session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, utility loss: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, utility+adversarial loss: %.4f, lr: %.2e\n' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_util_temp, loss_aux_temp, loss_adv_temp, loss_temp, lr)))
                train_summary_file.write(('\t\t\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f\n' % (loss_rpn_cls_temp, loss_rpn_box_temp, loss_rcnn_cls_temp, loss_rcnn_box_temp)))
                'Validation evaluation'
                loss_temp = 0
                loss_util_temp = 0
                loss_adv_temp = 0
                loss_aux_temp = 0
                acc_altitude_temp = 0
                acc_angle_temp = 0
                acc_weather_temp = 0
                loss_rpn_cls_temp = 0
                loss_rpn_box_temp = 0
                loss_rcnn_cls_temp = 0
                loss_rcnn_box_temp = 0
                fg_cnt = 0
                bg_cnt = 0
                with torch.no_grad():
                    for _ in itertools.repeat(None, args.n_minibatches_eval):
                        try:
                            data = next(data_iter_val)
                        except StopIteration:
                            data_iter_val = iter(dataloader_val)
                            data = next(data_iter_val)
                        im_data.data.resize_(data[0].size()).copy_(data[0])
                        im_info.data.resize_(data[1].size()).copy_(data[1])
                        meta_data.data.resize_(data[2].size()).copy_(data[2])
                        gt_boxes.data.resize_(data[3].size()).copy_(data[3])
                        num_boxes.data.resize_(data[4].size()).copy_(data[4])
                        try:
                            (rois, cls_prob, bbox_pred, rpn_loss_cls, rpn_loss_box, RCNN_loss_cls, RCNN_loss_bbox, RCNN_loss_altitude, RCNN_loss_altitude_adv, RCNN_acc_altitude, RCNN_loss_angle, RCNN_loss_angle_adv, RCNN_acc_angle, RCNN_loss_weather, RCNN_loss_weather_adv, RCNN_acc_weather, rois_label) = FPN(im_data, im_info, meta_data, gt_boxes, num_boxes, run_partial=False)
                        except Exception:
                            sys.exc_clear()
                            print(im_data)
                            print(im_info)
                            print(meta_data)
                            print(gt_boxes)
                            print('Catching TypeError: forward() takes at least 6 arguments (2 given)')
                            continue
                        loss_rpn = (rpn_loss_cls.mean() + rpn_loss_box.mean())
                        loss_rcnn = (RCNN_loss_cls.mean() + RCNN_loss_bbox.mean())
                        loss_altitude = RCNN_loss_altitude.mean()
                        loss_altitude_adv = RCNN_loss_altitude_adv.mean()
                        acc_altitude = RCNN_acc_altitude.mean()
                        loss_angle = RCNN_loss_angle.mean()
                        loss_angle_adv = RCNN_loss_angle_adv.mean()
                        acc_angle = RCNN_acc_angle.mean()
                        loss_weather = RCNN_loss_weather.mean()
                        loss_weather_adv = RCNN_loss_weather_adv.mean()
                        acc_weather = RCNN_acc_weather.mean()
                        rpn_loss_cls = rpn_loss_cls.mean()
                        rpn_loss_box = rpn_loss_box.mean()
                        RCNN_loss_cls = RCNN_loss_cls.mean()
                        RCNN_loss_bbox = RCNN_loss_bbox.mean()
                        loss_rpn_cls = (rpn_loss_cls / args.n_minibatches_eval)
                        loss_rpn_box = (rpn_loss_box / args.n_minibatches_eval)
                        loss_rcnn_cls = (RCNN_loss_cls / args.n_minibatches_eval)
                        loss_rcnn_box = (RCNN_loss_bbox / args.n_minibatches_eval)
                        loss_util = ((loss_rpn + loss_rcnn) / args.n_minibatches_eval)
                        loss_altitude_adv = (loss_altitude_adv / args.n_minibatches_eval)
                        loss_altitude_aux = (loss_altitude / args.n_minibatches_eval)
                        acc_altitude = (acc_altitude / args.n_minibatches_eval)
                        loss_angle_adv = (loss_angle_adv / args.n_minibatches_eval)
                        loss_angle_aux = (loss_angle / args.n_minibatches_eval)
                        acc_angle = (acc_angle / args.n_minibatches_eval)
                        loss_weather_adv = (loss_weather_adv / args.n_minibatches_eval)
                        loss_weather_aux = (loss_weather / args.n_minibatches_eval)
                        acc_weather = (acc_weather / args.n_minibatches_eval)
                        loss = (((loss_util + (args.gamma_altitude * loss_altitude_adv)) + (args.gamma_angle * loss_angle_adv)) + (args.gamma_weather * loss_weather_adv))
                        loss_adv = ((loss_altitude_adv + loss_angle_adv) + loss_weather_adv)
                        loss_aux = ((loss_altitude_aux + loss_angle_aux) + loss_weather_aux)
                        loss_util_temp += loss_util.item()
                        loss_adv_temp += loss_adv.item()
                        loss_aux_temp += loss_aux.item()
                        loss_temp += loss.item()
                        acc_altitude_temp += acc_altitude.item()
                        acc_angle_temp += acc_angle.item()
                        acc_weather_temp += acc_weather.item()
                        loss_rpn_cls_temp += loss_rpn_cls
                        loss_rpn_box_temp += loss_rpn_box
                        loss_rcnn_cls_temp += loss_rcnn_cls
                        loss_rcnn_box_temp += loss_rcnn_box
                        fg_cnt += torch.sum(rois_label.data.ne(0))
                        bg_cnt += (rois_label.data.numel() - fg_cnt)
                print(('**********DISPLAY VALIDATION**********: [session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, utility loss: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, utility+adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_util_temp, loss_aux_temp, loss_adv_temp, loss_temp, lr)))
                print(('\t\t\tfg/bg=(%d/%d), time cost: %f' % (fg_cnt, bg_cnt, (end - start))))
                print(('\t\t\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f' % (loss_rpn_cls_temp, loss_rpn_box_temp, loss_rcnn_cls_temp, loss_rcnn_box_temp)))
                if args.use_tfboard:
                    info = {'val_altitude_acc': acc_altitude_temp, 'val_angle_acc': acc_angle_temp, 'val_weather_acc': acc_weather_temp, 'val_loss': loss_temp, 'val_loss_util': loss_util_temp, 'val_loss_aux': loss_aux_temp, 'val_loss_adv': loss_adv_temp, 'val_loss_rpn_cls': loss_rpn_cls_temp, 'val_loss_rpn_box': loss_rpn_box_temp, 'val_loss_rcnn_cls': loss_rcnn_cls_temp, 'val_loss_rcnn_box': loss_rcnn_box_temp}
                    isRestarting = (lambda bool: ('Restart' if bool else 'NoRestart'))
                    logger.add_scalars('logs_altitude={}_angle={}_weather={}_{}/losses_val'.format(args.gamma_altitude, args.gamma_angle, args.gamma_weather, isRestarting(args.use_restarting)), info, niter)
                val_summary_file.write(('[session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather_accuracy: %.4f, utility loss: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, utility+adversarial loss: %.4f, lr: %.2e\n' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_util_temp, loss_aux_temp, loss_adv_temp, loss_temp, lr)))
                val_summary_file.write(('\t\t\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f\n' % (loss_rpn_cls_temp, loss_rpn_box_temp, loss_rcnn_cls_temp, loss_rcnn_box_temp)))
                start = time.time()
        if ((niter % args.save_iters) == 0):
            save_name = os.path.join(model_dir, 'fpn_{}_{}_{}_adv.pth'.format(args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch)))
            save_checkpoint({'session': args.session, 'epoch': ((niter // iters_per_epoch) + 1), 'model': (FPN.module.state_dict() if args.mGPUs else FPN.state_dict()), 'optimizer': optimizer.state_dict(), 'aux_optimizer': aux_optimizer.state_dict(), 'pooling_mode': cfg.POOLING_MODE, 'class_agnostic': args.class_agnostic}, save_name)
            print('save model: {}'.format(save_name))
        niter += 1
    train_summary_file.close()
    val_summary_file.close()
    if args.use_tfboard:
        logger.close()

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
    if:    elif:
         ...  =  ... .optim.SGD

idx = 33:------------------- similar code ------------------ index = 1, score = 3.0 
if (__name__ == '__main__'):
    args = get_args()
    print(vars(args))
    SEED = args.seed
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    dataloaders = load_data(args.root_path, args.src, args.tar, args.batch_size)
    model = DSAN(num_classes=args.nclass).cuda()
    correct = 0
    stop = 0
    if args.bottleneck:
        optimizer = torch.optim.SGD([{'params': model.feature_layers.parameters()}, {'params': model.bottle.parameters(), 'lr': args.lr[1]}, {'params': model.cls_fc.parameters(), 'lr': args.lr[2]}], lr=args.lr[0], momentum=args.momentum, weight_decay=args.decay)
    else:
        optimizer = torch.optim.SGD([{'params': model.feature_layers.parameters()}, {'params': model.cls_fc.parameters(), 'lr': args.lr[1]}], lr=args.lr[0], momentum=args.momentum, weight_decay=args.decay)
    for epoch in range(1, (args.nepoch + 1)):
        stop += 1
        for (index, param_group) in enumerate(optimizer.param_groups):
            param_group['lr'] = (args.lr[index] / math.pow((1 + ((10 * (epoch - 1)) / args.nepoch)), 0.75))
        train_epoch(epoch, model, dataloaders, optimizer)
        t_correct = test(model, dataloaders[(- 1)])
        if (t_correct > correct):
            correct = t_correct
            stop = 0
            torch.save(model, 'model.pkl')
        print(f'''{args.src}-{args.tar}: max correct: {correct} max accuracy: {((100.0 * correct) / len(dataloaders[(- 1)].dataset)):.2f}%
''')
        if (stop >= args.early_stop):
            print(f'Final test acc: {((100.0 * correct) / len(dataloaders[(- 1)].dataset)):.2f}%')
            break

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
    if:
         ...  =  ... .optim.SGD

idx = 34:------------------- similar code ------------------ index = 37, score = 3.0 
def main_worker(gpu, ngpus_per_node, args):
    global best_acc1
    args.gpu = gpu
    if (args.gpu is not None):
        print('Use GPU: {} for training'.format(args.gpu))
    if args.distributed:
        if ((args.dist_url == 'env://') and (args.rank == (- 1))):
            args.rank = int(os.environ['RANK'])
        if args.multiprocessing_distributed:
            args.rank = ((args.rank * ngpus_per_node) + gpu)
        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank)
    if args.pretrained:
        print("=> using pre-trained model '{}'".format(args.arch))
        model = models.__dict__[args.arch](pretrained=True)
    else:
        print("=> creating model '{}'".format(args.arch))
        import models_lpf.resnet
        import models_fconv_lpf.resnet
        import models_lpf.vgg
        import models_fconv_lpf.vgg
        if (args.arch == 'vgg11_bn_fconv'):
            model = vgg_fconv.vgg11_bn(num_classes=args.num_class)
        elif (args.arch == 'vgg13_bn_fconv'):
            model = vgg_fconv.vgg13_bn(num_classes=args.num_class)
        elif (args.arch == 'vgg16_bn_fconv'):
            model = vgg_fconv.vgg16_bn(num_classes=args.num_class)
        elif (args.arch == 'vgg19_bn_fconv'):
            model = vgg_fconv.vgg19_bn(num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg11_bn_lpf'):
            model = models_lpf.vgg11_bn(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg13_bn_lpf'):
            model = models_lpf.vgg.vgg13_bn(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg16_bn_lpf'):
            model = models_lpf.vgg.vgg16_bn(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg19_bn_lpf'):
            model = models_lpf.vgg.vgg19_bn(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg11_bn_fconv_lpf'):
            model = models_fconv_lpf.vgg.vgg11_bn(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg13_bn_fconv_lpf'):
            model = models_fconv_lpf.vgg.vgg13_bn(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg16_bn_fconv_lpf'):
            model = models_fconv_lpf.vgg.vgg16_bn(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg19_bn_fconv_lpf'):
            model = models_fconv_lpf.vgg.vgg19_bn(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch == 'vgg11_fconv'):
            model = vgg_fconv.vgg11(num_classes=args.num_class)
        elif (args.arch == 'vgg13_fconv'):
            model = vgg_fconv.vgg13(num_classes=args.num_class)
        elif (args.arch == 'vgg16_fconv'):
            model = vgg_fconv.vgg16(num_classes=args.num_class)
        elif (args.arch == 'vgg19_fconv'):
            model = vgg_fconv.vgg19(num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg11_lpf'):
            model = models_lpf.vgg.vgg11(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg13_lpf'):
            model = models_lpf.vgg.vgg13(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg16_lpf'):
            model = models_lpf.vgg.vgg16(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg19_lpf'):
            model = models_lpf.vgg.vgg19(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg11_fconv_lpf'):
            model = models_fconv_lpf.vgg.vgg11(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg13_fconv_lpf'):
            model = models_fconv_lpf.vgg.vgg13(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg16_fconv_lpf'):
            model = models_fconv_lpf.vgg.vgg16(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'vgg19_fconv_lpf'):
            model = models_fconv_lpf.vgg.vgg19(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch == 'resnet18_fconv'):
            model = resnet_fconv.resnet18(num_classes=args.num_class)
        elif (args.arch == 'resnet34_fconv'):
            model = resnet_fconv.resnet34(num_classes=args.num_class)
        elif (args.arch == 'resnet50_fconv'):
            model = resnet_fconv.resnet50(num_classes=args.num_class)
        elif (args.arch == 'resnet101_fconv'):
            model = resnet_fconv.resnet101(num_classes=args.num_class)
        elif (args.arch == 'resnet152_fconv'):
            model = resnet_fconv.resnet152(num_classes=args.num_class)
        elif (args.arch == 'resnext50_32x4d_fconv'):
            model = resnet_fconv.resnext50_32x4d(num_classes=args.num_class)
        elif (args.arch == 'resnext101_32x8d_fconv'):
            model = resnet_fconv.resnext101_32x8d(num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'resnet18_lpf'):
            model = models_lpf.resnet.resnet18(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'resnet34_lpf'):
            model = models_lpf.resnet.resnet34(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'resnet50_lpf'):
            model = models_lpf.resnet.resnet50(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'resnet101_lpf'):
            model = models_lpf.resnet.resnet101(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'resnet152_lpf'):
            model = models_lpf.resnet.resnet152(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'resnet18_fconv_lpf'):
            model = models_fconv_lpf.resnet.resnet18(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'resnet34_fconv_lpf'):
            model = models_fconv_lpf.resnet.resnet34(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'resnet50_fconv_lpf'):
            model = models_fconv_lpf.resnet.resnet50(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'resnet101_fconv_lpf'):
            model = models_fconv_lpf.resnet.resnet101(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        elif (args.arch[:(- 1)] == 'resnet152_fconv_lpf'):
            model = models_fconv_lpf.resnet.resnet152(filter_size=int(args.arch[(- 1)]), num_classes=args.num_class)
        else:
            model = models.__dict__[args.arch]()
    if (args.weights is not None):
        print(('=> using saved weights [%s]' % args.weights))
        weights = torch.load(args.weights)
        model.load_state_dict(weights['state_dict'])
    if args.distributed:
        if (args.gpu is not None):
            torch.cuda.set_device(args.gpu)
            model.cuda(args.gpu)
            args.batch_size = int((args.batch_size / ngpus_per_node))
            args.workers = int((args.workers / ngpus_per_node))
            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
        else:
            model.cuda()
            model = torch.nn.parallel.DistributedDataParallel(model)
    elif (args.gpu is not None):
        torch.cuda.set_device(args.gpu)
        model = model.cuda(args.gpu)
    elif (args.arch.startswith('alexnet') or args.arch.startswith('vgg')):
        model.features = torch.nn.DataParallel(model.features)
        model.cuda()
    else:
        model = torch.nn.DataParallel(model).cuda()
    criterion = nn.CrossEntropyLoss().cuda(args.gpu)
    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    if args.resume:
        if os.path.isfile(args.resume):
            print("=> loading checkpoint '{}'".format(args.resume))
            checkpoint = torch.load(args.resume)
            model.load_state_dict(checkpoint['state_dict'], strict=False)
            if ('optimizer' in checkpoint.keys()):
                args.start_epoch = checkpoint['epoch']
                best_acc1 = checkpoint['best_acc1']
                if (args.gpu is not None):
                    best_acc1 = best_acc1.to(args.gpu)
                optimizer.load_state_dict(checkpoint['optimizer'])
            else:
                print('  No optimizer saved')
            print("=> loaded checkpoint '{}' (epoch {})".format(args.resume, checkpoint['epoch']))
        else:
            print("=> no checkpoint found at '{}'".format(args.resume))
    cudnn.benchmark = True
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]
    normalize = transforms.Normalize(mean=mean, std=std)
    if args.no_data_aug:
        train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.Resize((256 + args.shift_inc)), transforms.CenterCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]))
    else:
        train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]))
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    else:
        train_sampler = None
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler)
    crop_size = ((256 + args.shift_inc) if (args.evaluate_shift or args.evaluate_diagonal or args.evaluate_save) else 224)
    args.batch_size = (1 if (args.evaluate_diagonal or args.evaluate_save) else args.batch_size)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize((256 + args.shift_inc)), transforms.CenterCrop(crop_size), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    if args.val_debug:
        train_loader = val_loader
    if args.embed:
        embed()
    if (args.save_weights is not None):
        print(("=> saving 'deparallelized' weights [%s]" % args.save_weights))
        if (args.gpu is not None):
            torch.save({'state_dict': model.state_dict()}, args.save_weights)
        elif ((args.arch[:7] == 'alexnet') or (args.arch[:3] == 'vgg')):
            model.features = model.features.module
            torch.save({'state_dict': model.state_dict()}, args.save_weights)
        else:
            torch.save({'state_dict': model.module.state_dict()}, args.save_weights)
        return
    if args.evaluate:
        validate(val_loader, model, criterion, args)
        return
    if args.evaluate_shift:
        validate_shift(val_loader, model, args)
        return
    if args.evaluate_diagonal:
        validate_diagonal(val_loader, model, args)
        return
    if args.evaluate_save:
        validate_save(val_loader, mean, std, args)
        return
    if args.cos_lr:
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)
        for epoch in range(args.start_epoch):
            scheduler.step()
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        if (not args.cos_lr):
            adjust_learning_rate(optimizer, epoch, args)
        else:
            scheduler.step()
            print(('[%03d] %.5f' % (epoch, scheduler.get_lr()[0])))
        train(train_loader, model, criterion, optimizer, epoch, args)
        acc1 = validate(val_loader, model, criterion, args)
        is_best = (acc1 > best_acc1)
        best_acc1 = max(acc1, best_acc1)
        if ((not args.multiprocessing_distributed) or (args.multiprocessing_distributed and ((args.rank % ngpus_per_node) == 0))):
            save_checkpoint({'epoch': (epoch + 1), 'arch': args.arch, 'state_dict': model.state_dict(), 'best_acc1': best_acc1, 'optimizer': optimizer.state_dict()}, is_best, epoch, out_dir=args.out_dir)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 35:------------------- similar code ------------------ index = 2, score = 3.0 
def get_optimizer(parameters, cfg):
    if (cfg.method == 'sgd'):
        optimizer = torch.optim.SGD(filter((lambda p: p.requires_grad), parameters), lr=cfg.lr, momentum=0.9, weight_decay=cfg.weight_decay)
    elif (cfg.method == 'adam'):
        optimizer = torch.optim.Adam(filter((lambda p: p.requires_grad), parameters), lr=cfg.lr, weight_decay=cfg.weight_decay)
    elif (cfg.method == 'rmsprop'):
        optimizer = torch.optim.RMSprop(filter((lambda p: p.requires_grad), parameters), lr=cfg.lr, weight_decay=cfg.weight_decay)
    elif (cfg.method == 'adadelta'):
        optimizer = torch.optim.Adadelta(filter((lambda p: p.requires_grad), parameters), lr=cfg.lr, weight_decay=cfg.weight_decay)
    else:
        raise NotImplementedError
    return optimizer

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
    if:
         ...  =  ... .optim.SGD

idx = 36:------------------- similar code ------------------ index = 35, score = 3.0 
def train_model(x_train, y_train_e, x_val, y_val_e, num_epochs, learning_rate):
    '\n    Train the model using the data given, along with the parameters given.\n\n    @param x_train       is the training dataset.\n    @param y_train_e     is an np.array of the training labels.\n    @param x_val         is the validation set.\n    @param y_val_e       is the np.array of validation labels.\n    @param num_epochs    for the model to use.\n    @param learning_rate for the model to use.\n    @return              the best trained model thus far (based on validation accuracy).\n    '
    y_train_e += 1
    y_val_e += 1
    model = nn.Linear(x_val.shape[1], 3)
    model.float()
    criterion = nn.CrossEntropyLoss(reduction='sum')
    if USE_CUDA:
        model = model.cuda()
        criterion = criterion.cuda()
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    best_model = None
    best_accuracy = float('-inf')
    for epoch in range(num_epochs):
        for i in range(len(x_train)):
            tmp = x_train[i].reshape((- 1), len(x_train[i]))
            output = model(tmp)
            loss = criterion(output, y_train_e[i])
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        with torch.no_grad():
            preds = test_model(model, x_val)
        preds = preds
        y_val_e = y_val_e.cpu()
        acc = accuracy_score(y_val_e, preds)
        if (acc > best_accuracy):
            print('Best acc: {:.3f}'.format(acc))
            best_accuracy = acc
            best_model = copy.deepcopy(model)
        print('Epoch: {}, validation accuracy: {:.3f}'.format((epoch + 1), acc))
    return best_model

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 37:------------------- similar code ------------------ index = 3, score = 3.0 
if (__name__ == '__main__'):
    model = models.DeepCoral(num_classes=31)
    correct = 0
    print(model)
    if cuda:
        model.cuda()
    model = load_pretrain(model)
    optimizer = torch.optim.SGD([{'params': model.sharedNet.parameters()}, {'params': model.cls_fc.parameters(), 'lr': lr[1]}], lr=lr[0], momentum=momentum, weight_decay=l2_decay)
    for epoch in range(1, (epochs + 1)):
        for (index, param_group) in enumerate(optimizer.param_groups):
            param_group['lr'] = (lr[index] / math.pow((1 + ((10 * (epoch - 1)) / epochs)), 0.75))
        train(epoch, model, optimizer)
        t_correct = test(model)
        if (t_correct > correct):
            correct = t_correct
        print('source: {} to target: {} max correct: {} max accuracy{: .2f}%\n'.format(source_name, target_name, correct, ((100.0 * correct) / len_target_dataset)))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
     ...  =  ... .optim.SGD

idx = 38:------------------- similar code ------------------ index = 4, score = 3.0 
def optimizer(params):
    return torch.optim.SGD(params, lr=0.1)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
    return  ... .optim.SGD

idx = 39:------------------- similar code ------------------ index = 33, score = 3.0 
def main():
    if (not torch.cuda.is_available()):
        sys.exit(1)
    start_t = time.time()
    cudnn.benchmark = True
    cudnn.enabled = True
    logging.info('args = %s', args)
    model = MobileNetV1()
    logging.info(model)
    model = nn.DataParallel(model).cuda()
    criterion = nn.CrossEntropyLoss()
    criterion = criterion.cuda()
    criterion_smooth = CrossEntropyLabelSmooth(CLASSES, args.label_smooth)
    criterion_smooth = criterion_smooth.cuda()
    all_parameters = model.parameters()
    weight_parameters = []
    for (pname, p) in model.named_parameters():
        if (('fc' in pname) or ('conv1' in pname) or ('pwconv' in pname)):
            weight_parameters.append(p)
    weight_parameters_id = list(map(id, weight_parameters))
    other_parameters = list(filter((lambda p: (id(p) not in weight_parameters_id)), all_parameters))
    optimizer = torch.optim.SGD([{'params': other_parameters}, {'params': weight_parameters, 'weight_decay': args.weight_decay}], args.learning_rate, momentum=args.momentum)
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, (lambda step: (1.0 - (step / args.epochs))), last_epoch=(- 1))
    start_epoch = 0
    best_top1_acc = 0
    checkpoint_tar = os.path.join(args.save, 'checkpoint.pth.tar')
    if os.path.exists(checkpoint_tar):
        logging.info('loading checkpoint {} ..........'.format(checkpoint_tar))
        checkpoint = torch.load(checkpoint_tar)
        start_epoch = checkpoint['epoch']
        best_top1_acc = checkpoint['best_top1_acc']
        model.load_state_dict(checkpoint['state_dict'])
        logging.info('loaded checkpoint {} epoch = {}'.format(checkpoint_tar, checkpoint['epoch']))
    for epoch in range(start_epoch):
        scheduler.step()
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    crop_scale = 0.08
    lighting_param = 0.1
    train_transforms = transforms.Compose([transforms.RandomResizedCrop(224, scale=(crop_scale, 1.0)), Lighting(lighting_param), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize])
    train_dataset = datasets.ImageFolder(traindir, transform=train_transforms)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    epoch = start_epoch
    while (epoch < args.epochs):
        (train_obj, train_top1_acc, train_top5_acc) = train(epoch, train_loader, model, criterion_smooth, optimizer, scheduler)
        (valid_obj, valid_top1_acc, valid_top5_acc) = validate(epoch, val_loader, model, criterion, args)
        is_best = False
        if (valid_top1_acc > best_top1_acc):
            best_top1_acc = valid_top1_acc
            is_best = True
        save_checkpoint({'epoch': epoch, 'state_dict': model.state_dict(), 'best_top1_acc': best_top1_acc, 'optimizer': optimizer.state_dict()}, is_best, args.save)
        epoch += 1
    training_time = ((time.time() - start_t) / 36000)
    print('total training time = {} hours'.format(training_time))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 40:------------------- similar code ------------------ index = 5, score = 3.0 
def test_target_net(self):
    torch.manual_seed(2)
    model = nn.Sequential(nn.Linear(1, 1))
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
    q = QNetwork(model, optimizer, target=FixedTarget(3))
    inputs = State(torch.tensor([1.0]).unsqueeze(0))

    def loss(policy_value):
        target = (policy_value - 1)
        return smooth_l1_loss(policy_value, target.detach())
    policy_value = q(inputs)
    target_value = q.target(inputs).item()
    np.testing.assert_equal(policy_value.item(), (- 0.008584141731262207))
    np.testing.assert_equal(target_value, (- 0.008584141731262207))
    q.reinforce(loss(policy_value))
    policy_value = q(inputs)
    target_value = q.target(inputs).item()
    np.testing.assert_equal(policy_value.item(), (- 0.20858412981033325))
    np.testing.assert_equal(target_value, (- 0.008584141731262207))
    q.reinforce(loss(policy_value))
    policy_value = q(inputs)
    target_value = q.target(inputs).item()
    np.testing.assert_equal(policy_value.item(), (- 0.4085841178894043))
    np.testing.assert_equal(target_value, (- 0.008584141731262207))
    q.reinforce(loss(policy_value))
    policy_value = q(inputs)
    target_value = q.target(inputs).item()
    np.testing.assert_equal(policy_value.item(), (- 0.6085841655731201))
    np.testing.assert_equal(target_value, (- 0.6085841655731201))
    q.reinforce(loss(policy_value))
    policy_value = q(inputs)
    target_value = q.target(inputs).item()
    np.testing.assert_equal(policy_value.item(), (- 0.8085841536521912))
    np.testing.assert_equal(target_value, (- 0.6085841655731201))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
     ...  =  ... .optim.SGD

idx = 41:------------------- similar code ------------------ index = 6, score = 3.0 
def main_worker(gpu, ngpus_per_node, args, writer, log_dir):
    global best_acc1
    args.gpu = gpu
    if (args.gpu is not None):
        print('Use GPU: {} for training'.format(args.gpu))
    if args.distributed:
        if ((args.dist_url == 'env://') and (args.rank == (- 1))):
            args.rank = int(os.environ['RANK'])
        if args.multiprocessing_distributed:
            args.rank = ((args.rank * ngpus_per_node) + gpu)
        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank)
    print("=> creating model '{}'".format(args.arch))
    model = models.__dict__[args.arch]()
    print("=> creating teacher model '{}'".format(args.arch))
    teacher_model = models.__dict__[args.arch_teacher](pretrained=True)
    if args.distributed:
        if (args.gpu is not None):
            torch.cuda.set_device(args.gpu)
            model.cuda(args.gpu)
            teacher_model.cuda(args.gpu)
            args.batch_size = int((args.batch_size / ngpus_per_node))
            args.workers = int((((args.workers + ngpus_per_node) - 1) / ngpus_per_node))
            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
            teacher_model = torch.nn.parallel.DistributedDataParallel(teacher_model, device_ids=[args.gpu])
        else:
            model.cuda()
            teacher_model.cuda()
            model = torch.nn.parallel.DistributedDataParallel(model)
            teacher_model = torch.nn.parallel.DistributedDataParallel(teacher_model)
    elif (args.gpu is not None):
        torch.cuda.set_device(args.gpu)
        model = model.cuda(args.gpu)
        teacher_model = teacher_model.cuda(args.gpu)
    elif (args.arch.startswith('alexnet') or args.arch.startswith('vgg')):
        model.features = torch.nn.DataParallel(model.features)
        model.cuda()
        teacher_model.features = torch.nn.DataParallel(teacher_model.features)
        teacher_model.cuda()
    else:
        model = torch.nn.DataParallel(model).cuda()
        teacher_model = torch.nn.DataParallel(teacher_model).cuda()
    criterion = nn.CrossEntropyLoss().cuda(args.gpu)
    optimizer = torch.optim.SGD(model.parameters(), (args.lr * (args.batch_size / 256)), momentum=args.momentum, weight_decay=args.weight_decay)
    if args.resume:
        if os.path.isfile(args.resume):
            print("=> loading checkpoint '{}'".format(args.resume))
            checkpoint = torch.load(args.resume)
            args.start_epoch = checkpoint['epoch']
            best_acc1 = checkpoint['best_acc1']
            if (args.gpu is not None):
                best_acc1 = best_acc1.to(args.gpu)
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            print("=> loaded checkpoint '{}' (epoch {})".format(args.resume, checkpoint['epoch']))
        else:
            print("=> no checkpoint found at '{}'".format(args.resume))
    cudnn.benchmark = True
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]))
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    else:
        train_sampler = None
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    if args.evaluate:
        validate(val_loader, model, criterion, args)
        return
    scheduler = MultiStepLR(optimizer, milestones=[30, 60, 80], gamma=0.1)
    (acc1, acc5, test_loss) = validate(val_loader, teacher_model, criterion, args)
    print('>>>>>>>>>>>>>>>The teacher accuracy, top1:{}, top5:{}>>>>>>>>>>>>'.format(acc1, acc5))
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        scheduler.step(epoch)
        (train_acc1, train_acc5, train_loss_CE) = train(train_loader, model, teacher_model, criterion, optimizer, epoch, args)
        (acc1, acc5, test_loss) = validate(val_loader, model, criterion, args)
        is_best = (acc1 > best_acc1)
        best_acc1 = max(acc1, best_acc1)
        if ((not args.multiprocessing_distributed) or (args.multiprocessing_distributed and ((args.rank % ngpus_per_node) == 0))):
            save_checkpoint({'epoch': (epoch + 1), 'arch': args.arch, 'state_dict': model.state_dict(), 'best_acc1': best_acc1, 'optimizer': optimizer.state_dict()}, is_best, dir=log_dir)
        writer.add_scalar('Train_acc_top1', train_acc1, epoch)
        writer.add_scalar('Train_acc_top5', train_acc5, epoch)
        writer.add_scalar('Train_loss_CE', train_loss_CE, epoch)
        writer.add_scalar('Test_acc_top1', acc1, epoch)
        writer.add_scalar('Test_acc_top5', acc5, epoch)
        writer.add_scalar('Test_loss_CE', test_loss, epoch)
    writer.close()

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 42:------------------- similar code ------------------ index = 31, score = 3.0 
def main():
    global best_prec1, args
    args.distributed = False
    if ('WORLD_SIZE' in os.environ):
        args.distributed = (int(os.environ['WORLD_SIZE']) > 1)
    args.gpu = 0
    args.world_size = 1
    if args.distributed:
        args.gpu = (args.local_rank % torch.cuda.device_count())
        torch.cuda.set_device(args.gpu)
        torch.distributed.init_process_group(backend='nccl', init_method='env://')
        args.world_size = torch.distributed.get_world_size()
    if args.fp16:
        assert torch.backends.cudnn.enabled, 'fp16 mode requires cudnn backend to be enabled.'
    if (args.static_loss_scale != 1.0):
        if (not args.fp16):
            print('Warning:  static_loss_scale != 1.0 is only necessary with --fp16. Resetting static_loss_scale to 1.0')
            args.static_loss_scale = 1.0
    if args.pretrained:
        print("=> using pre-trained model '{}'".format(args.arch))
        model = models.__dict__[args.arch](pretrained=True)
    else:
        print("=> creating model '{}'".format(args.arch))
        model = models.__dict__[args.arch]()
    if args.sync_bn:
        import apex
        print('using apex synced BN')
        model = apex.parallel.convert_syncbn_model(model)
    model = model.cuda()
    if args.fp16:
        model = network_to_half(model)
    if args.distributed:
        model = DDP(model, delay_allreduce=True)
    global model_params, master_params
    if args.fp16:
        (model_params, master_params) = prep_param_lists(model)
    else:
        master_params = list(model.parameters())
    criterion = nn.CrossEntropyLoss().cuda()
    args.lr = ((args.lr * float((args.batch_size * args.world_size))) / 256.0)
    optimizer = torch.optim.SGD(master_params, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    if args.resume:

        def resume():
            if os.path.isfile(args.resume):
                print("=> loading checkpoint '{}'".format(args.resume))
                checkpoint = torch.load(args.resume, map_location=(lambda storage, loc: storage.cuda(args.gpu)))
                args.start_epoch = checkpoint['epoch']
                best_prec1 = checkpoint['best_prec1']
                model.load_state_dict(checkpoint['state_dict'])
                if args.fp16:
                    saved_master_params = checkpoint['master_params']
                    for (master, saved) in zip(master_params, saved_master_params):
                        master.data.copy_(saved.data)
                optimizer.load_state_dict(checkpoint['optimizer'])
                print("=> loaded checkpoint '{}' (epoch {})".format(args.resume, checkpoint['epoch']))
            else:
                print("=> no checkpoint found at '{}'".format(args.resume))
        resume()
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    if (args.arch == 'inception_v3'):
        crop_size = 299
        val_size = 320
    else:
        crop_size = 224
        val_size = 256
    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(crop_size), transforms.RandomHorizontalFlip()]))
    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(val_size), transforms.CenterCrop(crop_size)]))
    train_sampler = None
    val_sampler = None
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True, sampler=val_sampler, collate_fn=fast_collate)
    if args.evaluate:
        validate(val_loader, model, criterion)
        return
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        train(train_loader, model, criterion, optimizer, epoch)
        if args.prof:
            break
        prec1 = validate(val_loader, model, criterion)
        if (args.local_rank == 0):
            is_best = (prec1 > best_prec1)
            best_prec1 = max(prec1, best_prec1)

            def create_and_save_checkpoint():
                checkpoint_dict = {'epoch': (epoch + 1), 'arch': args.arch, 'state_dict': model.state_dict(), 'best_prec1': best_prec1, 'optimizer': optimizer.state_dict()}
                if args.fp16:
                    checkpoint_dict['master_params'] = master_params
                save_checkpoint(checkpoint_dict, is_best)
            create_and_save_checkpoint()

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 43:------------------- similar code ------------------ index = 29, score = 3.0 
if (__name__ == '__main__'):
    data_path = sys.argv[1]
    current_fold = sys.argv[2]
    organ_number = int(sys.argv[3])
    low_range = int(sys.argv[4])
    high_range = int(sys.argv[5])
    slice_threshold = float(sys.argv[6])
    slice_thickness = int(sys.argv[7])
    organ_ID = int(sys.argv[8])
    plane = sys.argv[9]
    GPU_ID = int(sys.argv[10])
    learning_rate1 = float(sys.argv[11])
    learning_rate_m1 = int(sys.argv[12])
    learning_rate2 = float(sys.argv[13])
    learning_rate_m2 = int(sys.argv[14])
    crop_margin = int(sys.argv[15])
    crop_prob = float(sys.argv[16])
    crop_sample_batch = int(sys.argv[17])
    snapshot_path = os.path.join(snapshot_path, ((((('SIJ_training_' + sys.argv[11]) + 'x') + str(learning_rate_m1)) + ',') + str(crop_margin)))
    epoch = {}
    epoch['S'] = int(sys.argv[18])
    epoch['I'] = int(sys.argv[19])
    epoch['J'] = int(sys.argv[20])
    epoch['lr_decay'] = int(sys.argv[21])
    timestamp = sys.argv[22]
    if (not os.path.exists(snapshot_path)):
        os.makedirs(snapshot_path)
    FCN_weights = os.path.join(pretrained_model_path, 'fcn8s_from_caffe.pth')
    if (not os.path.isfile(FCN_weights)):
        raise RuntimeError('Please Download <http://drive.google.com/uc?id=0B9P1L--7Wd2vT0FtdThWREhjNkU> from the Internet ...')
    from Data import DataLayer
    training_set = DataLayer(data_path=data_path, current_fold=int(current_fold), organ_number=organ_number, low_range=low_range, high_range=high_range, slice_threshold=slice_threshold, slice_thickness=slice_thickness, organ_ID=organ_ID, plane=plane)
    batch_size = 4
    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'
    trainloader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=16, drop_last=True)
    print((current_fold + plane), len(trainloader))
    print(epoch)
    RSTN_model = nn.DataParallel(RSTN(crop_margin=crop_margin, crop_prob=crop_prob, crop_sample_batch=crop_sample_batch), device_ids=[0, 1, 2, 3])
    RSTN_snapshot = {}
    model_parameters = filter((lambda p: p.requires_grad), RSTN_model.parameters())
    params = sum([np.prod(p.size()) for p in model_parameters])
    print('model parameters:', params)
    optimizer = torch.optim.SGD([{'params': get_parameters(RSTN_model, coarse=True, bias=False, parallel=True)}, {'params': get_parameters(RSTN_model, coarse=True, bias=True, parallel=True), 'lr': (learning_rate1 * 2), 'weight_decay': 0}, {'params': get_parameters(RSTN_model, coarse=False, bias=False, parallel=True), 'lr': (learning_rate1 * 10)}, {'params': get_parameters(RSTN_model, coarse=False, bias=True, parallel=True), 'lr': (learning_rate1 * 20), 'weight_decay': 0}], lr=learning_rate1, momentum=0.99, weight_decay=0.0005)
    criterion = DSC_loss()
    COARSE_WEIGHT = (1 / 3)
    RSTN_model = RSTN_model.cuda()
    RSTN_model.train()
    for mode in ['S', 'I', 'J']:
        if (mode == 'S'):
            RSTN_dict = RSTN_model.state_dict()
            pretrained_dict = torch.load(FCN_weights)
            pretrained_dict_coarse = {('module.coarse_model.' + k): v for (k, v) in pretrained_dict.items() if ((('module.coarse_model.' + k) in RSTN_dict) and ('score' not in k))}
            pretrained_dict_fine = {('module.fine_model.' + k): v for (k, v) in pretrained_dict.items() if ((('module.fine_model.' + k) in RSTN_dict) and ('score' not in k))}
            RSTN_dict.update(pretrained_dict_coarse)
            RSTN_dict.update(pretrained_dict_fine)
            RSTN_model.load_state_dict(RSTN_dict)
            print((plane + mode), 'load pre-trained FCN8s model successfully!')
        elif (mode == 'I'):
            print((plane + mode), 'load S model successfully!')
        elif (mode == 'J'):
            print((plane + mode), 'load I model successfully!')
        else:
            raise ValueError("wrong value of mode, should be in ['S', 'I', 'J']")
        try:
            for e in range(epoch[mode]):
                total_loss = 0.0
                total_coarse_loss = 0.0
                total_fine_loss = 0.0
                start = time.time()
                for (index, (image, label)) in enumerate(trainloader):
                    start_it = time.time()
                    optimizer.zero_grad()
                    (image, label) = (image.cuda().float(), label.cuda().float())
                    (coarse_prob, fine_prob) = RSTN_model(image, label, mode=mode)
                    coarse_loss = criterion(coarse_prob, label)
                    fine_loss = criterion(fine_prob, label)
                    loss = ((COARSE_WEIGHT * coarse_loss) + ((1 - COARSE_WEIGHT) * fine_loss))
                    total_loss += loss.item()
                    total_coarse_loss += coarse_loss.item()
                    total_fine_loss += fine_loss.item()
                    loss.backward()
                    optimizer.step()
                    print(((current_fold + plane) + mode), ('Epoch[%d/%d], Iter[%05d], Coarse/Fine/Avg Loss %.4f/%.4f/%.4f, Time Elapsed %.2fs' % ((e + 1), epoch[mode], index, coarse_loss.item(), fine_loss.item(), loss.item(), (time.time() - start_it))))
                    del image, label, coarse_prob, fine_prob, loss, coarse_loss, fine_loss
                if ((mode == 'J') and (((e + 1) % epoch['lr_decay']) == 0)):
                    print('lr decay')
                    for param_group in optimizer.param_groups:
                        param_group['lr'] *= 0.5
                print(((current_fold + plane) + mode), ('Epoch[%d], Total Coarse/Fine/Avg Loss %.4f/%.4f/%.4f, Time elapsed %.2fs' % ((e + 1), (total_coarse_loss / len(trainloader)), (total_fine_loss / len(trainloader)), (total_loss / len(trainloader)), (time.time() - start))))
        except KeyboardInterrupt:
            print(('!' * 10), 'save before quitting ...')
        finally:
            snapshot_name = ((((((((('FD' + current_fold) + ':') + plane) + mode) + str(slice_thickness)) + '_') + str(organ_ID)) + '_') + timestamp)
            RSTN_snapshot[mode] = (os.path.join(snapshot_path, snapshot_name) + '.pkl')
            torch.save(RSTN_model.state_dict(), RSTN_snapshot[mode])
            print(('#' * 10), (((('end of ' + current_fold) + plane) + mode) + ' training stage!'))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
     ...  =  ... .optim.SGD

idx = 44:------------------- similar code ------------------ index = 13, score = 3.0 
def initialize_optimizer_and_scheduler(self):
    self.optimizer = torch.optim.SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay, momentum=0.99, nesterov=True)
    self.lr_scheduler = lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.2, patience=self.lr_scheduler_patience, verbose=True, threshold=self.lr_scheduler_eps, threshold_mode='abs')

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
 =  ... .optim.SGD

idx = 45:------------------- similar code ------------------ index = 28, score = 3.0 
def initialize_optimizer_and_scheduler(self):
    assert (self.network is not None), 'self.initialize_network must be called first'
    self.optimizer = torch.optim.SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay, momentum=0.98, nesterov=True)
    self.lr_scheduler = None

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
 =  ... .optim.SGD

idx = 46:------------------- similar code ------------------ index = 27, score = 3.0 
def setUp(self):
    torch.manual_seed(2)
    self.model = nn.Sequential(nn.Linear(STATE_DIM, ACTIONS))
    optimizer = torch.optim.SGD(self.model.parameters(), lr=0.1)
    self.policy = SoftmaxPolicy(self.model, optimizer)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
     ...  =  ... .optim.SGD

idx = 47:------------------- similar code ------------------ index = 26, score = 3.0 
def initialize_optimizer_and_scheduler(self):
    assert (self.network is not None), 'self.initialize_network must be called first'
    self.optimizer = torch.optim.SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay, momentum=0.95, nesterov=True)
    self.lr_scheduler = None

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
 =  ... .optim.SGD

idx = 48:------------------- similar code ------------------ index = 8, score = 3.0 
def initialize_optimizer_and_scheduler(self):
    assert (self.network is not None), 'self.initialize_network must be called first'
    self.optimizer = torch.optim.SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay, momentum=0.99, nesterov=True)
    self.lr_scheduler = None

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
 =  ... .optim.SGD

idx = 49:------------------- similar code ------------------ index = 25, score = 3.0 
def main():
    if (not torch.cuda.is_available()):
        sys.exit(1)
    start_t = time.time()
    cudnn.benchmark = True
    cudnn.enabled = True
    logging.info('args = %s', args)
    model = ResNet50()
    logging.info(model)
    model = nn.DataParallel(model).cuda()
    criterion = nn.CrossEntropyLoss()
    criterion = criterion.cuda()
    criterion_smooth = CrossEntropyLabelSmooth(CLASSES, args.label_smooth)
    criterion_smooth = criterion_smooth.cuda()
    all_parameters = model.parameters()
    weight_parameters = []
    for (pname, p) in model.named_parameters():
        if (('fc' in pname) or ('conv' in pname)):
            weight_parameters.append(p)
    weight_parameters_id = list(map(id, weight_parameters))
    other_parameters = list(filter((lambda p: (id(p) not in weight_parameters_id)), all_parameters))
    optimizer = torch.optim.SGD([{'params': other_parameters}, {'params': weight_parameters, 'weight_decay': args.weight_decay}], args.learning_rate, momentum=args.momentum)
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[(args.epochs // 4), (args.epochs // 2), ((args.epochs // 4) * 3)], gamma=0.1)
    start_epoch = 0
    best_top1_acc = 0
    checkpoint_tar = os.path.join(args.save, 'checkpoint.pth.tar')
    if os.path.exists(checkpoint_tar):
        logging.info('loading checkpoint {} ..........'.format(checkpoint_tar))
        checkpoint = torch.load(checkpoint_tar)
        start_epoch = checkpoint['epoch']
        best_top1_acc = checkpoint['best_top1_acc']
        model.load_state_dict(checkpoint['state_dict'])
        logging.info('loaded checkpoint {} epoch = {}'.format(checkpoint_tar, checkpoint['epoch']))
    for epoch in range(start_epoch):
        scheduler.step()
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    crop_scale = 0.08
    lighting_param = 0.1
    train_transforms = transforms.Compose([transforms.RandomResizedCrop(224, scale=(crop_scale, 1.0)), Lighting(lighting_param), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize])
    train_dataset = datasets.ImageFolder(traindir, transform=train_transforms)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    epoch = start_epoch
    while (epoch < args.epochs):
        (train_obj, train_top1_acc, train_top5_acc) = train(epoch, train_loader, model, criterion_smooth, optimizer, scheduler)
        (valid_obj, valid_top1_acc, valid_top5_acc) = validate(epoch, val_loader, model, criterion, args)
        is_best = False
        if (valid_top1_acc > best_top1_acc):
            best_top1_acc = valid_top1_acc
            is_best = True
        save_checkpoint({'epoch': epoch, 'state_dict': model.state_dict(), 'best_top1_acc': best_top1_acc, 'optimizer': optimizer.state_dict()}, is_best, args.save)
        epoch += 1
    training_time = ((time.time() - start_t) / 36000)
    print('total training time = {} hours'.format(training_time))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 50:------------------- similar code ------------------ index = 24, score = 3.0 
@pytest.fixture
def setUp():
    env = GymEnvironment('LunarLanderContinuous-v2', append_time=True)
    Action.set_action_space(env.action_space)
    latent_dim = 32
    num_samples = 5
    encoder_model = fc_bcq_encoder(env, latent_dim=latent_dim)
    decoder_model = fc_bcq_decoder(env, latent_dim=latent_dim)
    encoder_optimizer = torch.optim.SGD(encoder_model.parameters(), lr=0.1)
    decoder_optimizer = torch.optim.SGD(decoder_model.parameters(), lr=0.1)
    encoder = BcqEncoder(model=encoder_model, latent_dim=latent_dim, optimizer=encoder_optimizer)
    decoder = BcqDecoder(model=decoder_model, latent_dim=latent_dim, space=env.action_space, optimizer=decoder_optimizer)
    sample_states = State.from_list([env.reset() for _ in range(num_samples)])
    sample_actions = Action(torch.tensor([env.action_space.sample() for _ in range(num_samples)]))
    (yield (encoder, decoder, sample_states, sample_actions))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 51:------------------- similar code ------------------ index = 9, score = 3.0 
if (__name__ == '__main__'):
    data_path = sys.argv[1]
    current_fold = sys.argv[2]
    organ_number = int(sys.argv[3])
    low_range = int(sys.argv[4])
    high_range = int(sys.argv[5])
    slice_threshold = float(sys.argv[6])
    slice_thickness = int(sys.argv[7])
    organ_ID = int(sys.argv[8])
    plane = sys.argv[9]
    GPU_ID = int(sys.argv[10])
    learning_rate1 = float(sys.argv[11])
    learning_rate_m1 = int(sys.argv[12])
    learning_rate2 = float(sys.argv[13])
    learning_rate_m2 = int(sys.argv[14])
    crop_margin = int(sys.argv[15])
    crop_prob = float(sys.argv[16])
    crop_sample_batch = int(sys.argv[17])
    snapshot_path = os.path.join(snapshot_path, ((((('SIJ_training_' + sys.argv[11]) + 'x') + str(learning_rate_m1)) + ',') + str(crop_margin)))
    epoch = {}
    epoch['S'] = int(sys.argv[18])
    epoch['I'] = int(sys.argv[19])
    epoch['J'] = int(sys.argv[20])
    epoch['lr_decay'] = int(sys.argv[21])
    timestamp = sys.argv[22]
    if (not os.path.exists(snapshot_path)):
        os.makedirs(snapshot_path)
    FCN_weights = os.path.join(pretrained_model_path, 'fcn8s_from_caffe.pth')
    if (not os.path.isfile(FCN_weights)):
        raise RuntimeError('Please Download <http://drive.google.com/uc?id=0B9P1L--7Wd2vT0FtdThWREhjNkU> from the Internet ...')
    from Data import DataLayer
    training_set = DataLayer(data_path=data_path, current_fold=int(current_fold), organ_number=organ_number, low_range=low_range, high_range=high_range, slice_threshold=slice_threshold, slice_thickness=slice_thickness, organ_ID=organ_ID, plane=plane)
    batch_size = 1
    os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_ID)
    trainloader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=16, drop_last=True)
    print((current_fold + plane), len(trainloader))
    print(epoch)
    RSTN_model = RSTN(crop_margin=crop_margin, crop_prob=crop_prob, crop_sample_batch=crop_sample_batch)
    RSTN_snapshot = {}
    model_parameters = filter((lambda p: p.requires_grad), RSTN_model.parameters())
    params = sum([np.prod(p.size()) for p in model_parameters])
    print('model parameters:', params)
    optimizer = torch.optim.SGD([{'params': get_parameters(RSTN_model, coarse=True, bias=False, parallel=False)}, {'params': get_parameters(RSTN_model, coarse=True, bias=True, parallel=False), 'lr': (learning_rate1 * 2), 'weight_decay': 0}, {'params': get_parameters(RSTN_model, coarse=False, bias=False, parallel=False), 'lr': (learning_rate1 * 10)}, {'params': get_parameters(RSTN_model, coarse=False, bias=True, parallel=False), 'lr': (learning_rate1 * 20), 'weight_decay': 0}], lr=learning_rate1, momentum=0.99, weight_decay=0.0005)
    criterion = DSC_loss()
    COARSE_WEIGHT = (1 / 3)
    RSTN_model = RSTN_model.cuda()
    RSTN_model.train()
    for mode in ['S', 'I', 'J']:
        if (mode == 'S'):
            RSTN_dict = RSTN_model.state_dict()
            pretrained_dict = torch.load(FCN_weights)
            pretrained_dict_coarse = {('coarse_model.' + k): v for (k, v) in pretrained_dict.items() if ((('coarse_model.' + k) in RSTN_dict) and ('score' not in k))}
            pretrained_dict_fine = {('fine_model.' + k): v for (k, v) in pretrained_dict.items() if ((('fine_model.' + k) in RSTN_dict) and ('score' not in k))}
            RSTN_dict.update(pretrained_dict_coarse)
            RSTN_dict.update(pretrained_dict_fine)
            RSTN_model.load_state_dict(RSTN_dict)
            print((plane + mode), 'load pre-trained FCN8s model successfully!')
        elif (mode == 'I'):
            print((plane + mode), 'load S model successfully!')
        elif (mode == 'J'):
            print((plane + mode), 'load I model successfully!')
        else:
            raise ValueError("wrong value of mode, should be in ['S', 'I', 'J']")
        try:
            for e in range(epoch[mode]):
                total_loss = 0.0
                total_coarse_loss = 0.0
                total_fine_loss = 0.0
                start = time.time()
                for (index, (image, label)) in enumerate(trainloader):
                    start_it = time.time()
                    optimizer.zero_grad()
                    (image, label) = (image.cuda().float(), label.cuda().float())
                    (coarse_prob, fine_prob) = RSTN_model(image, label, mode=mode)
                    coarse_loss = criterion(coarse_prob, label)
                    fine_loss = criterion(fine_prob, label)
                    loss = ((COARSE_WEIGHT * coarse_loss) + ((1 - COARSE_WEIGHT) * fine_loss))
                    total_loss += loss.item()
                    total_coarse_loss += coarse_loss.item()
                    total_fine_loss += fine_loss.item()
                    loss.backward()
                    optimizer.step()
                    print(((current_fold + plane) + mode), ('Epoch[%d/%d], Iter[%05d], Coarse/Fine/Avg Loss %.4f/%.4f/%.4f, Time Elapsed %.2fs' % ((e + 1), epoch[mode], index, coarse_loss.item(), fine_loss.item(), loss.item(), (time.time() - start_it))))
                    del image, label, coarse_prob, fine_prob, loss, coarse_loss, fine_loss
                if ((mode == 'J') and (((e + 1) % epoch['lr_decay']) == 0)):
                    print('lr decay')
                    for param_group in optimizer.param_groups:
                        param_group['lr'] *= 0.5
                print(((current_fold + plane) + mode), ('Epoch[%d], Total Coarse/Fine/Avg Loss %.4f/%.4f/%.4f, Time elapsed %.2fs' % ((e + 1), (total_coarse_loss / len(trainloader)), (total_fine_loss / len(trainloader)), (total_loss / len(trainloader)), (time.time() - start))))
        except KeyboardInterrupt:
            print(('!' * 10), 'save before quitting ...')
        finally:
            snapshot_name = ((((((((('FD' + current_fold) + ':') + plane) + mode) + str(slice_thickness)) + '_') + str(organ_ID)) + '_') + timestamp)
            RSTN_snapshot[mode] = (os.path.join(snapshot_path, snapshot_name) + '.pkl')
            torch.save(RSTN_model.state_dict(), RSTN_snapshot[mode])
            print(('#' * 10), (((('end of ' + current_fold) + plane) + mode) + ' training stage!'))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
     ...  =  ... .optim.SGD

idx = 52:------------------- similar code ------------------ index = 23, score = 3.0 
def main():
    if (not torch.cuda.is_available()):
        sys.exit(1)
    start_t = time.time()
    cudnn.benchmark = True
    cudnn.enabled = True
    logging.info('args = %s', args)
    model = ResNet50()
    logging.info(model)
    model = nn.DataParallel(model).cuda()
    criterion = nn.CrossEntropyLoss()
    criterion = criterion.cuda()
    criterion_smooth = CrossEntropyLabelSmooth(CLASSES, args.label_smooth)
    criterion_smooth = criterion_smooth.cuda()
    all_parameters = model.parameters()
    weight_parameters = []
    for (pname, p) in model.named_parameters():
        if (('fc' in pname) or ('conv' in pname)):
            weight_parameters.append(p)
    weight_parameters_id = list(map(id, weight_parameters))
    other_parameters = list(filter((lambda p: (id(p) not in weight_parameters_id)), all_parameters))
    optimizer = torch.optim.SGD([{'params': other_parameters}, {'params': weight_parameters, 'weight_decay': args.weight_decay}], args.learning_rate, momentum=args.momentum)
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[(args.epochs // 4), (args.epochs // 2), ((args.epochs // 4) * 3)], gamma=0.1)
    start_epoch = 0
    best_top1_acc = 0
    checkpoint_tar = os.path.join(args.save, 'checkpoint.pth.tar')
    if os.path.exists(checkpoint_tar):
        logging.info('loading checkpoint {} ..........'.format(checkpoint_tar))
        checkpoint = torch.load(checkpoint_tar)
        start_epoch = checkpoint['epoch']
        best_top1_acc = checkpoint['best_top1_acc']
        model.load_state_dict(checkpoint['state_dict'])
        logging.info('loaded checkpoint {} epoch = {}'.format(checkpoint_tar, checkpoint['epoch']))
    for epoch in range(start_epoch):
        scheduler.step()
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    crop_scale = 0.08
    lighting_param = 0.1
    train_transforms = transforms.Compose([transforms.RandomResizedCrop(224, scale=(crop_scale, 1.0)), Lighting(lighting_param), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize])
    train_dataset = datasets.ImageFolder(traindir, transform=train_transforms)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    epoch = start_epoch
    while (epoch < args.epochs):
        (train_obj, train_top1_acc, train_top5_acc, epoch) = train(epoch, train_loader, model, criterion_smooth, optimizer, scheduler)
        (valid_obj, valid_top1_acc, valid_top5_acc) = validate(epoch, val_loader, model, criterion, args)
        is_best = False
        if (valid_top1_acc > best_top1_acc):
            best_top1_acc = valid_top1_acc
            is_best = True
        save_checkpoint({'epoch': epoch, 'state_dict': model.state_dict(), 'best_top1_acc': best_top1_acc, 'optimizer': optimizer.state_dict()}, is_best, args.save)
        epoch += 1
    training_time = ((time.time() - start_t) / 36000)
    print('total training time = {} hours'.format(training_time))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 53:------------------- similar code ------------------ index = 22, score = 3.0 
def add_optimizer(self, parameters):
    '\n\t\tCreates an optimizer for training the parameters\n\n\t\tParameters\n\t\t----------\n\t\tparameters:         The parameters of the model\n\t\t\n\t\tReturns\n\t\t-------\n\t\tReturns an optimizer for learning the parameters of the model\n\t\t\n\t\t'
    if (self.p.opt == 'adam'):
        return torch.optim.Adam(parameters, lr=self.p.lr, weight_decay=self.p.l2)
    else:
        return torch.optim.SGD(parameters, lr=self.p.lr, weight_decay=self.p.l2)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
    if:    else:
        return  ... .optim.SGD

idx = 54:------------------- similar code ------------------ index = 21, score = 3.0 
if (__name__ == '__main__'):
    device = torch.device('cuda')
    keys = ('device', 'x_inducing_var', 'f_prior', 'n_inducing', 'add_cov_diag', 'standard_cross_entropy')
    values = (device, args.x_inducing_var, args.f_prior, args.n_inducing, args.add_cov_diag, args.standard_cross_entropy)
    fvi_args = dict(zip(keys, values))
    FVI = FVI_seg(x_size=(H_crop, W_crop), num_classes=num_classes, **fvi_args).to(device)
    optimizer = torch.optim.SGD(FVI.parameters(), lr=args.lr, momentum=0.9, weight_decay=0.0001)
    if args.load:
        model_load_dir = os.path.join(args.base_dir, 'FVI_CV/model_{}_{}.bin'.format(args.dataset, exp_name))
        optimizer_load_dir = os.path.join(args.base_dir, 'FVI_CV/optimizer_{}_{}.bin'.format(args.dataset, exp_name))
        FVI.load_state_dict(torch.load(model_load_dir))
        optimizer.load_state_dict(torch.load(optimizer_load_dir))
        print('Loading FVI segmentation model..')
    if args.training_mode:
        print('Training FVI segmentation for {} epochs'.format(args.n_epochs))
        train(args.n_epochs, train_loader, FVI)
    if args.test_mode:
        print('Evaluating FVI segmentation on test set')
        model_load_dir = os.path.join(args.base_dir, 'FVI_CV/models_test/model_{}_fvi_seg_test.bin'.format(args.dataset))
        FVI.load_state_dict(torch.load(model_load_dir))
        (error, mIOU) = test(FVI, test_loader, num_classes, args.dataset, exp_name, mkdir=True)
        print('Test Error: {:.5f} || Test Mean IOU: {:.5f}'.format(error, mIOU))
        np.savetxt('{}_{}_epoch_{}_test_error.txt'.format(args.dataset, exp_name, (- 1)), [error])
        np.savetxt('{}_{}_epoch_{}_test_mIOU.txt'.format(args.dataset, exp_name, (- 1)), [mIOU])
    if args.test_runtime_mode:
        model_load_dir = os.path.join(args.base_dir, 'FVI_CV/models_test/model_{}_fvi_seg_test.bin'.format(args.dataset))
        FVI.load_state_dict(torch.load(model_load_dir))
        run_runtime_seg(FVI, test_loader, exp_name, 50)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
     ...  =  ... .optim.SGD

idx = 55:------------------- similar code ------------------ index = 11, score = 3.0 
def main_worker(gpu, ngpus_per_node, args):
    global best_acc1
    args.gpu = gpu
    if (args.gpu is not None):
        print('Use GPU: {} for training'.format(args.gpu))
    if args.distributed:
        if ((args.dist_url == 'env://') and (args.rank == (- 1))):
            args.rank = int(os.environ['RANK'])
        if args.multiprocessing_distributed:
            args.rank = ((args.rank * ngpus_per_node) + gpu)
        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank)
    if args.pretrained:
        print("=> using pre-trained model '{}'".format(args.arch))
        model = models.__dict__[args.arch](pretrained=True)
    else:
        print("=> creating model '{}'".format(args.arch))
        if (args.arch == 'vgg11_bn_fconv'):
            model = vgg_fconv.vgg11_bn(num_classes=args.num_class)
        elif (args.arch == 'vgg13_bn_fconv'):
            model = vgg_fconv.vgg13_bn(num_classes=args.num_class)
        elif (args.arch == 'vgg16_bn_fconv'):
            model = vgg_fconv.vgg16_bn(num_classes=args.num_class)
        elif (args.arch == 'vgg19_bn_fconv'):
            model = vgg_fconv.vgg19_bn(num_classes=args.num_class)
        elif (args.arch == 'vgg11_fconv'):
            model = vgg_fconv.vgg11(num_classes=args.num_class)
        elif (args.arch == 'vgg13_fconv'):
            model = vgg_fconv.vgg13(num_classes=args.num_class)
        elif (args.arch == 'vgg16_fconv'):
            model = vgg_fconv.vgg16(num_classes=args.num_class)
        elif (args.arch == 'vgg19_fconv'):
            model = vgg_fconv.vgg19(num_classes=args.num_class)
        elif (args.arch == 'resnet18_fconv'):
            model = resnet_fconv.resnet18(num_classes=args.num_class)
        elif (args.arch == 'resnet34_fconv'):
            model = resnet_fconv.resnet34(num_classes=args.num_class)
        elif (args.arch == 'resnet50_fconv'):
            model = resnet_fconv.resnet50(num_classes=args.num_class)
        elif (args.arch == 'resnet101_fconv'):
            model = resnet_fconv.resnet101(num_classes=args.num_class)
        elif (args.arch == 'resnet152_fconv'):
            model = resnet_fconv.resnet152(num_classes=args.num_class)
        else:
            model = models.__dict__[args.arch]()
    if args.distributed:
        if (args.gpu is not None):
            torch.cuda.set_device(args.gpu)
            model.cuda(args.gpu)
            args.batch_size = int((args.batch_size / ngpus_per_node))
            print('batch size per GPU', args.batch_size)
            args.workers = int((((args.workers + ngpus_per_node) - 1) / ngpus_per_node))
            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
        else:
            model.cuda()
            model = torch.nn.parallel.DistributedDataParallel(model)
    elif (args.gpu is not None):
        torch.cuda.set_device(args.gpu)
        model = model.cuda(args.gpu)
    elif (args.arch.startswith('alexnet') or args.arch.startswith('vgg')):
        model.features = torch.nn.DataParallel(model.features)
        model.cuda()
    else:
        model = torch.nn.DataParallel(model).cuda()
    criterion = nn.CrossEntropyLoss().cuda(args.gpu)
    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    if args.resume:
        if os.path.isfile(args.resume):
            print("=> loading checkpoint '{}'".format(args.resume))
            checkpoint = torch.load(args.resume)
            args.start_epoch = checkpoint['epoch']
            best_acc1 = checkpoint['best_acc1']
            if (args.gpu is not None):
                best_acc1 = best_acc1.to(args.gpu)
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            print("=> loaded checkpoint '{}' (epoch {})".format(args.resume, checkpoint['epoch']))
        else:
            print("=> no checkpoint found at '{}'".format(args.resume))
    cudnn.benchmark = True
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]))
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    else:
        train_sampler = None
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    if args.evaluate:
        validate(val_loader, model, criterion, args)
        return
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        adjust_learning_rate(optimizer, epoch, args)
        train(train_loader, model, criterion, optimizer, epoch, args)
        acc1 = validate(val_loader, model, criterion, args)
        is_best = (acc1 > best_acc1)
        best_acc1 = max(acc1, best_acc1)
        if ((not args.multiprocessing_distributed) or (args.multiprocessing_distributed and ((args.rank % ngpus_per_node) == 0))):
            save_checkpoint({'epoch': (epoch + 1), 'arch': args.arch, 'state_dict': model.state_dict(), 'best_acc1': best_acc1, 'optimizer': optimizer.state_dict()}, is_best, args)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 56:------------------- similar code ------------------ index = 42, score = 3.0 
def main():
    exp_name = experiment_name_non_mnist(dataset=args.dataset, arch=args.arch, epochs=args.epochs, dropout=args.dropout, batch_size=args.batch_size, lr=args.learning_rate, momentum=args.momentum, decay=args.decay, data_aug=args.data_aug, train=args.train, alpha=args.alpha, job_id=args.job_id, add_name=args.add_name, patchup_mode=args.patchup_type, keep_prob=args.keep_prob, gamma=args.gamma, k=args.k, dropblock_all=args.drop_block_all)
    exp_dir = os.path.join(args.root_dir, exp_name)
    if (not os.path.exists(exp_dir)):
        os.makedirs(exp_dir)
    copy_script_to_folder(os.path.abspath(__file__), exp_dir)
    result_png_path = os.path.join(exp_dir, 'results.png')
    global best_acc
    global best_model
    log = open(os.path.join(exp_dir, 'log.txt'.format(args.manualSeed)), 'w')
    print_log('save path : {}'.format(exp_dir), log)
    state = {k: v for (k, v) in args._get_kwargs()}
    print_log(state, log)
    print_log('Random Seed: {}'.format(args.manualSeed), log)
    print_log('python version : {}'.format(sys.version.replace('\n', ' ')), log)
    print_log('torch  version : {}'.format(torch.__version__), log)
    print_log('cudnn  version : {}'.format(torch.backends.cudnn.version()), log)
    if torch.cuda.is_available():
        device = torch.device('cuda')
    else:
        device = torch.device('cpu')
    num_workers = 0
    if (device == torch.device('cuda')):
        num_workers = 2
    per_img_std = False
    (train_loader, valid_loader, _, test_loader, num_classes) = load_data_subset(args.data_aug, args.batch_size, num_workers, args.dataset, args.data_dir, labels_per_class=args.labels_per_class, valid_labels_per_class=args.valid_labels_per_class)
    if (args.dataset == 'tiny-imagenet-200'):
        stride = 2
    else:
        stride = 1
    drop_block = args.drop_block
    keep_prob = args.keep_prob
    gamma = args.gamma
    patchup_block = args.patchup_block
    patchup_prob = args.patchup_prob
    print_log("=> creating model '{}'".format(args.arch), log)
    net = models.__dict__[args.arch](num_classes, args.dropout, per_img_std, stride, drop_block, keep_prob, gamma, patchup_block).to(device)
    print_log('=> network :\n {}'.format(net), log)
    args.num_classes = num_classes
    optimizer = torch.optim.SGD(net.parameters(), state['learning_rate'], momentum=state['momentum'], weight_decay=state['decay'], nesterov=True)
    recorder = RecorderMeter(args.epochs)
    if args.resume:
        if os.path.isfile(args.resume):
            print_log("=> loading checkpoint '{}'".format(args.resume), log)
            checkpoint = torch.load(args.resume)
            recorder = checkpoint['recorder']
            args.start_epoch = checkpoint['epoch']
            net.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            best_acc = recorder.max_accuracy(False)
            print_log("=> loaded checkpoint '{}' accuracy={} (epoch {})".format(args.resume, best_acc, checkpoint['epoch']), log)
        else:
            print_log("=> no checkpoint found at '{}'".format(args.resume), log)
    else:
        print_log('=> do not use any checkpoint for {} model'.format(args.arch), log)
    if args.evaluate:
        validate(test_loader, net, criterion, log)
        return
    start_time = time.time()
    epoch_time = AverageMeter()
    train_loss = []
    train_acc = []
    test_loss = []
    test_acc = []
    for epoch in range(args.start_epoch, args.epochs):
        current_learning_rate = adjust_learning_rate(optimizer, epoch, args.step_factors, args.schedule)
        (need_hour, need_mins, need_secs) = convert_secs2time((epoch_time.avg * (args.epochs - epoch)))
        need_time = '[Need: {:02d}:{:02d}:{:02d}]'.format(need_hour, need_mins, need_secs)
        print_log(('\n==>>{:s} [Epoch={:03d}/{:03d}] {:s} [learning_rate={:6.4f}]'.format(time_string(), epoch, args.epochs, need_time, current_learning_rate) + ' [Best Accuracy={:.2f}, Error={:.2f}]'.format(recorder.max_accuracy(False), (100 - recorder.max_accuracy(False)))), log)
        (tr_acc, tr_acc5, tr_los) = train(train_loader, net, optimizer, epoch, args, log)
        (val_acc, top5_avg, error1, val_los) = validate(valid_loader, net, log)
        train_loss.append(tr_los)
        train_acc.append(tr_acc)
        test_loss.append(val_los)
        test_acc.append(val_acc)
        dummy = recorder.update(epoch, tr_los, tr_acc, val_los, val_acc)
        is_best = False
        if (val_acc > best_acc):
            is_best = True
            (best_acc, best_top5_avg, best_error1, best_val_los) = (val_acc, top5_avg, error1, val_los)
            best_model = models.__dict__[args.arch](num_classes, args.dropout, per_img_std, stride).to(device)
            best_model.load_state_dict(net.state_dict())
            best_model.eval()
        if args.checkpoint:
            save_checkpoint({'epoch': (epoch + 1), 'arch': args.arch, 'state_dict': net.state_dict(), 'recorder': recorder, 'optimizer': optimizer.state_dict()}, is_best, exp_dir, 'checkpoint.pth.tar')
        epoch_time.update((time.time() - start_time))
        start_time = time.time()
        recorder.plot_curve(result_png_path)
        train_log = OrderedDict()
        train_log['train_loss'] = train_loss
        train_log['train_acc'] = train_acc
        train_log['test_loss'] = test_loss
        train_log['test_acc'] = test_acc
        pickle.dump(train_log, open(os.path.join(exp_dir, 'log.pkl'), 'wb'))
        plotting(exp_dir)
    print_log('best model stat on validation set:', log)
    validate(valid_loader, best_model, log)
    print_log('best model stat on test set:', log)
    validate(test_loader, best_model, log)
    if args.affine_test:
        affine_data_loaders = load_transformed_test_sets(args.affine_path, batch_size=100, workers=2)
        for t_loader in affine_data_loaders:
            print_log(f'model performance on {t_loader.transformer} test set:', log)
            validate(t_loader, best_model, log)
    if args.fsgm_attack:
        epsilons = [0, 0.05, 0.1, 0.12, 0.15, 0.18, 0.2]
        accuracies = []
        for eps in epsilons:
            result = run_test_adversarial(best_model, test_loader, eps)
            accuracies.append(result)
            print_log(result, log)
        print_log('the FSGM result :', log)
        print_log(accuracies, log)
    log.close()

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 57:------------------- similar code ------------------ index = 19, score = 3.0 
def initialize_optimizer_and_scheduler(self):
    assert (self.network is not None), 'self.initialize_network must be called first'
    self.optimizer = torch.optim.SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay, momentum=0.9, nesterov=True)
    self.lr_scheduler = None

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
 =  ... .optim.SGD

idx = 58:------------------- similar code ------------------ index = 50, score = 3.0 
if (__name__ == '__main__'):
    model = models.DANNet(num_classes=31)
    correct = 0
    print(model)
    if cuda:
        model.cuda()
    model = load_pretrain(model)
    optimizer = torch.optim.SGD([{'params': model.sharedNet.parameters()}, {'params': model.cls_fc.parameters(), 'lr': lr[1]}], lr=lr[0], momentum=momentum, weight_decay=l2_decay)
    for epoch in range(1, (epochs + 1)):
        for (index, param_group) in enumerate(optimizer.param_groups):
            param_group['lr'] = (lr[index] / math.pow((1 + ((10 * (epoch - 1)) / epochs)), 0.75))
        train(epoch, model, optimizer)
        t_correct = test(model)
        if (t_correct > correct):
            correct = t_correct
        print('source: {} to target: {} max correct: {} max accuracy{: .2f}%\n'.format(source_name, target_name, correct, ((100.0 * correct) / len_target_dataset)))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
     ...  =  ... .optim.SGD

idx = 59:------------------- similar code ------------------ index = 44, score = 3.0 
def train(model):
    source1_iter = iter(source1_loader)
    source2_iter = iter(source2_loader)
    source3_iter = iter(source3_loader)
    target_iter = iter(target_train_loader)
    correct = 0
    optimizer = torch.optim.SGD([{'params': model.sharedNet.parameters()}, {'params': model.cls_fc_son1.parameters(), 'lr': lr[1]}, {'params': model.cls_fc_son2.parameters(), 'lr': lr[1]}, {'params': model.cls_fc_son3.parameters(), 'lr': lr[1]}, {'params': model.sonnet1.parameters(), 'lr': lr[1]}, {'params': model.sonnet2.parameters(), 'lr': lr[1]}, {'params': model.sonnet3.parameters(), 'lr': lr[1]}], lr=lr[0], momentum=momentum, weight_decay=l2_decay)
    for i in range(1, (iteration + 1)):
        model.train()
        optimizer.param_groups[0]['lr'] = (lr[0] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        optimizer.param_groups[1]['lr'] = (lr[1] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        optimizer.param_groups[2]['lr'] = (lr[1] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        optimizer.param_groups[3]['lr'] = (lr[1] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        optimizer.param_groups[4]['lr'] = (lr[1] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        optimizer.param_groups[5]['lr'] = (lr[1] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        optimizer.param_groups[6]['lr'] = (lr[1] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        try:
            (source_data, source_label) = source1_iter.next()
        except Exception as err:
            source1_iter = iter(source1_loader)
            (source_data, source_label) = source1_iter.next()
        try:
            (target_data, __) = target_iter.next()
        except Exception as err:
            target_iter = iter(target_train_loader)
            (target_data, __) = target_iter.next()
        if cuda:
            (source_data, source_label) = (source_data.cuda(), source_label.cuda())
            target_data = target_data.cuda()
        (source_data, source_label) = (Variable(source_data), Variable(source_label))
        target_data = Variable(target_data)
        optimizer.zero_grad()
        (cls_loss, mmd_loss, l1_loss) = model(source_data, target_data, source_label, mark=1)
        gamma = ((2 / (1 + math.exp((((- 10) * i) / iteration)))) - 1)
        loss = (cls_loss + (gamma * (mmd_loss + l1_loss)))
        loss.backward()
        optimizer.step()
        if ((i % log_interval) == 0):
            print('Train source1 iter: {} [({:.0f}%)]\tLoss: {:.6f}\tsoft_Loss: {:.6f}\tmmd_Loss: {:.6f}\tl1_Loss: {:.6f}'.format(i, ((100.0 * i) / iteration), loss.item(), cls_loss.item(), mmd_loss.item(), l1_loss.item()))
        try:
            (source_data, source_label) = source2_iter.next()
        except Exception as err:
            source2_iter = iter(source2_loader)
            (source_data, source_label) = source2_iter.next()
        try:
            (target_data, __) = target_iter.next()
        except Exception as err:
            target_iter = iter(target_train_loader)
            (target_data, __) = target_iter.next()
        if cuda:
            (source_data, source_label) = (source_data.cuda(), source_label.cuda())
            target_data = target_data.cuda()
        (source_data, source_label) = (Variable(source_data), Variable(source_label))
        target_data = Variable(target_data)
        optimizer.zero_grad()
        (cls_loss, mmd_loss, l1_loss) = model(source_data, target_data, source_label, mark=2)
        gamma = ((2 / (1 + math.exp((((- 10) * i) / iteration)))) - 1)
        loss = (cls_loss + (gamma * (mmd_loss + l1_loss)))
        loss.backward()
        optimizer.step()
        if ((i % log_interval) == 0):
            print('Train source2 iter: {} [({:.0f}%)]\tLoss: {:.6f}\tsoft_Loss: {:.6f}\tmmd_Loss: {:.6f}\tl1_Loss: {:.6f}'.format(i, ((100.0 * i) / iteration), loss.item(), cls_loss.item(), mmd_loss.item(), l1_loss.item()))
        try:
            (source_data, source_label) = source3_iter.next()
        except Exception as err:
            source3_iter = iter(source3_loader)
            (source_data, source_label) = source3_iter.next()
        try:
            (target_data, __) = target_iter.next()
        except Exception as err:
            target_iter = iter(target_train_loader)
            (target_data, __) = target_iter.next()
        if cuda:
            (source_data, source_label) = (source_data.cuda(), source_label.cuda())
            target_data = target_data.cuda()
        (source_data, source_label) = (Variable(source_data), Variable(source_label))
        target_data = Variable(target_data)
        optimizer.zero_grad()
        (cls_loss, mmd_loss, l1_loss) = model(source_data, target_data, source_label, mark=3)
        gamma = ((2 / (1 + math.exp((((- 10) * i) / iteration)))) - 1)
        loss = (cls_loss + (gamma * (mmd_loss + l1_loss)))
        loss.backward()
        optimizer.step()
        if ((i % log_interval) == 0):
            print('Train source3 iter: {} [({:.0f}%)]\tLoss: {:.6f}\tsoft_Loss: {:.6f}\tmmd_Loss: {:.6f}\tl1_Loss: {:.6f}'.format(i, ((100.0 * i) / iteration), loss.item(), cls_loss.item(), mmd_loss.item(), l1_loss.item()))
        if ((i % (log_interval * 20)) == 0):
            t_correct = test(model)
            if (t_correct > correct):
                correct = t_correct
            print(source1_name, source2_name, source3_name, 'to', target_name, ('%s max correct:' % target_name), correct.item(), '\n')

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
     ...  =  ... .optim.SGD

idx = 60:------------------- similar code ------------------ index = 45, score = 3.0 
def main():
    global best_prec1, args
    args.distributed = False
    if ('WORLD_SIZE' in os.environ):
        args.distributed = (int(os.environ['WORLD_SIZE']) > 1)
    args.gpu = 0
    args.world_size = 1
    if args.distributed:
        args.gpu = (args.local_rank % torch.cuda.device_count())
        torch.cuda.set_device(args.gpu)
        torch.distributed.init_process_group(backend='nccl', init_method='env://')
        args.world_size = torch.distributed.get_world_size()
    if args.fp16:
        assert torch.backends.cudnn.enabled, 'fp16 mode requires cudnn backend to be enabled.'
    if (args.static_loss_scale != 1.0):
        if (not args.fp16):
            print('Warning:  if --fp16 is not used, static_loss_scale will be ignored.')
    if args.pretrained:
        print("=> using pre-trained model '{}'".format(args.arch))
        model = models.__dict__[args.arch](pretrained=True)
    else:
        print("=> creating model '{}'".format(args.arch))
        model = models.__dict__[args.arch]()
    if args.sync_bn:
        import apex
        print('using apex synced BN')
        model = apex.parallel.convert_syncbn_model(model)
    model = model.cuda()
    if args.fp16:
        model = network_to_half(model)
    if args.distributed:
        model = DDP(model, delay_allreduce=True)
    criterion = nn.CrossEntropyLoss().cuda()
    args.lr = ((args.lr * float((args.batch_size * args.world_size))) / 256.0)
    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    if args.fp16:
        optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.static_loss_scale, dynamic_loss_scale=args.dynamic_loss_scale)
    if args.resume:

        def resume():
            if os.path.isfile(args.resume):
                print("=> loading checkpoint '{}'".format(args.resume))
                checkpoint = torch.load(args.resume, map_location=(lambda storage, loc: storage.cuda(args.gpu)))
                args.start_epoch = checkpoint['epoch']
                best_prec1 = checkpoint['best_prec1']
                model.load_state_dict(checkpoint['state_dict'])
                optimizer.load_state_dict(checkpoint['optimizer'])
                print("=> loaded checkpoint '{}' (epoch {})".format(args.resume, checkpoint['epoch']))
            else:
                print("=> no checkpoint found at '{}'".format(args.resume))
        resume()
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    if (args.arch == 'inception_v3'):
        crop_size = 299
        val_size = 320
    else:
        crop_size = 224
        val_size = 256
    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(crop_size), transforms.RandomHorizontalFlip()]))
    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(val_size), transforms.CenterCrop(crop_size)]))
    train_sampler = None
    val_sampler = None
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True, sampler=val_sampler, collate_fn=fast_collate)
    if args.evaluate:
        validate(val_loader, model, criterion)
        return
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        train(train_loader, model, criterion, optimizer, epoch)
        if args.prof:
            break
        prec1 = validate(val_loader, model, criterion)
        if (args.local_rank == 0):
            is_best = (prec1 > best_prec1)
            best_prec1 = max(prec1, best_prec1)
            save_checkpoint({'epoch': (epoch + 1), 'arch': args.arch, 'state_dict': model.state_dict(), 'best_prec1': best_prec1, 'optimizer': optimizer.state_dict()}, is_best)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 61:------------------- similar code ------------------ index = 75, score = 3.0 
def train(model):
    source1_iter = iter(source1_loader)
    source2_iter = iter(source2_loader)
    target_iter = iter(target_train_loader)
    correct = 0
    optimizer = torch.optim.SGD([{'params': model.sharedNet.parameters()}, {'params': model.cls_fc_son1.parameters(), 'lr': lr[1]}, {'params': model.cls_fc_son2.parameters(), 'lr': lr[1]}, {'params': model.sonnet1.parameters(), 'lr': lr[1]}, {'params': model.sonnet2.parameters(), 'lr': lr[1]}], lr=lr[0], momentum=momentum, weight_decay=l2_decay)
    for i in range(1, (iteration + 1)):
        model.train()
        optimizer.param_groups[0]['lr'] = (lr[0] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        optimizer.param_groups[1]['lr'] = (lr[1] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        optimizer.param_groups[2]['lr'] = (lr[1] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        optimizer.param_groups[3]['lr'] = (lr[1] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        optimizer.param_groups[4]['lr'] = (lr[1] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        try:
            (source_data, source_label) = source1_iter.next()
        except Exception as err:
            source1_iter = iter(source1_loader)
            (source_data, source_label) = source1_iter.next()
        try:
            (target_data, __) = target_iter.next()
        except Exception as err:
            target_iter = iter(target_train_loader)
            (target_data, __) = target_iter.next()
        if cuda:
            (source_data, source_label) = (source_data.cuda(), source_label.cuda())
            target_data = target_data.cuda()
        (source_data, source_label) = (Variable(source_data), Variable(source_label))
        target_data = Variable(target_data)
        optimizer.zero_grad()
        (cls_loss, mmd_loss, l1_loss) = model(source_data, target_data, source_label, mark=1)
        gamma = ((2 / (1 + math.exp((((- 10) * i) / iteration)))) - 1)
        loss = (cls_loss + (gamma * (mmd_loss + l1_loss)))
        loss.backward()
        optimizer.step()
        if ((i % log_interval) == 0):
            print('Train source1 iter: {} [({:.0f}%)]\tLoss: {:.6f}\tsoft_Loss: {:.6f}\tmmd_Loss: {:.6f}\tl1_Loss: {:.6f}'.format(i, ((100.0 * i) / iteration), loss.item(), cls_loss.item(), mmd_loss.item(), l1_loss.item()))
        try:
            (source_data, source_label) = source2_iter.next()
        except Exception as err:
            source2_iter = iter(source2_loader)
            (source_data, source_label) = source2_iter.next()
        try:
            (target_data, __) = target_iter.next()
        except Exception as err:
            target_iter = iter(target_train_loader)
            (target_data, __) = target_iter.next()
        if cuda:
            (source_data, source_label) = (source_data.cuda(), source_label.cuda())
            target_data = target_data.cuda()
        (source_data, source_label) = (Variable(source_data), Variable(source_label))
        target_data = Variable(target_data)
        optimizer.zero_grad()
        (cls_loss, mmd_loss, l1_loss) = model(source_data, target_data, source_label, mark=2)
        gamma = ((2 / (1 + math.exp((((- 10) * i) / iteration)))) - 1)
        loss = (cls_loss + (gamma * (mmd_loss + l1_loss)))
        loss.backward()
        optimizer.step()
        if ((i % log_interval) == 0):
            print('Train source2 iter: {} [({:.0f}%)]\tLoss: {:.6f}\tsoft_Loss: {:.6f}\tmmd_Loss: {:.6f}\tl1_Loss: {:.6f}'.format(i, ((100.0 * i) / iteration), loss.item(), cls_loss.item(), mmd_loss.item(), l1_loss.item()))
        if ((i % (log_interval * 20)) == 0):
            t_correct = test(model)
            if (t_correct > correct):
                correct = t_correct
            print(source1_name, source2_name, 'to', target_name, ('%s max correct:' % target_name), correct.item(), '\n')

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
     ...  =  ... .optim.SGD

idx = 62:------------------- similar code ------------------ index = 76, score = 3.0 
def main_worker(gpu, ngpus_per_node, args):
    global best_acc1
    args.gpu = gpu
    if (args.gpu is not None):
        print('Use GPU: {} for training'.format(args.gpu))
    if args.distributed:
        if ((args.dist_url == 'env://') and (args.rank == (- 1))):
            args.rank = int(os.environ['RANK'])
        if args.multiprocessing_distributed:
            args.rank = ((args.rank * ngpus_per_node) + gpu)
        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank)
    if args.pretrained:
        print("=> using pre-trained model '{}'".format(args.arch))
        model = models.__dict__[args.arch](pretrained=True)
    else:
        print("=> creating model '{}'".format(args.arch))
        model = models.__dict__[args.arch]()
    if args.distributed:
        if (args.gpu is not None):
            torch.cuda.set_device(args.gpu)
            model.cuda(args.gpu)
            args.batch_size = int((args.batch_size / ngpus_per_node))
            args.workers = int((((args.workers + ngpus_per_node) - 1) / ngpus_per_node))
            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
        else:
            model.cuda()
            model = torch.nn.parallel.DistributedDataParallel(model)
    elif (args.gpu is not None):
        torch.cuda.set_device(args.gpu)
        model = model.cuda(args.gpu)
    elif (args.arch.startswith('alexnet') or args.arch.startswith('vgg')):
        model.features = torch.nn.DataParallel(model.features)
        model.cuda()
    else:
        model = torch.nn.DataParallel(model).cuda()
    criterion = nn.CrossEntropyLoss().cuda(args.gpu)
    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    if args.resume:
        if os.path.isfile(args.resume):
            print("=> loading checkpoint '{}'".format(args.resume))
            if (args.gpu is None):
                checkpoint = torch.load(args.resume)
            else:
                loc = 'cuda:{}'.format(args.gpu)
                checkpoint = torch.load(args.resume, map_location=loc)
            args.start_epoch = checkpoint['epoch']
            best_acc1 = checkpoint['best_acc1']
            if (args.gpu is not None):
                best_acc1 = best_acc1.to(args.gpu)
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            print("=> loaded checkpoint '{}' (epoch {})".format(args.resume, checkpoint['epoch']))
        else:
            print("=> no checkpoint found at '{}'".format(args.resume))
    cudnn.benchmark = True
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]))
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    else:
        train_sampler = None
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    if args.evaluate:
        validate(val_loader, model, criterion, args)
        return
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        adjust_learning_rate(optimizer, epoch, args)
        train(train_loader, model, criterion, optimizer, epoch, args)
        acc1 = validate(val_loader, model, criterion, args)
        is_best = (acc1 > best_acc1)
        best_acc1 = max(acc1, best_acc1)
        if ((not args.multiprocessing_distributed) or (args.multiprocessing_distributed and ((args.rank % ngpus_per_node) == 0))):
            save_checkpoint({'epoch': (epoch + 1), 'arch': args.arch, 'state_dict': model.state_dict(), 'best_acc1': best_acc1, 'optimizer': optimizer.state_dict()}, is_best)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 63:------------------- similar code ------------------ index = 78, score = 3.0 
def initialize_optimizer_and_scheduler(self):
    assert (self.network is not None), 'self.initialize_network must be called first'
    self.optimizer = torch.optim.SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay, momentum=0.99, nesterov=True)
    self.lr_scheduler = None

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
 =  ... .optim.SGD

idx = 64:------------------- similar code ------------------ index = 80, score = 3.0 
def _fit_classifier(self, optimizer='adam', learning_rate=0.0004, weight_decay=0.0001, epochs=10):
    'Fits the last layer of the network using the cached features.'
    logging.info('Fitting final classifier...')
    if (not hasattr(self.model.classifier, 'input_features')):
        raise ValueError('You need to run `cache_features` on model before running `fit_classifier`')
    targets = self.model.classifier.targets.to(self.device)
    features = self.model.classifier.input_features.to(self.device)
    dataset = torch.utils.data.TensorDataset(features, targets)
    data_loader = _get_loader(dataset, **self.loader_opts)
    if (optimizer == 'adam'):
        optimizer = torch.optim.Adam(self.model.fc.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif (optimizer == 'sgd'):
        optimizer = torch.optim.SGD(self.model.fc.parameters(), lr=learning_rate, weight_decay=weight_decay)
    else:
        raise ValueError(f'Unsupported optimizer {optimizer}')
    loss_fn = nn.CrossEntropyLoss()
    for epoch in tqdm(range(epochs), desc='Fitting classifier', leave=False):
        metrics = AverageMeter()
        for (data, target) in data_loader:
            optimizer.zero_grad()
            output = self.model.classifier(data)
            loss = loss_fn(self.model.classifier(data), target)
            error = get_error(output, target)
            loss.backward()
            optimizer.step()
            metrics.update(n=data.size(0), loss=loss.item(), error=error)
        logging.info((f'[epoch {epoch}]: ' + '\t'.join((f'{k}: {v}' for (k, v) in metrics.avg.items()))))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
    if:    elif:
         ...  =  ... .optim.SGD

idx = 65:------------------- similar code ------------------ index = 81, score = 3.0 
def train(model):
    src_iter = iter(src_loader)
    tgt_iter = iter(tgt_train_loader)
    correct = 0
    optimizer = torch.optim.SGD([{'params': model.sharedNet.parameters()}, {'params': model.cls_fc.parameters(), 'lr': lr[1]}], lr=lr[0], momentum=momentum, weight_decay=l2_decay)
    for i in range(1, (iteration + 1)):
        model.train()
        for (index, param_group) in enumerate(optimizer.param_groups):
            param_group['lr'] = (lr[index] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        try:
            (src_data, src_label) = src_iter.next()
        except Exception as err:
            src_iter = iter(src_loader)
            (src_data, src_label) = src_iter.next()
        try:
            (tgt_data, _) = tgt_iter.next()
        except Exception as err:
            tgt_iter = iter(tgt_train_loader)
            (tgt_data, _) = tgt_iter.next()
        if cuda:
            (src_data, src_label) = (src_data.cuda(), src_label.cuda())
            tgt_data = tgt_data.cuda()
        optimizer.zero_grad()
        (src_pred, mmd_loss) = model(src_data, tgt_data)
        cls_loss = F.nll_loss(F.log_softmax(src_pred, dim=1), src_label)
        lambd = ((2 / (1 + math.exp((((- 10) * i) / iteration)))) - 1)
        loss = (cls_loss + (lambd * mmd_loss))
        loss.backward()
        optimizer.step()
        if ((i % log_interval) == 0):
            print('Train iter: {} [({:.0f}%)]\tLoss: {:.6f}\tsoft_Loss: {:.6f}\tmmd_Loss: {:.6f}'.format(i, ((100.0 * i) / iteration), loss.item(), cls_loss.item(), mmd_loss.item()))
        if ((i % (log_interval * 20)) == 0):
            t_correct = test(model)
            if (t_correct > correct):
                correct = t_correct
            print('src: {} to tgt: {} max correct: {} max accuracy{: .2f}%\n'.format(src_name, tgt_name, correct, ((100.0 * correct) / tgt_dataset_len)))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
     ...  =  ... .optim.SGD

idx = 66:------------------- similar code ------------------ index = 82, score = 3.0 
def train(model):
    src_iter = iter(src_loader)
    tgt_iter = iter(tgt_train_loader)
    correct = 0
    optimizer = torch.optim.SGD([{'params': model.sharedNet.parameters()}, {'params': model.cls_fc.parameters(), 'lr': lr[1]}], lr=lr[0], momentum=momentum, weight_decay=l2_decay)
    for i in range(1, (iteration + 1)):
        model.train()
        for (index, param_group) in enumerate(optimizer.param_groups):
            param_group['lr'] = (lr[index] / math.pow((1 + ((10 * (i - 1)) / iteration)), 0.75))
        try:
            (src_data, src_label) = src_iter.next()
        except Exception as err:
            src_iter = iter(src_loader)
            (src_data, src_label) = src_iter.next()
        try:
            (tgt_data, _) = tgt_iter.next()
        except Exception as err:
            tgt_iter = iter(tgt_train_loader)
            (tgt_data, _) = tgt_iter.next()
        if cuda:
            (src_data, src_label) = (src_data.cuda(), src_label.cuda())
            tgt_data = tgt_data.cuda()
        optimizer.zero_grad()
        (src_pred, coral_loss) = model(src_data, tgt_data)
        cls_loss = F.nll_loss(F.log_softmax(src_pred, dim=1), src_label)
        lambd = ((2 / (1 + math.exp((((- 10) * i) / iteration)))) - 1)
        loss = (cls_loss + (lambd * coral_loss))
        loss.backward()
        optimizer.step()
        if ((i % log_interval) == 0):
            print('Train iter: {} [({:.0f}%)]\ttotal_Loss: {:.6f}\tcls_Loss: {:.6f}\tcoral_Loss: {:.6f}'.format(i, ((100.0 * i) / iteration), loss.item(), cls_loss.item(), coral_loss.item()))
        if ((i % (log_interval * 20)) == 0):
            t_correct = test(model)
            if (t_correct > correct):
                correct = t_correct
            print('src: {} to tgt: {} max correct: {} max accuracy{: .2f}%\n'.format(src_name, tgt_name, correct, ((100.0 * correct) / tgt_dataset_len)))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
     ...  =  ... .optim.SGD

idx = 67:------------------- similar code ------------------ index = 83, score = 3.0 
@pytest.fixture(scope='function')
def simple_runner():
    model = nn.Linear(2, 1)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.02, momentum=0.95)
    runner = Runner(model=model, optimizer=optimizer, scheduler=None, batch_processor=SimpleBatchProcessor(), hooks=[ProgressBarLoggerHook(bar_width=10), TextLoggerHook(), IterTimerHook(), LogBufferHook()], work_dir=tempfile.mkdtemp())
    return runner

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 68:------------------- similar code ------------------ index = 85, score = 3.0 
def main():
    if (not torch.cuda.is_available()):
        sys.exit(1)
    start_t = time.time()
    cudnn.benchmark = True
    cudnn.enabled = True
    logging.info('args = %s', args)
    model = MobileNetV2()
    logging.info(model)
    model = nn.DataParallel(model).cuda()
    criterion = nn.CrossEntropyLoss()
    criterion = criterion.cuda()
    criterion_smooth = CrossEntropyLabelSmooth(CLASSES, args.label_smooth)
    criterion_smooth = criterion_smooth.cuda()
    all_parameters = model.parameters()
    weight_parameters = []
    for (pname, p) in model.named_parameters():
        if (('fc' in pname) or ('conv1' in pname) or ('pwconv' in pname)):
            weight_parameters.append(p)
    weight_parameters_id = list(map(id, weight_parameters))
    other_parameters = list(filter((lambda p: (id(p) not in weight_parameters_id)), all_parameters))
    optimizer = torch.optim.SGD([{'params': other_parameters}, {'params': weight_parameters, 'weight_decay': args.weight_decay}], args.learning_rate, momentum=args.momentum)
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, (lambda step: (1.0 - (step / args.epochs))), last_epoch=(- 1))
    start_epoch = 0
    best_top1_acc = 0
    checkpoint_tar = os.path.join(args.save, 'checkpoint.pth.tar')
    if os.path.exists(checkpoint_tar):
        logging.info('loading checkpoint {} ..........'.format(checkpoint_tar))
        checkpoint = torch.load(checkpoint_tar)
        start_epoch = checkpoint['epoch']
        best_top1_acc = checkpoint['best_top1_acc']
        model.load_state_dict(checkpoint['state_dict'])
        logging.info('loaded checkpoint {} epoch = {}'.format(checkpoint_tar, checkpoint['epoch']))
    for epoch in range(start_epoch):
        scheduler.step()
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    crop_scale = 0.08
    lighting_param = 0.1
    train_transforms = transforms.Compose([transforms.RandomResizedCrop(224, scale=(crop_scale, 1.0)), Lighting(lighting_param), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize])
    train_dataset = datasets.ImageFolder(traindir, transform=train_transforms)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    epoch = start_epoch
    while (epoch < args.epochs):
        (train_obj, train_top1_acc, train_top5_acc, epoch) = train(epoch, train_loader, model, criterion_smooth, optimizer, scheduler)
        (valid_obj, valid_top1_acc, valid_top5_acc) = validate(epoch, val_loader, model, criterion, args)
        is_best = False
        if (valid_top1_acc > best_top1_acc):
            best_top1_acc = valid_top1_acc
            is_best = True
        save_checkpoint({'epoch': epoch, 'state_dict': model.state_dict(), 'best_top1_acc': best_top1_acc, 'optimizer': optimizer.state_dict()}, is_best, args.save)
        epoch += 1
    training_time = ((time.time() - start_t) / 36000)
    print('total training time = {} hours'.format(training_time))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 69:------------------- similar code ------------------ index = 86, score = 3.0 
if (__name__ == '__main__'):
    model = models.RevGrad(num_classes=31)
    correct = 0
    print(model)
    if cuda:
        model.cuda()
    model = load_pretrain(model)
    optimizer_fea = torch.optim.SGD([{'params': model.sharedNet.parameters()}, {'params': model.cls_fc.parameters(), 'lr': lr[1]}], lr=lr[0], momentum=momentum, weight_decay=l2_decay)
    optimizer_critic = torch.optim.SGD([{'params': model.domain_fc.parameters(), 'lr': lr[1]}], lr=lr[1], momentum=momentum, weight_decay=l2_decay)
    for epoch in range(1, (epochs + 1)):
        train(epoch, model, optimizer_fea, optimizer_critic)
        t_correct = test(model)
        if (t_correct > correct):
            correct = t_correct
        print('source: {} to target: {} max correct: {} max accuracy{: .2f}%\n'.format(source_name, target_name, correct, ((100.0 * correct) / len_target_dataset)))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
     ...  =  ... .optim.SGD

idx = 70:------------------- similar code ------------------ index = 87, score = 3.0 
def initialize_optimizer_and_scheduler(self):
    if self.threeD:
        momentum = 0.99
    else:
        momentum = 0.9
    assert (self.network is not None), 'self.initialize_network must be called first'
    self.optimizer = torch.optim.SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay, momentum=momentum, nesterov=True)
    self.lr_scheduler = None

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
 =  ... .optim.SGD

idx = 71:------------------- similar code ------------------ index = 88, score = 3.0 
def main(logger, args):
    if (not torch.cuda.is_available()):
        raise Exception('need gpu to train network!')
    if (args.seed is not None):
        random.seed(args.seed)
        torch.cuda.manual_seed_all(args.seed)
        cudnn.deterministic = True
    gpus = torch.cuda.device_count()
    logger.info(f'use {gpus} gpus')
    logger.info(f'args: {args}')
    cudnn.benchmark = True
    cudnn.enabled = True
    start_time = time.time()
    logger.info('start loading data')
    train_loader = DataLoader(Config.train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)
    val_loader = DataLoader(Config.val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)
    logger.info('finish loading data')
    logger.info(f"creating model '{args.network}'")
    model = densenet161(**{'pretrained': args.pretrained, 'num_classes': args.num_classes})
    flops_input = torch.randn(1, 3, args.input_image_size, args.input_image_size)
    (flops, params) = profile(model, inputs=(flops_input,))
    (flops, params) = clever_format([flops, params], '%.3f')
    logger.info(f"model: '{args.network}', flops: {flops}, params: {params}")
    for (name, param) in model.named_parameters():
        logger.info(f'{name},{param.requires_grad}')
    model = model.cuda()
    criterion = nn.CrossEntropyLoss().cuda()
    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.milestones, gamma=0.1)
    if args.apex:
        (model, optimizer) = amp.initialize(model, optimizer, opt_level='O1')
    model = nn.DataParallel(model)
    if args.evaluate:
        if (not os.path.isfile(args.evaluate)):
            raise Exception(f'{args.resume} is not a file, please check it again')
        logger.info('start only evaluating')
        logger.info(f'start resuming model from {args.evaluate}')
        checkpoint = torch.load(args.evaluate, map_location=torch.device('cpu'))
        model.load_state_dict(checkpoint['model_state_dict'])
        (acc1, acc5, throughput) = validate(val_loader, model, args)
        logger.info(f"epoch {checkpoint['epoch']:0>3d}, top1 acc: {acc1:.2f}%, top5 acc: {acc5:.2f}%, throughput: {throughput:.2f}sample/s")
        return
    start_epoch = 1
    if os.path.exists(args.resume):
        logger.info(f'start resuming model from {args.resume}')
        checkpoint = torch.load(args.resume, map_location=torch.device('cpu'))
        start_epoch += checkpoint['epoch']
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        logger.info(f"finish resuming model from {args.resume}, epoch {checkpoint['epoch']}, loss: {checkpoint['loss']:3f}, lr: {checkpoint['lr']:.6f}, top1_acc: {checkpoint['acc1']}%")
    if (not os.path.exists(args.checkpoints)):
        os.makedirs(args.checkpoints)
    logger.info('start training')
    for epoch in range(start_epoch, (args.epochs + 1)):
        (acc1, acc5, losses) = train(train_loader, model, criterion, optimizer, scheduler, epoch, logger, args)
        logger.info(f'train: epoch {epoch:0>3d}, top1 acc: {acc1:.2f}%, top5 acc: {acc5:.2f}%, losses: {losses:.2f}')
        (acc1, acc5, throughput) = validate(val_loader, model, args)
        logger.info(f'val: epoch {epoch:0>3d}, top1 acc: {acc1:.2f}%, top5 acc: {acc5:.2f}%, throughput: {throughput:.2f}sample/s')
        torch.save({'epoch': epoch, 'acc1': acc1, 'loss': losses, 'lr': scheduler.get_lr()[0], 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict()}, os.path.join(args.checkpoints, 'latest.pth'))
        if (epoch == args.epochs):
            torch.save(model.module.state_dict(), os.path.join(args.checkpoints, '{}-epoch{}-acc{}.pth'.format(args.network, epoch, acc1)))
    training_time = ((time.time() - start_time) / 3600)
    logger.info(f'finish training, total training time: {training_time:.2f} hours')

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 72:------------------- similar code ------------------ index = 90, score = 3.0 
def train_scan(inference_vectorizer, train_Xy, val_Xy, test_Xy, epochs=1, batch_size=1, patience=3):
    scan_net = ScanNet(inference_vectorizer)
    if USE_CUDA:
        scan_net = scan_net.cuda()
    optimizer = torch.optim.SGD(scan_net.parameters(), lr=0.01)
    criterion = nn.BCELoss(reduction='sum')
    bow = Bag_of_words(inference_vectorizer)
    total_epoch_loss = []
    for epoch in range(epochs):
        if early_stopping(total_epoch_loss, patience):
            break
        epoch_loss = 0
        epoch_samples = sample_train(train_Xy)
        for i in range(0, len(epoch_samples), batch_size):
            instances = epoch_samples[i:(i + batch_size)]
            ys = torch.FloatTensor([inst['y'] for inst in instances])
            sens = torch.FloatTensor([bow.transform(inst['sentence_span']) for inst in instances])
            optimizer.zero_grad()
            if USE_CUDA:
                sens = sens.cuda()
                ys = ys.cuda()
            tags = scan_net(sens)
            loss = criterion(tags, ys)
            if (loss.item() != loss.item()):
                import pdb
                pdb.set_trace()
            epoch_loss += loss.item()
            loss.backward()
            optimizer.step()
        with torch.no_grad():
            instances = val_Xy
            y_hat = []
            val_loss = 0
            y_true = [inst['y'] for inst in val_Xy]
            for i in range(0, len(instances), batch_size):
                batch_instances = instances[i:(i + batch_size)]
                sens = torch.FloatTensor([bow.transform(inst['sentence_span']) for inst in batch_instances])
                ys = torch.FloatTensor([inst['y'] for inst in batch_instances])
                if USE_CUDA:
                    sens = sens.cuda()
                    ys = ys.cuda()
                tags = scan_net(sens, batch_size=len(batch_instances))
                val_loss += criterion(tags, ys)
                y_hat = np.append(y_hat, tags.data.cpu().numpy())
            y_hat = [(1 if (y > 0.5) else 0) for y in y_hat]
            acc = accuracy_score(y_true, y_hat)
            f1 = f1_score(y_true, y_hat)
            prc = precision_score(y_true, y_hat)
            rc = recall_score(y_true, y_hat)
            print('epoch {}. train loss: {:.2f}; val loss: {:.2f}; val acc: {:.2f}; val f1: {:.2f}; val precision: {:.2f}; val recall: {:.2f}'.format(epoch, epoch_loss, val_loss, acc, f1, prc, rc))
            total_epoch_loss.append(val_loss)
    return scan_net

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 73:------------------- similar code ------------------ index = 91, score = 3.0 
def setUp(self):
    torch.manual_seed(2)
    self.model = nn.Sequential(nn.Linear(STATE_DIM, 1))
    optimizer = torch.optim.SGD(self.model.parameters(), lr=0.1)
    self.v = VNetwork(self.model, optimizer)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
     ...  =  ... .optim.SGD

idx = 74:------------------- similar code ------------------ index = 92, score = 3.0 
if (__name__ == '__main__'):
    "Train a simple Hybrid Scattering + CNN model on CIFAR.\n\n        Three models are demoed:\n        'linear' - scattering + linear model\n        'mlp' - scattering + MLP\n        'cnn' - scattering + CNN\n\n        scattering 1st order can also be set by the mode\n        Scattering features are normalized by batch normalization.\n        The model achieves around 88% testing accuracy after 10 epochs.\n\n        scatter 1st order + linear achieves 64% in 90 epochs\n        scatter 2nd order + linear achieves 70.5% in 90 epochs\n\n        scatter + cnn achieves 88% in 15 epochs\n\n    "
    parser = argparse.ArgumentParser(description='MNIST scattering  + hybrid examples')
    parser.add_argument('--mode', type=int, default=1, help='scattering 1st or 2nd order')
    parser.add_argument('--classifier', type=str, default='cnn', help='classifier model')
    args = parser.parse_args()
    assert (args.classifier in ['linear', 'mlp', 'cnn'])
    use_cuda = torch.cuda.is_available()
    device = torch.device(('cuda' if use_cuda else 'cpu'))
    if (args.mode == 1):
        scattering = Scattering2D(J=2, shape=(32, 32), max_order=1)
        K = (17 * 3)
    else:
        scattering = Scattering2D(J=2, shape=(32, 32))
        K = (81 * 3)
    scattering = scattering.to(device)
    model = Scattering2dCNN(K, args.classifier).to(device)
    num_workers = 4
    if use_cuda:
        pin_memory = True
    else:
        pin_memory = False
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    train_loader = torch.utils.data.DataLoader(datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=True, transform=transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 4), transforms.ToTensor(), normalize]), download=True), batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)
    test_loader = torch.utils.data.DataLoader(datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=False, transform=transforms.Compose([transforms.ToTensor(), normalize])), batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)
    lr = 0.1
    for epoch in range(0, 90):
        if ((epoch % 20) == 0):
            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)
            lr *= 0.2
        train(model, device, train_loader, optimizer, (epoch + 1), scattering)
        test(model, device, test_loader, scattering)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
    for  ...  in:
        if:
             ...  =  ... .optim.SGD

idx = 75:------------------- similar code ------------------ index = 93, score = 3.0 
def __init__(self, args, cuda=None, train_id='None', logger=None):
    self.args = args
    os.environ['CUDA_VISIBLE_DEVICES'] = self.args.gpu
    self.cuda = (cuda and torch.cuda.is_available())
    self.device = torch.device(('cuda' if self.cuda else 'cpu'))
    self.train_id = train_id
    self.logger = logger
    self.current_MIoU = 0
    self.best_MIou = 0
    self.best_source_MIou = 0
    self.current_epoch = 0
    self.current_iter = 0
    self.second_best_MIou = 0
    self.writer = SummaryWriter(self.args.checkpoint_dir)
    self.Eval = Eval(self.args.num_classes)
    self.loss = nn.CrossEntropyLoss(weight=None, ignore_index=(- 1))
    self.loss.to(self.device)
    (self.model, params) = get_model(self.args)
    self.model = nn.DataParallel(self.model, device_ids=[0])
    self.model.to(self.device)
    if (self.args.optim == 'SGD'):
        self.optimizer = torch.optim.SGD(params=params, momentum=self.args.momentum, weight_decay=self.args.weight_decay)
    elif (self.args.optim == 'Adam'):
        self.optimizer = torch.optim.Adam(params, betas=(0.9, 0.99), weight_decay=self.args.weight_decay)
    if (self.args.dataset == 'cityscapes'):
        self.dataloader = City_DataLoader(self.args)
    elif (self.args.dataset == 'gta5'):
        self.dataloader = GTA5_DataLoader(self.args)
    else:
        self.dataloader = SYNTHIA_DataLoader(self.args)
    self.dataloader.num_iterations = min(self.dataloader.num_iterations, ITER_MAX)
    print(self.args.iter_max, self.dataloader.num_iterations)
    self.epoch_num = (ceil((self.args.iter_max / self.dataloader.num_iterations)) if (self.args.iter_stop is None) else ceil((self.args.iter_stop / self.dataloader.num_iterations)))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
    if:
 =  ... .optim.SGD

idx = 76:------------------- similar code ------------------ index = 96, score = 3.0 
def main():
    global best_prec1, args
    args.distributed = False
    if ('WORLD_SIZE' in os.environ):
        args.distributed = (int(os.environ['WORLD_SIZE']) > 1)
    args.gpu = 0
    args.world_size = 1
    if args.distributed:
        args.gpu = (args.local_rank % torch.cuda.device_count())
        torch.cuda.set_device(args.gpu)
        torch.distributed.init_process_group(backend='nccl', init_method='env://')
        args.world_size = torch.distributed.get_world_size()
    if args.fp16:
        assert torch.backends.cudnn.enabled, 'fp16 mode requires cudnn backend to be enabled.'
    if args.pretrained:
        print("=> using pre-trained model '{}'".format(args.arch))
        model = models.__dict__[args.arch](pretrained=True)
    else:
        print("=> creating model '{}'".format(args.arch))
        model = models.__dict__[args.arch]()
    if args.sync_bn:
        import apex
        print('using apex synced BN')
        model = apex.parallel.convert_syncbn_model(model)
    model = model.cuda()
    if args.distributed:
        model = DDP(model, delay_allreduce=True)
    criterion = nn.CrossEntropyLoss().cuda()
    args.lr = ((args.lr * float((args.batch_size * args.world_size))) / 256.0)
    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    if args.resume:

        def resume():
            if os.path.isfile(args.resume):
                print("=> loading checkpoint '{}'".format(args.resume))
                checkpoint = torch.load(args.resume, map_location=(lambda storage, loc: storage.cuda(args.gpu)))
                args.start_epoch = checkpoint['epoch']
                best_prec1 = checkpoint['best_prec1']
                model.load_state_dict(checkpoint['state_dict'])
                optimizer.load_state_dict(checkpoint['optimizer'])
                print("=> loaded checkpoint '{}' (epoch {})".format(args.resume, checkpoint['epoch']))
            else:
                print("=> no checkpoint found at '{}'".format(args.resume))
        resume()
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    if (args.arch == 'inception_v3'):
        crop_size = 299
        val_size = 320
    else:
        crop_size = 224
        val_size = 256
    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(crop_size), transforms.RandomHorizontalFlip()]))
    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(val_size), transforms.CenterCrop(crop_size)]))
    train_sampler = None
    val_sampler = None
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True, sampler=val_sampler, collate_fn=fast_collate)
    if args.evaluate:
        validate(val_loader, model, criterion)
        return
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        train(train_loader, model, criterion, optimizer, epoch)
        if args.prof:
            break
        prec1 = validate(val_loader, model, criterion)
        if (args.local_rank == 0):
            is_best = (prec1 > best_prec1)
            best_prec1 = max(prec1, best_prec1)
            save_checkpoint({'epoch': (epoch + 1), 'arch': args.arch, 'state_dict': model.state_dict(), 'best_prec1': best_prec1, 'optimizer': optimizer.state_dict()}, is_best)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 77:------------------- similar code ------------------ index = 97, score = 3.0 
def main():
    if (not torch.cuda.is_available()):
        sys.exit(1)
    start_t = time.time()
    cudnn.benchmark = True
    cudnn.enabled = True
    logging.info('args = %s', args)
    model = MobileNetV2()
    logging.info(model)
    model = nn.DataParallel(model).cuda()
    criterion = nn.CrossEntropyLoss()
    criterion = criterion.cuda()
    criterion_smooth = CrossEntropyLabelSmooth(CLASSES, args.label_smooth)
    criterion_smooth = criterion_smooth.cuda()
    all_parameters = model.parameters()
    weight_parameters = []
    for (pname, p) in model.named_parameters():
        if (('fc' in pname) or ('conv1' in pname) or ('pwconv' in pname)):
            weight_parameters.append(p)
    weight_parameters_id = list(map(id, weight_parameters))
    other_parameters = list(filter((lambda p: (id(p) not in weight_parameters_id)), all_parameters))
    optimizer = torch.optim.SGD([{'params': other_parameters}, {'params': weight_parameters, 'weight_decay': args.weight_decay}], args.learning_rate, momentum=args.momentum)
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, (lambda step: (1.0 - (step / args.epochs))), last_epoch=(- 1))
    start_epoch = 0
    best_top1_acc = 0
    checkpoint_tar = os.path.join(args.save, 'checkpoint.pth.tar')
    if os.path.exists(checkpoint_tar):
        logging.info('loading checkpoint {} ..........'.format(checkpoint_tar))
        checkpoint = torch.load(checkpoint_tar)
        start_epoch = checkpoint['epoch']
        best_top1_acc = checkpoint['best_top1_acc']
        model.load_state_dict(checkpoint['state_dict'])
        logging.info('loaded checkpoint {} epoch = {}'.format(checkpoint_tar, checkpoint['epoch']))
    for epoch in range(start_epoch):
        scheduler.step()
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    crop_scale = 0.08
    lighting_param = 0.1
    train_transforms = transforms.Compose([transforms.RandomResizedCrop(224, scale=(crop_scale, 1.0)), Lighting(lighting_param), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize])
    train_dataset = datasets.ImageFolder(traindir, transform=train_transforms)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    epoch = start_epoch
    while (epoch < args.epochs):
        (train_obj, train_top1_acc, train_top5_acc) = train(epoch, train_loader, model, criterion_smooth, optimizer, scheduler)
        (valid_obj, valid_top1_acc, valid_top5_acc) = validate(epoch, val_loader, model, criterion, args)
        is_best = False
        if (valid_top1_acc > best_top1_acc):
            best_top1_acc = valid_top1_acc
            is_best = True
        save_checkpoint({'epoch': epoch, 'state_dict': model.state_dict(), 'best_top1_acc': best_top1_acc, 'optimizer': optimizer.state_dict()}, is_best, args.save)
        epoch += 1
    training_time = ((time.time() - start_t) / 36000)
    print('total training time = {} hours'.format(training_time))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 78:------------------- similar code ------------------ index = 74, score = 3.0 
if (__name__ == '__main__'):
    model = models.MRANNet(num_classes=31)
    print(model)
    if args.cuda:
        model.cuda()
    (train_loader, unsuptrain_loader, test_loader) = load_data()
    correct = 0
    optimizer = torch.optim.SGD([{'params': model.sharedNet.parameters()}, {'params': model.Inception.parameters(), 'lr': args.lr[1]}], lr=args.lr[0], momentum=args.momentum, weight_decay=args.l2_decay)
    for epoch in range(1, (args.epochs + 1)):
        for (index, param_group) in enumerate(optimizer.param_groups):
            param_group['lr'] = (args.lr[index] / math.pow((1 + ((10 * (epoch - 1)) / args.epochs)), 0.75))
        train(epoch, model, train_loader, unsuptrain_loader, optimizer)
        t_correct = test(model, test_loader)
        if (t_correct > correct):
            correct = t_correct
        print(('%s max correct:' % args.test_dir), correct.item())
        print(args.source_dir, 'to', args.test_dir)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
     ...  =  ... .optim.SGD

idx = 79:------------------- similar code ------------------ index = 0, score = 3.0 
def train_eval_train_test(self, module, t):
    model = module(t).cuda()
    dummy_optimizer = torch.optim.SGD(model.parameters(), lr=1.0)

    def training_step():
        for param in model.parameters():
            param.grad = None
        loss = model(self.x).sum()
        self.handle._default_scaler._loss_scale = 1.0
        with self.handle.scale_loss(loss, dummy_optimizer) as scaled_loss:
            scaled_loss.backward()
        self.assertEqual(len([p.grad for p in model.parameters() if (p.grad is not None)]), 1)
        self.assertEqual(model.weight.grad.type(), model.weight.type())
        reference_grad = get_reference_grad(self.x, model.weight, model.ops)
        if (model.weight.grad.type() == 'torch.cuda.HalfTensor'):
            self.assertTrue(torch.allclose(model.weight.grad.float(), reference_grad))
        elif (model.weight.grad.type() == 'torch.cuda.FloatTensor'):
            self.assertTrue(torch.allclose(model.weight.grad.float(), reference_grad))
        else:
            raise RuntimeError('model.weight.grad.type = {}'.format(model.weight.grad.type()))
        model.weight.data -= 1.0
    training_step()
    with torch.no_grad():
        loss = model(self.x).sum()
    training_step()

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD


idx = 80:------------------- similar code ------------------ index = 69, score = 3.0 
def train(train_Xy, n_epochs=4, batch_size=4):
    tokenizer = RobertaTokenizer.from_pretrained('allenai/biomed_roberta_base')
    model = RobertaForSequenceClassification.from_pretrained('allenai/biomed_roberta_base').to(device=device)
    from transformers import AdamW
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    best_val = np.inf
    train_epoch_loss = 0
    for epoch in range(n_epochs):
        model.train()
        print('on epoch ', epoch)
        train_epoch_loss = 0
        (batch_X, batch_y) = ([], [])
        cur_batch_size = 0
        for (i, article) in enumerate(train_Xy):
            if ((i % 100) == 0):
                print('on article', i)
            (cur_X, cur_y) = instances_from_article(article, max_instances=(batch_size - cur_batch_size))
            batch_X.extend(cur_X)
            batch_y.extend(cur_y)
            cur_batch_size += len(cur_X)
            if (cur_batch_size >= batch_size):
                optimizer.zero_grad()
                batch_X_tensor = tokenizer.batch_encode_plus(batch_X[:batch_size], max_length=512, add_special_tokens=True, pad_to_max_length=True)
                batch_y_tensor = torch.tensor(batch_y[:batch_size])
                (loss, logits) = model(torch.tensor(batch_X_tensor['input_ids']).to(device=device), attention_mask=torch.tensor(batch_X_tensor['attention_mask']).to(device=device), labels=batch_y_tensor.to(device=device))
                train_epoch_loss += loss.cpu().detach().numpy()
                loss.backward()
                optimizer.step()
                cur_batch_size = 0
                (batch_X, batch_y) = ([], [])
        print('total epoch train loss {}'.format(train_epoch_loss))
        print('evaluating on val...')
        model.eval()
        (total_correct, total_preds) = (0, 0)
        val_loss = 0
        for (j, article) in enumerate(val_Xy):
            (val_X, val_y) = instances_from_article(article, max_instances=batch_size)
            val_X_tensor = tokenizer.batch_encode_plus(val_X[:batch_size], max_length=512, add_special_tokens=True, pad_to_max_length=True)
            val_y_tensor = torch.tensor(val_y[:batch_size])
            (loss, logits) = model(torch.tensor(val_X_tensor['input_ids']).to(device=device), attention_mask=torch.tensor(val_X_tensor['attention_mask']).to(device=device), labels=torch.tensor(val_y_tensor).to(device=device))
            val_loss += loss.cpu().detach().numpy()
            class_preds = torch.argmax(logits, dim=1).detach().cpu()
            total_correct += (class_preds == val_y_tensor).sum()
            total_preds += len(val_X)
        val_acc = (total_correct / float(total_preds))
        print('val loss, acc after epoch {} is: {}, {}'.format(epoch, val_loss, val_acc))
        if (val_loss < best_val):
            print('new best loss: {}'.format(val_loss))
            best_val = val_loss
            torch.save(model.state_dict(), 'inference.model')

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 81:------------------- similar code ------------------ index = 56, score = 3.0 
def main():
    'Main function'
    args = parse_args()
    print('Called with args:')
    print(args)
    if (not torch.cuda.is_available()):
        sys.exit('Need a CUDA device to run the code.')
    if (args.cuda or (cfg.NUM_GPUS > 0)):
        cfg.CUDA = True
    else:
        raise ValueError('Need Cuda device to run !')
    if (args.dataset == 'vrd'):
        cfg.TRAIN.DATASETS = ('vrd_train',)
        cfg.TEST.DATASETS = ('vrd_val',)
        cfg.MODEL.NUM_CLASSES = 101
        cfg.MODEL.NUM_PRD_CLASSES = 70
    elif (args.dataset == 'vg_mini'):
        cfg.TRAIN.DATASETS = ('vg_train_mini',)
        cfg.MODEL.NUM_CLASSES = 151
        cfg.MODEL.NUM_PRD_CLASSES = 50
    elif (args.dataset == 'vg'):
        cfg.TRAIN.DATASETS = ('vg_train',)
        cfg.MODEL.NUM_CLASSES = 151
        cfg.MODEL.NUM_PRD_CLASSES = 50
    elif (args.dataset == 'oi_rel'):
        cfg.TRAIN.DATASETS = ('oi_rel_train',)
        cfg.MODEL.NUM_CLASSES = 58
        cfg.MODEL.NUM_PRD_CLASSES = 9
    elif (args.dataset == 'oi_rel_mini'):
        cfg.TRAIN.DATASETS = ('oi_rel_train_mini',)
        cfg.MODEL.NUM_CLASSES = 58
        cfg.MODEL.NUM_PRD_CLASSES = 9
    else:
        raise ValueError('Unexpected args.dataset: {}'.format(args.dataset))
    cfg_from_file(args.cfg_file)
    if (args.set_cfgs is not None):
        cfg_from_list(args.set_cfgs)
    Generalized_RCNN = importlib.import_module(('modeling_rel.' + cfg.MODEL.TYPE)).Generalized_RCNN
    from core.test_engine_rel_mps import get_metrics_det_boxes, get_metrics_gt_boxes
    original_batch_size = (cfg.NUM_GPUS * cfg.TRAIN.IMS_PER_BATCH)
    original_num_gpus = cfg.NUM_GPUS
    original_ims_per_batch = cfg.TRAIN.IMS_PER_BATCH
    if (args.batch_size is None):
        args.batch_size = original_batch_size
    cfg.NUM_GPUS = torch.cuda.device_count()
    assert ((args.batch_size % cfg.NUM_GPUS) == 0), ('batch_size: %d, NUM_GPUS: %d' % (args.batch_size, cfg.NUM_GPUS))
    cfg.TRAIN.IMS_PER_BATCH = (args.batch_size // cfg.NUM_GPUS)
    effective_batch_size = (args.iter_size * args.batch_size)
    print(('effective_batch_size = batch_size * iter_size = %d * %d' % (args.batch_size, args.iter_size)))
    print('Adaptive config changes:')
    print(('    effective_batch_size: %d --> %d' % (original_batch_size, effective_batch_size)))
    print(('    NUM_GPUS:             %d --> %d' % (original_num_gpus, cfg.NUM_GPUS)))
    print(('    IMS_PER_BATCH:        %d --> %d' % (original_ims_per_batch, cfg.TRAIN.IMS_PER_BATCH)))
    old_base_lr = cfg.SOLVER.BASE_LR
    cfg.SOLVER.BASE_LR *= (args.batch_size / original_batch_size)
    print('Adjust BASE_LR linearly according to batch_size change:\n    BASE_LR: {} --> {}'.format(old_base_lr, cfg.SOLVER.BASE_LR))
    step_scale = (original_batch_size / effective_batch_size)
    old_solver_steps = cfg.SOLVER.STEPS
    old_max_iter = cfg.SOLVER.MAX_ITER
    cfg.SOLVER.STEPS = list(map((lambda x: int(((x * step_scale) + 0.5))), cfg.SOLVER.STEPS))
    cfg.SOLVER.MAX_ITER = int(((cfg.SOLVER.MAX_ITER * step_scale) + 0.5))
    print('Adjust SOLVER.STEPS and SOLVER.MAX_ITER linearly based on effective_batch_size change:\n    SOLVER.STEPS: {} --> {}\n    SOLVER.MAX_ITER: {} --> {}'.format(old_solver_steps, cfg.SOLVER.STEPS, old_max_iter, cfg.SOLVER.MAX_ITER))
    if (cfg.FPN.FPN_ON and cfg.MODEL.FASTER_RCNN):
        cfg.FPN.RPN_COLLECT_SCALE = (cfg.TRAIN.IMS_PER_BATCH / original_ims_per_batch)
        print('Scale FPN rpn_proposals collect size directly propotional to the change of IMS_PER_BATCH:\n    cfg.FPN.RPN_COLLECT_SCALE: {}'.format(cfg.FPN.RPN_COLLECT_SCALE))
    if (args.num_workers is not None):
        cfg.DATA_LOADER.NUM_THREADS = args.num_workers
    print(('Number of data loading threads: %d' % cfg.DATA_LOADER.NUM_THREADS))
    if (args.optimizer is not None):
        cfg.SOLVER.TYPE = args.optimizer
    if (args.lr is not None):
        cfg.SOLVER.BASE_LR = args.lr
    if (args.lr_decay_gamma is not None):
        cfg.SOLVER.GAMMA = args.lr_decay_gamma
    assert_and_infer_cfg()
    timers = defaultdict(Timer)
    timers['roidb'].tic()
    (roidb, ratio_list, ratio_index, ds) = combined_roidb_for_training(cfg.TRAIN.DATASETS, cfg.TRAIN.PROPOSAL_FILES)
    timers['roidb'].toc()
    roidb_size = len(roidb)
    logger.info('{:d} roidb entries'.format(roidb_size))
    logger.info('Takes %.2f sec(s) to construct roidb', timers['roidb'].average_time)
    train_size = ((roidb_size // args.batch_size) * args.batch_size)
    batchSampler = BatchSampler(sampler=MinibatchSampler(ratio_list, ratio_index), batch_size=args.batch_size, drop_last=True)
    dataset = RoiDataLoader(roidb, cfg.MODEL.NUM_CLASSES, training=True)
    dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=batchSampler, num_workers=cfg.DATA_LOADER.NUM_THREADS, collate_fn=collate_minibatch)
    dataiterator = iter(dataloader)
    maskRCNN = Generalized_RCNN()
    if cfg.CUDA:
        maskRCNN.cuda()
    gn_params = []
    backbone_bias_params = []
    backbone_bias_param_names = []
    prd_branch_bias_params = []
    prd_branch_bias_param_names = []
    backbone_nonbias_params = []
    backbone_nonbias_param_names = []
    prd_branch_nonbias_params = []
    prd_branch_nonbias_param_names = []
    for (key, value) in dict(maskRCNN.named_parameters()).items():
        if value.requires_grad:
            if ('gn' in key):
                gn_params.append(value)
            elif (('Conv_Body' in key) or ('Box_Head' in key) or ('Box_Outs' in key) or ('RPN' in key)):
                if ('bias' in key):
                    backbone_bias_params.append(value)
                    backbone_bias_param_names.append(key)
                else:
                    backbone_nonbias_params.append(value)
                    backbone_nonbias_param_names.append(key)
            elif ('bias' in key):
                prd_branch_bias_params.append(value)
                prd_branch_bias_param_names.append(key)
            else:
                prd_branch_nonbias_params.append(value)
                prd_branch_nonbias_param_names.append(key)
    params = [{'params': backbone_nonbias_params, 'lr': 0, 'weight_decay': cfg.SOLVER.WEIGHT_DECAY}, {'params': backbone_bias_params, 'lr': (0 * (cfg.SOLVER.BIAS_DOUBLE_LR + 1)), 'weight_decay': (cfg.SOLVER.WEIGHT_DECAY if cfg.SOLVER.BIAS_WEIGHT_DECAY else 0)}, {'params': prd_branch_nonbias_params, 'lr': 0, 'weight_decay': cfg.SOLVER.WEIGHT_DECAY}, {'params': prd_branch_bias_params, 'lr': (0 * (cfg.SOLVER.BIAS_DOUBLE_LR + 1)), 'weight_decay': (cfg.SOLVER.WEIGHT_DECAY if cfg.SOLVER.BIAS_WEIGHT_DECAY else 0)}, {'params': gn_params, 'lr': 0, 'weight_decay': cfg.SOLVER.WEIGHT_DECAY_GN}]
    if (cfg.SOLVER.TYPE == 'SGD'):
        optimizer = torch.optim.SGD(params, momentum=cfg.SOLVER.MOMENTUM)
    elif (cfg.SOLVER.TYPE == 'Adam'):
        optimizer = torch.optim.Adam(params)
    if args.load_ckpt:
        load_name = args.load_ckpt
        logging.info('loading checkpoint %s', load_name)
        checkpoint = torch.load(load_name, map_location=(lambda storage, loc: storage))
        net_utils_rel.load_ckpt_rel(maskRCNN, checkpoint['model'])
        if args.resume:
            args.start_step = (checkpoint['step'] + 1)
            if ('train_size' in checkpoint):
                if (checkpoint['train_size'] != train_size):
                    print(('train_size value: %d different from the one in checkpoint: %d' % (train_size, checkpoint['train_size'])))
            optimizer.load_state_dict(checkpoint['optimizer'])
            misc_utils.load_optimizer_state_dict(optimizer, checkpoint['optimizer'])
        del checkpoint
        torch.cuda.empty_cache()
    if args.load_detectron:
        logging.info('loading Detectron weights %s', args.load_detectron)
        load_detectron_weight(maskRCNN, args.load_detectron)
    lr = optimizer.param_groups[2]['lr']
    backbone_lr = optimizer.param_groups[0]['lr']
    device_ids = list(range(torch.cuda.device_count()))
    maskRCNN_one_gpu = mynn.DataParallel(maskRCNN, cpu_keywords=['im_info', 'roidb'], minibatch=True, device_ids=[device_ids[0]])
    maskRCNN = mynn.DataParallel(maskRCNN, cpu_keywords=['im_info', 'roidb'], minibatch=True)
    args.run_name = (((((misc_utils.get_run_name() + '_') + args.exp) + '_') + '_step_with_prd_cls_v') + str(cfg.MODEL.SUBTYPE))
    output_dir = misc_utils.get_output_dir(args, args.run_name)
    args.cfg_filename = os.path.basename(args.cfg_file)
    if (not args.no_save):
        if (not os.path.exists(output_dir)):
            os.makedirs(output_dir)
        blob = {'cfg': yaml.dump(cfg), 'args': args}
        with open(os.path.join(output_dir, 'config_and_args.pkl'), 'wb') as f:
            pickle.dump(blob, f, pickle.HIGHEST_PROTOCOL)
        if args.use_tfboard:
            from tensorboardX import SummaryWriter
            tblogger = SummaryWriter(output_dir)
    maskRCNN.train()
    CHECKPOINT_PERIOD = (ds.len // effective_batch_size)
    decay_steps_ind = None
    for i in range(1, len(cfg.SOLVER.STEPS)):
        if (cfg.SOLVER.STEPS[i] >= args.start_step):
            decay_steps_ind = i
            break
    if (decay_steps_ind is None):
        decay_steps_ind = len(cfg.SOLVER.STEPS)
    training_stats = TrainingStats(args, args.disp_interval, (tblogger if (args.use_tfboard and (not args.no_save)) else None))
    try:
        logger.info('Training starts !')
        step = args.start_step
        for step in range(args.start_step, cfg.SOLVER.MAX_ITER):
            if (step < cfg.SOLVER.WARM_UP_ITERS):
                method = cfg.SOLVER.WARM_UP_METHOD
                if (method == 'constant'):
                    warmup_factor = cfg.SOLVER.WARM_UP_FACTOR
                elif (method == 'linear'):
                    alpha = (step / cfg.SOLVER.WARM_UP_ITERS)
                    warmup_factor = ((cfg.SOLVER.WARM_UP_FACTOR * (1 - alpha)) + alpha)
                else:
                    raise KeyError('Unknown SOLVER.WARM_UP_METHOD: {}'.format(method))
                lr_new = (cfg.SOLVER.BASE_LR * warmup_factor)
                net_utils_rel.update_learning_rate_rel(optimizer, lr, lr_new)
                lr = optimizer.param_groups[2]['lr']
                backbone_lr = optimizer.param_groups[0]['lr']
                assert (lr == lr_new)
            elif (step == cfg.SOLVER.WARM_UP_ITERS):
                net_utils_rel.update_learning_rate_rel(optimizer, lr, cfg.SOLVER.BASE_LR)
                lr = optimizer.param_groups[2]['lr']
                backbone_lr = optimizer.param_groups[0]['lr']
                assert (lr == cfg.SOLVER.BASE_LR)
            if ((decay_steps_ind < len(cfg.SOLVER.STEPS)) and (step == cfg.SOLVER.STEPS[decay_steps_ind])):
                logger.info('Decay the learning on step %d', step)
                lr_new = (lr * cfg.SOLVER.GAMMA)
                net_utils_rel.update_learning_rate_rel(optimizer, lr, lr_new)
                lr = optimizer.param_groups[2]['lr']
                backbone_lr = optimizer.param_groups[0]['lr']
                assert (lr == lr_new)
                decay_steps_ind += 1
            training_stats.IterTic()
            optimizer.zero_grad()
            for inner_iter in range(args.iter_size):
                try:
                    input_data = next(dataiterator)
                except StopIteration:
                    dataiterator = iter(dataloader)
                    input_data = next(dataiterator)
                for key in input_data:
                    if (key != 'roidb'):
                        input_data[key] = list(map(Variable, input_data[key]))
                net_outputs = maskRCNN(**input_data)
                training_stats.UpdateIterStats(net_outputs, inner_iter)
                loss = net_outputs['total_loss']
                loss.backward()
            optimizer.step()
            training_stats.IterToc()
            if (step == args.start_step):
                for (n, p) in maskRCNN.named_parameters():
                    if ((p.requires_grad == True) and (p.grad is None)):
                        logger.warning('The module was defined but no-use!')
                        logger.warning(n)
            training_stats.LogIterStats(step, lr, backbone_lr)
            if ((int((step + 1)) % CHECKPOINT_PERIOD) == 0):
                save_ckpt(output_dir, args, step, train_size, maskRCNN, optimizer)
                metrics = get_metrics_gt_boxes(maskRCNN_one_gpu, timers, cfg.TEST.DATASETS[0])
                maskRCNN.train()
                tblogger.add_scalar((args.dataset + '_metrics'), metrics, step)
        save_ckpt(output_dir, args, step, train_size, maskRCNN, optimizer)
    except (RuntimeError, KeyboardInterrupt):
        del dataiterator
        logger.info('Save ckpt on exception ...')
        save_ckpt(output_dir, args, step, train_size, maskRCNN, optimizer)
        logger.info('Save ckpt done.')
        stack_trace = traceback.format_exc()
        print(stack_trace)
    finally:
        if (args.use_tfboard and (not args.no_save)):
            tblogger.close()

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
    if:
         ...  =  ... .optim.SGD

idx = 82:------------------- similar code ------------------ index = 46, score = 3.0 
if (__name__ == '__main__'):
    args = parse_args()
    print('Called with args:')
    print(args)
    assert (args.dataset == 'uav')
    args.imdb_name = 'uav_2017_trainval'
    args.imdbval_name = 'uav_2017_test'
    args.set_cfgs = ['ANCHOR_SCALES', '[1, 2, 4, 8, 16]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '107']
    args.cfg_file = ('cfgs/{}_ls.yml'.format(args.net) if args.large_scale else 'cfgs/{}.yml'.format(args.net))
    if (args.cfg_file is not None):
        cfg_from_file(args.cfg_file)
    if (args.set_cfgs is not None):
        cfg_from_list(args.set_cfgs)
    print('Using config:')
    pprint.pprint(cfg)
    np.random.seed(cfg.RNG_SEED)
    if (torch.cuda.is_available() and (not args.cuda)):
        print('WARNING: You have a CUDA device, so you should probably run with --cuda')
    cfg.TRAIN.USE_FLIPPED = False
    cfg.USE_GPU_NMS = args.cuda
    'Dataloader for the training'
    (imdb_train, roidb_train, ratio_list_train, ratio_index_train) = combined_roidb(args.imdb_name)
    train_size = len(roidb_train)
    print('{:d} roidb entries'.format(len(roidb_train)))
    sampler_batch = sampler(train_size, args.batch_size)
    dataset_train = roibatchLoader(roidb_train, ratio_list_train, ratio_index_train, args.batch_size, imdb_train.num_classes, training=True)
    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=args.batch_size, sampler=sampler_batch, num_workers=args.num_workers)
    'Dataloader for the validation/testing'
    (imdb_val, roidb_val, ratio_list_val, ratio_index_val) = combined_roidb(args.imdbval_name)
    val_size = len(roidb_val)
    print('{:d} roidb entries'.format(len(roidb_train)))
    sampler_batch = sampler(val_size, args.batch_size)
    dataset_val = roibatchLoader(roidb_val, ratio_list_val, ratio_index_val, args.batch_size, imdb_val.num_classes, training=True)
    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=args.batch_size, sampler=sampler_batch, num_workers=args.num_workers)
    im_data = torch.FloatTensor(1)
    im_info = torch.FloatTensor(1)
    meta_data = torch.FloatTensor(1)
    num_boxes = torch.LongTensor(1)
    gt_boxes = torch.FloatTensor(1)
    if args.cuda:
        im_data = im_data.cuda()
        im_info = im_info.cuda()
        meta_data = meta_data.cuda()
        num_boxes = num_boxes.cuda()
        gt_boxes = gt_boxes.cuda()
    im_data = Variable(im_data)
    im_info = Variable(im_info)
    meta_data = Variable(meta_data)
    num_boxes = Variable(num_boxes)
    gt_boxes = Variable(gt_boxes)
    if args.cuda:
        cfg.CUDA = True
    if (args.net == 'vgg16'):
        fasterRCNN = vgg16(imdb_train.classes, pretrained=True, class_agnostic=args.class_agnostic)
    elif (args.net == 'res101'):
        fasterRCNN = resnet(imdb_train.classes, 101, pretrained=True, class_agnostic=args.class_agnostic)
    elif (args.net == 'res50'):
        fasterRCNN = resnet(imdb_train.classes, 50, pretrained=True, class_agnostic=args.class_agnostic)
    elif (args.net == 'res152'):
        fasterRCNN = resnet(imdb_train.classes, 152, pretrained=True, class_agnostic=args.class_agnostic)
    else:
        print('network is not defined')
        pdb.set_trace()
    fasterRCNN.create_architecture()
    lr = cfg.TRAIN.LEARNING_RATE
    lr = args.lr
    params_util = []
    params_adv = []
    params_aux = []
    params_keys_util = []
    params_keys_adv = []
    params_keys_aux = []
    for (key, value) in dict(fasterRCNN.named_parameters()).items():
        if (value.requires_grad and (not any(((name in key) for name in ['RCNN_weather', 'RCNN_altitude', 'RCNN_angle', 'RCNN_weather_score', 'RCNN_altitude_score', 'RCNN_angle_score'])))):
            params_keys_util.append(key)
            if ('bias' in key):
                params_util += [{'params': [value], 'lr': (lr * (cfg.TRAIN.DOUBLE_BIAS + 1)), 'weight_decay': ((cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY) or 0)}]
            else:
                params_util += [{'params': [value], 'lr': lr, 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]
        if (value.requires_grad and any(((name in key) for name in ['RCNN_weather', 'RCNN_altitude', 'RCNN_angle', 'RCNN_weather_score', 'RCNN_altitude_score', 'RCNN_angle_score']))):
            params_keys_aux.append(key)
            if ('bias' in key):
                params_aux += [{'params': [value], 'lr': (lr * (cfg.TRAIN.DOUBLE_BIAS + 1)), 'weight_decay': ((cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY) or 0)}]
            else:
                params_aux += [{'params': [value], 'lr': lr, 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]
        if (value.requires_grad and any(((name in key) for name in ['RCNN_base']))):
            params_keys_adv.append(key)
            if ('bias' in key):
                params_adv += [{'params': [value], 'lr': ((lr * 0.1) * (cfg.TRAIN.DOUBLE_BIAS + 1)), 'weight_decay': ((cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY) or 0)}]
            else:
                params_adv += [{'params': [value], 'lr': (lr * 0.1), 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]
    if (args.optimizer == 'adam'):
        lr = (lr * 0.1)
        optimizer = torch.optim.Adam(params_util)
    elif (args.optimizer == 'sgd'):
        optimizer = torch.optim.SGD(params_util, momentum=cfg.TRAIN.MOMENTUM)
    aux_optimizer = torch.optim.Adam(params_aux)
    if ((args.gamma_altitude > 1e-10) and (args.gamma_angle > 1e-10) and (args.gamma_weather > 1e-10)):
        nuisance_type = 'A+V+W'
    elif ((args.gamma_altitude > 1e-10) and (args.gamma_angle > 1e-10)):
        nuisance_type = 'A+V'
    elif ((args.gamma_altitude > 1e-10) and (args.gamma_weather > 1e-10)):
        nuisance_type = 'A+W'
    elif (args.gamma_altitude > 1e-10):
        nuisance_type = 'A'
    elif (args.gamma_angle > 1e-10):
        nuisance_type = 'V'
    elif (args.gamma_weather > 1e-10):
        nuisance_type = 'W'
    else:
        nuisance_type = 'Baseline'
    model_dir = os.path.join(args.model_dir, nuisance_type, 'altitude={}_angle={}_weather={}'.format(str(args.gamma_altitude), str(args.gamma_angle), str(args.gamma_weather)))
    summary_dir = os.path.join(args.summary_dir, nuisance_type, 'altitude={}_angle={}_weather={}'.format(str(args.gamma_altitude), str(args.gamma_angle), str(args.gamma_weather)))
    if args.resume:
        load_name = os.path.join(model_dir, 'faster_rcnn_{}_{}_{}_adv.pth'.format(args.checksession, args.checkepoch, args.checkpoint))
        print(('loading checkpoint %s' % load_name))
        checkpoint = torch.load(load_name)
        args.session = checkpoint['session']
        args.start_epoch = checkpoint['epoch']
        model_dict = fasterRCNN.state_dict()
        model_dict.update(checkpoint['model'])
        fasterRCNN.load_state_dict(model_dict)
        optimizer.load_state_dict(checkpoint['optimizer'])
        for state in optimizer.state.values():
            for (k, v) in state.items():
                if isinstance(v, torch.Tensor):
                    state[k] = v.cuda()
        aux_optimizer.load_state_dict(checkpoint['aux_optimizer'])
        for state in aux_optimizer.state.values():
            for (k, v) in state.items():
                if isinstance(v, torch.Tensor):
                    state[k] = v.cuda()
        print(('loaded checkpoint %s' % load_name))
    else:
        load_name = os.path.join(os.path.join(args.model_dir, 'Pretrained'), 'faster_rcnn_{}_{}_{}.pth'.format(args.checksession, args.checkepoch, args.checkpoint))
        print(('loading checkpoint %s' % load_name))
        checkpoint = torch.load(load_name)
        args.session = checkpoint['session']
        args.start_epoch = checkpoint['epoch']
        model_dict = fasterRCNN.state_dict()
        model_dict.update(checkpoint['model'])
        fasterRCNN.load_state_dict(model_dict)
        print(('loaded checkpoint %s' % load_name))
    if args.mGPUs:
        fasterRCNN = nn.DataParallel(fasterRCNN)
    if args.cuda:
        fasterRCNN.cuda()
    iters_per_epoch = int((train_size / args.batch_size))
    if args.use_tfboard:
        from tensorboardX import SummaryWriter
        logger = SummaryWriter('logs')
    niter = args.niter
    fasterRCNN.train()
    data_iter_train = iter(dataloader_train)
    data_iter_val = iter(dataloader_val)
    if (not os.path.exists(model_dir)):
        os.makedirs(model_dir)
    if (not os.path.exists(summary_dir)):
        os.makedirs(summary_dir)
    train_summary_file = open(os.path.join(summary_dir, 'train_summary.txt'), 'w', 0)
    val_summary_file = open(os.path.join(summary_dir, 'val_summary.txt'), 'w', 0)
    while (niter < (iters_per_epoch * ((args.max_epochs - args.start_epoch) + 1))):
        fasterRCNN.zero_grad()
        start = time.time()
        if ((niter % ((args.lr_decay_step + 1) * iters_per_epoch)) == 0):
            adjust_learning_rate(optimizer, args.lr_decay_gamma)
            lr *= args.lr_decay_gamma
        if ((niter == 0) or (args.use_restarting and ((niter % args.restarting_iters) == 0))):
            '\n            Restarting\n            '
            caffe_state_dict = torch.load('data/pretrained_model/resnet101_caffe.pth')
            rcnn_altitude = fasterRCNN.module.RCNN_altitude
            rcnn_altitude.load_state_dict({('0' + k[6:]): v for (k, v) in caffe_state_dict.items() if (('layer4' in k) and (('0' + k[6:]) in rcnn_altitude.state_dict()))})
            rcnn_angle = fasterRCNN.module.RCNN_angle
            rcnn_angle.load_state_dict({('0' + k[6:]): v for (k, v) in caffe_state_dict.items() if (('layer4' in k) and (('0' + k[6:]) in rcnn_angle.state_dict()))})
            rcnn_weather = fasterRCNN.module.RCNN_weather
            rcnn_weather.load_state_dict({('0' + k[6:]): v for (k, v) in caffe_state_dict.items() if (('layer4' in k) and (('0' + k[6:]) in rcnn_weather.state_dict()))})
            for _ in itertools.repeat(None, args.retraining_steps):
                loss_adv_temp = 0
                loss_aux_temp = 0
                acc_altitude_temp = 0
                acc_angle_temp = 0
                acc_weather_temp = 0
                aux_optimizer.zero_grad()
                for _ in itertools.repeat(None, args.n_minibatches):
                    try:
                        data = next(data_iter_train)
                    except StopIteration:
                        data_iter_train = iter(dataloader_train)
                        data = next(data_iter_train)
                    im_data.data.resize_(data[0].size()).copy_(data[0])
                    im_info.data.resize_(data[1].size()).copy_(data[1])
                    meta_data.data.resize_(data[2].size()).copy_(data[2])
                    gt_boxes.data.resize_(data[3].size()).copy_(data[3])
                    num_boxes.data.resize_(data[4].size()).copy_(data[4])
                    (RCNN_loss_altitude, RCNN_loss_altitude_adv, RCNN_acc_altitude, RCNN_loss_angle, RCNN_loss_angle_adv, RCNN_acc_angle, RCNN_loss_weather, RCNN_loss_weather_adv, RCNN_acc_weather) = fasterRCNN(im_data, im_info, meta_data, gt_boxes, num_boxes, run_partial=True)
                    loss_altitude = RCNN_loss_altitude.mean()
                    loss_altitude_adv = RCNN_loss_altitude_adv.mean()
                    acc_altitude = RCNN_acc_altitude.mean()
                    loss_angle = RCNN_loss_angle.mean()
                    loss_angle_adv = RCNN_loss_angle_adv.mean()
                    acc_angle = RCNN_acc_angle.mean()
                    loss_weather = RCNN_loss_weather.mean()
                    loss_weather_adv = RCNN_loss_weather_adv.mean()
                    acc_weather = RCNN_acc_weather.mean()
                    acc_altitude = (acc_altitude / args.n_minibatches)
                    loss_altitude_adv = (loss_altitude_adv / args.n_minibatches)
                    loss_altitude_aux = (loss_altitude / args.n_minibatches)
                    acc_angle = (acc_angle / args.n_minibatches)
                    loss_angle_adv = (loss_angle_adv / args.n_minibatches)
                    loss_angle_aux = (loss_angle / args.n_minibatches)
                    acc_weather = (acc_weather / args.n_minibatches)
                    loss_weather_adv = (loss_weather_adv / args.n_minibatches)
                    loss_weather_aux = (loss_weather / args.n_minibatches)
                    loss_adv = ((loss_altitude_adv + loss_angle_adv) + loss_weather_adv)
                    loss_aux = ((loss_altitude_aux + loss_angle_aux) + loss_weather_aux)
                    loss_adv_temp += loss_adv.item()
                    loss_aux_temp += loss_aux.item()
                    acc_altitude_temp += acc_altitude.item()
                    acc_angle_temp += acc_angle.item()
                    acc_weather_temp += acc_weather.item()
                    loss_aux.backward(retain_graph=False)
                aux_optimizer.step()
                if (niter == 0):
                    print(('Initialization (Auxiliary): [session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_aux_temp, loss_adv_temp, lr)))
                else:
                    print(('Restarting (Auxiliary): [session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_aux_temp, loss_adv_temp, lr)))
        if args.use_adversarial_loss:
            loss_temp = 0
            loss_util_temp = 0
            loss_adv_temp = 0
            loss_aux_temp = 0
            optimizer.zero_grad()
            for _ in itertools.repeat(None, args.n_minibatches):
                try:
                    data = next(data_iter_train)
                except StopIteration:
                    data_iter_train = iter(dataloader_train)
                    data = next(data_iter_train)
                im_data.data.resize_(data[0].size()).copy_(data[0])
                im_info.data.resize_(data[1].size()).copy_(data[1])
                meta_data.data.resize_(data[2].size()).copy_(data[2])
                gt_boxes.data.resize_(data[3].size()).copy_(data[3])
                num_boxes.data.resize_(data[4].size()).copy_(data[4])
                (rois, cls_prob, bbox_pred, rpn_loss_cls, rpn_loss_box, RCNN_loss_cls, RCNN_loss_bbox, RCNN_loss_altitude, RCNN_loss_altitude_adv, RCNN_acc_altitude, RCNN_loss_angle, RCNN_loss_angle_adv, RCNN_acc_angle, RCNN_loss_weather, RCNN_loss_weather_adv, RCNN_acc_weather, rois_label) = fasterRCNN(im_data, im_info, meta_data, gt_boxes, num_boxes, run_partial=False)
                loss_rpn = (rpn_loss_cls.mean() + rpn_loss_box.mean())
                loss_rcnn = (RCNN_loss_cls.mean() + RCNN_loss_bbox.mean())
                loss_altitude = RCNN_loss_altitude.mean()
                loss_altitude_adv = RCNN_loss_altitude_adv.mean()
                loss_angle = RCNN_loss_angle.mean()
                loss_angle_adv = RCNN_loss_angle_adv.mean()
                loss_weather = RCNN_loss_weather.mean()
                loss_weather_adv = RCNN_loss_weather_adv.mean()
                loss_util = ((loss_rpn + loss_rcnn) / args.n_minibatches)
                loss_altitude_adv = (loss_altitude_adv / args.n_minibatches)
                loss_altitude_aux = (loss_altitude / args.n_minibatches)
                loss_angle_adv = (loss_angle_adv / args.n_minibatches)
                loss_angle_aux = (loss_angle / args.n_minibatches)
                loss_weather_adv = (loss_weather_adv / args.n_minibatches)
                loss_weather_aux = (loss_weather / args.n_minibatches)
                loss_adv = ((loss_altitude_adv + loss_angle_adv) + loss_weather_adv)
                loss_aux = ((loss_altitude_aux + loss_angle_aux) + loss_weather_aux)
                loss = (((loss_util + (args.gamma_altitude * loss_altitude_adv)) + (args.gamma_angle * loss_angle_adv)) + (args.gamma_weather * loss_weather_adv))
                loss_util_temp += loss_util.item()
                loss_adv_temp += loss_adv.item()
                loss_aux_temp += loss_aux.item()
                loss_temp += loss.item()
                loss.backward(retain_graph=False)
            optimizer.step()
            print(('Alternating Training (Utility + Adversarial): [session %d][epoch %2d][iter %4d/%4d] utility loss: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, utility+adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, loss_util_temp, loss_aux_temp, loss_adv_temp, loss_temp, lr)))
        if args.monitor_discriminator:
            while True:
                loss_adv_temp = 0
                loss_aux_temp = 0
                acc_altitude_temp = 0
                acc_angle_temp = 0
                acc_weather_temp = 0
                aux_optimizer.zero_grad()
                for _ in itertools.repeat(None, args.n_minibatches):
                    try:
                        data = next(data_iter_train)
                    except StopIteration:
                        data_iter_train = iter(dataloader_train)
                        data = next(data_iter_train)
                    im_data.data.resize_(data[0].size()).copy_(data[0])
                    im_info.data.resize_(data[1].size()).copy_(data[1])
                    meta_data.data.resize_(data[2].size()).copy_(data[2])
                    gt_boxes.data.resize_(data[3].size()).copy_(data[3])
                    num_boxes.data.resize_(data[4].size()).copy_(data[4])
                    (RCNN_loss_altitude, RCNN_loss_altitude_adv, RCNN_acc_altitude, RCNN_loss_angle, RCNN_loss_angle_adv, RCNN_acc_angle, RCNN_loss_weather, RCNN_loss_weather_adv, RCNN_acc_weather) = fasterRCNN(im_data, im_info, meta_data, gt_boxes, num_boxes, run_partial=True)
                    loss_altitude = RCNN_loss_altitude.mean()
                    loss_altitude_adv = RCNN_loss_altitude_adv.mean()
                    acc_altitude = RCNN_acc_altitude.mean()
                    loss_angle = RCNN_loss_angle.mean()
                    loss_angle_adv = RCNN_loss_angle_adv.mean()
                    acc_angle = RCNN_acc_angle.mean()
                    loss_weather = RCNN_loss_weather.mean()
                    loss_weather_adv = RCNN_loss_weather_adv.mean()
                    acc_weather = RCNN_acc_weather.mean()
                    acc_altitude = (acc_altitude / args.n_minibatches)
                    loss_altitude_adv = (loss_altitude_adv / args.n_minibatches)
                    loss_altitude_aux = (loss_altitude / args.n_minibatches)
                    acc_angle = (acc_angle / args.n_minibatches)
                    loss_angle_adv = (loss_angle_adv / args.n_minibatches)
                    loss_angle_aux = (loss_angle / args.n_minibatches)
                    acc_weather = (acc_weather / args.n_minibatches)
                    loss_weather_adv = (loss_weather_adv / args.n_minibatches)
                    loss_weather_aux = (loss_weather / args.n_minibatches)
                    loss_adv = ((loss_altitude_adv + loss_angle_adv) + loss_weather_adv)
                    loss_aux = ((loss_altitude_aux + loss_angle_aux) + loss_weather_aux)
                    loss_adv_temp += loss_adv.item()
                    loss_aux_temp += loss_aux.item()
                    acc_altitude_temp += acc_altitude.item()
                    acc_angle_temp += acc_angle.item()
                    acc_weather_temp += acc_weather.item()
                    loss_aux.backward(retain_graph=False)
                aux_optimizer.step()
                print(('Alternating Training (Auxiliary): [session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_aux_temp, loss_adv_temp, lr)))
                if ((acc_angle_temp > args.angle_thresh) and (acc_altitude_temp > args.altitude_thresh) and (acc_weather_temp > args.weather_thresh)):
                    break
        if args.eval_display:
            if ((niter % args.disp_interval) == 0):
                end = time.time()
                'Training evaluation'
                loss_temp = 0
                loss_util_temp = 0
                loss_adv_temp = 0
                loss_aux_temp = 0
                acc_altitude_temp = 0
                acc_angle_temp = 0
                acc_weather_temp = 0
                loss_rpn_cls_temp = 0
                loss_rpn_box_temp = 0
                loss_rcnn_cls_temp = 0
                loss_rcnn_box_temp = 0
                fg_cnt = 0
                bg_cnt = 0
                with torch.no_grad():
                    for _ in itertools.repeat(None, args.n_minibatches_eval):
                        try:
                            data = next(data_iter_train)
                        except StopIteration:
                            data_iter_train = iter(dataloader_train)
                            data = next(data_iter_train)
                        im_data.data.resize_(data[0].size()).copy_(data[0])
                        im_info.data.resize_(data[1].size()).copy_(data[1])
                        meta_data.data.resize_(data[2].size()).copy_(data[2])
                        gt_boxes.data.resize_(data[3].size()).copy_(data[3])
                        num_boxes.data.resize_(data[4].size()).copy_(data[4])
                        (rois, cls_prob, bbox_pred, rpn_loss_cls, rpn_loss_box, RCNN_loss_cls, RCNN_loss_bbox, RCNN_loss_altitude, RCNN_loss_altitude_adv, RCNN_acc_altitude, RCNN_loss_angle, RCNN_loss_angle_adv, RCNN_acc_angle, RCNN_loss_weather, RCNN_loss_weather_adv, RCNN_acc_weather, rois_label) = fasterRCNN(im_data, im_info, meta_data, gt_boxes, num_boxes, run_partial=False)
                        loss_rpn = (rpn_loss_cls.mean() + rpn_loss_box.mean())
                        loss_rcnn = (RCNN_loss_cls.mean() + RCNN_loss_bbox.mean())
                        loss_altitude = RCNN_loss_altitude.mean()
                        loss_altitude_adv = RCNN_loss_altitude_adv.mean()
                        acc_altitude = RCNN_acc_altitude.mean()
                        loss_angle = RCNN_loss_angle.mean()
                        loss_angle_adv = RCNN_loss_angle_adv.mean()
                        acc_angle = RCNN_acc_angle.mean()
                        loss_weather = RCNN_loss_weather.mean()
                        loss_weather_adv = RCNN_loss_weather_adv.mean()
                        acc_weather = RCNN_acc_weather.mean()
                        rpn_loss_cls = rpn_loss_cls.mean()
                        rpn_loss_box = rpn_loss_box.mean()
                        RCNN_loss_cls = RCNN_loss_cls.mean()
                        RCNN_loss_bbox = RCNN_loss_bbox.mean()
                        loss_rpn_cls = (rpn_loss_cls / args.n_minibatches_eval)
                        loss_rpn_box = (rpn_loss_box / args.n_minibatches_eval)
                        loss_rcnn_cls = (RCNN_loss_cls / args.n_minibatches_eval)
                        loss_rcnn_box = (RCNN_loss_bbox / args.n_minibatches_eval)
                        loss_util = ((loss_rpn + loss_rcnn) / args.n_minibatches_eval)
                        loss_altitude_adv = (loss_altitude_adv / args.n_minibatches_eval)
                        loss_altitude_aux = (loss_altitude / args.n_minibatches_eval)
                        acc_altitude = (acc_altitude / args.n_minibatches_eval)
                        loss_angle_adv = (loss_angle_adv / args.n_minibatches_eval)
                        loss_angle_aux = (loss_angle / args.n_minibatches_eval)
                        acc_angle = (acc_angle / args.n_minibatches_eval)
                        loss_weather_adv = (loss_weather_adv / args.n_minibatches_eval)
                        loss_weather_aux = (loss_weather / args.n_minibatches_eval)
                        acc_weather = (acc_weather / args.n_minibatches_eval)
                        loss = (((loss_util + (args.gamma_altitude * loss_altitude_adv)) + (args.gamma_angle * loss_angle_adv)) + (args.gamma_weather * loss_weather_adv))
                        loss_adv = ((loss_altitude_adv + loss_angle_adv) + loss_weather_adv)
                        loss_aux = ((loss_altitude_aux + loss_angle_aux) + loss_weather_aux)
                        loss_util_temp += loss_util.item()
                        loss_adv_temp += loss_adv.item()
                        loss_aux_temp += loss_aux.item()
                        loss_temp += loss.item()
                        acc_altitude_temp += acc_altitude.item()
                        acc_angle_temp += acc_angle.item()
                        acc_weather_temp += acc_weather.item()
                        loss_rpn_cls_temp += loss_rpn_cls
                        loss_rpn_box_temp += loss_rpn_box
                        loss_rcnn_cls_temp += loss_rcnn_cls
                        loss_rcnn_box_temp += loss_rcnn_box
                        fg_cnt += torch.sum(rois_label.data.ne(0))
                        bg_cnt += (rois_label.data.numel() - fg_cnt)
                print(('**********DISPLAY TRAINING**********: [session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, utility loss: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, utility+adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_util_temp, loss_aux_temp, loss_adv_temp, loss_temp, lr)))
                print(('\t\t\tfg/bg=(%d/%d), time cost: %f' % (fg_cnt, bg_cnt, (end - start))))
                print(('\t\t\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f' % (loss_rpn_cls_temp, loss_rpn_box_temp, loss_rcnn_cls_temp, loss_rcnn_box_temp)))
                if args.use_tfboard:
                    info = {'training_altitude_acc': acc_altitude_temp, 'training_angle_acc': acc_angle_temp, 'training_weather_acc': acc_weather_temp, 'train_loss': loss_temp, 'train_loss_util': loss_util_temp, 'train_loss_aux': loss_aux_temp, 'train_loss_adv': loss_adv_temp, 'train_loss_rpn_cls': loss_rpn_cls_temp, 'train_loss_rpn_box': loss_rpn_box_temp, 'train_loss_rcnn_cls': loss_rcnn_cls_temp, 'train_loss_rcnn_box': loss_rcnn_box_temp}
                    logger.add_scalars('logs_altitude={}_angle={}_weather=_{}/losses_train'.format(args.gamma_altitude, args.gamma_angle, args.gamma_weather), info, niter)
                train_summary_file.write(('[session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, utility loss: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, utility+adversarial loss: %.4f, lr: %.2e\n' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_util_temp, loss_aux_temp, loss_adv_temp, loss_temp, lr)))
                train_summary_file.write(('\t\t\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f\n' % (loss_rpn_cls_temp, loss_rpn_box_temp, loss_rcnn_cls_temp, loss_rcnn_box_temp)))
                'Validation evaluation'
                loss_temp = 0
                loss_util_temp = 0
                loss_adv_temp = 0
                loss_aux_temp = 0
                acc_altitude_temp = 0
                acc_angle_temp = 0
                acc_weather_temp = 0
                loss_rpn_cls_temp = 0
                loss_rpn_box_temp = 0
                loss_rcnn_cls_temp = 0
                loss_rcnn_box_temp = 0
                fg_cnt = 0
                bg_cnt = 0
                with torch.no_grad():
                    for _ in itertools.repeat(None, args.n_minibatches_eval):
                        try:
                            data = next(data_iter_val)
                        except StopIteration:
                            data_iter_val = iter(dataloader_val)
                            data = next(data_iter_val)
                        im_data.data.resize_(data[0].size()).copy_(data[0])
                        im_info.data.resize_(data[1].size()).copy_(data[1])
                        meta_data.data.resize_(data[2].size()).copy_(data[2])
                        gt_boxes.data.resize_(data[3].size()).copy_(data[3])
                        num_boxes.data.resize_(data[4].size()).copy_(data[4])
                        (rois, cls_prob, bbox_pred, rpn_loss_cls, rpn_loss_box, RCNN_loss_cls, RCNN_loss_bbox, RCNN_loss_altitude, RCNN_loss_altitude_adv, RCNN_acc_altitude, RCNN_loss_angle, RCNN_loss_angle_adv, RCNN_acc_angle, RCNN_loss_weather, RCNN_loss_weather_adv, RCNN_acc_weather, rois_label) = fasterRCNN(im_data, im_info, meta_data, gt_boxes, num_boxes, run_partial=False)
                        loss_rpn = (rpn_loss_cls.mean() + rpn_loss_box.mean())
                        loss_rcnn = (RCNN_loss_cls.mean() + RCNN_loss_bbox.mean())
                        loss_altitude = RCNN_loss_altitude.mean()
                        loss_altitude_adv = RCNN_loss_altitude_adv.mean()
                        acc_altitude = RCNN_acc_altitude.mean()
                        loss_angle = RCNN_loss_angle.mean()
                        loss_angle_adv = RCNN_loss_angle_adv.mean()
                        acc_angle = RCNN_acc_angle.mean()
                        loss_weather = RCNN_loss_weather.mean()
                        loss_weather_adv = RCNN_loss_weather_adv.mean()
                        acc_weather = RCNN_acc_weather.mean()
                        rpn_loss_cls = rpn_loss_cls.mean()
                        rpn_loss_box = rpn_loss_box.mean()
                        RCNN_loss_cls = RCNN_loss_cls.mean()
                        RCNN_loss_bbox = RCNN_loss_bbox.mean()
                        loss_rpn_cls = (rpn_loss_cls / args.n_minibatches_eval)
                        loss_rpn_box = (rpn_loss_box / args.n_minibatches_eval)
                        loss_rcnn_cls = (RCNN_loss_cls / args.n_minibatches_eval)
                        loss_rcnn_box = (RCNN_loss_bbox / args.n_minibatches_eval)
                        loss_util = ((loss_rpn + loss_rcnn) / args.n_minibatches_eval)
                        loss_altitude_adv = (loss_altitude_adv / args.n_minibatches_eval)
                        loss_altitude_aux = (loss_altitude / args.n_minibatches_eval)
                        acc_altitude = (acc_altitude / args.n_minibatches_eval)
                        loss_angle_adv = (loss_angle_adv / args.n_minibatches_eval)
                        loss_angle_aux = (loss_angle / args.n_minibatches_eval)
                        acc_angle = (acc_angle / args.n_minibatches_eval)
                        loss_weather_adv = (loss_weather_adv / args.n_minibatches_eval)
                        loss_weather_aux = (loss_weather / args.n_minibatches_eval)
                        acc_weather = (acc_weather / args.n_minibatches_eval)
                        loss = (((loss_util + (args.gamma_altitude * loss_altitude_adv)) + (args.gamma_angle * loss_angle_adv)) + (args.gamma_weather * loss_weather_adv))
                        loss_adv = ((loss_altitude_adv + loss_angle_adv) + loss_weather_adv)
                        loss_aux = ((loss_altitude_aux + loss_angle_aux) + loss_weather_aux)
                        loss_util_temp += loss_util.item()
                        loss_adv_temp += loss_adv.item()
                        loss_aux_temp += loss_aux.item()
                        loss_temp += loss.item()
                        acc_altitude_temp += acc_altitude.item()
                        acc_angle_temp += acc_angle.item()
                        acc_weather_temp += acc_weather.item()
                        loss_rpn_cls_temp += loss_rpn_cls
                        loss_rpn_box_temp += loss_rpn_box
                        loss_rcnn_cls_temp += loss_rcnn_cls
                        loss_rcnn_box_temp += loss_rcnn_box
                        fg_cnt += torch.sum(rois_label.data.ne(0))
                        bg_cnt += (rois_label.data.numel() - fg_cnt)
                print(('**********DISPLAY VALIDATION**********: [session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather accuracy: %.4f, utility loss: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, utility+adversarial loss: %.4f, lr: %.2e' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_util_temp, loss_aux_temp, loss_adv_temp, loss_temp, lr)))
                print(('\t\t\tfg/bg=(%d/%d), time cost: %f' % (fg_cnt, bg_cnt, (end - start))))
                print(('\t\t\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f' % (loss_rpn_cls_temp, loss_rpn_box_temp, loss_rcnn_cls_temp, loss_rcnn_box_temp)))
                if args.use_tfboard:
                    info = {'val_altitude_acc': acc_altitude_temp, 'val_angle_acc': acc_angle_temp, 'val_weather_acc': acc_weather_temp, 'val_loss': loss_temp, 'val_loss_util': loss_util_temp, 'val_loss_aux': loss_aux_temp, 'val_loss_adv': loss_adv_temp, 'val_loss_rpn_cls': loss_rpn_cls_temp, 'val_loss_rpn_box': loss_rpn_box_temp, 'val_loss_rcnn_cls': loss_rcnn_cls_temp, 'val_loss_rcnn_box': loss_rcnn_box_temp}
                    logger.add_scalars('logs_altitude={}_angle={}_weather={}_{}/losses_val'.format(args.gamma_altitude, args.gamma_angle, args.gamma_weather), info, niter)
                val_summary_file.write(('[session %d][epoch %2d][iter %4d/%4d] altitude accuracy: %.4f, angle accuracy: %.4f, weather_accuracy: %.4f, utility loss: %.4f, auxiliary loss: %.4f, adversarial loss: %.4f, utility+adversarial loss: %.4f, lr: %.2e\n' % (args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch), iters_per_epoch, acc_altitude_temp, acc_angle_temp, acc_weather_temp, loss_util_temp, loss_aux_temp, loss_adv_temp, loss_temp, lr)))
                val_summary_file.write(('\t\t\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f\n' % (loss_rpn_cls_temp, loss_rpn_box_temp, loss_rcnn_cls_temp, loss_rcnn_box_temp)))
                start = time.time()
        if ((niter % args.save_iters) == 0):
            save_name = os.path.join(model_dir, 'faster_rcnn_{}_{}_{}_adv.pth'.format(args.session, ((niter // iters_per_epoch) + 1), (niter % iters_per_epoch)))
            save_checkpoint({'session': args.session, 'epoch': ((niter // iters_per_epoch) + 1), 'model': (fasterRCNN.module.state_dict() if args.mGPUs else fasterRCNN.state_dict()), 'optimizer': optimizer.state_dict(), 'aux_optimizer': aux_optimizer.state_dict(), 'pooling_mode': cfg.POOLING_MODE, 'class_agnostic': args.class_agnostic}, save_name)
            print('save model: {}'.format(save_name))
        niter += 1
    train_summary_file.close()
    val_summary_file.close()
    if args.use_tfboard:
        logger.close()

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
    if:    elif:
         ...  =  ... .optim.SGD

idx = 83:------------------- similar code ------------------ index = 47, score = 3.0 
def main():
    if (not torch.cuda.is_available()):
        sys.exit(1)
    start_t = time.time()
    cudnn.benchmark = True
    cudnn.enabled = True
    logging.info('args = %s', args)
    model = MobileNetV1()
    logging.info(model)
    model = nn.DataParallel(model).cuda()
    criterion = nn.CrossEntropyLoss()
    criterion = criterion.cuda()
    criterion_smooth = CrossEntropyLabelSmooth(CLASSES, args.label_smooth)
    criterion_smooth = criterion_smooth.cuda()
    all_parameters = model.parameters()
    weight_parameters = []
    for (pname, p) in model.named_parameters():
        if (('fc' in pname) or ('conv1' in pname) or ('pwconv' in pname)):
            weight_parameters.append(p)
    weight_parameters_id = list(map(id, weight_parameters))
    other_parameters = list(filter((lambda p: (id(p) not in weight_parameters_id)), all_parameters))
    optimizer = torch.optim.SGD([{'params': other_parameters}, {'params': weight_parameters, 'weight_decay': args.weight_decay}], args.learning_rate, momentum=args.momentum)
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, (lambda step: (1.0 - (step / args.epochs))), last_epoch=(- 1))
    start_epoch = 0
    best_top1_acc = 0
    checkpoint_tar = os.path.join(args.save, 'checkpoint.pth.tar')
    if os.path.exists(checkpoint_tar):
        logging.info('loading checkpoint {} ..........'.format(checkpoint_tar))
        checkpoint = torch.load(checkpoint_tar)
        start_epoch = checkpoint['epoch']
        best_top1_acc = checkpoint['best_top1_acc']
        model.load_state_dict(checkpoint['state_dict'])
        logging.info('loaded checkpoint {} epoch = {}'.format(checkpoint_tar, checkpoint['epoch']))
    for epoch in range(start_epoch):
        scheduler.step()
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    crop_scale = 0.08
    lighting_param = 0.1
    train_transforms = transforms.Compose([transforms.RandomResizedCrop(224, scale=(crop_scale, 1.0)), Lighting(lighting_param), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize])
    train_dataset = datasets.ImageFolder(traindir, transform=train_transforms)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    epoch = start_epoch
    while (epoch < args.epochs):
        (train_obj, train_top1_acc, train_top5_acc, epoch) = train(epoch, train_loader, model, criterion_smooth, optimizer, scheduler)
        (valid_obj, valid_top1_acc, valid_top5_acc) = validate(epoch, val_loader, model, criterion, args)
        is_best = False
        if (valid_top1_acc > best_top1_acc):
            best_top1_acc = valid_top1_acc
            is_best = True
        save_checkpoint({'epoch': epoch, 'state_dict': model.state_dict(), 'best_top1_acc': best_top1_acc, 'optimizer': optimizer.state_dict()}, is_best, args.save)
        epoch += 1
    training_time = ((time.time() - start_t) / 36000)
    print('total training time = {} hours'.format(training_time))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 84:------------------- similar code ------------------ index = 48, score = 3.0 
def main(logger, args):
    if (not torch.cuda.is_available()):
        raise Exception('need gpu to train network!')
    if (args.seed is not None):
        random.seed(args.seed)
        torch.cuda.manual_seed_all(args.seed)
        cudnn.deterministic = True
    gpus = torch.cuda.device_count()
    logger.info(f'use {gpus} gpus')
    logger.info(f'args: {args}')
    cudnn.benchmark = True
    cudnn.enabled = True
    start_time = time.time()
    logger.info('start loading data')
    train_loader = DataLoader(Config.train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)
    val_loader = DataLoader(Config.val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)
    logger.info('finish loading data')
    logger.info(f"creating model '{args.network}'")
    model = resnet50(**{'pretrained': args.pretrained, 'num_classes': args.num_classes})
    flops_input = torch.randn(1, 3, args.input_image_size, args.input_image_size)
    (flops, params) = profile(model, inputs=(flops_input,))
    (flops, params) = clever_format([flops, params], '%.3f')
    logger.info(f"model: '{args.network}', flops: {flops}, params: {params}")
    for (name, param) in model.named_parameters():
        logger.info(f'{name},{param.requires_grad}')
    model = model.cuda()
    criterion = nn.CrossEntropyLoss().cuda()
    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.milestones, gamma=0.1)
    if args.apex:
        (model, optimizer) = amp.initialize(model, optimizer, opt_level='O1')
    model = nn.DataParallel(model)
    if args.evaluate:
        if (not os.path.isfile(args.evaluate)):
            raise Exception(f'{args.resume} is not a file, please check it again')
        logger.info('start only evaluating')
        logger.info(f'start resuming model from {args.evaluate}')
        checkpoint = torch.load(args.evaluate, map_location=torch.device('cpu'))
        model.load_state_dict(checkpoint['model_state_dict'])
        (acc1, acc5, throughput) = validate(val_loader, model, args)
        logger.info(f"epoch {checkpoint['epoch']:0>3d}, top1 acc: {acc1:.2f}%, top5 acc: {acc5:.2f}%, throughput: {throughput:.2f}sample/s")
        return
    start_epoch = 1
    if os.path.exists(args.resume):
        logger.info(f'start resuming model from {args.resume}')
        checkpoint = torch.load(args.resume, map_location=torch.device('cpu'))
        start_epoch += checkpoint['epoch']
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        logger.info(f"finish resuming model from {args.resume}, epoch {checkpoint['epoch']}, loss: {checkpoint['loss']:3f}, lr: {checkpoint['lr']:.6f}, top1_acc: {checkpoint['acc1']}%")
    if (not os.path.exists(args.checkpoints)):
        os.makedirs(args.checkpoints)
    logger.info('start training')
    for epoch in range(start_epoch, (args.epochs + 1)):
        (acc1, acc5, losses) = train(train_loader, model, criterion, optimizer, scheduler, epoch, logger, args)
        logger.info(f'train: epoch {epoch:0>3d}, top1 acc: {acc1:.2f}%, top5 acc: {acc5:.2f}%, losses: {losses:.2f}')
        (acc1, acc5, throughput) = validate(val_loader, model, args)
        logger.info(f'val: epoch {epoch:0>3d}, top1 acc: {acc1:.2f}%, top5 acc: {acc5:.2f}%, throughput: {throughput:.2f}sample/s')
        torch.save({'epoch': epoch, 'acc1': acc1, 'loss': losses, 'lr': scheduler.get_lr()[0], 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict()}, os.path.join(args.checkpoints, 'latest.pth'))
        if (epoch == args.epochs):
            torch.save(model.module.state_dict(), os.path.join(args.checkpoints, '{}-epoch{}-acc{}.pth'.format(args.network, epoch, acc1)))
    training_time = ((time.time() - start_time) / 3600)
    logger.info(f'finish training, total training time: {training_time:.2f} hours')

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 85:------------------- similar code ------------------ index = 49, score = 3.0 
def main_worker(gpu, ngpus_per_node, args):
    global best_acc1
    args.gpu = gpu
    if (args.gpu is not None):
        print('Use GPU: {} for training'.format(args.gpu))
    if args.distributed:
        if ((args.dist_url == 'env://') and (args.rank == (- 1))):
            args.rank = int(os.environ['RANK'])
        if args.multiprocessing_distributed:
            args.rank = ((args.rank * ngpus_per_node) + gpu)
        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank)
    if args.pretrained:
        print("=> using pre-trained model '{}'".format(args.arch))
        model = models.__dict__[args.arch](pretrained=True)
    else:
        print("=> creating model '{}'".format(args.arch))
        model = models.__dict__[args.arch]()
    if args.distributed:
        if (args.gpu is not None):
            torch.cuda.set_device(args.gpu)
            model.cuda(args.gpu)
            args.batch_size = int((args.batch_size / ngpus_per_node))
            args.workers = int((((args.workers + ngpus_per_node) - 1) / ngpus_per_node))
            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
        else:
            model.cuda()
            model = torch.nn.parallel.DistributedDataParallel(model)
    elif (args.gpu is not None):
        torch.cuda.set_device(args.gpu)
        model = model.cuda(args.gpu)
    elif (args.arch.startswith('alexnet') or args.arch.startswith('vgg')):
        model.features = torch.nn.DataParallel(model.features)
        model.cuda()
    else:
        model = torch.nn.DataParallel(model).cuda()
    criterion = nn.CrossEntropyLoss().cuda(args.gpu)
    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    if args.resume:
        if os.path.isfile(args.resume):
            print("=> loading checkpoint '{}'".format(args.resume))
            if (args.gpu is None):
                checkpoint = torch.load(args.resume)
            else:
                loc = 'cuda:{}'.format(args.gpu)
                checkpoint = torch.load(args.resume, map_location=loc)
            args.start_epoch = checkpoint['epoch']
            best_acc1 = checkpoint['best_acc1']
            if (args.gpu is not None):
                best_acc1 = best_acc1.to(args.gpu)
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            print("=> loaded checkpoint '{}' (epoch {})".format(args.resume, checkpoint['epoch']))
        else:
            print("=> no checkpoint found at '{}'".format(args.resume))
    cudnn.benchmark = True
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]))
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    else:
        train_sampler = None
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    if args.evaluate:
        validate(val_loader, model, criterion, args)
        return
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        adjust_learning_rate(optimizer, epoch, args)
        train(train_loader, model, criterion, optimizer, epoch, args)
        acc1 = validate(val_loader, model, criterion, args)
        is_best = (acc1 > best_acc1)
        best_acc1 = max(acc1, best_acc1)
        if ((not args.multiprocessing_distributed) or (args.multiprocessing_distributed and ((args.rank % ngpus_per_node) == 0))):
            save_checkpoint({'epoch': (epoch + 1), 'arch': args.arch, 'state_dict': model.state_dict(), 'best_acc1': best_acc1, 'optimizer': optimizer.state_dict()}, is_best, filename=((('model/' + args.modelname) + '_') + str(epoch)))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 86:------------------- similar code ------------------ index = 99, score = 3.0 
def main_worker(gpu, ngpus_per_node, argss):
    global args, best_acc1
    (args, best_acc1) = (argss, 0)
    if args.distributed:
        if ((args.dist_url == 'env://') and (args.rank == (- 1))):
            args.rank = int(os.environ['RANK'])
        if args.multiprocessing_distributed:
            args.rank = ((args.rank * ngpus_per_node) + gpu)
        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank)
    model = san(args.sa_type, args.layers, args.kernels, args.classes)
    criterion = nn.CrossEntropyLoss(ignore_index=args.ignore_label)
    optimizer = torch.optim.SGD(model.parameters(), lr=args.base_lr, momentum=args.momentum, weight_decay=args.weight_decay)
    if (args.scheduler == 'step'):
        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=args.step_epochs, gamma=0.1)
    elif (args.scheduler == 'cosine'):
        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    if main_process():
        global logger, writer
        logger = get_logger()
        writer = SummaryWriter(args.save_path)
        logger.info(args)
        logger.info('=> creating model ...')
        logger.info('Classes: {}'.format(args.classes))
        logger.info(model)
    if args.distributed:
        torch.cuda.set_device(gpu)
        args.batch_size = int((args.batch_size / ngpus_per_node))
        args.batch_size_val = int((args.batch_size_val / ngpus_per_node))
        args.workers = int((((args.workers + ngpus_per_node) - 1) / ngpus_per_node))
        model = torch.nn.parallel.DistributedDataParallel(model.cuda(), device_ids=[gpu])
    else:
        model = torch.nn.DataParallel(model.cuda())
    if args.weight:
        if os.path.isfile(args.weight):
            if main_process():
                logger.info("=> loading weight '{}'".format(args.weight))
            checkpoint = torch.load(args.weight)
            model.load_state_dict(checkpoint['state_dict'])
            if main_process():
                logger.info("=> loaded weight '{}'".format(args.weight))
        elif main_process():
            logger.info("=> no weight found at '{}'".format(args.weight))
    if args.resume:
        if os.path.isfile(args.resume):
            if main_process():
                logger.info("=> loading checkpoint '{}'".format(args.resume))
            checkpoint = torch.load(args.resume, map_location=(lambda storage, loc: storage.cuda(gpu)))
            args.start_epoch = checkpoint['epoch']
            best_acc1 = checkpoint['top1_val']
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            scheduler.load_state_dict(checkpoint['scheduler'])
            if main_process():
                logger.info("=> loaded checkpoint '{}' (epoch {})".format(args.resume, checkpoint['epoch']))
        elif main_process():
            logger.info("=> no checkpoint found at '{}'".format(args.resume))
    (mean, std) = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    train_transform = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean, std)])
    train_set = torchvision.datasets.ImageFolder(os.path.join(args.data_root, 'train'), train_transform)
    val_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean, std)])
    val_set = torchvision.datasets.ImageFolder(os.path.join(args.data_root, 'val'), val_transform)
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_set)
        val_sampler = torch.utils.data.distributed.DistributedSampler(val_set)
    else:
        train_sampler = None
        val_sampler = None
    train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler)
    val_loader = torch.utils.data.DataLoader(val_set, batch_size=args.batch_size_val, shuffle=False, num_workers=args.workers, pin_memory=True, sampler=val_sampler)
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        (loss_train, mIoU_train, mAcc_train, allAcc_train, top1_train, top5_train) = train(train_loader, model, criterion, optimizer, epoch)
        (loss_val, mIoU_val, mAcc_val, allAcc_val, top1_val, top5_val) = validate(val_loader, model, criterion)
        scheduler.step()
        epoch_log = (epoch + 1)
        if main_process():
            writer.add_scalar('loss_train', loss_train, epoch_log)
            writer.add_scalar('mIoU_train', mIoU_train, epoch_log)
            writer.add_scalar('mAcc_train', mAcc_train, epoch_log)
            writer.add_scalar('allAcc_train', allAcc_train, epoch_log)
            writer.add_scalar('top1_train', top1_train, epoch_log)
            writer.add_scalar('top5_train', top5_train, epoch_log)
            writer.add_scalar('loss_val', loss_val, epoch_log)
            writer.add_scalar('mIoU_val', mIoU_val, epoch_log)
            writer.add_scalar('mAcc_val', mAcc_val, epoch_log)
            writer.add_scalar('allAcc_val', allAcc_val, epoch_log)
            writer.add_scalar('top1_val', top1_val, epoch_log)
            writer.add_scalar('top5_val', top5_val, epoch_log)
        if (((epoch_log % args.save_freq) == 0) and main_process()):
            filename = (((args.save_path + '/train_epoch_') + str(epoch_log)) + '.pth')
            logger.info(('Saving checkpoint to: ' + filename))
            torch.save({'epoch': epoch_log, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'scheduler': scheduler.state_dict(), 'top1_val': top1_val, 'top5_val': top5_val}, filename)
            if (top1_val > best_acc1):
                best_acc1 = top1_val
                shutil.copyfile(filename, (args.save_path + '/model_best.pth'))
            if ((epoch_log / args.save_freq) > 2):
                deletename = (((args.save_path + '/train_epoch_') + str((epoch_log - (args.save_freq * 2)))) + '.pth')
                os.remove(deletename)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 87:------------------- similar code ------------------ index = 51, score = 3.0 
def main():
    global best_prec1, args
    args.distributed = False
    if ('WORLD_SIZE' in os.environ):
        args.distributed = (int(os.environ['WORLD_SIZE']) > 1)
    args.gpu = 0
    args.world_size = 1
    if args.distributed:
        args.gpu = (args.local_rank % torch.cuda.device_count())
        torch.cuda.set_device(args.gpu)
        torch.distributed.init_process_group(backend='nccl', init_method='env://')
        args.world_size = torch.distributed.get_world_size()
    if args.fp16:
        assert torch.backends.cudnn.enabled, 'fp16 mode requires cudnn backend to be enabled.'
    if args.pretrained:
        print("=> using pre-trained model '{}'".format(args.arch))
        model = models.__dict__[args.arch](pretrained=True)
    else:
        print("=> creating model '{}'".format(args.arch))
        model = models.__dict__[args.arch]()
    if args.sync_bn:
        import apex
        print('using apex synced BN')
        model = apex.parallel.convert_syncbn_model(model)
    model = model.cuda()
    if args.fp16:
        model = network_to_half(model)
    if args.distributed:
        global reducer
        reducer = Reducer(model)
    global model_params, master_params
    if args.fp16:
        (model_params, master_params) = prep_param_lists(model)
    else:
        master_params = list(model.parameters())
    criterion = nn.CrossEntropyLoss().cuda()
    args.lr = ((args.lr * float((args.batch_size * args.world_size))) / 256.0)
    optimizer = torch.optim.SGD(master_params, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    if args.resume:

        def resume():
            if os.path.isfile(args.resume):
                print("=> loading checkpoint '{}'".format(args.resume))
                checkpoint = torch.load(args.resume, map_location=(lambda storage, loc: storage.cuda(args.gpu)))
                args.start_epoch = checkpoint['epoch']
                best_prec1 = checkpoint['best_prec1']
                model.load_state_dict(checkpoint['state_dict'])
                if args.fp16:
                    saved_master_params = checkpoint['master_params']
                    for (master, saved) in zip(master_params, saved_master_params):
                        master.data.copy_(saved.data)
                optimizer.load_state_dict(checkpoint['optimizer'])
                print("=> loaded checkpoint '{}' (epoch {})".format(args.resume, checkpoint['epoch']))
            else:
                print("=> no checkpoint found at '{}'".format(args.resume))
        resume()
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    if (args.arch == 'inception_v3'):
        crop_size = 299
        val_size = 320
    else:
        crop_size = 224
        val_size = 256
    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(crop_size), transforms.RandomHorizontalFlip()]))
    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(val_size), transforms.CenterCrop(crop_size)]))
    train_sampler = None
    val_sampler = None
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True, sampler=val_sampler, collate_fn=fast_collate)
    if args.evaluate:
        validate(val_loader, model, criterion)
        return
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        train(train_loader, model, criterion, optimizer, epoch)
        if args.prof:
            break
        prec1 = validate(val_loader, model, criterion)
        if (args.local_rank == 0):
            is_best = (prec1 > best_prec1)
            best_prec1 = max(prec1, best_prec1)

            def create_and_save_checkpoint():
                checkpoint_dict = {'epoch': (epoch + 1), 'arch': args.arch, 'state_dict': model.state_dict(), 'best_prec1': best_prec1, 'optimizer': optimizer.state_dict()}
                if args.fp16:
                    checkpoint_dict['master_params'] = master_params
                save_checkpoint(checkpoint_dict, is_best)
            create_and_save_checkpoint()

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 88:------------------- similar code ------------------ index = 53, score = 3.0 
def main():
    'Train a simple Hybrid Resnet Scattering + CNN model on CIFAR.\n\n    '
    parser = argparse.ArgumentParser(description='CIFAR scattering  + hybrid examples')
    parser.add_argument('--mode', type=str, default='scattering', choices=['scattering', 'standard'], help='network_type')
    parser.add_argument('--num_samples', type=int, default=50, help='samples per class')
    parser.add_argument('--learning_schedule_multi', type=int, default=10, help='samples per class')
    parser.add_argument('--seed', type=int, default=0, help='seed for dataset subselection')
    parser.add_argument('--width', type=int, default=2, help='width factor for resnet')
    args = parser.parse_args()
    use_cuda = torch.cuda.is_available()
    device = torch.device(('cuda' if use_cuda else 'cpu'))
    if (args.mode == 'scattering'):
        scattering = Scattering2D(J=2, shape=(32, 32))
        K = (81 * 3)
        model = Scattering2dResNet(K, args.width).to(device)
        scattering = scattering.to(device)
    else:
        model = Scattering2dResNet(8, args.width, standard=True).to(device)
        scattering = Identity()
    num_workers = 4
    if use_cuda:
        pin_memory = True
    else:
        pin_memory = False
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    cifar_data = datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=True, transform=transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 4), transforms.ToTensor(), normalize]), download=True)
    prng = RandomState(args.seed)
    random_permute = prng.permutation(np.arange(0, 5000))[0:args.num_samples]
    indx = np.concatenate([np.where((np.array(cifar_data.targets) == classe))[0][random_permute] for classe in range(0, 10)])
    (cifar_data.data, cifar_data.targets) = (cifar_data.data[indx], list(np.array(cifar_data.targets)[indx]))
    train_loader = torch.utils.data.DataLoader(cifar_data, batch_size=32, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)
    test_loader = torch.utils.data.DataLoader(datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=False, transform=transforms.Compose([transforms.ToTensor(), normalize])), batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)
    lr = 0.1
    M = args.learning_schedule_multi
    drops = [(60 * M), (120 * M), (160 * M)]
    for epoch in range(0, (200 * M)):
        if ((epoch in drops) or (epoch == 0)):
            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)
            lr *= 0.2
        train(model, device, train_loader, optimizer, (epoch + 1), scattering)
        if ((epoch % 10) == 0):
            test(model, device, test_loader, scattering)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
    for  ...  in:
        if:
             ...  =  ... .optim.SGD

idx = 89:------------------- similar code ------------------ index = 57, score = 3.0 
if (__name__ == '__main__'):
    model = models.DDCNet(num_classes=31)
    correct = 0
    print(model)
    if cuda:
        model.cuda()
    model = load_pretrain(model)
    optimizer = torch.optim.SGD([{'params': model.sharedNet.parameters()}, {'params': model.cls_fc.parameters(), 'lr': lr[1]}], lr=lr[0], momentum=momentum, weight_decay=l2_decay)
    for epoch in range(1, (epochs + 1)):
        for (index, param_group) in enumerate(optimizer.param_groups):
            param_group['lr'] = (lr[index] / math.pow((1 + ((10 * (epoch - 1)) / epochs)), 0.75))
        train(epoch, model, optimizer)
        t_correct = test(model)
        if (t_correct > correct):
            correct = t_correct
        print('source: {} to target: {} max correct: {} max accuracy{: .2f}%\n'.format(source_name, target_name, correct, ((100.0 * correct) / len_target_dataset)))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
     ...  =  ... .optim.SGD

idx = 90:------------------- similar code ------------------ index = 58, score = 3.0 
if (__name__ == '__main__'):
    'Train a simple Hybrid Resnet Scattering + CNN model on CIFAR.\n\n        scattering 1st order can also be set by the mode\n        Scattering features are normalized by batch normalization.\n        The model achieves around 88% testing accuracy after 10 epochs.\n\n        scatter 1st order +\n        scatter 2nd order + linear achieves 70.5% in 90 epochs\n\n        scatter + cnn achieves 88% in 15 epochs\n\n    '
    parser = argparse.ArgumentParser(description='CIFAR scattering  + hybrid examples')
    parser.add_argument('--mode', type=int, default=1, help='scattering 1st or 2nd order')
    parser.add_argument('--width', type=int, default=2, help='width factor for resnet')
    args = parser.parse_args()
    use_cuda = torch.cuda.is_available()
    device = torch.device(('cuda' if use_cuda else 'cpu'))
    if (args.mode == 1):
        scattering = Scattering2D(J=2, shape=(32, 32), max_order=1)
        K = (17 * 3)
    else:
        scattering = Scattering2D(J=2, shape=(32, 32))
        K = (81 * 3)
    scattering = scattering.to(device)
    model = Scattering2dResNet(K, args.width).to(device)
    num_workers = 4
    if use_cuda:
        pin_memory = True
    else:
        pin_memory = False
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    train_loader = torch.utils.data.DataLoader(datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=True, transform=transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 4), transforms.ToTensor(), normalize]), download=True), batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)
    test_loader = torch.utils.data.DataLoader(datasets.CIFAR10(root=scattering_datasets.get_dataset_dir('CIFAR'), train=False, transform=transforms.Compose([transforms.ToTensor(), normalize])), batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)
    lr = 0.1
    for epoch in range(0, 90):
        if ((epoch % 20) == 0):
            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)
            lr *= 0.2
        train(model, device, train_loader, optimizer, (epoch + 1), scattering)
        test(model, device, test_loader, scattering)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
    for  ...  in:
        if:
             ...  =  ... .optim.SGD

idx = 91:------------------- similar code ------------------ index = 59, score = 3.0 
def main():
    global args, best_prec1
    args = parser.parse_args()
    model_weight = torch.load(FLAGS['pretrain_model'])
    model = resnet50(model_weight, num_classes=FLAGS['class_num'])
    model.eval()
    summary(model, torch.zeros((1, 3, 224, 224)))
    model = model.cuda()
    cudnn.benchmark = True
    model_teacher = ori_resnet50(num_classes=FLAGS['class_num'])
    model_teacher = torch.nn.DataParallel(model_teacher.cuda(), device_ids=range(torch.cuda.device_count()))
    model_teacher.load_state_dict(model_weight)
    criterion = nn.CrossEntropyLoss().cuda()
    optimizer = torch.optim.SGD(filter((lambda p: p.requires_grad), model.parameters()), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    data_transforms = {'train': transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]), 'val': transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}
    data_dir = FLAGS['data_base']
    print('| Preparing model...')
    dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}
    train_loader = torch.utils.data.DataLoader(dsets['train'], batch_size=FLAGS['batch_size'], shuffle=True, num_workers=8, pin_memory=True)
    val_loader = torch.utils.data.DataLoader(dsets['val'], batch_size=(4 * FLAGS['batch_size']), shuffle=False, num_workers=8, pin_memory=True)
    print('data_loader_success!')
    validate(val_loader, model, criterion)
    for epoch in range(args.start_epoch, args.epochs):
        train(train_loader, model_teacher, model, criterion, optimizer, epoch)
        prec1 = validate(val_loader, model, criterion)
        best_prec1 = max(prec1, best_prec1)
        folder_path = 'checkpoint/fine_tune'
        if (not os.path.exists(folder_path)):
            os.makedirs(folder_path)
        torch.save(model.state_dict(), (folder_path + '/model.pth'))
        print(('best acc is %.3f' % best_prec1))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 92:------------------- similar code ------------------ index = 60, score = 3.0 
def initialize_optimizer_and_scheduler(self):
    current_momentum = 0.99
    min_momentum = 0.9
    if (self.epoch > 800):
        current_momentum = (current_momentum - (((current_momentum - min_momentum) / 200) * (self.epoch - 800)))
    self.print_to_log_file('current momentum', current_momentum)
    assert (self.network is not None), 'self.initialize_network must be called first'
    if (self.optimizer is None):
        self.optimizer = torch.optim.SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay, momentum=0.99, nesterov=True)
    else:
        self.optimizer.param_groups[0]['momentum'] = current_momentum
    self.lr_scheduler = None

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
    if:
 =  ... .optim.SGD

idx = 93:------------------- similar code ------------------ index = 61, score = 3.0 
if (__name__ == '__main__'):
    parser = argparse.ArgumentParser(description='Trains ResNeXt on CIFAR', formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('data_path', type=str, help='Root for the Cifar dataset.')
    parser.add_argument('dataset', type=str, choices=['cifar10', 'cifar100'], help='Choose between Cifar10/100.')
    parser.add_argument('--epochs', '-e', type=int, default=300, help='Number of epochs to train.')
    parser.add_argument('--batch_size', '-b', type=int, default=128, help='Batch size.')
    parser.add_argument('--learning_rate', '-lr', type=float, default=0.1, help='The Learning Rate.')
    parser.add_argument('--momentum', '-m', type=float, default=0.9, help='Momentum.')
    parser.add_argument('--decay', '-d', type=float, default=0.0005, help='Weight decay (L2 penalty).')
    parser.add_argument('--test_bs', type=int, default=10)
    parser.add_argument('--schedule', type=int, nargs='+', default=[150, 225], help='Decrease learning rate at these epochs.')
    parser.add_argument('--gamma', type=float, default=0.1, help='LR is multiplied by gamma on schedule.')
    parser.add_argument('--save', '-s', type=str, default='./', help='Folder to save checkpoints.')
    parser.add_argument('--load', '-l', type=str, help='Checkpoint path to resume / test.')
    parser.add_argument('--test', '-t', action='store_true', help='Test only flag.')
    parser.add_argument('--depth', type=int, default=29, help='Model depth.')
    parser.add_argument('--cardinality', type=int, default=8, help='Model cardinality (group).')
    parser.add_argument('--base_width', type=int, default=64, help='Number of channels in each group.')
    parser.add_argument('--widen_factor', type=int, default=4, help='Widen factor. 4 -> 64, 8 -> 128, ...')
    parser.add_argument('--ngpu', type=int, default=1, help='0 = CPU.')
    parser.add_argument('--prefetch', type=int, default=2, help='Pre-fetching threads.')
    parser.add_argument('--log', type=str, default='./', help='Log folder.')
    args = parser.parse_args()
    if (not os.path.isdir(args.log)):
        os.makedirs(args.log)
    log = open(os.path.join(args.log, 'log.txt'), 'w')
    state = {k: v for (k, v) in args._get_kwargs()}
    log.write((json.dumps(state) + '\n'))
    args.epochs = ((args.epochs * 128) // args.batch_size)
    args.schedule = [((x * 128) // args.batch_size) for x in args.schedule]
    if (not os.path.isdir(args.data_path)):
        os.makedirs(args.data_path)
    mean = [(x / 255) for x in [125.3, 123.0, 113.9]]
    std = [(x / 255) for x in [63.0, 62.1, 66.7]]
    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(), transforms.Normalize(mean, std)])
    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])
    if (args.dataset == 'cifar10'):
        train_data = dset.CIFAR10(args.data_path, train=True, transform=train_transform, download=True)
        test_data = dset.CIFAR10(args.data_path, train=False, transform=test_transform, download=True)
        nlabels = 10
    else:
        train_data = dset.CIFAR100(args.data_path, train=True, transform=train_transform, download=True)
        test_data = dset.CIFAR100(args.data_path, train=False, transform=test_transform, download=True)
        nlabels = 100
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=args.prefetch, pin_memory=True)
    test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=False, num_workers=args.prefetch, pin_memory=True)
    if (not os.path.isdir(args.save)):
        os.makedirs(args.save)
    net = CifarResNeXt(args.cardinality, args.depth, nlabels, args.base_width, args.widen_factor)
    print(net)
    if (args.ngpu > 1):
        net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))
    if (args.ngpu > 0):
        net.cuda()
    optimizer = torch.optim.SGD(net.parameters(), state['learning_rate'], momentum=state['momentum'], weight_decay=state['decay'], nesterov=True)

    def train():
        net.train()
        loss_avg = 0.0
        for (batch_idx, (data, target)) in enumerate(train_loader):
            (data, target) = (torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda()))
            output = net(data)
            optimizer.zero_grad()
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()
            loss_avg = ((loss_avg * 0.2) + (float(loss) * 0.8))
        state['train_loss'] = loss_avg

    def test():
        net.eval()
        loss_avg = 0.0
        correct = 0
        for (batch_idx, (data, target)) in enumerate(test_loader):
            (data, target) = (torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda()))
            output = net(data)
            loss = F.cross_entropy(output, target)
            pred = output.data.max(1)[1]
            correct += float(pred.eq(target.data).sum())
            loss_avg += float(loss)
        state['test_loss'] = (loss_avg / len(test_loader))
        state['test_accuracy'] = (correct / len(test_loader.dataset))
    best_accuracy = 0.0
    for epoch in range(args.epochs):
        if (epoch in args.schedule):
            state['learning_rate'] *= args.gamma
            for param_group in optimizer.param_groups:
                param_group['lr'] = state['learning_rate']
        state['epoch'] = epoch
        train()
        test()
        if (state['test_accuracy'] > best_accuracy):
            best_accuracy = state['test_accuracy']
            torch.save(net.state_dict(), os.path.join(args.save, 'model.pytorch'))
        log.write(('%s\n' % json.dumps(state)))
        log.flush()
        print(state)
        print(('Best accuracy: %f' % best_accuracy))
    log.close()

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
if:
     ...  =  ... .optim.SGD


idx = 94:------------------- similar code ------------------ index = 63, score = 3.0 
def main_worker(gpu, ngpus_per_node, args):
    global best_acc1
    args.gpu = gpu
    if (args.gpu is not None):
        print('Use GPU: {} for training'.format(args.gpu))
    if args.distributed:
        if ((args.dist_url == 'env://') and (args.rank == (- 1))):
            args.rank = int(os.environ['RANK'])
        if args.multiprocessing_distributed:
            args.rank = ((args.rank * ngpus_per_node) + gpu)
        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank)
    if args.pretrained:
        print("=> using pre-trained model '{}'".format(args.arch))
        model = models.__dict__[args.arch](pretrained=True)
    else:
        print("=> creating model '{}'".format(args.arch))
        model = models.__dict__[args.arch]()
    if args.distributed:
        if (args.gpu is not None):
            torch.cuda.set_device(args.gpu)
            model.cuda(args.gpu)
            args.batch_size = int((args.batch_size / ngpus_per_node))
            args.workers = int((((args.workers + ngpus_per_node) - 1) / ngpus_per_node))
            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
        else:
            model.cuda()
            model = torch.nn.parallel.DistributedDataParallel(model)
    elif (args.gpu is not None):
        torch.cuda.set_device(args.gpu)
        model = model.cuda(args.gpu)
    elif (args.arch.startswith('alexnet') or args.arch.startswith('vgg')):
        model.features = torch.nn.DataParallel(model.features)
        model.cuda()
    else:
        model = torch.nn.DataParallel(model).cuda()
    criterion = nn.CrossEntropyLoss().cuda(args.gpu)
    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    if args.resume:
        if os.path.isfile(args.resume):
            print("=> loading checkpoint '{}'".format(args.resume))
            if (args.gpu is None):
                checkpoint = torch.load(args.resume)
            else:
                loc = 'cuda:{}'.format(args.gpu)
                checkpoint = torch.load(args.resume, map_location=loc)
            args.start_epoch = checkpoint['epoch']
            best_acc1 = checkpoint['best_acc1']
            if (args.gpu is not None):
                best_acc1 = best_acc1.to(args.gpu)
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            print("=> loaded checkpoint '{}' (epoch {})".format(args.resume, checkpoint['epoch']))
        else:
            print("=> no checkpoint found at '{}'".format(args.resume))
    cudnn.benchmark = True
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]))
    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    else:
        train_sampler = None
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler)
    val_loader = torch.utils.data.DataLoader(datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)
    if args.evaluate:
        validate(val_loader, model, criterion, args)
        return
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        adjust_learning_rate(optimizer, epoch, args)
        train(train_loader, model, criterion, optimizer, epoch, args)
        acc1 = validate(val_loader, model, criterion, args)
        is_best = (acc1 > best_acc1)
        best_acc1 = max(acc1, best_acc1)
        if ((not args.multiprocessing_distributed) or (args.multiprocessing_distributed and ((args.rank % ngpus_per_node) == 0))):
            save_checkpoint({'epoch': (epoch + 1), 'arch': args.arch, 'state_dict': model.state_dict(), 'best_acc1': best_acc1, 'optimizer': optimizer.state_dict()}, is_best)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD

idx = 95:------------------- similar code ------------------ index = 64, score = 3.0 
def __init__(self, args, params):
    super().__init__(args)
    self._optimizer = torch.optim.SGD(params, **self.optimizer_config)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
 =  ... .optim.SGD

idx = 96:------------------- similar code ------------------ index = 66, score = 3.0 
def setUp(self):
    torch.manual_seed(2)
    self.model = nn.Sequential(nn.Linear(STATE_DIM, ACTIONS))

    def optimizer(params):
        return torch.optim.SGD(params, lr=0.1)
    self.q = QNetwork(self.model, optimizer)

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ( ... ):
    def  ... ( ... ):
        return  ... .optim.SGD

idx = 97:------------------- similar code ------------------ index = 67, score = 3.0 
def train(self, train_data, step, loss, dropout=0.5):
    epochs = (self.initial_steps if (step == 0) else self.later_steps)
    init_lr = (0.1 if (step == 0) else 0.01)
    step_size = (self.step_size if (step == 0) else sys.maxsize)
    ' create model and dataloader '
    dataloader = self.get_dataloader(train_data, training=True)
    base_param_ids = set(map(id, self.model.module.CNN.base.parameters()))
    base_params_need_for_grad = filter((lambda p: p.requires_grad), self.model.module.CNN.base.parameters())
    new_params = [p for p in self.model.parameters() if (id(p) not in base_param_ids)]
    param_groups = [{'params': base_params_need_for_grad, 'lr_mult': 0.1}, {'params': new_params, 'lr_mult': 1.0}]
    optimizer = torch.optim.SGD(param_groups, lr=init_lr, momentum=0.9, weight_decay=0.0005, nesterov=True)

    def adjust_lr(epoch, step_size):
        lr = (init_lr / (10 ** (epoch // step_size)))
        for g in optimizer.param_groups:
            g['lr'] = (lr * g.get('lr_mult', 1))
    ' main training process '
    trainer = Trainer(self.model, self.criterion, fixed_layer=self.fixed_layer)
    for epoch in range(epochs):
        adjust_lr(epoch, step_size)
        trainer.train(epoch, dataloader, optimizer, print_freq=max(5, ((len(dataloader) // 30) * 10)))

------------------- similar code (pruned) ------------------ score = 0.26666666666666666 
def  ... ():
     ...  =  ... .optim.SGD


