------------------------- example 1 ------------------------ 
def forward(self, x):
    '\n                        This is where we get outputs from the input model.\n\n                        :param x: Input data.\n                        :type x: `torch.Tensor`\n                        :return: a list of output layers, where the last 2 layers are logit and final outputs.\n                        :rtype: `list`\n                        '
// your code ...
    if isinstance(self._model, nn.Sequential):
        for (_, module_) in self._model._modules.items():
// your code ...
    elif isinstance(self._model, nn.Module):
        x = self._model(x)
        result.append(x)
    else:
// your code ...

------------------------- example 2 ------------------------ 
def get_encoder(self, encoder, layer):
    if (layer == 0):
        return nn.Sequential(encoder.blocks['conv1_1'].conv, encoder.blocks['conv1_1'].bn, encoder.blocks['conv1_1'].act)
    elif (layer == 1):
        return nn.Sequential(encoder.blocks['conv1_1'].pool, *[b for (k, b) in encoder.blocks.items() if k.startswith('conv2_')])
    elif (layer == 2):
// your code ...
    elif (layer == 3):
// your code ...

------------------------- example 3 ------------------------ 
def pointwise(in_channels, out_channels):
    return nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True))

------------------------- example 4 ------------------------ 
def conv_1x1_bn(inp, oup):
    return nn.Sequential(nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.ReLU6(inplace=True))

------------------------- example 5 ------------------------ 
def get_encoder(self, encoder, layer):
    if (layer == 0):
// your code ...
    elif (layer == 1):
        return nn.Sequential(encoder.pool, encoder.layer1)
    elif (layer == 2):
// your code ...
    elif (layer == 3):
// your code ...

examples  ||  representativeness  ||  number of lines  || number of comments   ||  relevancy  
example1  ||          4           ||        8         ||         3        ||        0.25         
example2  ||          4           ||        7         ||         2        ||        0.2857142857142857         
example3  ||          4           ||        2         ||         0        ||        0.5         
example4  ||          7           ||        2         ||         0        ||        0.5         
example5  ||          5           ||        6         ||         3        ||        0.16666666666666666         

avg       ||          0.5762304921968787           ||        5.0         ||         1.6        ||         34.04761904761905        

idx = 0:------------------- similar code ------------------ index = 605, score = 7.0 
def forward(self, x):
    '\n                        This is where we get outputs from the input model.\n\n                        :param x: Input data.\n                        :type x: `torch.Tensor`\n                        :return: a list of output layers, where the last 2 layers are logit and final outputs.\n                        :rtype: `list`\n                        '
    import torch.nn as nn
    result = []
    if isinstance(self._model, nn.Sequential):
        for (_, module_) in self._model._modules.items():
            x = module_(x)
            result.append(x)
    elif isinstance(self._model, nn.Module):
        x = self._model(x)
        result.append(x)
    else:
        raise TypeError('The input model must inherit from `nn.Module`.')
    output_layer = nn.functional.softmax(x, dim=1)
    result.append(output_layer)
    return result

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if  ... (, nn.Sequential):
idx = 1:------------------- similar code ------------------ index = 538, score = 7.0 
@property
def get_layers(self):
    '\n                        Return the hidden layers in the model, if applicable.\n\n                        :return: The hidden layers in the model, input and output layers excluded.\n                        :rtype: `list`\n\n                        .. warning:: `get_layers` tries to infer the internal structure of the model.\n                                     This feature comes with no guarantees on the correctness of the result.\n                                     The intended order of the layers tries to match their order in the model, but this\n                                     is not guaranteed either. In addition, the function can only infer the internal\n                                     layers if the input model is of type `nn.Sequential`, otherwise, it will only\n                                     return the logit layer.\n                        '
    import torch.nn as nn
    result = []
    if isinstance(self._model, nn.Sequential):
        for (name, module_) in self._model._modules.items():
            result.append(((name + '_') + str(module_)))
    elif isinstance(self._model, nn.Module):
        result.append('logit_layer')
    else:
        raise TypeError('The input model must inherit from `nn.Module`.')
    return result

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if  ... (, nn.Sequential):
idx = 2:------------------- similar code ------------------ index = 663, score = 7.0 
def get_parameters(self, bias=False):
    import torch.nn as nn
    modules_skipped = (nn.ReLU, nn.MaxPool2d, nn.Dropout2d, nn.Sequential, FCN8s)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            if bias:
                (yield m.bias)
            else:
                (yield m.weight)
        elif isinstance(m, nn.ConvTranspose2d):
            if bias:
                assert (m.bias is None)
        elif isinstance(m, modules_skipped):
            continue
        else:
            raise ValueError(('Unexpected module: %s' % str(m)))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
     ...  = (,,, nn.Sequential,  ... )

idx = 3:------------------- similar code ------------------ index = 365, score = 7.0 
def _make_model_wrapper(self, model):
    try:
        import torch.nn as nn
        if (not hasattr(self, '_model_wrapper')):

            class ModelWrapper(nn.Module):
                '\n                    This is a wrapper for the input model.\n                    '

                def __init__(self, model):
                    '\n                        Initialization by storing the input model.\n\n                        :param model: PyTorch model. The forward function of the model must return the logit output.\n                        :type model: is instance of `torch.nn.Module`\n                        '
                    super(ModelWrapper, self).__init__()
                    self._model = model

                def forward(self, x):
                    '\n                        This is where we get outputs from the input model.\n\n                        :param x: Input data.\n                        :type x: `torch.Tensor`\n                        :return: a list of output layers, where the last 2 layers are logit and final outputs.\n                        :rtype: `list`\n                        '
                    import torch.nn as nn
                    result = []
                    if isinstance(self._model, nn.Sequential):
                        for (_, module_) in self._model._modules.items():
                            x = module_(x)
                            result.append(x)
                    elif isinstance(self._model, nn.Module):
                        x = self._model(x)
                        result.append(x)
                    else:
                        raise TypeError('The input model must inherit from `nn.Module`.')
                    output_layer = nn.functional.softmax(x, dim=1)
                    result.append(output_layer)
                    return result

                @property
                def get_layers(self):
                    '\n                        Return the hidden layers in the model, if applicable.\n\n                        :return: The hidden layers in the model, input and output layers excluded.\n                        :rtype: `list`\n\n                        .. warning:: `get_layers` tries to infer the internal structure of the model.\n                                     This feature comes with no guarantees on the correctness of the result.\n                                     The intended order of the layers tries to match their order in the model, but this\n                                     is not guaranteed either. In addition, the function can only infer the internal\n                                     layers if the input model is of type `nn.Sequential`, otherwise, it will only\n                                     return the logit layer.\n                        '
                    import torch.nn as nn
                    result = []
                    if isinstance(self._model, nn.Sequential):
                        for (name, module_) in self._model._modules.items():
                            result.append(((name + '_') + str(module_)))
                    elif isinstance(self._model, nn.Module):
                        result.append('logit_layer')
                    else:
                        raise TypeError('The input model must inherit from `nn.Module`.')
                    return result
            self._model_wrapper = ModelWrapper
        return self._model_wrapper(model)
    except ImportError:
        raise ImportError('Could not find PyTorch (`torch`) installation.')

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    try:
        if:

            class  ... ():
                def  ... ():
                    if  ... (, nn.Sequential):
idx = 4:------------------- similar code ------------------ index = 522, score = 6.0 
def get_encoder(self, encoder, layer):
    if (layer == 0):
        return nn.Sequential(encoder.blocks['conv1_1'].conv, encoder.blocks['conv1_1'].bn, encoder.blocks['conv1_1'].act)
    elif (layer == 1):
        return nn.Sequential(encoder.blocks['conv1_1'].pool, *[b for (k, b) in encoder.blocks.items() if k.startswith('conv2_')])
    elif (layer == 2):
        return nn.Sequential(*[b for (k, b) in encoder.blocks.items() if k.startswith('conv3_')])
    elif (layer == 3):
        return nn.Sequential(*[b for (k, b) in encoder.blocks.items() if k.startswith('conv4_')])
    elif (layer == 4):
        return nn.Sequential(*[b for (k, b) in encoder.blocks.items() if k.startswith('conv5_')])

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        return nn.Sequential

idx = 5:------------------- similar code ------------------ index = 524, score = 6.0 
def get_encoder(self, encoder, layer):
    if (layer == 0):
        return nn.Sequential(encoder.features.conv0, encoder.features.norm0, encoder.features.relu0)
    elif (layer == 1):
        return nn.Sequential(encoder.features.pool0, encoder.features.denseblock1)
    elif (layer == 2):
        return nn.Sequential(encoder.features.transition1, encoder.features.denseblock2)
    elif (layer == 3):
        return nn.Sequential(encoder.features.transition2, encoder.features.denseblock3)
    elif (layer == 4):
        return nn.Sequential(encoder.features.transition3, encoder.features.denseblock4, encoder.features.norm5, nn.ReLU())

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        return nn.Sequential

idx = 6:------------------- similar code ------------------ index = 207, score = 6.0 
def pointwise(in_channels, out_channels):
    return nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 7:------------------- similar code ------------------ index = 532, score = 6.0 
def conv_1x1_bn(inp, oup):
    return nn.Sequential(nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.ReLU6(inplace=True))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 8:------------------- similar code ------------------ index = 203, score = 6.0 
def get_encoder(self, encoder, layer):
    if (layer == 0):
        return encoder.layer0
    elif (layer == 1):
        return nn.Sequential(encoder.pool, encoder.layer1)
    elif (layer == 2):
        return encoder.layer2
    elif (layer == 3):
        return encoder.layer3
    elif (layer == 4):
        return encoder.layer4

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:    elif:
        return nn.Sequential

idx = 9:------------------- similar code ------------------ index = 216, score = 6.0 
def _make_layer(self, block, channel_num):
    layers = []
    for i in range(cfg.GAN.R_NUM):
        layers.append(block(channel_num))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 10:------------------- similar code ------------------ index = 200, score = 6.0 
def make_pool_layer(dim):
    return nn.Sequential()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return nn.Sequential()

idx = 11:------------------- similar code ------------------ index = 542, score = 6.0 
def get_encoder(self, encoder, layer):
    if (layer == 0):
        return nn.Sequential(encoder.conv1, encoder.bn1, encoder.relu)
    elif (layer == 1):
        return nn.Sequential(encoder.maxpool, encoder.layer1)
    elif (layer == 2):
        return encoder.layer2
    elif (layer == 3):
        return encoder.layer3
    elif (layer == 4):
        return encoder.layer4

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        return nn.Sequential

idx = 12:------------------- similar code ------------------ index = 196, score = 6.0 
def init_weights(self, pretrained=None):
    super(CascadeRCNN, self).init_weights(pretrained)
    self.backbone.init_weights(pretrained=pretrained)
    if self.with_neck:
        if isinstance(self.neck, nn.Sequential):
            for m in self.neck:
                m.init_weights()
        else:
            self.neck.init_weights()
    if self.with_rpn:
        self.rpn_head.init_weights()
    if self.with_shared_head:
        self.shared_head.init_weights(pretrained=pretrained)
    for i in range(self.num_stages):
        if self.with_bbox:
            self.bbox_roi_extractor[i].init_weights()
            self.bbox_head[i].init_weights()
        if self.with_mask:
            if (not self.share_roi_extractor):
                self.mask_roi_extractor[i].init_weights()
            self.mask_head[i].init_weights()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        if  ... (, nn.Sequential):
idx = 13:------------------- similar code ------------------ index = 192, score = 6.0 
def convbn_3d(in_planes, out_planes, kernel_size, stride, pad, dilation):
    return nn.Sequential(Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=pad, dilation=dilation, bias=False), nn.BatchNorm3d(out_planes))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 14:------------------- similar code ------------------ index = 189, score = 6.0 
def _make_branch(self, in_ch, out_ch, stride=1):
    return nn.Sequential(nn.ReLU(inplace=False), nn.Conv2d(in_ch, out_ch, 3, padding=1, stride=stride, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=False), nn.Conv2d(out_ch, out_ch, 3, padding=1, stride=1, bias=False), nn.BatchNorm2d(out_ch))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 15:------------------- similar code ------------------ index = 554, score = 6.0 
def __init__(self, in_channels, kernel_size):
    assert (kernel_size >= 2), 'kernel_size out of range: {}'.format(kernel_size)
    super(DeConv, self).__init__()

    def convt(in_channels):
        stride = 2
        padding = ((kernel_size - 1) // 2)
        output_padding = (kernel_size % 2)
        assert (((((- 2) - (2 * padding)) + kernel_size) + output_padding) == 0), 'deconv parameters incorrect'
        module_name = 'deconv{}'.format(kernel_size)
        return nn.Sequential(collections.OrderedDict([(module_name, nn.ConvTranspose2d(in_channels, (in_channels // 2), kernel_size, stride, padding, output_padding, bias=False)), ('batchnorm', nn.BatchNorm2d((in_channels // 2))), ('relu', nn.ReLU(inplace=True))]))
    self.layer1 = convt(in_channels)
    self.layer2 = convt((in_channels // 2))
    self.layer3 = convt((in_channels // (2 ** 2)))
    self.layer4 = convt((in_channels // (2 ** 3)))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    def  ... ( ... ):
        return nn.Sequential

idx = 16:------------------- similar code ------------------ index = 539, score = 6.0 
def conv_dw(inp, oup, stride, alpha_in=0.5, alpha_out=0.5):
    return nn.Sequential(Conv_BN_ACT(inp, inp, kernel_size=3, stride=stride, padding=1, groups=inp, bias=False, alpha_in=alpha_in, alpha_out=(alpha_in if (alpha_out != alpha_in) else alpha_out)), Conv_BN_ACT(inp, oup, kernel_size=1, alpha_in=alpha_in, alpha_out=alpha_out))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 17:------------------- similar code ------------------ index = 517, score = 6.0 
def init_weights(self, pretrained=None):
    super(SingleStageDetector, self).init_weights(pretrained)
    self.backbone.init_weights(pretrained=pretrained)
    if self.with_neck:
        if isinstance(self.neck, nn.Sequential):
            for m in self.neck:
                m.init_weights()
        else:
            self.neck.init_weights()
    self.bbox_head.init_weights()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        if  ... (, nn.Sequential):
idx = 18:------------------- similar code ------------------ index = 557, score = 6.0 
def register_hook(module):

    def hook(module, input, output=None):
        class_name = str(module.__class__).split('.')[(- 1)].split("'")[0]
        module_idx = len(summary)
        m_key = f'{class_name}-{(module_idx + 1)}'
        summary[m_key] = OrderedDict()
        summary[m_key]['input_shape'] = list(input[0].size())
        summary[m_key]['input_shape'][0] = batch_size
        if ((show_input is False) and (output is not None)):
            if isinstance(output, (list, tuple)):
                for out in output:
                    if isinstance(out, torch.Tensor):
                        summary[m_key]['output_shape'] = [([(- 1)] + list(out.size())[1:])][0]
                    else:
                        summary[m_key]['output_shape'] = [([(- 1)] + list(out[0].size())[1:])][0]
            else:
                summary[m_key]['output_shape'] = list(output.size())
                summary[m_key]['output_shape'][0] = batch_size
        params = 0
        if (hasattr(module, 'weight') and hasattr(module.weight, 'size')):
            params += torch.prod(torch.LongTensor(list(module.weight.size())))
            summary[m_key]['trainable'] = module.weight.requires_grad
        if (hasattr(module, 'bias') and hasattr(module.bias, 'size')):
            params += torch.prod(torch.LongTensor(list(module.bias.size())))
        summary[m_key]['nb_params'] = params
    if ((not isinstance(module, nn.Sequential)) and (not isinstance(module, nn.ModuleList)) and (not (module == model))):
        if (show_input is True):
            hooks.append(module.register_forward_pre_hook(hook))
        else:
            hooks.append(module.register_forward_hook(hook))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):

    if ((not  ... ( ... , nn.Sequential)) and and):
idx = 19:------------------- similar code ------------------ index = 489, score = 6.0 
def conv_3x3_bn(inp, oup, stride):
    return nn.Sequential(nn.Conv2d(inp, oup, 3, stride, 1, bias=False), nn.BatchNorm2d(oup), nn.ReLU6(inplace=True))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 20:------------------- similar code ------------------ index = 234, score = 6.0 
def _make_btlayer(self, bt, inChannels, outChannels, numBlock, t=6, stride=1):
    layers = []
    layers.append(bt(inChannels, outChannels, t, stride))
    for i in range(1, numBlock):
        layers.append(bt(outChannels, outChannels, t, 1))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 21:------------------- similar code ------------------ index = 492, score = 6.0 
def fc_v(env, hidden1=400, hidden2=300):
    return nn.Sequential(nn.Linear(env.state_space.shape[0], hidden1), nn.LeakyReLU(), nn.Linear(hidden1, hidden2), nn.LeakyReLU(), nn.Linear(hidden2, 1))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 22:------------------- similar code ------------------ index = 493, score = 6.0 
def pyramidal_make_layer(self, block, block_depth, stride=1):
    downsample = None
    if (stride != 1):
        downsample = nn.AvgPool2d((2, 2), stride=(2, 2), ceil_mode=True)
    layers = []
    self.featuremap_dim = (self.featuremap_dim + self.addrate)
    layers.append(block(self.input_featuremap_dim, int(round(self.featuremap_dim)), stride, downsample))
    for i in range(1, block_depth):
        temp_featuremap_dim = (self.featuremap_dim + self.addrate)
        layers.append(block((int(round(self.featuremap_dim)) * block.outchannel_ratio), int(round(temp_featuremap_dim)), 1))
        self.featuremap_dim = temp_featuremap_dim
    self.input_featuremap_dim = (int(round(self.featuremap_dim)) * block.outchannel_ratio)
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 23:------------------- similar code ------------------ index = 232, score = 6.0 
def last_zero_init(m):
    if isinstance(m, nn.Sequential):
        constant_init(m[(- 1)], val=0)
    else:
        constant_init(m, val=0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if  ... ( ... , nn.Sequential):
idx = 24:------------------- similar code ------------------ index = 226, score = 6.0 
def _make_conv_layers(self, channels, convs, stride=1, dilation=1, BatchNorm=None):
    modules = []
    for i in range(convs):
        modules.extend([nn.Conv2d(self.inplanes, channels, kernel_size=3, stride=(stride if (i == 0) else 1), padding=dilation, bias=False, dilation=dilation), BatchNorm(channels), nn.ReLU(inplace=True)])
        self.inplanes = channels
    return nn.Sequential(*modules)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 25:------------------- similar code ------------------ index = 506, score = 6.0 
def build_conv_block(self, dim, padding_type, norm_layer, use_bias):
    conv_block = []
    p = 0
    if (padding_type == 'reflect'):
        conv_block += [nn.ReflectionPad2d(1)]
    elif (padding_type == 'replicate'):
        conv_block += [nn.ReplicationPad2d(1)]
    elif (padding_type == 'zero'):
        p = 1
    else:
        raise NotImplementedError(('padding [%s] is not implemented' % padding_type))
    conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]
    p = 0
    if (padding_type == 'reflect'):
        conv_block += [nn.ReflectionPad2d(1)]
    elif (padding_type == 'replicate'):
        conv_block += [nn.ReplicationPad2d(1)]
    elif (padding_type == 'zero'):
        p = 1
    else:
        raise NotImplementedError(('padding [%s] is not implemented' % padding_type))
    conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]
    return nn.Sequential(*conv_block)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 26:------------------- similar code ------------------ index = 218, score = 6.0 
def residual_bottleneck(feature_dim=256, num_blocks=1, l2norm=True, final_conv=False, norm_scale=1.0, out_dim=None):
    if (out_dim is None):
        out_dim = feature_dim
    feat_layers = []
    for i in range(num_blocks):
        planes = (feature_dim if (i < ((num_blocks - 1) + int(final_conv))) else (out_dim // 4))
        feat_layers.append(Bottleneck((4 * feature_dim), planes))
    if final_conv:
        feat_layers.append(nn.Conv2d((4 * feature_dim), out_dim, kernel_size=3, padding=1, bias=False))
    if l2norm:
        feat_layers.append(InstanceL2Norm(scale=norm_scale))
    return nn.Sequential(*feat_layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 27:------------------- similar code ------------------ index = 560, score = 6.0 
def convbn_3d(in_planes, out_planes, kernel_size, stride, pad, dilation=1):
    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride, dilation=dilation, bias=False), nn.BatchNorm3d(out_planes))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 28:------------------- similar code ------------------ index = 187, score = 6.0 
def _make_layer(self, n_units, n_ch, w_base, cardinary, stride=1):
    layers = []
    (mid_ch, out_ch) = (((n_ch * (w_base // 64)) * cardinary), (n_ch * 4))
    for i in range(n_units):
        layers.append(ShakeBottleNeck(self.in_ch, mid_ch, out_ch, cardinary, stride=stride))
        (self.in_ch, stride) = (out_ch, 1)
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 29:------------------- similar code ------------------ index = 484, score = 6.0 
def conv_1x1_bn(inp, oup):
    return nn.Sequential(nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.ReLU6(inplace=True))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 30:------------------- similar code ------------------ index = 607, score = 6.0 
def conv_3x3_bn(inp, oup, stride):
    return nn.Sequential(nn.Conv2d(inp, oup, 3, stride, 1, bias=False), nn.BatchNorm2d(oup), nn.ReLU6(inplace=True))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 31:------------------- similar code ------------------ index = 153, score = 6.0 
def convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):
    return nn.Sequential(Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=pad, dilation=dilation, bias=False), nn.BatchNorm2d(out_planes))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 32:------------------- similar code ------------------ index = 151, score = 6.0 
def build(cfg, registry, default_args=None):
    if isinstance(cfg, list):
        modules = [build_from_cfg(cfg_, registry, default_args) for cfg_ in cfg]
        return nn.Sequential(*modules)
    else:
        return build_from_cfg(cfg, registry, default_args)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        return nn.Sequential

idx = 33:------------------- similar code ------------------ index = 147, score = 6.0 
def fc_bcq_deterministic_policy(env, hidden1=400, hidden2=300):
    return nn.Sequential(nn.Linear((env.state_space.shape[0] + env.action_space.shape[0]), hidden1), nn.LeakyReLU(), nn.Linear(hidden1, hidden2), nn.LeakyReLU(), nn.Linear(hidden2, env.action_space.shape[0]))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 34:------------------- similar code ------------------ index = 627, score = 6.0 
def last_zero_init(m):
    if isinstance(m, nn.Sequential):
        constant_init(m[(- 1)], val=0)
    else:
        constant_init(m, val=0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if  ... ( ... , nn.Sequential):
idx = 35:------------------- similar code ------------------ index = 134, score = 6.0 
def get_cnn_miniimagenet(hidden_size, n_classes):

    def conv_layer(ic, oc):
        return nn.Sequential(nn.Conv2d(ic, oc, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(oc, momentum=1.0, affine=True, track_running_stats=False))
    net = nn.Sequential(conv_layer(3, hidden_size), conv_layer(hidden_size, hidden_size), conv_layer(hidden_size, hidden_size), conv_layer(hidden_size, hidden_size), nn.Flatten(), nn.Linear(((hidden_size * 5) * 5), n_classes))
    initialize(net)
    return net

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():

    def  ... ():
        return nn.Sequential

idx = 36:------------------- similar code ------------------ index = 132, score = 6.0 
def _make_layer(self, block, planes, num_blocks, stride):
    strides = ([stride] + ([1] * (num_blocks - 1)))
    layers = []
    for stride in strides:
        layers.append(block(self.in_planes, planes, stride))
        self.in_planes = (planes * block.expansion)
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 37:------------------- similar code ------------------ index = 633, score = 6.0 
def conv_1x1_bn(inp, oup):
    return nn.Sequential(nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), h_swish())

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 38:------------------- similar code ------------------ index = 130, score = 6.0 
def _make_layer_revr(inp_dim, out_dim, modules):
    layers = [residual(inp_dim, inp_dim) for _ in range((modules - 1))]
    layers += [residual(inp_dim, out_dim)]
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 39:------------------- similar code ------------------ index = 640, score = 6.0 
def make_layer_revr(k, inp_dim, out_dim, modules, layer=convolution, **kwargs):
    layers = []
    for _ in range((modules - 1)):
        layers.append(layer(k, inp_dim, inp_dim, **kwargs))
    layers.append(layer(k, inp_dim, out_dim, **kwargs))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 40:------------------- similar code ------------------ index = 606, score = 6.0 
def build(cfg, registry, default_args=None):
    if isinstance(cfg, list):
        modules = [build_from_cfg(cfg_, registry, default_args) for cfg_ in cfg]
        return nn.Sequential(*modules)
    else:
        return build_from_cfg(cfg, registry, default_args)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        return nn.Sequential

idx = 41:------------------- similar code ------------------ index = 604, score = 6.0 
def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):
    strides = ([stride] + ([1] * (num_blocks - 1)))
    layers = []
    for stride in strides:
        layers.append(block(self.in_planes, planes, dropout_rate, stride))
        self.in_planes = planes
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 42:------------------- similar code ------------------ index = 173, score = 6.0 
def conv_1x1_bn(inp, oup):
    return nn.Sequential(nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.ReLU6(inplace=True))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 43:------------------- similar code ------------------ index = 185, score = 6.0 
def fc_deterministic_policy(env, hidden1=400, hidden2=300):
    return nn.Sequential(nn.Linear(env.state_space.shape[0], hidden1), nn.LeakyReLU(), nn.Linear(hidden1, hidden2), nn.LeakyReLU(), nn.Linear(hidden2, env.action_space.shape[0]))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 44:------------------- similar code ------------------ index = 572, score = 6.0 
def make_layers(cfg, batch_norm=False):
    layers = list()
    in_channels = 3
    for v in cfg:
        if (v == 'M'):
            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
        else:
            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
            if batch_norm:
                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]
            else:
                layers += [conv2d, nn.ReLU(inplace=True)]
            in_channels = v
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 45:------------------- similar code ------------------ index = 574, score = 6.0 
def conv_3x3_bn(inp, oup, stride):
    return nn.Sequential(nn.Conv2d(inp, oup, 3, stride, 1, bias=False), nn.BatchNorm2d(oup), h_swish())

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 46:------------------- similar code ------------------ index = 575, score = 6.0 
def _create_fcnn(input_size, hidden_size, output_size, activation_function, dropout=0, final_gain=1.0):
    assert (activation_function in ACTIVATION_FUNCTIONS.keys())
    (network_dims, layers) = ((input_size, hidden_size, hidden_size), [])
    for l in range((len(network_dims) - 1)):
        layer = nn.Linear(network_dims[l], network_dims[(l + 1)])
        nn.init.orthogonal_(layer.weight, gain=nn.init.calculate_gain(activation_function))
        nn.init.constant_(layer.bias, 0)
        layers.append(layer)
        if (dropout > 0):
            layers.append(nn.Dropout(p=dropout))
        layers.append(ACTIVATION_FUNCTIONS[activation_function]())
    final_layer = nn.Linear(network_dims[(- 1)], output_size)
    nn.init.orthogonal_(final_layer.weight, gain=final_gain)
    nn.init.constant_(final_layer.bias, 0)
    layers.append(final_layer)
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 47:------------------- similar code ------------------ index = 599, score = 6.0 
def conv(in_channels, out_channels, kernel_size):
    padding = ((kernel_size - 1) // 2)
    assert ((2 * padding) == (kernel_size - 1)), 'parameters incorrect. kernel={}, padding={}'.format(kernel_size, padding)
    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 48:------------------- similar code ------------------ index = 170, score = 6.0 
def _make_layer(self, block, norm_layer, blocks, mid_out_channels, stride=1):
    layers = []
    has_proj = (True if (stride > 1) else False)
    layers.append(block(self.in_channels, mid_out_channels, has_proj, stride=stride, norm_layer=norm_layer))
    self.in_channels = (mid_out_channels * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.in_channels, mid_out_channels, has_proj=False, stride=1, norm_layer=norm_layer))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 49:------------------- similar code ------------------ index = 169, score = 6.0 
def convt_dw(channels, kernel_size):
    stride = 2
    padding = ((kernel_size - 1) // 2)
    output_padding = (kernel_size % 2)
    assert (((((- 2) - (2 * padding)) + kernel_size) + output_padding) == 0), 'deconv parameters incorrect'
    return nn.Sequential(nn.ConvTranspose2d(channels, channels, kernel_size, stride, padding, output_padding, bias=False, groups=channels), nn.BatchNorm2d(channels), nn.ReLU(inplace=True))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 50:------------------- similar code ------------------ index = 591, score = 6.0 
def network_to_half(network):
    '\n    Convert model to half precision in a batchnorm-safe way.\n\n    Retained for legacy purposes. It is recommended to use FP16Model.\n    '
    return nn.Sequential(tofp16(), BN_convert_float(network.half()))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return nn.Sequential

idx = 51:------------------- similar code ------------------ index = 594, score = 6.0 
def conv_bn_relu(in_c, out_c):
    return nn.Sequential(nn.Conv2d(in_c, out_c, 3, padding=1, bias=False), nn.BatchNorm2d(out_c), nn.ReLU(True))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 52:------------------- similar code ------------------ index = 643, score = 6.0 
def lin_layer(n_in, n_out, dropout):
    return nn.Sequential(nn.Linear(n_in, n_out), GELU(), nn.Dropout(dropout))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn.Sequential

idx = 53:------------------- similar code ------------------ index = 318, score = 6.0 
def _init_perception_model(self, observation_space):
    if ('rgb' in observation_space.spaces):
        self._n_input_rgb = observation_space.spaces['rgb'].shape[2]
    else:
        self._n_input_rgb = 0
    if ('depth' in observation_space.spaces):
        self._n_input_depth = observation_space.spaces['depth'].shape[2]
    else:
        self._n_input_depth = 0
    if ('global_map' in observation_space.spaces):
        self._n_input_global_map = observation_space.spaces['global_map'].shape[0]
    else:
        self._n_input_global_map = 0
    if ('local_map' in observation_space.spaces):
        self._n_input_local_map = observation_space.spaces['local_map'].shape[0]
    else:
        self._n_input_local_map = 0
    if (self._n_input_rgb > 0):
        cnn_dims = np.array(observation_space.spaces['rgb'].shape[:2], dtype=np.float32)
    elif (self._n_input_depth > 0):
        cnn_dims = np.array(observation_space.spaces['depth'].shape[:2], dtype=np.float32)
    elif (self._n_input_global_map > 0):
        cnn_dims = np.array(observation_space.spaces['global_map'].shape[1:3], dtype=np.float32)
    elif (self._n_input_local_map > 0):
        cnn_dims = np.array(observation_space.spaces['local_map'].shape[1:3], dtype=np.float32)
    if self.is_blind:
        return nn.Sequential()
    else:
        for (_, kernel_size, stride, padding) in self._cnn_layers_params:
            cnn_dims = self._conv_output_dim(dimension=cnn_dims, padding=np.array([padding, padding], dtype=np.float32), dilation=np.array([1, 1], dtype=np.float32), kernel_size=np.array([kernel_size, kernel_size], dtype=np.float32), stride=np.array([stride, stride], dtype=np.float32))
        cnn_layers = []
        prev_out_channels = None
        for (i, (out_channels, kernel_size, stride, padding)) in enumerate(self._cnn_layers_params):
            if (i == 0):
                in_channels = (((self._n_input_rgb + self._n_input_depth) + self._n_input_global_map) + self._n_input_local_map)
            else:
                in_channels = prev_out_channels
            cnn_layers.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding))
            if (i != (len(self._cnn_layers_params) - 1)):
                cnn_layers.append(nn.ReLU())
            prev_out_channels = out_channels
        cnn_layers += [Flatten(), nn.Linear(((self._cnn_layers_params[(- 1)][0] * cnn_dims[0]) * cnn_dims[1]), self._single_branch_size), nn.ReLU()]
        return nn.Sequential(*cnn_layers)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        return nn.Sequential()

idx = 54:------------------- similar code ------------------ index = 215, score = 6.0 
def __init__(self, n_head, d_k, d_in):
    super().__init__()
    self.n_head = n_head
    self.d_k = d_k
    self.d_in = d_in
    self.fc1_q = nn.Linear(d_in, (n_head * d_k))
    nn.init.normal_(self.fc1_q.weight, mean=0, std=np.sqrt((2.0 / d_k)))
    self.fc1_k = nn.Linear(d_in, (n_head * d_k))
    nn.init.normal_(self.fc1_k.weight, mean=0, std=np.sqrt((2.0 / d_k)))
    self.fc2 = nn.Sequential(nn.BatchNorm1d((n_head * d_k)), nn.Linear((n_head * d_k), (n_head * d_k)))
    self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Sequential

idx = 55:------------------- similar code ------------------ index = 496, score = 6.0 
def _make_fuse_layers(self):
    if (self.num_branches == 1):
        return None
    num_branches = self.num_branches
    num_inchannels = self.num_inchannels
    fuse_layers = []
    for i in range((num_branches if self.multi_scale_output else 1)):
        fuse_layer = []
        for j in range(num_branches):
            if (j > i):
                fuse_layer.append(nn.Sequential(nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False), nn.BatchNorm2d(num_inchannels[i]), nn.Upsample(scale_factor=(2 ** (j - i)), mode='nearest')))
            elif (j == i):
                fuse_layer.append(None)
            else:
                conv3x3s = []
                for k in range((i - j)):
                    if (k == ((i - j) - 1)):
                        num_outchannels_conv3x3 = num_inchannels[i]
                        conv3x3s.append(nn.Sequential(nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False), nn.BatchNorm2d(num_outchannels_conv3x3)))
                    else:
                        num_outchannels_conv3x3 = num_inchannels[j]
                        conv3x3s.append(nn.Sequential(nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False), nn.BatchNorm2d(num_outchannels_conv3x3), nn.ReLU(False)))
                fuse_layer.append(nn.Sequential(*conv3x3s))
        fuse_layers.append(nn.ModuleList(fuse_layer))
    return nn.ModuleList(fuse_layers)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
        for  ...  in:
            if:
                 ... . ... (nn.Sequential)

idx = 56:------------------- similar code ------------------ index = 227, score = 6.0 
def __init__(self, nc, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):
    '\n        The init function of the  MobileNet V2 Module\n\n        :param nc: Network controller\n        :param num_classes: the number of output classes\n        :param width_mult: The width multiple\n        :param inverted_residual_setting: A list of the block configurations\n        :param round_nearest: Rounding to nearest value\n        '
    super(MobileNetV2, self).__init__()
    block = InvertedResidual
    input_channel = 32
    last_channel = 1280
    if (inverted_residual_setting is None):
        inverted_residual_setting = [[1, 16, 1, 1], [6, 24, 2, 2], [6, 32, 3, 2], [6, 64, 4, 2], [6, 96, 3, 1], [6, 160, 3, 2], [6, 320, 1, 1]]
    if ((len(inverted_residual_setting) == 0) or (len(inverted_residual_setting[0]) != 4)):
        raise ValueError('inverted_residual_setting should be non-empty or a 4-element list, got {}'.format(inverted_residual_setting))
    input_channel = _make_divisible((input_channel * width_mult), round_nearest)
    self.last_channel = _make_divisible((last_channel * max(1.0, width_mult)), round_nearest)
    features = [ConvBNNonLinear(nc, 3, input_channel, stride=2)]
    for (t, c, n, s) in inverted_residual_setting:
        output_channel = _make_divisible((c * width_mult), round_nearest)
        for i in range(n):
            stride = (s if (i == 0) else 1)
            features.append(block(nc, input_channel, output_channel, stride, expand_ratio=t))
            input_channel = output_channel
    features.append(ConvBNNonLinear(nc, input_channel, self.last_channel, kernel_size=1))
    self.features = nn.Sequential(*features)
    self.classifier = nn.Sequential(nn.Dropout(0.2), layers.FullyConnected(nc, self.last_channel, num_classes))
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out')
            if (m.bias is not None):
                nn.init.zeros_(m.bias)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.ones_(m.weight)
            nn.init.zeros_(m.bias)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
            nn.init.zeros_(m.bias)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Sequential

idx = 57:------------------- similar code ------------------ index = 219, score = 6.0 
def __init__(self, params):
    super(CoILICRA, self).__init__()
    self.params = params
    number_first_layer_channels = 0
    for (_, sizes) in g_conf.SENSORS.items():
        number_first_layer_channels += (sizes[0] * g_conf.NUMBER_FRAMES_FUSION)
    sensor_input_shape = next(iter(g_conf.SENSORS.values()))
    sensor_input_shape = [number_first_layer_channels, sensor_input_shape[1], sensor_input_shape[2]]
    if ('conv' in params['perception']):
        perception_convs = Conv(params={'channels': ([number_first_layer_channels] + params['perception']['conv']['channels']), 'kernels': params['perception']['conv']['kernels'], 'strides': params['perception']['conv']['strides'], 'dropouts': params['perception']['conv']['dropouts'], 'end_layer': True})
        perception_fc = FC(params={'neurons': ([perception_convs.get_conv_output(sensor_input_shape)] + params['perception']['fc']['neurons']), 'dropouts': params['perception']['fc']['dropouts'], 'end_layer': False})
        self.perception = nn.Sequential(*[perception_convs, perception_fc])
        number_output_neurons = params['perception']['fc']['neurons'][(- 1)]
    elif ('res' in params['perception']):
        resnet_module = importlib.import_module('network.models.building_blocks.resnet')
        resnet_module = getattr(resnet_module, params['perception']['res']['name'])
        self.perception = resnet_module(pretrained=g_conf.PRE_TRAINED, num_classes=params['perception']['res']['num_classes'])
        number_output_neurons = params['perception']['res']['num_classes']
    else:
        raise ValueError('invalid convolution layer type')
    self.measurements = FC(params={'neurons': ([len(g_conf.INPUTS)] + params['measurements']['fc']['neurons']), 'dropouts': params['measurements']['fc']['dropouts'], 'end_layer': False})
    self.join = Join(params={'after_process': FC(params={'neurons': ([(params['measurements']['fc']['neurons'][(- 1)] + number_output_neurons)] + params['join']['fc']['neurons']), 'dropouts': params['join']['fc']['dropouts'], 'end_layer': False}), 'mode': 'cat'})
    self.speed_branch = FC(params={'neurons': (([params['join']['fc']['neurons'][(- 1)]] + params['speed_branch']['fc']['neurons']) + [1]), 'dropouts': (params['speed_branch']['fc']['dropouts'] + [0.0]), 'end_layer': True})
    branch_fc_vector = []
    for i in range(params['branches']['number_of_branches']):
        branch_fc_vector.append(FC(params={'neurons': (([params['join']['fc']['neurons'][(- 1)]] + params['branches']['fc']['neurons']) + [len(g_conf.TARGETS)]), 'dropouts': (params['branches']['fc']['dropouts'] + [0.0]), 'end_layer': True}))
    self.branches = Branching(branch_fc_vector)
    if ('conv' in params['perception']):
        for m in self.modules():
            if (isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear)):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0.1)
    else:
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0.1)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
 = nn.Sequential

idx = 58:------------------- similar code ------------------ index = 179, score = 6.0 
def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):
    super(_Context_voted_module, self).__init__()
    assert (dimension in [1, 2, 3])
    self.dimension = dimension
    self.sub_sample = sub_sample
    self.in_channels = in_channels
    self.inter_channels = inter_channels
    if (self.inter_channels is None):
        self.inter_channels = (in_channels // 2)
        if (self.inter_channels == 0):
            self.inter_channels = 1
    if (dimension == 3):
        conv_nd = nn.Conv3d
        max_pool = nn.MaxPool3d
        bn = nn.BatchNorm3d
    elif (dimension == 2):
        conv_nd = nn.Conv2d
        max_pool = nn.MaxPool2d
        bn = nn.BatchNorm2d
    else:
        conv_nd = nn.Conv1d
        max_pool = nn.MaxPool1d
        bn = nn.BatchNorm1d
    self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1, stride=1, padding=0)
    if bn_layer:
        self.W = nn.Sequential(conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0), bn(self.in_channels))
        nn.init.constant_(self.W[1].weight, 0)
        nn.init.constant_(self.W[1].bias, 0)
    else:
        self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0)
        nn.init.constant_(self.W.weight, 0)
        nn.init.constant_(self.W.bias, 0)
    self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1, stride=1, padding=0)
    self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1, stride=1, padding=0)
    if sub_sample:
        self.g = nn.Sequential(self.g, max_pool(kernel_size=2))
        self.phi = nn.Sequential(self.phi, max_pool(kernel_size=2))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if  ... :
 = nn.Sequential

idx = 59:------------------- similar code ------------------ index = 568, score = 6.0 
def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):
    super(DenseNet, self).__init__()
    self.features = nn.Sequential(OrderedDict([('conv0', nn.Conv2d(4, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)), ('norm0', nn.BatchNorm2d(num_init_features)), ('relu0', nn.ReLU(inplace=True)), ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))
    num_features = num_init_features
    for (i, num_layers) in enumerate(block_config):
        block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)
        self.features.add_module(('denseblock%d' % (i + 1)), block)
        num_features = (num_features + (num_layers * growth_rate))
        if (i != (len(block_config) - 1)):
            trans = _Transition(num_input_features=num_features, num_output_features=(num_features // 2))
            self.features.add_module(('transition%d' % (i + 1)), trans)
            num_features = (num_features // 2)
    self.features.add_module('norm5', nn.BatchNorm2d(num_features))
    self.classifier = nn.Linear(num_features, num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal(m.weight.data)
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Sequential

idx = 60:------------------- similar code ------------------ index = 590, score = 6.0 
def _make_fuse_layers(self):
    if (self.num_branches == 1):
        return None
    num_branches = self.num_branches
    in_channels = self.in_channels
    fuse_layers = []
    num_out_branches = (num_branches if self.multiscale_output else 1)
    for i in range(num_out_branches):
        fuse_layer = []
        for j in range(num_branches):
            if (j > i):
                fuse_layer.append(nn.Sequential(build_conv_layer(self.conv_cfg, in_channels[j], in_channels[i], kernel_size=1, stride=1, padding=0, bias=False), build_norm_layer(self.norm_cfg, in_channels[i])[1], nn.Upsample(scale_factor=(2 ** (j - i)), mode='nearest')))
            elif (j == i):
                fuse_layer.append(None)
            else:
                conv_downsamples = []
                for k in range((i - j)):
                    if (k == ((i - j) - 1)):
                        conv_downsamples.append(nn.Sequential(build_conv_layer(self.conv_cfg, in_channels[j], in_channels[i], kernel_size=3, stride=2, padding=1, bias=False), build_norm_layer(self.norm_cfg, in_channels[i])[1]))
                    else:
                        conv_downsamples.append(nn.Sequential(build_conv_layer(self.conv_cfg, in_channels[j], in_channels[j], kernel_size=3, stride=2, padding=1, bias=False), build_norm_layer(self.norm_cfg, in_channels[j])[1], nn.ReLU(inplace=False)))
                fuse_layer.append(nn.Sequential(*conv_downsamples))
        fuse_layers.append(nn.ModuleList(fuse_layer))
    return nn.ModuleList(fuse_layers)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
        for  ...  in:
            if:
                 ... . ... (nn.Sequential)

idx = 61:------------------- similar code ------------------ index = 321, score = 6.0 
def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer):
    num_branches_cur = len(num_channels_cur_layer)
    num_branches_pre = len(num_channels_pre_layer)
    transition_layers = []
    for i in range(num_branches_cur):
        if (i < num_branches_pre):
            if (num_channels_cur_layer[i] != num_channels_pre_layer[i]):
                transition_layers.append(nn.Sequential(build_conv_layer(self.conv_cfg, num_channels_pre_layer[i], num_channels_cur_layer[i], kernel_size=3, stride=1, padding=1, bias=False), build_norm_layer(self.norm_cfg, num_channels_cur_layer[i])[1], nn.ReLU(inplace=True)))
            else:
                transition_layers.append(None)
        else:
            conv_downsamples = []
            for j in range(((i + 1) - num_branches_pre)):
                in_channels = num_channels_pre_layer[(- 1)]
                out_channels = (num_channels_cur_layer[i] if (j == (i - num_branches_pre)) else in_channels)
                conv_downsamples.append(nn.Sequential(build_conv_layer(self.conv_cfg, in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False), build_norm_layer(self.norm_cfg, out_channels)[1], nn.ReLU(inplace=True)))
            transition_layers.append(nn.Sequential(*conv_downsamples))
    return nn.ModuleList(transition_layers)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
        if:
            if:
                 ... . ... (nn.Sequential)

idx = 62:------------------- similar code ------------------ index = 381, score = 6.0 
def __init__(self, num_classes=1000, width_mult=1.0, filter_size=1):
    super(MobileNetV2, self).__init__()
    block = InvertedResidual
    input_channel = 32
    last_channel = 1280
    inverted_residual_setting = [[1, 16, 1, 1], [6, 24, 2, 2], [6, 32, 3, 2], [6, 64, 4, 2], [6, 96, 3, 1], [6, 160, 3, 2], [6, 320, 1, 1]]
    input_channel = int((input_channel * width_mult))
    self.last_channel = int((last_channel * max(1.0, width_mult)))
    features = [ConvBNReLU(3, input_channel, stride=2)]
    for (t, c, n, s) in inverted_residual_setting:
        output_channel = int((c * width_mult))
        for i in range(n):
            stride = (s if (i == 0) else 1)
            features.append(block(input_channel, output_channel, stride, expand_ratio=t, filter_size=filter_size))
            input_channel = output_channel
    features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))
    self.features = nn.Sequential(*features)
    self.classifier = nn.Sequential(nn.Linear(self.last_channel, num_classes))
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out')
            if (m.bias is not None):
                nn.init.zeros_(m.bias)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.ones_(m.weight)
            nn.init.zeros_(m.bias)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
            nn.init.zeros_(m.bias)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Sequential

idx = 63:------------------- similar code ------------------ index = 527, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1, norm_layer=None):
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(conv1x1(self.inplanes, (planes * block.expansion), stride), norm_layer((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample, norm_layer))
    self.inplanes = (planes * block.expansion)
    for _ in range(1, blocks):
        layers.append(block(self.inplanes, planes, norm_layer=norm_layer))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 64:------------------- similar code ------------------ index = 536, score = 6.0 
def _make_layer(self, block, planes, blocks, shortcut_type, cardinality, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        if (shortcut_type == 'A'):
            downsample = partial(downsample_basic_block, planes=(planes * block.expansion), stride=stride)
        else:
            downsample = nn.Sequential(nn.Conv3d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm3d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, cardinality, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes, cardinality))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
        if:        else:
             ...  =  ... .Sequential
    return nn

idx = 65:------------------- similar code ------------------ index = 0, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 66:------------------- similar code ------------------ index = 201, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1, dilation__=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion)) or (dilation__ == 2) or (dilation__ == 4)):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion), affine=affine_par))
    for i in downsample._modules['1'].parameters():
        i.requires_grad = False
    layers = []
    layers.append(block(self.inplanes, planes, stride, dilation_=dilation__, downsample=downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes, dilation_=dilation__))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 67:------------------- similar code ------------------ index = 543, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 68:------------------- similar code ------------------ index = 198, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1, att_type=None):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample, use_cbam=(att_type == 'CBAM')))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes, use_cbam=(att_type == 'CBAM')))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 69:------------------- similar code ------------------ index = 191, score = 6.0 
def _make_layer(self, block, norm_layer, planes, blocks, inplace=True, stride=1, bn_eps=1e-05, bn_momentum=0.1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), norm_layer((planes * block.expansion), eps=bn_eps, momentum=bn_momentum))
    layers = []
    layers.append(block(self.inplanes, planes, stride, norm_layer, bn_eps, bn_momentum, downsample, inplace))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes, norm_layer=norm_layer, bn_eps=bn_eps, bn_momentum=bn_momentum, inplace=inplace))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 70:------------------- similar code ------------------ index = 497, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1, dilation=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride=stride, dilation=dilation, downsample=downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes, stride=1, dilation=dilation))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 71:------------------- similar code ------------------ index = 486, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 72:------------------- similar code ------------------ index = 235, score = 6.0 
def make_res_layer(block, inplanes, planes, blocks, stride=1, dilation=1, style='pytorch', with_cp=False, conv_cfg=None, norm_cfg=dict(type='BN'), dcn=None, gcb=None, gen_attention=None, gen_attention_blocks=[]):
    downsample = None
    if ((stride != 1) or (inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(build_conv_layer(conv_cfg, inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), build_norm_layer(norm_cfg, (planes * block.expansion))[1])
    layers = []
    layers.append(block(inplanes=inplanes, planes=planes, stride=stride, dilation=dilation, downsample=downsample, style=style, with_cp=with_cp, conv_cfg=conv_cfg, norm_cfg=norm_cfg, dcn=dcn, gcb=gcb, gen_attention=(gen_attention if (0 in gen_attention_blocks) else None)))
    inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(inplanes=inplanes, planes=planes, stride=1, dilation=dilation, style=style, with_cp=with_cp, conv_cfg=conv_cfg, norm_cfg=norm_cfg, dcn=dcn, gcb=gcb, gen_attention=(gen_attention if (i in gen_attention_blocks) else None)))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 73:------------------- similar code ------------------ index = 495, score = 6.0 
def _make_layer_slow(self, block, planes, blocks, stride=1, head_conv=1):
    downsample = None
    if ((stride != 1) or ((self.slow_inplanes + (int((self.slow_inplanes * self.beta)) * 2)) != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv3d((self.slow_inplanes + (int((self.slow_inplanes * self.beta)) * 2)), (planes * block.expansion), kernel_size=1, stride=(1, stride, stride), bias=False))
    layers = []
    layers.append(block((self.slow_inplanes + (int((self.slow_inplanes * self.beta)) * 2)), planes, stride, downsample, head_conv=head_conv))
    self.slow_inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.slow_inplanes, planes, head_conv=head_conv))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 74:------------------- similar code ------------------ index = 516, score = 6.0 
def _make_layer_fast(self, block, planes, blocks, shortcut_type, stride=1, head_conv=1):
    downsample = None
    if ((stride != 1) or (self.fast_inplanes != (planes * block.expansion))):
        if (shortcut_type == 'A'):
            downsample = partial(downsample_basic_block, planes=(planes * block.expansion), stride=stride)
        else:
            downsample = nn.Sequential(nn.Conv3d(self.fast_inplanes, (planes * block.expansion), kernel_size=1, stride=(1, stride, stride), bias=False))
    layers = []
    layers.append(block(self.fast_inplanes, planes, stride, downsample, head_conv=head_conv))
    self.fast_inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.fast_inplanes, planes, head_conv=head_conv))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
        if:        else:
             ...  =  ... .Sequential
    return nn

idx = 75:------------------- similar code ------------------ index = 500, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1, dilation=1, BatchNorm=None):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), BatchNorm((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample, BatchNorm=BatchNorm))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes, dilation=(dilation, dilation), BatchNorm=BatchNorm))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 76:------------------- similar code ------------------ index = 507, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1, dilate=False):
    norm_layer = self._norm_layer
    downsample = None
    previous_dilation = self.dilation
    if dilate:
        self.dilation *= stride
        stride = 1
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(conv1x1(self.inplanes, (planes * block.expansion), stride), norm_layer((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer))
    self.inplanes = (planes * block.expansion)
    for _ in range(1, blocks):
        layers.append(block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 77:------------------- similar code ------------------ index = 515, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False))
    layers = list()
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 78:------------------- similar code ------------------ index = 555, score = 6.0 
def _make_MG_unit(self, block, planes, blocks, stride=1, dilation=1, BatchNorm=None):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), BatchNorm((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, dilation=(blocks[0] * dilation), downsample=downsample, BatchNorm=BatchNorm))
    self.inplanes = (planes * block.expansion)
    for i in range(1, len(blocks)):
        layers.append(block(self.inplanes, planes, stride=1, dilation=(blocks[i] * dilation), BatchNorm=BatchNorm))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 79:------------------- similar code ------------------ index = 152, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion), momentum=BN_MOMENTUM))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 80:------------------- similar code ------------------ index = 612, score = 6.0 
def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        if (shortcut_type == 'A'):
            downsample = partial(downsample_basic_block, planes=(planes * block.expansion), stride=stride)
        else:
            downsample = nn.Sequential(nn.Conv3d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm3d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
        if:        else:
             ...  =  ... .Sequential
    return nn

idx = 81:------------------- similar code ------------------ index = 145, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 82:------------------- similar code ------------------ index = 622, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 83:------------------- similar code ------------------ index = 138, score = 6.0 
def _make_layer_scene(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes_scene != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes_scene, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes_scene, planes, stride, downsample))
    self.inplanes_scene = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes_scene, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 84:------------------- similar code ------------------ index = 137, score = 6.0 
def _make_layer(self, block, inplanes, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(build_conv_layer(self.conv_cfg, inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), build_norm_layer(self.norm_cfg, (planes * block.expansion))[1])
    layers = []
    layers.append(block(inplanes, planes, stride, downsample=downsample, with_cp=self.with_cp, norm_cfg=self.norm_cfg, conv_cfg=self.conv_cfg))
    inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(inplanes, planes, with_cp=self.with_cp, norm_cfg=self.norm_cfg, conv_cfg=self.conv_cfg))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 85:------------------- similar code ------------------ index = 628, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 86:------------------- similar code ------------------ index = 635, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 87:------------------- similar code ------------------ index = 126, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 88:------------------- similar code ------------------ index = 154, score = 6.0 
def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1):
    downsample = None
    if ((stride != 1) or (self.num_inchannels[branch_index] != (num_channels[branch_index] * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.num_inchannels[branch_index], (num_channels[branch_index] * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((num_channels[branch_index] * block.expansion)))
    layers = []
    layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index], stride, downsample))
    self.num_inchannels[branch_index] = (num_channels[branch_index] * block.expansion)
    for i in range(1, num_blocks[branch_index]):
        layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index]))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 89:------------------- similar code ------------------ index = 184, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1, norm_layer=None, pyconv_kernels=[3], pyconv_groups=[1]):
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    downsample = None
    if ((stride != 1) and (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=stride, padding=1), conv1x1(self.inplanes, (planes * block.expansion)), norm_layer((planes * block.expansion)))
    elif (self.inplanes != (planes * block.expansion)):
        downsample = nn.Sequential(conv1x1(self.inplanes, (planes * block.expansion)), norm_layer((planes * block.expansion)))
    elif (stride != 1):
        downsample = nn.MaxPool2d(kernel_size=3, stride=stride, padding=1)
    layers = []
    layers.append(block(self.inplanes, planes, stride=stride, downsample=downsample, norm_layer=norm_layer, pyconv_kernels=pyconv_kernels, pyconv_groups=pyconv_groups))
    self.inplanes = (planes * block.expansion)
    for _ in range(1, blocks):
        layers.append(block(self.inplanes, planes, norm_layer=norm_layer, pyconv_kernels=pyconv_kernels, pyconv_groups=pyconv_groups))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 90:------------------- similar code ------------------ index = 177, score = 6.0 
def _make_level(self, block, inplanes, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (inplanes != planes)):
        downsample = nn.Sequential(nn.MaxPool2d(stride, stride=stride), nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False), nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))
    layers = []
    layers.append(block(inplanes, planes, stride, downsample=downsample))
    for i in range(1, blocks):
        layers.append(block(inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 91:------------------- similar code ------------------ index = 579, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 92:------------------- similar code ------------------ index = 583, score = 6.0 
def _make_layer_slow(self, block, planes, blocks, shortcut_type, stride=1, head_conv=1):
    downsample = None
    if ((stride != 1) or ((self.slow_inplanes + (int((self.slow_inplanes * self.beta)) * 2)) != (planes * block.expansion))):
        if (shortcut_type == 'A'):
            downsample = partial(downsample_basic_block, planes=(planes * block.expansion), stride=stride)
        else:
            downsample = nn.Sequential(nn.Conv3d((self.slow_inplanes + (int((self.slow_inplanes * self.beta)) * 2)), (planes * block.expansion), kernel_size=1, stride=(1, stride, stride), bias=False))
    layers = []
    layers.append(block((self.slow_inplanes + (int((self.slow_inplanes * self.beta)) * 2)), planes, stride, downsample, head_conv=head_conv))
    self.slow_inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.slow_inplanes, planes, head_conv=head_conv))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
        if:        else:
             ...  =  ... .Sequential
    return nn

idx = 93:------------------- similar code ------------------ index = 589, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != planes)):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = planes
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 94:------------------- similar code ------------------ index = 162, score = 6.0 
def _make_layer(self, block, planes, blocks, groups, reduction, stride=1, downsample_kernel_size=1, downsample_padding=0):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=downsample_kernel_size, stride=stride, padding=downsample_padding, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, groups, reduction, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes, groups, reduction))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 95:------------------- similar code ------------------ index = 238, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 96:------------------- similar code ------------------ index = 483, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 97:------------------- similar code ------------------ index = 372, score = 6.0 
def _make_layer_face(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes_face != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes_face, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes_face, planes, stride, downsample))
    self.inplanes_face = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes_face, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 98:------------------- similar code ------------------ index = 376, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1, dilation=1, multi_grid=1, bn_momentum=0.0003, BatchNorm2d=nn.BatchNorm2d):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), BatchNorm2d((planes * block.expansion), affine=True, momentum=bn_momentum))
    layers = []
    generate_multi_grid = (lambda index, grids: (grids[(index % len(grids))] if isinstance(grids, tuple) else 1))
    layers.append(block(self.inplanes, planes, stride, dilation=dilation, downsample=downsample, multi_grid=generate_multi_grid(0, multi_grid), bn_momentum=bn_momentum, BatchNorm2d=BatchNorm2d))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes, dilation=dilation, multi_grid=generate_multi_grid(i, multi_grid), bn_momentum=bn_momentum, BatchNorm2d=BatchNorm2d))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

idx = 99:------------------- similar code ------------------ index = 316, score = 6.0 
def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if ((stride != 1) or (self.inplanes != (planes * block.expansion))):
        downsample = nn.Sequential(nn.Conv2d(self.inplanes, (planes * block.expansion), kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d((planes * block.expansion)))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = (planes * block.expansion)
    for i in range(1, blocks):
        layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    if:
         ...  =  ... .Sequential
    return nn

