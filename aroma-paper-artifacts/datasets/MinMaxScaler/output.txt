------------------------- example 1 ------------------------ 
def run(delta, observation_window, n_snapshots, censoring_ratio=0.5, single_snapshot=False):
    logging.basicConfig(level=logging.INFO, format='%(asctime)s: %(message)s', datefmt='%H:%M:%S')
    with open('data/delicious/user_contacts-timestamps.dat') as usr_usr:
        usr_dataset = usr_usr.read().splitlines()
    with open('data/delicious/user_taggedbookmarks-timestamps.dat') as usr_bm_tg:
        usr_bm_tg_dataset = usr_bm_tg.read().splitlines()
    delta = timestamp_delta_generator(months=delta)
    observation_end = datetime(2010, 10, 1).timestamp()
    observation_begin = (observation_end - timestamp_delta_generator(months=observation_window))
    feature_end = observation_begin
    feature_begin = (feature_end - (n_snapshots * delta))
    indexer = generate_indexer(usr_dataset, usr_bm_tg_dataset, feature_begin, feature_end)
    (contact_sparse, save_sparse, attach_sparse) = parse_dataset(usr_dataset, usr_bm_tg_dataset, feature_begin, feature_end, indexer)
    (observed_samples, censored_samples) = sample_generator(usr_dataset, observation_begin, observation_end, contact_sparse, indexer, censoring_ratio)
    (X, Y, T) = extract_features(contact_sparse, save_sparse, attach_sparse, observed_samples, censored_samples)
    X_list = [X]
    if (not single_snapshot):
        for t in range(int((feature_end - delta)), int(feature_begin), (- int(delta))):
            (contact_sparse, save_sparse, attach_sparse) = parse_dataset(usr_dataset, usr_bm_tg_dataset, feature_begin, t, indexer)
            (X, _, _) = extract_features(contact_sparse, save_sparse, attach_sparse, observed_samples, censored_samples)
            X_list = ([X] + X_list)
        for i in range(1, len(X_list)):
            X_list[i] -= X_list[(i - 1)]
    scaler = MinMaxScaler(copy=False)
    for X in X_list:
        scaler.fit_transform(X)
    X = np.stack(X_list, axis=1)
    T /= delta
    return (X, Y, T)

------------------------- example 2 ------------------------ 
if (__name__ == '__main__'):
    parser = build_parser()
    args = parser.parse_args()
    model_name = args.model_name
    model_type = ('double' if (model_name == 'double_albert') else 'siamese')
    checkpoint_dir = args.checkpoint_dir
    log_dir = args.log_dir
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    main_logger = init_logger(log_dir, f'train_main_{model_name}.log')
    test = pd.read_csv(f'{args.data_dir}test.csv')
    train = pd.read_csv(f'{args.data_dir}train.csv')
    for col in TARGETS:
        train[col] = train[col].rank(method='average')
    train[TARGETS] = MinMaxScaler().fit_transform(train[TARGETS])
    y = train[TARGETS].values
    (ids_train, seg_ids_train) = tokenize(train, pretrained_model_str=pretrained_models[model_name])
    (cat_features_train, _) = get_ohe_categorical_features(train, test, 'category')
    device = 'cuda'
    num_workers = 10
    n_folds = 10
    lr = 0.001
    n_epochs = 10
    bs = 2
    grad_accum = 4
    weight_decay = 0.01
    loss_fn = nn.BCEWithLogitsLoss()
    init_seed()
    folds = GroupKFold(n_splits=n_folds).split(X=train['question_body'], groups=train['question_body'])
    oofs = np.zeros((len(train), N_TARGETS))
    main_logger.info(f'Start training model {model_name}...')
    for (fold_id, (train_index, valid_index)) in enumerate(folds):
        main_logger.info(f'Fold {(fold_id + 1)} started at {time.ctime()}')
        fold_logger = init_logger(log_dir, f'train_fold_{(fold_id + 1)}_{model_name}.log')
        train_loader = DataLoader(TextDataset(cat_features_train, ids_train['question'], ids_train['answer'], seg_ids_train['question'], seg_ids_train['answer'], train_index, y), batch_size=bs, shuffle=True, num_workers=num_workers)
        valid_loader = DataLoader(TextDataset(cat_features_train, ids_train['question'], ids_train['answer'], seg_ids_train['question'], seg_ids_train['answer'], valid_index, y), batch_size=bs, shuffle=False, num_workers=num_workers)
        model = models[model_name]()
        optimizer = get_optimizer(model, lr, weight_decay, model_type)
        scheduler = OneCycleLR(optimizer, n_epochs=n_epochs, n_batches=len(train_loader))
        learner = Learner(model, optimizer, train_loader, valid_loader, loss_fn, device, n_epochs, f'{model_name}_fold_{(fold_id + 1)}', checkpoint_dir, scheduler, metric_spec={'spearmanr': spearmanr_torch}, monitor_metric=True, minimize_score=False, logger=fold_logger, grad_accum=grad_accum, batch_step_scheduler=True)
        learner.train()
        oofs[valid_index] = infer(learner.model, valid_loader, learner.best_checkpoint_file, device)
    main_logger.info(f'Finished training {model_name}')
    ix = np.where((train.groupby('question_body')['host'].transform('count') == 1))[0]
    main_logger.info('CVs:')
    main_logger.info(get_cvs(oofs, y, ix))
    os.makedirs('oofs/', exist_ok=True)
    pd.DataFrame(oofs, columns=TARGETS).to_csv(f'oofs/{model_name}_oofs.csv')

------------------------- example 3 ------------------------ 
def run(delta, observation_window, n_snapshots, censoring_ratio=0.5, single_snapshot=False):
    logging.basicConfig(level=logging.INFO, format='%(asctime)s: %(message)s', datefmt='%H:%M:%S')
    conf_list = {'db': ['KDD', 'PKDD', 'ICDM', 'SDM', 'PAKDD', 'SIGMOD', 'VLDB', 'ICDE', 'PODS', 'EDBT', 'SIGIR', 'ECIR', 'ACL', 'WWW', 'CIKM', 'NIPS', 'ICML', 'ECML', 'AAAI', 'IJCAI'], 'th': ['STOC', 'FOCS', 'COLT', 'LICS', 'SCG', 'SODA', 'SPAA', 'PODC', 'ISSAC', 'CRYPTO', 'EUROCRYPT', 'CONCUR', 'ICALP', 'STACS', 'COCO', 'WADS', 'MFCS', 'SWAT', 'ESA', 'IPCO', 'LFCS', 'ALT', 'EUROCOLT', 'WDAG', 'ISTCS', 'FSTTCS', 'LATIN', 'RECOMB', 'CADE', 'ISIT', 'MEGA', 'ASIAN', 'CCCG', 'FCT', 'WG', 'CIAC', 'ICCI', 'CATS', 'COCOON', 'GD', 'ISAAC', 'SIROCCO', 'WEA', 'ALENEX', 'FTP', 'CSL', 'DMTCS']}[path]
    observation_end = 2016
    observation_begin = (observation_end - observation_window)
    feature_end = observation_begin
    feature_begin = (feature_end - (delta * n_snapshots))
    (papers_feat_window, papers_obs_window, counter) = generate_papers('data/dblp/dblp.txt', feature_begin, feature_end, observation_begin, observation_end, conf_list)
    (W, C, I, P) = parse_dataset(papers_feat_window, feature_begin, feature_end, counter)
    (observed_samples, censored_samples) = generate_samples(papers_obs_window, censoring_ratio, W, C)
    (X, Y, T) = extract_features(W, C, P, I, observed_samples, censored_samples)
    X_list = [X]
    if (not single_snapshot):
        for t in range((feature_end - delta), feature_begin, (- delta)):
            (W, C, I, P) = parse_dataset(papers_feat_window, feature_begin, t, counter)
            (X, _, _) = extract_features(W, C, P, I, observed_samples, censored_samples)
            X_list = ([X] + X_list)
        for i in range(1, len(X_list)):
            X_list[i] -= X_list[(i - 1)]
    scaler = MinMaxScaler(copy=False)
    for X in X_list:
        scaler.fit_transform(X)
    X = np.stack(X_list, axis=1)
    T = (T - observation_begin)
    return (X, Y, T)

examples  ||  representativeness  ||  number of lines  || number of comments   ||  relevancy  
example1  ||          2           ||        29         ||         0        ||        0.06896551724137931         
example2  ||          2           ||        48         ||         0        ||        0.041666666666666664         
example3  ||          2           ||        25         ||         0        ||        0.08         

avg       ||          18.181818181818183           ||        34.0         ||         0.0        ||         6.354406130268199        

idx = 0:------------------- similar code ------------------ index = 5, score = 1.0 
def run(delta, observation_window, n_snapshots, censoring_ratio=0.5, single_snapshot=False):
    logging.basicConfig(level=logging.INFO, format='%(asctime)s: %(message)s', datefmt='%H:%M:%S')
    with open('data/delicious/user_contacts-timestamps.dat') as usr_usr:
        usr_dataset = usr_usr.read().splitlines()
    with open('data/delicious/user_taggedbookmarks-timestamps.dat') as usr_bm_tg:
        usr_bm_tg_dataset = usr_bm_tg.read().splitlines()
    delta = timestamp_delta_generator(months=delta)
    observation_end = datetime(2010, 10, 1).timestamp()
    observation_begin = (observation_end - timestamp_delta_generator(months=observation_window))
    feature_end = observation_begin
    feature_begin = (feature_end - (n_snapshots * delta))
    indexer = generate_indexer(usr_dataset, usr_bm_tg_dataset, feature_begin, feature_end)
    (contact_sparse, save_sparse, attach_sparse) = parse_dataset(usr_dataset, usr_bm_tg_dataset, feature_begin, feature_end, indexer)
    (observed_samples, censored_samples) = sample_generator(usr_dataset, observation_begin, observation_end, contact_sparse, indexer, censoring_ratio)
    (X, Y, T) = extract_features(contact_sparse, save_sparse, attach_sparse, observed_samples, censored_samples)
    X_list = [X]
    if (not single_snapshot):
        for t in range(int((feature_end - delta)), int(feature_begin), (- int(delta))):
            (contact_sparse, save_sparse, attach_sparse) = parse_dataset(usr_dataset, usr_bm_tg_dataset, feature_begin, t, indexer)
            (X, _, _) = extract_features(contact_sparse, save_sparse, attach_sparse, observed_samples, censored_samples)
            X_list = ([X] + X_list)
        for i in range(1, len(X_list)):
            X_list[i] -= X_list[(i - 1)]
    scaler = MinMaxScaler(copy=False)
    for X in X_list:
        scaler.fit_transform(X)
    X = np.stack(X_list, axis=1)
    T /= delta
    return (X, Y, T)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = MinMaxScaler

idx = 1:------------------- similar code ------------------ index = 4, score = 1.0 
def __init__(self, dataset=None, data_idx=None, sliding=None, expand_function=None, output_index=None, method_statistic=0):
    '\n        :param data_idx:\n        :param sliding:\n        :param output_index:\n        :param method_statistic:\n        :param minmax_scaler:\n        '
    self.original_dataset = dataset
    self.dimension = dataset.shape[1]
    self.original_dataset_len = len(dataset)
    self.dataset_len = (self.original_dataset_len - sliding)
    self.train_idx = int((data_idx[0] * self.dataset_len))
    self.train_len = self.train_idx
    self.valid_idx = (self.train_idx + int((data_idx[1] * self.dataset_len)))
    self.valid_len = (self.valid_idx - self.train_idx)
    self.test_idx = self.dataset_len
    self.test_len = ((self.dataset_len - self.train_len) - self.valid_len)
    self.sliding = sliding
    self.expand_function = expand_function
    self.output_index = output_index
    self.method_statistic = method_statistic
    self.minmax_scaler = MinMaxScaler()

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
 = MinMaxScaler()

idx = 2:------------------- similar code ------------------ index = 3, score = 1.0 
if (__name__ == '__main__'):
    parser = build_parser()
    args = parser.parse_args()
    model_name = args.model_name
    model_type = ('double' if (model_name == 'double_albert') else 'siamese')
    checkpoint_dir = args.checkpoint_dir
    log_dir = args.log_dir
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    main_logger = init_logger(log_dir, f'train_main_{model_name}.log')
    test = pd.read_csv(f'{args.data_dir}test.csv')
    train = pd.read_csv(f'{args.data_dir}train.csv')
    for col in TARGETS:
        train[col] = train[col].rank(method='average')
    train[TARGETS] = MinMaxScaler().fit_transform(train[TARGETS])
    y = train[TARGETS].values
    (ids_train, seg_ids_train) = tokenize(train, pretrained_model_str=pretrained_models[model_name])
    (cat_features_train, _) = get_ohe_categorical_features(train, test, 'category')
    device = 'cuda'
    num_workers = 10
    n_folds = 10
    lr = 0.001
    n_epochs = 10
    bs = 2
    grad_accum = 4
    weight_decay = 0.01
    loss_fn = nn.BCEWithLogitsLoss()
    init_seed()
    folds = GroupKFold(n_splits=n_folds).split(X=train['question_body'], groups=train['question_body'])
    oofs = np.zeros((len(train), N_TARGETS))
    main_logger.info(f'Start training model {model_name}...')
    for (fold_id, (train_index, valid_index)) in enumerate(folds):
        main_logger.info(f'Fold {(fold_id + 1)} started at {time.ctime()}')
        fold_logger = init_logger(log_dir, f'train_fold_{(fold_id + 1)}_{model_name}.log')
        train_loader = DataLoader(TextDataset(cat_features_train, ids_train['question'], ids_train['answer'], seg_ids_train['question'], seg_ids_train['answer'], train_index, y), batch_size=bs, shuffle=True, num_workers=num_workers)
        valid_loader = DataLoader(TextDataset(cat_features_train, ids_train['question'], ids_train['answer'], seg_ids_train['question'], seg_ids_train['answer'], valid_index, y), batch_size=bs, shuffle=False, num_workers=num_workers)
        model = models[model_name]()
        optimizer = get_optimizer(model, lr, weight_decay, model_type)
        scheduler = OneCycleLR(optimizer, n_epochs=n_epochs, n_batches=len(train_loader))
        learner = Learner(model, optimizer, train_loader, valid_loader, loss_fn, device, n_epochs, f'{model_name}_fold_{(fold_id + 1)}', checkpoint_dir, scheduler, metric_spec={'spearmanr': spearmanr_torch}, monitor_metric=True, minimize_score=False, logger=fold_logger, grad_accum=grad_accum, batch_step_scheduler=True)
        learner.train()
        oofs[valid_index] = infer(learner.model, valid_loader, learner.best_checkpoint_file, device)
    main_logger.info(f'Finished training {model_name}')
    ix = np.where((train.groupby('question_body')['host'].transform('count') == 1))[0]
    main_logger.info('CVs:')
    main_logger.info(get_cvs(oofs, y, ix))
    os.makedirs('oofs/', exist_ok=True)
    pd.DataFrame(oofs, columns=TARGETS).to_csv(f'oofs/{model_name}_oofs.csv')

------------------- similar code (pruned) ------------------ score = 0.2 
if:
 = MinMaxScaler()

idx = 3:------------------- similar code ------------------ index = 2, score = 1.0 
def run(delta, observation_window, n_snapshots, censoring_ratio=0.5, single_snapshot=False):
    logging.basicConfig(level=logging.INFO, format='%(asctime)s: %(message)s', datefmt='%H:%M:%S')
    conf_list = {'db': ['KDD', 'PKDD', 'ICDM', 'SDM', 'PAKDD', 'SIGMOD', 'VLDB', 'ICDE', 'PODS', 'EDBT', 'SIGIR', 'ECIR', 'ACL', 'WWW', 'CIKM', 'NIPS', 'ICML', 'ECML', 'AAAI', 'IJCAI'], 'th': ['STOC', 'FOCS', 'COLT', 'LICS', 'SCG', 'SODA', 'SPAA', 'PODC', 'ISSAC', 'CRYPTO', 'EUROCRYPT', 'CONCUR', 'ICALP', 'STACS', 'COCO', 'WADS', 'MFCS', 'SWAT', 'ESA', 'IPCO', 'LFCS', 'ALT', 'EUROCOLT', 'WDAG', 'ISTCS', 'FSTTCS', 'LATIN', 'RECOMB', 'CADE', 'ISIT', 'MEGA', 'ASIAN', 'CCCG', 'FCT', 'WG', 'CIAC', 'ICCI', 'CATS', 'COCOON', 'GD', 'ISAAC', 'SIROCCO', 'WEA', 'ALENEX', 'FTP', 'CSL', 'DMTCS']}[path]
    observation_end = 2016
    observation_begin = (observation_end - observation_window)
    feature_end = observation_begin
    feature_begin = (feature_end - (delta * n_snapshots))
    (papers_feat_window, papers_obs_window, counter) = generate_papers('data/dblp/dblp.txt', feature_begin, feature_end, observation_begin, observation_end, conf_list)
    (W, C, I, P) = parse_dataset(papers_feat_window, feature_begin, feature_end, counter)
    (observed_samples, censored_samples) = generate_samples(papers_obs_window, censoring_ratio, W, C)
    (X, Y, T) = extract_features(W, C, P, I, observed_samples, censored_samples)
    X_list = [X]
    if (not single_snapshot):
        for t in range((feature_end - delta), feature_begin, (- delta)):
            (W, C, I, P) = parse_dataset(papers_feat_window, feature_begin, t, counter)
            (X, _, _) = extract_features(W, C, P, I, observed_samples, censored_samples)
            X_list = ([X] + X_list)
        for i in range(1, len(X_list)):
            X_list[i] -= X_list[(i - 1)]
    scaler = MinMaxScaler(copy=False)
    for X in X_list:
        scaler.fit_transform(X)
    X = np.stack(X_list, axis=1)
    T = (T - observation_begin)
    return (X, Y, T)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = MinMaxScaler

idx = 4:------------------- similar code ------------------ index = 1, score = 1.0 
if (__name__ == '__main__'):
    parser = build_parser()
    args = parser.parse_args()
    model_name = args.model_name
    model_type = ('double' if (model_name == 'double_albert') else 'siamese')
    checkpoint_dir = args.checkpoint_dir
    log_dir = args.log_dir
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    main_logger = init_logger(log_dir, f'finetune_main_{model_name}.log')
    test = pd.read_csv(f'{args.data_dir}test.csv')
    train = pd.read_csv(f'{args.data_dir}train.csv')
    for col in TARGETS:
        train[col] = train[col].rank(method='average')
    train[TARGETS] = MinMaxScaler().fit_transform(train[TARGETS])
    y = train[TARGETS].values
    (ids_train, seg_ids_train) = tokenize(train, pretrained_model_str=pretrained_models[model_name])
    (cat_features_train, _) = get_ohe_categorical_features(train, test, 'category')
    device = 'cuda'
    num_workers = 10
    n_folds = 10
    lr = 1e-05
    n_epochs = 10
    bs = 2
    grad_accum = 4
    weight_decay = 0.01
    loss_fn = nn.BCEWithLogitsLoss()
    init_seed()
    folds = GroupKFold(n_splits=n_folds).split(X=train['question_body'], groups=train['question_body'])
    oofs = np.zeros((len(train), N_TARGETS))
    main_logger.info(f'Start finetuning model {model_name}...')
    for (fold_id, (train_index, valid_index)) in enumerate(folds):
        main_logger.info(f'Fold {(fold_id + 1)} started at {time.ctime()}')
        fold_logger = init_logger(log_dir, f'finetune_fold_{(fold_id + 1)}_{model_name}.log')
        loader = DataLoader(TextDataset(cat_features_train, ids_train['question'], ids_train['answer'], seg_ids_train['question'], seg_ids_train['answer'], np.arange(len(train)), y), batch_size=bs, shuffle=False, num_workers=num_workers)
        model = models[model_name]()
        checkpoint_file = f'{checkpoint_dir}{model_name}_fold_{(fold_id + 1)}_best.pth'
        fold_logger.info(f'Precompute transformer outputs for model {model_name}...')
        (q_outputs, a_outputs) = get_model_outputs(model, loader, checkpoint_file, device, model_type)
        train_loader = DataLoader(TransformerOutputDataset(cat_features_train, q_outputs, a_outputs, train_index, y), batch_size=bs, shuffle=True, num_workers=num_workers)
        valid_loader = DataLoader(TransformerOutputDataset(cat_features_train, q_outputs, a_outputs, valid_index, y), batch_size=bs, shuffle=False, num_workers=num_workers, drop_last=False)
        optimizer = AdamW(get_optimizer_param_groups(model.head, lr, weight_decay))
        learner = Learner(model.head, optimizer, train_loader, valid_loader, loss_fn, device, n_epochs, f'{model_name}_head_fold_{(fold_id + 1)}', checkpoint_dir, scheduler=None, metric_spec={'spearmanr': spearmanr_torch}, monitor_metric=True, minimize_score=False, logger=fold_logger, grad_accum=grad_accum, batch_step_scheduler=False, eval_at_start=True)
        learner.train()
        oofs[valid_index] = infer(learner.model, valid_loader, learner.best_checkpoint_file, device)
        head_checkpoint_file = f'{checkpoint_dir}{model_name}_head_fold_{(fold_id + 1)}_best.pth'
        checkpoint = torch.load(head_checkpoint_file)
        model.head.load_state_dict(checkpoint['model_state_dict'])
        model.half()
        tuned_checkpoint_file = f'{checkpoint_dir}{model_name}_tuned_fold_{(fold_id + 1)}_best.pth'
        torch.save({'model_state_dict': model.state_dict()}, tuned_checkpoint_file)
    main_logger.info(f'Finished tuning {model_name}')
    ix = np.where((train.groupby('question_body')['host'].transform('count') == 1))[0]
    main_logger.info('CVs:')
    main_logger.info(get_cvs(oofs, y, ix))
    os.makedirs('oofs/', exist_ok=True)
    pd.DataFrame(oofs, columns=TARGETS).to_csv(f'oofs/{model_name}_tuned_oofs.csv')

------------------- similar code (pruned) ------------------ score = 0.2 
if:
 = MinMaxScaler()

idx = 5:------------------- similar code ------------------ index = 0, score = 1.0 
def run(delta, observation_window, n_snapshots, censoring_ratio=0.5, single_snapshot=False):
    logging.basicConfig(level=logging.INFO, format='%(asctime)s: %(message)s', datefmt='%H:%M:%S')
    with open('data/movielens/user_ratedmovies-timestamps.dat') as user_rates_movies_ds:
        user_rates_movies_ds = user_rates_movies_ds.read().splitlines()
    with open('data/movielens/user_taggedmovies-timestamps.dat') as user_tags_movies_ds:
        user_tags_movies_ds = user_tags_movies_ds.read().splitlines()
    with open('data/movielens/movie_actors.dat', encoding='latin-1') as movie_actor_ds:
        movie_actor_ds = movie_actor_ds.read().splitlines()
    with open('data/movielens/movie_directors.dat', encoding='latin-1') as movie_director_ds:
        movie_director_ds = movie_director_ds.read().splitlines()
    with open('data/movielens/movie_genres.dat') as movie_genre_ds:
        movie_genre_ds = movie_genre_ds.read().splitlines()
    with open('data/movielens/movie_countries.dat') as movie_countries_ds:
        movie_countries_ds = movie_countries_ds.read().splitlines()
    delta = timestamp_delta_generator(months=delta)
    observation_end = datetime(2009, 1, 1).timestamp()
    observation_begin = (observation_end - timestamp_delta_generator(months=observation_window))
    feature_end = observation_begin
    feature_begin = (feature_end - (n_snapshots * delta))
    indexer = generate_indexer(user_rates_movies_ds, user_tags_movies_ds, movie_actor_ds, movie_director_ds, movie_genre_ds, movie_countries_ds, feature_begin, feature_end)
    (rate_sparse, attach_sparse, played_by_sparse, directed_by_sparse, has_genre_sparse, produced_in_sparse) = parse_dataset(user_rates_movies_ds, user_tags_movies_ds, movie_actor_ds, movie_director_ds, movie_genre_ds, movie_countries_ds, feature_begin, feature_end, indexer)
    (observed_samples, censored_samples) = sample_generator(user_rates_movies_ds, observation_begin, observation_end, rate_sparse, indexer, censoring_ratio)
    (X, Y, T) = extract_features(rate_sparse, attach_sparse, played_by_sparse, directed_by_sparse, has_genre_sparse, produced_in_sparse, observed_samples, censored_samples)
    X_list = [X]
    if (not single_snapshot):
        for t in range(int((feature_end - delta)), int(feature_begin), (- int(delta))):
            (rate_sparse, attach_sparse, played_by_sparse, directed_by_sparse, has_genre_sparse, produced_in_sparse) = parse_dataset(user_rates_movies_ds, user_tags_movies_ds, movie_actor_ds, movie_director_ds, movie_genre_ds, movie_countries_ds, feature_begin, t, indexer)
            (X, _, _) = extract_features(rate_sparse, attach_sparse, played_by_sparse, directed_by_sparse, has_genre_sparse, produced_in_sparse, observed_samples, censored_samples)
            X_list = ([X] + X_list)
        for i in range(1, len(X_list)):
            X_list[i] -= X_list[(i - 1)]
    scaler = MinMaxScaler(copy=False)
    for X in X_list:
        scaler.fit_transform(X)
    X = np.stack(X_list, axis=1)
    T /= delta
    return (X, Y, T)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = MinMaxScaler

