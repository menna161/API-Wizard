------------------------- example 1 ------------------------ 
def init_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear)):
            kaiming_init(m)
    for m in self.modules():
        if isinstance(m, nn.ConvTranspose2d):
            normal_init(m, std=0.001)
    nn.init.constant_(self.deconv2.bias, (- np.log((0.99 / 0.01))))

------------------------- example 2 ------------------------ 
def init_weights(self):
    super(ConvFCBBoxHead, self).init_weights()
    for module_list in [self.shared_fcs, self.cls_fcs, self.reg_fcs]:
        for m in module_list.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

------------------------- example 3 ------------------------ 
def _init_weights(self, module):
    ' Initialize the weights '
    if isinstance(module, nn.Linear):
        module.weight.data.normal_(mean=0.0, std=0.02)
    elif isinstance(module, nn.LayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if (isinstance(module, nn.Linear) and (module.bias is not None)):
        module.bias.data.zero_()

------------------------- example 4 ------------------------ 
def prune_layer(layer, index, dim=None):
    ' Prune a Conv1D or nn.Linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    '
    if isinstance(layer, nn.Linear):
        return prune_linear_layer(layer, index, dim=(0 if (dim is None) else dim))
    elif isinstance(layer, Conv1D):
        return prune_conv1d_layer(layer, index, dim=(1 if (dim is None) else dim))
    else:
// your code ...


------------------------- example 5 ------------------------ 
def weights_normal_init(model, dev=0.01):
    if isinstance(model, list):
        for m in model:
            weights_normal_init(m, dev)
    else:
        for m in model.modules():
            if isinstance(m, nn.Conv2d):
                m.weight.data.normal_(0.0, dev)
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0.0, dev)

examples  ||  representativeness  ||  number of lines  || number of comments 
example1  ||          3           ||        8         ||         0        
example2  ||          7           ||        7         ||         0        
example3  ||          2           ||        9         ||         0        
example4  ||          2           ||        8         ||         1        
example5  ||          4           ||        10         ||         0        

avg       ||          3.6           ||        8.4         ||         0.2        

idx = 0:------------------- similar code ------------------ index = 138, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear)):
            kaiming_init(m)
    for m in self.modules():
        if isinstance(m, nn.ConvTranspose2d):
            normal_init(m, std=0.001)
    nn.init.constant_(self.deconv2.bias, (- np.log((0.99 / 0.01))))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if ( or  ... ( ... , nn.Linear)):
idx = 1:------------------- similar code ------------------ index = 135, score = 7.0 
def init_normal(m):
    if (type(m) == nn.Linear):
        nn.init.uniform_(m.weight, 0, 0.01)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if ( == nn.Linear):
idx = 2:------------------- similar code ------------------ index = 78, score = 7.0 
def init_weights(self):
    super(ConvFCBBoxHead, self).init_weights()
    for module_list in [self.shared_fcs, self.cls_fcs, self.reg_fcs]:
        for m in module_list.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        for  ...  in:
            if  ... ( ... , nn.Linear):
idx = 3:------------------- similar code ------------------ index = 152, score = 7.0 
def __init__(self, word_embeddings_out_dim: int):
    super(Duet, self).__init__()
    NUM_HIDDEN_NODES = word_embeddings_out_dim
    POOLING_KERNEL_WIDTH_QUERY = 18
    POOLING_KERNEL_WIDTH_DOC = 100
    DROPOUT_RATE = 0
    NUM_POOLING_WINDOWS_DOC = 99
    MAX_DOC_TERMS = 2000
    MAX_QUERY_TERMS = 30
    self.cosine_module = CosineMatrixAttention()
    self.duet_local = nn.Sequential(nn.Conv1d(MAX_DOC_TERMS, NUM_HIDDEN_NODES, kernel_size=1), nn.ReLU(), Flatten(), nn.Dropout(p=DROPOUT_RATE), nn.Linear((NUM_HIDDEN_NODES * MAX_QUERY_TERMS), NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE), nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE))
    self.duet_dist_q = nn.Sequential(nn.Conv1d(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES, kernel_size=3), nn.ReLU(), nn.MaxPool1d(POOLING_KERNEL_WIDTH_QUERY), Flatten(), nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES), nn.ReLU())
    self.duet_dist_d = nn.Sequential(nn.Conv1d(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES, kernel_size=3), nn.ReLU(), nn.MaxPool1d(POOLING_KERNEL_WIDTH_DOC, stride=1), nn.Conv1d(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES, kernel_size=1), nn.ReLU())
    self.duet_dist = nn.Sequential(Flatten(), nn.Dropout(p=DROPOUT_RATE), nn.Linear((NUM_HIDDEN_NODES * NUM_POOLING_WINDOWS_DOC), NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE), nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE))
    self.duet_comb = nn.Sequential(nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE), nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE), nn.Linear(NUM_HIDDEN_NODES, 1), nn.ReLU())

    def init_normal(m):
        if (type(m) == nn.Linear):
            nn.init.uniform_(m.weight, 0, 0.01)
    self.duet_comb.apply(init_normal)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
 =  ... . ... (,,,, nn,,,,,)
    def  ... ( ... ):
        if ( ==  ... .Linear):
idx = 4:------------------- similar code ------------------ index = 41, score = 7.0 
def _init_weights(self, module):
    ' Initialize the weights '
    if isinstance(module, nn.Linear):
        torch.nn.init.xavier_uniform_(module.weight.data)
    elif isinstance(module, nn.LayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if (isinstance(module, nn.Linear) and (module.bias is not None)):
        module.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if  ... ( ... , nn.Linear):
idx = 5:------------------- similar code ------------------ index = 132, score = 7.0 
def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):
    super(DenseNet, self).__init__()
    self.features = nn.Sequential(OrderedDict([('conv0', nn.Conv2d(4, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)), ('norm0', nn.BatchNorm2d(num_init_features)), ('relu0', nn.ReLU(inplace=True)), ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))
    num_features = num_init_features
    for (i, num_layers) in enumerate(block_config):
        block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)
        self.features.add_module(('denseblock%d' % (i + 1)), block)
        num_features = (num_features + (num_layers * growth_rate))
        if (i != (len(block_config) - 1)):
            trans = _Transition(num_input_features=num_features, num_output_features=(num_features // 2))
            self.features.add_module(('transition%d' % (i + 1)), trans)
            num_features = (num_features // 2)
    self.features.add_module('norm5', nn.BatchNorm2d(num_features))
    self.classifier = nn.Linear(num_features, num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal(m.weight.data)
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
 = nn
    for  ...  in:
        if:        elif  ... ( ... ,  ... .Linear):
idx = 6:------------------- similar code ------------------ index = 168, score = 7.0 
def _init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 7:------------------- similar code ------------------ index = 200, score = 6.0 
def _init_weights(self, module):
    ' Initialize the weights '
    if isinstance(module, nn.Linear):
        module.weight.data.normal_(mean=0.0, std=0.02)
    elif isinstance(module, nn.LayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if (isinstance(module, nn.Linear) and (module.bias is not None)):
        module.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if  ... ( ... , nn.Linear):
idx = 8:------------------- similar code ------------------ index = 73, score = 6.0 
def weights_init(m):
    'custom weights initialization.'
    cname = m.__class__
    if ((cname == nn.Linear) or (cname == nn.Conv2d) or (cname == nn.ConvTranspose2d)):
        m.weight.data.normal_(0.0, 0.02)
    elif (cname == nn.BatchNorm2d):
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)
    else:
        print(('%s is not initialized.' % cname))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if (( ...  == nn.Linear) or or):
idx = 9:------------------- similar code ------------------ index = 172, score = 6.0 
def _initialize_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear)):
            m.weight.data = nn.init.kaiming_normal_(m.weight.data)
            if (m.bias is not None):
                m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if ( or  ... ( ... , nn.Linear)):
idx = 10:------------------- similar code ------------------ index = 53, score = 6.0 
def _initialize_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear)):
            m.weight.data = nn.init.kaiming_normal_(m.weight.data)
            if (m.bias is not None):
                m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if ( or  ... ( ... , nn.Linear)):
idx = 11:------------------- similar code ------------------ index = 157, score = 6.0 
def _initialize_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear)):
            m.weight.data = nn.init.kaiming_normal_(m.weight.data)
            if (m.bias is not None):
                m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if ( or  ... ( ... , nn.Linear)):
idx = 12:------------------- similar code ------------------ index = 61, score = 6.0 
def _initialize_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear)):
            m.weight.data = nn.init.kaiming_normal_(m.weight.data)
            if (m.bias is not None):
                m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if ( or  ... ( ... , nn.Linear)):
idx = 13:------------------- similar code ------------------ index = 36, score = 6.0 
def init_weights(self, pretrained=None):
    if isinstance(pretrained, str):
        logger = get_root_logger()
        load_checkpoint(self, pretrained, strict=False, logger=logger)
    elif (pretrained is None):
        for m in self.features.modules():
            if isinstance(m, nn.Conv2d):
                kaiming_init(m)
            elif isinstance(m, nn.BatchNorm2d):
                constant_init(m, 1)
            elif isinstance(m, nn.Linear):
                normal_init(m, std=0.01)
    else:
        raise TypeError('pretrained must be a str or None')
    for m in self.extra.modules():
        if isinstance(m, nn.Conv2d):
            xavier_init(m, distribution='uniform')
    constant_init(self.l2_norm, self.l2_norm.scale)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:    elif:
        for  ...  in:
            if:            elif  ... ( ... , nn.Linear):
idx = 14:------------------- similar code ------------------ index = 76, score = 6.0 
def prune_layer(layer, index, dim=None):
    ' Prune a Conv1D or nn.Linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    '
    if isinstance(layer, nn.Linear):
        return prune_linear_layer(layer, index, dim=(0 if (dim is None) else dim))
    elif isinstance(layer, Conv1D):
        return prune_conv1d_layer(layer, index, dim=(1 if (dim is None) else dim))
    else:
        raise ValueError("Can't prune layer of class {}".format(layer.__class__))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if  ... ( ... , nn.Linear):
idx = 15:------------------- similar code ------------------ index = 154, score = 6.0 
def weights_normal_init(model, dev=0.01):
    if isinstance(model, list):
        for m in model:
            weights_normal_init(m, dev)
    else:
        for m in model.modules():
            if isinstance(m, nn.Conv2d):
                m.weight.data.normal_(0.0, dev)
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0.0, dev)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:    else:
        for  ...  in:
            if:            elif  ... ( ... , nn.Linear):
idx = 16:------------------- similar code ------------------ index = 151, score = 6.0 
def _init_weights(self, module):
    ' Initialize the weights '
    if isinstance(module, (nn.Linear, nn.Embedding)):
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
    elif isinstance(module, AlbertLayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if (isinstance(module, nn.Linear) and (module.bias is not None)):
        module.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if  ... ( ... , (nn.Linear,)):
idx = 17:------------------- similar code ------------------ index = 56, score = 6.0 
def __init__(self, relu6=True):
    super(MobileNet, self).__init__()

    def relu(relu6):
        if relu6:
            return nn.ReLU6(inplace=True)
        else:
            return nn.ReLU(inplace=True)

    def conv_bn(inp, oup, stride, relu6):
        return nn.Sequential(nn.Conv2d(inp, oup, 3, stride, 1, bias=False), nn.BatchNorm2d(oup), relu(relu6))

    def conv_dw(inp, oup, stride, relu6):
        return nn.Sequential(nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False), nn.BatchNorm2d(inp), relu(relu6), nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), relu(relu6))
    self.model = nn.Sequential(conv_bn(3, 32, 2, relu6), conv_dw(32, 64, 1, relu6), conv_dw(64, 128, 2, relu6), conv_dw(128, 128, 1, relu6), conv_dw(128, 256, 2, relu6), conv_dw(256, 256, 1, relu6), conv_dw(256, 512, 2, relu6), conv_dw(512, 512, 1, relu6), conv_dw(512, 512, 1, relu6), conv_dw(512, 512, 1, relu6), conv_dw(512, 512, 1, relu6), conv_dw(512, 512, 1, relu6), conv_dw(512, 1024, 2, relu6), conv_dw(1024, 1024, 1, relu6), nn.AvgPool2d(7))
    self.fc = nn.Linear(1024, 1000)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    def  ... ( ... ):
        if  ... :
            return nn
 =  ... .Linear

idx = 18:------------------- similar code ------------------ index = 137, score = 6.0 
def init_weights(self):
    normal_init(self.fc_cls, std=0.01)
    normal_init(self.fc_reg, std=0.001)
    for m in self.fc_branch.modules():
        if isinstance(m, nn.Linear):
            xavier_init(m, distribution='uniform')

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 19:------------------- similar code ------------------ index = 21, score = 6.0 
def _init_weights(self, module):
    ' Initialize the weights '
    if isinstance(module, (nn.Linear, nn.Embedding)):
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
    elif isinstance(module, AlbertLayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if (isinstance(module, nn.Linear) and (module.bias is not None)):
        module.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if  ... ( ... , (nn.Linear,)):
idx = 20:------------------- similar code ------------------ index = 116, score = 6.0 
def weights_normal_init(model, dev=0.01):
    if isinstance(model, list):
        for m in model:
            weights_normal_init(m, dev)
    else:
        for m in model.modules():
            if isinstance(m, nn.Conv2d):
                m.weight.data.normal_(0.0, dev)
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0.0, dev)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:    else:
        for  ...  in:
            if:            elif  ... ( ... , nn.Linear):
idx = 21:------------------- similar code ------------------ index = 190, score = 6.0 
def _init_weights(self, module):
    ' Initialize the weights '
    if isinstance(module, (nn.Linear, nn.Embedding)):
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
    elif isinstance(module, BertLayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if (isinstance(module, nn.Linear) and (module.bias is not None)):
        module.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if  ... ( ... , (nn.Linear,)):
idx = 22:------------------- similar code ------------------ index = 179, score = 6.0 
def weights_normal_init(model, dev=0.01):
    if isinstance(model, list):
        for m in model:
            weights_normal_init(m, dev)
    else:
        for m in model.modules():
            if isinstance(m, nn.Conv2d):
                m.weight.data.normal_(0.0, dev)
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0.0, dev)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:    else:
        for  ...  in:
            if:            elif  ... ( ... , nn.Linear):
idx = 23:------------------- similar code ------------------ index = 93, score = 6.0 
def __init__(self, maxdisp):
    super(PSMNet, self).__init__()
    self.maxdisp = maxdisp
    self.feature_extraction = feature_extraction()
    self.feature_disp_pre = SparseConvNet()
    self.dres0 = nn.Sequential(convbn_3d(96, 32, 3, 1, 1), nn.ReLU(inplace=True), convbn_3d(32, 32, 3, 1, 1), nn.ReLU(inplace=True))
    self.dres1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1), nn.ReLU(inplace=True), convbn_3d(32, 32, 3, 1, 1))
    self.dres2 = hourglass(32)
    self.dres3 = hourglass(32)
    self.dres4 = hourglass(32)
    self.classif1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1), nn.ReLU(inplace=True), nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1, bias=False))
    self.classif2 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1), nn.ReLU(inplace=True), nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1, bias=False))
    self.classif3 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1), nn.ReLU(inplace=True), nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1, bias=False))
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = ((m.kernel_size[0] * m.kernel_size[1]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
        elif isinstance(m, nn.Conv3d):
            n = (((m.kernel_size[0] * m.kernel_size[1]) * m.kernel_size[2]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm3d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 24:------------------- similar code ------------------ index = 62, score = 6.0 
def __init__(self):
    super(Dense, self).__init__()
    self.fc1 = nn.Linear(784, 200)
    self.fc2 = nn.Linear(200, 100)
    self.fc3 = nn.Linear(100, 70)
    self.fc4 = nn.Linear(70, 30)
    self.fc5 = nn.Linear(30, 10)
    torch.nn.init.xavier_uniform_(self.fc1.weight)
    torch.nn.init.xavier_uniform_(self.fc2.weight)
    torch.nn.init.xavier_uniform_(self.fc3.weight)
    torch.nn.init.xavier_uniform_(self.fc4.weight)
    torch.nn.init.xavier_uniform_(self.fc5.weight)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = nn.Linear

idx = 25:------------------- similar code ------------------ index = 68, score = 6.0 
def __init__(self, bert_model: Union[(str, AutoModel)], dropout: float=0.0, trainable: bool=True, sample_train_type='lambdaloss', sample_n=1, sample_context='ck', top_k_chunks=3, chunk_size=50, overlap=7, padding_idx: int=0) -> None:
    super().__init__()
    if isinstance(bert_model, str):
        self.bert_model = AutoModel.from_pretrained(bert_model)
    else:
        self.bert_model = bert_model
    for p in self.bert_model.parameters():
        p.requires_grad = trainable
    self._classification_layer = torch.nn.Linear(self.bert_model.config.hidden_size, 1)
    self.top_k_chunks = top_k_chunks
    self.top_k_scoring = nn.Parameter(torch.full([1, self.top_k_chunks], 1, dtype=torch.float32, requires_grad=True))
    self.padding_idx = padding_idx
    self.chunk_size = chunk_size
    self.overlap = overlap
    self.extended_chunk_size = (self.chunk_size + (2 * self.overlap))
    self.sample_train_type = sample_train_type
    self.sample_n = sample_n
    self.sample_context = sample_context
    if (self.sample_context == 'ck'):
        i = 3
        self.sample_cnn3 = nn.Sequential(nn.ConstantPad1d((0, (i - 1)), 0), nn.Conv1d(kernel_size=i, in_channels=self.bert_model.config.dim, out_channels=self.bert_model.config.dim), nn.ReLU())
    elif (self.sample_context == 'ck-small'):
        i = 3
        self.sample_projector = nn.Linear(self.bert_model.config.dim, 384)
        self.sample_cnn3 = nn.Sequential(nn.ConstantPad1d((0, (i - 1)), 0), nn.Conv1d(kernel_size=i, in_channels=384, out_channels=128), nn.ReLU())
    elif (self.sample_context == 'tk'):
        self.tk_projector = nn.Linear(self.bert_model.config.dim, 384)
        encoder_layer = nn.TransformerEncoderLayer(384, 8, dim_feedforward=384, dropout=0)
        self.tk_contextualizer = nn.TransformerEncoder(encoder_layer, 1, norm=None)
        self.tK_mixer = nn.Parameter(torch.full([1], 0.5, dtype=torch.float32, requires_grad=True))
    self.sampling_binweights = nn.Linear(11, 1, bias=True)
    torch.nn.init.uniform_(self.sampling_binweights.weight, (- 0.01), 0.01)
    self.kernel_alpha_scaler = nn.Parameter(torch.full([1, 1, 11], 1, dtype=torch.float32, requires_grad=True))
    self.register_buffer('mu', nn.Parameter(torch.tensor([1.0, 0.9, 0.7, 0.5, 0.3, 0.1, (- 0.1), (- 0.3), (- 0.5), (- 0.7), (- 0.9)]), requires_grad=False).view(1, 1, 1, (- 1)))
    self.register_buffer('sigma', nn.Parameter(torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]), requires_grad=False).view(1, 1, 1, (- 1)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... () -> None:
    if:    elif:
 = nn.Linear

idx = 26:------------------- similar code ------------------ index = 30, score = 6.0 
def __init__(self, input_num_channels: int, internal_size: int, output_size: int, crop_size: int, use_convolution: bool, final_nonlinearity: bool):
    super(MapDistributionEmbedder, self).__init__()
    self._current_position_transformer: map_transformer.MapTransformer = map_transformer.MapTransformer(source_map_size=environment_util.ENVIRONMENT_WIDTH, dest_map_size=environment_util.PADDED_WIDTH, world_size_px=environment_util.ENVIRONMENT_WIDTH, world_size_m=environment_util.ENVIRONMENT_WIDTH)
    if (torch.cuda.device_count() >= 1):
        self._current_position_transformer = self._current_position_transformer.cuda()
    self._final_nonlinearity = final_nonlinearity
    self._convolution_layer = None
    linear_in_size = ((input_num_channels * crop_size) * crop_size)
    if use_convolution:
        self._convolution_layer = nn.Conv2d(input_num_channels, (2 * input_num_channels), kernel_size=3, padding=1, stride=2, bias=True)
        self._convolution_activation = nn.LeakyReLU()
        self._convolution_normalization = nn.InstanceNorm2d((2 * input_num_channels))
        linear_in_size = int((((2 * input_num_channels) * math.ceil((crop_size / 2))) * math.ceil((crop_size / 2))))
    self._state_embedding_layer_1: nn.Module = nn.Linear(linear_in_size, internal_size)
    torch.nn.init.orthogonal_(self._state_embedding_layer_1.weight, torch.nn.init.calculate_gain('leaky_relu'))
    self._state_embedding_layer_1.bias.data.fill_(0)
    self._nonlinearity: nn.Module = nn.LeakyReLU()
    self._state_embedding_layer_2: nn.Module = nn.Linear((internal_size + linear_in_size), output_size)
    torch.nn.init.orthogonal_(self._state_embedding_layer_2.weight, torch.nn.init.calculate_gain('leaky_relu'))
    self._state_embedding_layer_2.bias.data.fill_(0)
    self._crop_size: int = crop_size

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
: = nn.Linear

idx = 27:------------------- similar code ------------------ index = 54, score = 6.0 
def __init__(self, word_embeddings_out_dim: int, n_grams: int, n_kernels: int, conv_out_dim: int):
    super(Conv_KNRM, self).__init__()
    self.mu = Variable(torch.cuda.FloatTensor(self.kernel_mus(n_kernels)), requires_grad=False).view(1, 1, 1, n_kernels)
    self.sigma = Variable(torch.cuda.FloatTensor(self.kernel_sigmas(n_kernels)), requires_grad=False).view(1, 1, 1, n_kernels)
    self.convolutions = []
    for i in range(1, (n_grams + 1)):
        self.convolutions.append(nn.Sequential(nn.ConstantPad1d((0, (i - 1)), 0), nn.Conv1d(kernel_size=i, in_channels=word_embeddings_out_dim, out_channels=conv_out_dim), nn.ReLU()))
    self.convolutions = nn.ModuleList(self.convolutions)
    self.cosine_module = CosineMatrixAttention()
    self.dense = nn.Linear(((n_kernels * n_grams) * n_grams), 1, bias=False)
    torch.nn.init.uniform_(self.dense.weight, (- 0.014), 0.014)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 28:------------------- similar code ------------------ index = 180, score = 6.0 
def __init__(self, cfg) -> None:
    super().__init__(cfg)
    if isinstance(cfg.bert_model, str):
        self.bert_model = AutoModel.from_pretrained(cfg.bert_model)
    else:
        self.bert_model = cfg.bert_model
    self._classification_layer = torch.nn.Linear(self.bert_model.config.hidden_size, 1)
    self.top_k_chunks = cfg.top_k_chunks
    self.top_k_scoring = nn.Parameter(torch.full([1, self.top_k_chunks], 1, dtype=torch.float32, requires_grad=True))
    self.padding_idx = cfg.padding_idx
    self.chunk_size = cfg.chunk_size
    self.overlap = cfg.overlap
    self.extended_chunk_size = (self.chunk_size + (2 * self.overlap))
    self.sample_n = cfg.sample_n
    self.sample_context = cfg.sample_context
    if (self.sample_context == 'ck'):
        i = 3
        self.sample_cnn3 = nn.Sequential(nn.ConstantPad1d((0, (i - 1)), 0), nn.Conv1d(kernel_size=i, in_channels=self.bert_model.config.dim, out_channels=self.bert_model.config.dim), nn.ReLU())
    elif (self.sample_context == 'ck-small'):
        i = 3
        self.sample_projector = nn.Linear(self.bert_model.config.dim, 384)
        self.sample_cnn3 = nn.Sequential(nn.ConstantPad1d((0, (i - 1)), 0), nn.Conv1d(kernel_size=i, in_channels=384, out_channels=128), nn.ReLU())
    self.sampling_binweights = nn.Linear(11, 1, bias=True)
    torch.nn.init.uniform_(self.sampling_binweights.weight, (- 0.01), 0.01)
    self.kernel_alpha_scaler = nn.Parameter(torch.full([1, 1, 11], 1, dtype=torch.float32, requires_grad=True))
    self.register_buffer('mu', nn.Parameter(torch.tensor([1.0, 0.9, 0.7, 0.5, 0.3, 0.1, (- 0.1), (- 0.3), (- 0.5), (- 0.7), (- 0.9)]), requires_grad=False).view(1, 1, 1, (- 1)))
    self.register_buffer('sigma', nn.Parameter(torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]), requires_grad=False).view(1, 1, 1, (- 1)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... () -> None:
    if:    elif:
 = nn.Linear

idx = 29:------------------- similar code ------------------ index = 109, score = 6.0 
def __init__(self, n_kernels: int):
    super(KNRM, self).__init__()
    self.mu = Variable(torch.cuda.FloatTensor(self.kernel_mus(n_kernels)), requires_grad=False).view(1, 1, 1, n_kernels)
    self.sigma = Variable(torch.cuda.FloatTensor(self.kernel_sigmas(n_kernels)), requires_grad=False).view(1, 1, 1, n_kernels)
    self.cosine_module = CosineMatrixAttention()
    self.dense = nn.Linear(n_kernels, 1, bias=False)
    torch.nn.init.uniform_(self.dense.weight, (- 0.014), 0.014)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 30:------------------- similar code ------------------ index = 192, score = 6.0 
def __init__(self, args: model_args.ModelArgs, input_vocabulary: List[str], auxiliaries: List[auxiliary.Auxiliary], load_pretrained: bool=True, end_to_end: bool=False):
    super(ActionGeneratorModel, self).__init__()
    self._args: model_args.ModelArgs = args
    self._end_to_end = end_to_end
    self._plan_predictor: Optional[plan_predictor_model.PlanPredictorModel] = None
    if (self._end_to_end or load_pretrained):
        self._plan_predictor: plan_predictor_model.PlanPredictorModel = plan_predictor_model.PlanPredictorModel(args, input_vocabulary, auxiliaries)
    self._output_layer = None
    self._rnn = None
    self._action_embedder = None
    if self._args.get_decoder_args().use_recurrence():
        self._action_embedder: word_embedder.WordEmbedder = word_embedder.WordEmbedder(self._args.get_decoder_args().get_action_embedding_size(), [str(action) for action in agent_actions.AGENT_ACTIONS], add_unk=False)
        self._rnn: nn.Module = nn.LSTM((self._args.get_decoder_args().get_state_internal_size() + self._args.get_decoder_args().get_action_embedding_size()), self._args.get_decoder_args().get_hidden_size(), self._args.get_decoder_args().get_num_layers(), batch_first=True)
        self._output_layer: nn.Module = nn.Linear((self._args.get_decoder_args().get_hidden_size() + self._args.get_decoder_args().get_state_internal_size()), len(agent_actions.AGENT_ACTIONS))
        torch.nn.init.orthogonal_(self._output_layer.weight, torch.nn.init.calculate_gain('leaky_relu'))
        self._output_layer.bias.data.fill_(0)
    distribution_num_channels: int = 0
    if self._args.get_decoder_args().use_trajectory_distribution():
        distribution_num_channels += 1
    if self._args.get_decoder_args().use_goal_probabilities():
        distribution_num_channels += 1
    if self._args.get_decoder_args().use_obstacle_probabilities():
        distribution_num_channels += 1
    if self._args.get_decoder_args().use_avoid_probabilities():
        distribution_num_channels += 1
    self._map_distribution_embedder: map_distribution_embedder.MapDistributionEmbedder = map_distribution_embedder.MapDistributionEmbedder(distribution_num_channels, self._args.get_decoder_args().get_state_internal_size(), (self._args.get_decoder_args().get_state_internal_size() if self._args.get_decoder_args().use_recurrence() else len(agent_actions.AGENT_ACTIONS)), self._args.get_decoder_args().get_crop_size(), self._args.get_decoder_args().convolution_encode_map_distributions(), self._args.get_decoder_args().use_recurrence())
    if load_pretrained:
        if self._args.get_decoder_args().pretrained_generator():
            initialization.load_pretrained_parameters(self._args.get_decoder_args().pretrained_action_generator_filepath(), module=self)
        if self._args.get_decoder_args().pretrained_plan_predictor():
            initialization.load_pretrained_parameters(self._args.get_decoder_args().pretrained_plan_predictor_filepath(), module=self._plan_predictor)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
: = nn.Linear

idx = 31:------------------- similar code ------------------ index = 88, score = 5.0 
def __init__(self, pretrained='imagenet', **kwargs):
    super(SeResNext50_9ch_Unet, self).__init__()
    encoder_filters = [64, 256, 512, 1024, 2048]
    decoder_filters = [64, 96, 128, 256, 512]
    self.conv6 = ConvRelu(encoder_filters[(- 1)], decoder_filters[(- 1)])
    self.conv6_2 = nn.Sequential(ConvRelu(((((decoder_filters[(- 1)] + encoder_filters[(- 2)]) + 2) + 27) + 2), decoder_filters[(- 1)]), SCSEModule(decoder_filters[(- 1)], reduction=16, concat=True))
    self.conv7 = ConvRelu((decoder_filters[(- 1)] * 2), decoder_filters[(- 2)])
    self.conv7_2 = nn.Sequential(ConvRelu(((((decoder_filters[(- 2)] + encoder_filters[(- 3)]) + 2) + 27) + 2), decoder_filters[(- 2)]), SCSEModule(decoder_filters[(- 2)], reduction=16, concat=True))
    self.conv8 = ConvRelu((decoder_filters[(- 2)] * 2), decoder_filters[(- 3)])
    self.conv8_2 = nn.Sequential(ConvRelu(((((decoder_filters[(- 3)] + encoder_filters[(- 4)]) + 2) + 27) + 2), decoder_filters[(- 3)]), SCSEModule(decoder_filters[(- 3)], reduction=16, concat=True))
    self.conv9 = ConvRelu((decoder_filters[(- 3)] * 2), decoder_filters[(- 4)])
    self.conv9_2 = nn.Sequential(ConvRelu(((((decoder_filters[(- 4)] + encoder_filters[(- 5)]) + 2) + 27) + 2), decoder_filters[(- 4)]), SCSEModule(decoder_filters[(- 4)], reduction=16, concat=True))
    self.conv10 = ConvRelu(((((decoder_filters[(- 4)] * 2) + 2) + 2) + 27), decoder_filters[(- 5)])
    self.res = nn.Conv2d(((decoder_filters[(- 5)] + 2) + 2), 4, 1, stride=1, padding=0)
    self.off_nadir = nn.Sequential(nn.Linear(encoder_filters[(- 1)], 64), nn.ReLU(inplace=True), nn.Linear(64, 1))
    self._initialize_weights()
    encoder = se_resnext50_32x4d(pretrained=pretrained)
    conv1_new = nn.Conv2d(9, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    _w = encoder.layer0.conv1.state_dict()
    _w['weight'] = torch.cat([(0.8 * _w['weight']), (0.1 * _w['weight']), (0.1 * _w['weight'])], 1)
    conv1_new.load_state_dict(_w)
    self.conv1 = nn.Sequential(conv1_new, encoder.layer0.bn1, encoder.layer0.relu1)
    self.conv2 = nn.Sequential(encoder.pool, encoder.layer1)
    self.conv3 = encoder.layer2
    self.conv4 = encoder.layer3
    self.conv5 = encoder.layer4

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... (nn.Linear,,)

idx = 32:------------------- similar code ------------------ index = 86, score = 5.0 
def __init__(self, dim, coverage=False, attn_type='dot', attn_func='softmax'):
    super(GlobalAttention, self).__init__()
    self.dim = dim
    assert (attn_type in ['dot', 'general', 'mlp']), 'Please select a valid attention type (got {:s}).'.format(attn_type)
    self.attn_type = attn_type
    assert (attn_func in ['softmax', 'sparsemax']), 'Please select a valid attention function.'
    self.attn_func = attn_func
    if (self.attn_type == 'general'):
        self.linear_in = nn.Linear(dim, dim, bias=False)
    elif (self.attn_type == 'mlp'):
        self.linear_context = nn.Linear(dim, dim, bias=False)
        self.linear_query = nn.Linear(dim, dim, bias=True)
        self.v = nn.Linear(dim, 1, bias=False)
    out_bias = (self.attn_type == 'mlp')
    self.linear_out = nn.Linear((dim * 2), dim, bias=out_bias)
    if coverage:
        self.linear_cover = nn.Linear(1, dim, bias=False)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
 = nn.Linear

idx = 33:------------------- similar code ------------------ index = 87, score = 5.0 
def __init__(self, embed_dim, ff_embed_dim, num_heads, dropout, with_external=False, weights_dropout=True):
    super(TransformerLayer, self).__init__()
    self.self_attn = MultiheadAttention(embed_dim, num_heads, dropout, weights_dropout)
    self.fc1 = nn.Linear(embed_dim, ff_embed_dim)
    self.fc2 = nn.Linear(ff_embed_dim, embed_dim)
    self.attn_layer_norm = LayerNorm(embed_dim)
    self.ff_layer_norm = LayerNorm(embed_dim)
    self.with_external = with_external
    self.dropout = dropout
    if self.with_external:
        self.external_attn = MultiheadAttention(embed_dim, num_heads, dropout, weights_dropout)
        self.external_layer_norm = LayerNorm(embed_dim)
    self.reset_parameters()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 34:------------------- similar code ------------------ index = 57, score = 5.0 
def __init__(self, in_dim, hid_dim, out_dim, dropout):
    super(SimpleClassifier, self).__init__()
    layers = [weight_norm(nn.Linear(in_dim, hid_dim), dim=None), nn.ReLU(), nn.Dropout(dropout, inplace=True), weight_norm(nn.Linear(hid_dim, out_dim), dim=None)]
    self.main = nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = [ ... (nn.Linear,),,,]

idx = 35:------------------- similar code ------------------ index = 58, score = 5.0 
def __init__(self, num_embeddings, embedding_dim=50, hidden_size=50, output_size=1, num_layers=1, dropout=0.2):
    super().__init__()
    self.emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)
    self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)
    self.linear = nn.Linear(hidden_size, output_size)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 36:------------------- similar code ------------------ index = 85, score = 5.0 
def __init__(self, word_vec_size, word_vocab_size, word_padding_idx, position_encoding=False, feat_merge='concat', feat_vec_exponent=0.7, feat_vec_size=(- 1), feat_padding_idx=[], feat_vocab_sizes=[], dropout=0, sparse=False, fix_word_vecs=False):
    self._validate_args(feat_merge, feat_vocab_sizes, feat_vec_exponent, feat_vec_size, feat_padding_idx)
    if (feat_padding_idx is None):
        feat_padding_idx = []
    self.word_padding_idx = word_padding_idx
    self.word_vec_size = word_vec_size
    vocab_sizes = [word_vocab_size]
    emb_dims = [word_vec_size]
    pad_indices = [word_padding_idx]
    if (feat_merge == 'sum'):
        feat_dims = ([word_vec_size] * len(feat_vocab_sizes))
    elif (feat_vec_size > 0):
        feat_dims = ([feat_vec_size] * len(feat_vocab_sizes))
    else:
        feat_dims = [int((vocab ** feat_vec_exponent)) for vocab in feat_vocab_sizes]
    vocab_sizes.extend(feat_vocab_sizes)
    emb_dims.extend(feat_dims)
    pad_indices.extend(feat_padding_idx)
    emb_params = zip(vocab_sizes, emb_dims, pad_indices)
    embeddings = [nn.Embedding(vocab, dim, padding_idx=pad, sparse=sparse) for (vocab, dim, pad) in emb_params]
    emb_luts = Elementwise(feat_merge, embeddings)
    self.embedding_size = (sum(emb_dims) if (feat_merge == 'concat') else word_vec_size)
    super(Embeddings, self).__init__()
    self.make_embedding = nn.Sequential()
    self.make_embedding.add_module('emb_luts', emb_luts)
    if ((feat_merge == 'mlp') and (len(feat_vocab_sizes) > 0)):
        in_dim = sum(emb_dims)
        mlp = nn.Sequential(nn.Linear(in_dim, word_vec_size), nn.ReLU())
        self.make_embedding.add_module('mlp', mlp)
    self.position_encoding = position_encoding
    if self.position_encoding:
        pe = PositionalEncoding(dropout, self.embedding_size)
        self.make_embedding.add_module('pe', pe)
    if fix_word_vecs:
        self.word_lut.weight.requires_grad = False

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  =  ... . ... (nn.Linear,)

idx = 37:------------------- similar code ------------------ index = 89, score = 5.0 
def __init__(self, params):
    specs = ResNet.specs[params['model']]
    Model.check_parameters(params, {'name': specs['name'], 'input_shape': (3, 224, 224), 'num_classes': 1000, 'phase': 'training', 'dtype': 'float32'})
    Model.__init__(self, params)
    if (specs['num_layers'] >= 50):
        filter_list = [64, 256, 512, 1024, 2048]
        bottle_neck = True
    else:
        filter_list = [64, 64, 128, 256, 512]
        bottle_neck = False
    self.features = nn.Sequential(nn.Conv2d(3, filter_list[0], kernel_size=7, stride=2, padding=3, bias=False), nn.BatchNorm2d(filter_list[0], eps=2e-05, momentum=0.9, affine=True), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
    num_prev_channels = filter_list[0]
    for i in range(len(specs['units'])):
        self.features.add_module(('stage%d_unit%d' % ((i + 1), 1)), ResnetModule(num_prev_channels, filter_list[(i + 1)], (1 if (i == 0) else 2), False, bottle_neck))
        num_prev_channels = filter_list[(i + 1)]
        for j in range((specs['units'][i] - 1)):
            self.features.add_module(('stage%d_unit%d' % ((i + 1), (j + 2))), ResnetModule(num_prev_channels, filter_list[(i + 1)], 1, True, bottle_neck))
    self.features.add_module('pool1', nn.AvgPool2d(kernel_size=7, padding=0))
    self.num_output_channels = filter_list[(- 1)]
    self.classifier = nn.Sequential(nn.Linear(self.num_output_channels, self.num_classes))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... (nn.Linear)

idx = 38:------------------- similar code ------------------ index = 90, score = 5.0 
def __init__(self, num_shared_convs=0, num_shared_fcs=0, num_cls_convs=0, num_cls_fcs=0, num_reg_convs=0, num_reg_fcs=0, conv_out_channels=256, fc_out_channels=1024, conv_cfg=None, norm_cfg=None, *args, **kwargs):
    super(ConvFCBBoxHead, self).__init__(*args, **kwargs)
    assert ((((((num_shared_convs + num_shared_fcs) + num_cls_convs) + num_cls_fcs) + num_reg_convs) + num_reg_fcs) > 0)
    if ((num_cls_convs > 0) or (num_reg_convs > 0)):
        assert (num_shared_fcs == 0)
    if (not self.with_cls):
        assert ((num_cls_convs == 0) and (num_cls_fcs == 0))
    if (not self.with_reg):
        assert ((num_reg_convs == 0) and (num_reg_fcs == 0))
    self.num_shared_convs = num_shared_convs
    self.num_shared_fcs = num_shared_fcs
    self.num_cls_convs = num_cls_convs
    self.num_cls_fcs = num_cls_fcs
    self.num_reg_convs = num_reg_convs
    self.num_reg_fcs = num_reg_fcs
    self.conv_out_channels = conv_out_channels
    self.fc_out_channels = fc_out_channels
    self.conv_cfg = conv_cfg
    self.norm_cfg = norm_cfg
    (self.shared_convs, self.shared_fcs, last_layer_dim) = self._add_conv_fc_branch(self.num_shared_convs, self.num_shared_fcs, self.in_channels, True)
    self.shared_out_channels = last_layer_dim
    (self.cls_convs, self.cls_fcs, self.cls_last_dim) = self._add_conv_fc_branch(self.num_cls_convs, self.num_cls_fcs, self.shared_out_channels)
    (self.reg_convs, self.reg_fcs, self.reg_last_dim) = self._add_conv_fc_branch(self.num_reg_convs, self.num_reg_fcs, self.shared_out_channels)
    if ((self.num_shared_fcs == 0) and (not self.with_avg_pool)):
        if (self.num_cls_fcs == 0):
            self.cls_last_dim *= self.roi_feat_area
        if (self.num_reg_fcs == 0):
            self.reg_last_dim *= self.roi_feat_area
    self.relu = nn.ReLU(inplace=True)
    if self.with_cls:
        self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes)
    if self.with_reg:
        out_dim_reg = (4 if self.reg_class_agnostic else (4 * self.num_classes))
        self.fc_reg = nn.Linear(self.reg_last_dim, out_dim_reg)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
 = nn.Linear

idx = 39:------------------- similar code ------------------ index = 92, score = 5.0 
def __init__(self, cardinality, depth, nlabels, base_width, widen_factor=4):
    ' Constructor\n\n        Args:\n            cardinality: number of convolution groups.\n            depth: number of layers.\n            nlabels: number of classes\n            base_width: base number of channels in each group.\n            widen_factor: factor to adjust the channel dimensionality\n        '
    super(CifarResNeXt, self).__init__()
    self.cardinality = cardinality
    self.depth = depth
    self.block_depth = ((self.depth - 2) // 9)
    self.base_width = base_width
    self.widen_factor = widen_factor
    self.nlabels = nlabels
    self.output_size = 64
    self.stages = [64, (64 * self.widen_factor), (128 * self.widen_factor), (256 * self.widen_factor)]
    self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)
    self.bn_1 = nn.BatchNorm2d(64)
    self.stage_1 = self.block('stage_1', self.stages[0], self.stages[1], 1)
    self.stage_2 = self.block('stage_2', self.stages[1], self.stages[2], 2)
    self.stage_3 = self.block('stage_3', self.stages[2], self.stages[3], 2)
    self.classifier = nn.Linear(self.stages[3], nlabels)
    init.kaiming_normal(self.classifier.weight)
    for key in self.state_dict():
        if (key.split('.')[(- 1)] == 'weight'):
            if ('conv' in key):
                init.kaiming_normal(self.state_dict()[key], mode='fan_out')
            if ('bn' in key):
                self.state_dict()[key][...] = 1
        elif (key.split('.')[(- 1)] == 'bias'):
            self.state_dict()[key][...] = 0

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 40:------------------- similar code ------------------ index = 55, score = 5.0 
def __init__(self, config):
    super(AlbertForMultipleChoice, self).__init__(config)
    self.bert = AlbertModel(config)
    self.dropout = nn.Dropout(config.hidden_dropout_prob)
    self.classifier = nn.Linear(config.hidden_size, 1)
    self.init_weights()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 41:------------------- similar code ------------------ index = 59, score = 5.0 
def __init__(self, num_tokens, dim, seq_len, depth, emb_dim=None, memory_layers=None, enhanced_recurrence=True, mem_len=None, cmem_len=None, cmem_ratio=4, heads=8, gru_gated_residual=True, mogrify_gru=False, attn_dropout=0.0, ff_glu=False, ff_dropout=0.0, attn_layer_dropout=0.0, reconstruction_attn_dropout=0.0, reconstruction_loss_weight=1.0):
    super().__init__()
    emb_dim = default(emb_dim, dim)
    mem_len = default(mem_len, seq_len)
    cmem_len = default(cmem_len, (mem_len // cmem_ratio))
    memory_layers = default(memory_layers, list(range(1, (depth + 1))))
    assert (mem_len >= seq_len), 'length of memory should be at least the sequence length'
    assert (cmem_len >= (mem_len // cmem_ratio)), f'length of compressed memory should be at least the memory length divided by the compression ratio {int((mem_len // cmem_ratio))}'
    assert all([((layer > 0) and (layer <= depth)) for layer in memory_layers]), 'one of the indicated memory layers is invalid'
    self.seq_len = seq_len
    self.depth = depth
    self.memory_layers = list(memory_layers)
    self.enhanced_recurrence = enhanced_recurrence
    self.token_emb = nn.Embedding(num_tokens, emb_dim)
    self.to_model_dim = (nn.Identity() if (emb_dim == dim) else nn.Linear(emb_dim, dim))
    seq_and_mem_len = ((seq_len + mem_len) + cmem_len)
    self.pos_emb = nn.Parameter(torch.zeros(heads, seq_and_mem_len, (dim // heads)))
    self.to_logits = nn.Sequential((nn.Identity() if (emb_dim == dim) else nn.Linear(dim, emb_dim)), nn.Linear(emb_dim, num_tokens))
    wrapper = (partial(GRUGating, dim, mogrify=mogrify_gru) if gru_gated_residual else Residual)
    self.attn_layers = nn.ModuleList([wrapper(PreNorm(dim, SelfAttention(dim, seq_len, mem_len, cmem_len, cmem_ratio, heads, dropout=attn_layer_dropout, attn_dropout=attn_dropout, reconstruction_attn_dropout=reconstruction_attn_dropout))) for _ in range(depth)])
    self.ff_layers = nn.ModuleList([wrapper(PreNorm(dim, FeedForward(dim, dropout=ff_dropout, glu=ff_glu))) for _ in range(depth)])
    self.reconstruction_loss_weight = reconstruction_loss_weight

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = ( if else nn.Linear)

idx = 42:------------------- similar code ------------------ index = 60, score = 5.0 
def _initialize_bridge(self, rnn_type, hidden_size, num_layers):
    number_of_states = (2 if (rnn_type == 'LSTM') else 1)
    self.total_hidden_dim = (hidden_size * num_layers)
    self.bridge = nn.ModuleList([nn.Linear(self.total_hidden_dim, self.total_hidden_dim, bias=True) for _ in range(number_of_states)])

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... ([nn.Linear])

idx = 43:------------------- similar code ------------------ index = 84, score = 5.0 
def __init__(self, num, p):
    super(_FCN, self).__init__()
    self.features = nn.Sequential(nn.Conv3d(1, num, 4, 1, 0, bias=False), nn.MaxPool3d(2, 1, 0), nn.BatchNorm3d(num), nn.LeakyReLU(), nn.Dropout(0.1), nn.Conv3d(num, (2 * num), 4, 1, 0, bias=False), nn.MaxPool3d(2, 2, 0), nn.BatchNorm3d((2 * num)), nn.LeakyReLU(), nn.Dropout(0.1), nn.Conv3d((2 * num), (4 * num), 3, 1, 0, bias=False), nn.MaxPool3d(2, 2, 0), nn.BatchNorm3d((4 * num)), nn.LeakyReLU(), nn.Dropout(0.1), nn.Conv3d((4 * num), (8 * num), 3, 1, 0, bias=False), nn.MaxPool3d(2, 1, 0), nn.BatchNorm3d((8 * num)), nn.LeakyReLU())
    self.classifier = nn.Sequential(nn.Dropout(p), nn.Linear(((((8 * num) * 6) * 6) * 6), 30), nn.LeakyReLU(), nn.Dropout(p), nn.Linear(30, 2))
    self.feature_length = ((((8 * num) * 6) * 6) * 6)
    self.num = num

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... (, nn.Linear,,,)

idx = 44:------------------- similar code ------------------ index = 77, score = 5.0 
def __init__(self, neural_ir_model: nn.Module, representation_size: int, vocab_size: int):
    super(PreTrain_MLM_POD_Head, self).__init__()
    self.neural_ir_model = neural_ir_model
    self.mlm_head = nn.Linear(representation_size, vocab_size)
    self.loss_fct = torch.nn.CrossEntropyLoss()
    self.loss_fct2 = torch.nn.BCEWithLogitsLoss(reduction='none')
    self.loss_fct3 = torch.nn.BCEWithLogitsLoss(reduction='mean')
    self.apply(self._init_weights)
    self.check_inter_passage_pod = False
    self.doc_pass_pod = True

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 45:------------------- similar code ------------------ index = 83, score = 5.0 
def __init__(self, with_avg_pool=False, with_cls=True, with_reg=True, roi_feat_size=7, in_channels=256, num_classes=81, target_means=[0.0, 0.0, 0.0, 0.0], target_stds=[0.1, 0.1, 0.2, 0.2], reg_class_agnostic=False, loss_cls=dict(type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0), loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)):
    super(BBoxHead, self).__init__()
    assert (with_cls or with_reg)
    self.with_avg_pool = with_avg_pool
    self.with_cls = with_cls
    self.with_reg = with_reg
    self.roi_feat_size = _pair(roi_feat_size)
    self.roi_feat_area = (self.roi_feat_size[0] * self.roi_feat_size[1])
    self.in_channels = in_channels
    self.num_classes = num_classes
    self.target_means = target_means
    self.target_stds = target_stds
    self.reg_class_agnostic = reg_class_agnostic
    self.fp16_enabled = False
    self.loss_cls = build_loss(loss_cls)
    self.loss_bbox = build_loss(loss_bbox)
    in_channels = self.in_channels
    if self.with_avg_pool:
        self.avg_pool = nn.AvgPool2d(self.roi_feat_size)
    else:
        in_channels *= self.roi_feat_area
    if self.with_cls:
        self.fc_cls = nn.Linear(in_channels, num_classes)
    if self.with_reg:
        out_dim_reg = (4 if reg_class_agnostic else (4 * num_classes))
        self.fc_reg = nn.Linear(in_channels, out_dim_reg)
    self.debug_imgs = None

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
 = nn.Linear

idx = 46:------------------- similar code ------------------ index = 63, score = 5.0 
def __init__(self, block, layers, num_classes=1000):
    self.inplanes = 64
    super(ResNet, self).__init__()
    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = nn.BatchNorm2d(64)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
    self.avgpool = nn.AvgPool2d(7)
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = ((m.kernel_size[0] * m.kernel_size[1]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 47:------------------- similar code ------------------ index = 65, score = 5.0 
def __init__(self, bert_model: Union[(str, AutoModel)], dropout: float=0.0, trainable: bool=True, parade_aggregate_layers=2, parade_aggregate_type='tf', chunk_size=50, overlap=7, padding_idx: int=0) -> None:
    super().__init__()
    if isinstance(bert_model, str):
        self.bert_model = AutoModel.from_pretrained(bert_model)
    else:
        self.bert_model = bert_model
    for p in self.bert_model.parameters():
        p.requires_grad = trainable
    self._dropout = torch.nn.Dropout(p=dropout)
    self.padding_idx = padding_idx
    self.chunk_size = chunk_size
    self.overlap = overlap
    self.extended_chunk_size = (self.chunk_size + (2 * self.overlap))
    self.parade_aggregate_type = parade_aggregate_type
    if (parade_aggregate_type == 'tf'):
        encoder_layer = nn.TransformerEncoderLayer(self.bert_model.config.dim, self.bert_model.config.num_attention_heads, dim_feedforward=self.bert_model.config.hidden_dim, dropout=self.bert_model.config.dropout)
        self.parade_aggregate_tf = nn.TransformerEncoder(encoder_layer, parade_aggregate_layers, norm=None)
    self.score_reduction = nn.Linear(self.bert_model.config.dim, 1)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... () -> None:
 = nn.Linear

idx = 48:------------------- similar code ------------------ index = 82, score = 5.0 
def __init__(self, neural_ir_model: nn.Module, representation_size: int, vocab_size: int):
    super(PreTrain_MLM_Head, self).__init__()
    self.neural_ir_model = neural_ir_model
    self.mlm_head = nn.Linear(representation_size, vocab_size)
    self.loss_fct = torch.nn.CrossEntropyLoss()
    self.apply(self._init_weights)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 49:------------------- similar code ------------------ index = 66, score = 5.0 
def __init__(self, in_channels, k=2, n=4, num_classes=10):
    super(Scattering2dResNet, self).__init__()
    self.inplanes = (16 * k)
    self.ichannels = (16 * k)
    self.K = in_channels
    self.init_conv = nn.Sequential(nn.BatchNorm2d(in_channels, eps=1e-05, affine=False), nn.Conv2d(in_channels, self.ichannels, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(self.ichannels), nn.ReLU(True))
    self.layer2 = self._make_layer(BasicBlock, (32 * k), n)
    self.layer3 = self._make_layer(BasicBlock, (64 * k), n)
    self.avgpool = nn.AdaptiveAvgPool2d(2)
    self.fc = nn.Linear(((64 * k) * 4), num_classes)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 50:------------------- similar code ------------------ index = 67, score = 5.0 
def __init__(self, num_embeddings, embedding_dim=300, hidden_size=300, output_size=1, num_layers=1, dropout=0.2):
    super().__init__()
    model = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)
    weights = torch.FloatTensor(model.vectors)
    self.emb = nn.Embedding.from_pretrained(weights)
    self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)
    self.linear = nn.Linear(hidden_size, output_size)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 51:------------------- similar code ------------------ index = 81, score = 5.0 
def __init__(self, config):
    super(AlbertForQuestionAnswering, self).__init__(config)
    self.num_labels = config.num_labels
    self.bert = AlbertModel(config)
    self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)
    self.init_weights()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 52:------------------- similar code ------------------ index = 80, score = 5.0 
def __init__(self, params):
    Model.check_parameters(params, {'name': 'Overfeat', 'input_shape': (3, 231, 231), 'num_classes': 1000, 'phase': 'training', 'dtype': 'float32'})
    Model.__init__(self, params)
    self.features = nn.Sequential(nn.Conv2d(self.input_shape[0], 96, kernel_size=11, stride=4), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(96, 256, kernel_size=5), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(256, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(512, 1024, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(1024, 1024, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2))
    self.classifier = nn.Sequential(nn.Linear(((1024 * 6) * 6), 3072), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(3072, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, self.num_classes))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... (nn.Linear,,,,,,)

idx = 53:------------------- similar code ------------------ index = 69, score = 5.0 
def __init__(self, args: model_args.ModelArgs, input_vocabulary: List[str], auxiliaries: List[auxiliary.Auxiliary]):
    super(PlanPredictorModel, self).__init__()
    self._args: model_args.ModelArgs = args
    self._env_feature_channels: int = 0
    self._auxiliaries: List[auxiliary.Auxiliary] = auxiliaries
    state_rep_args: state_representation_args.StateRepresentationArgs = self._args.get_state_rep_args()
    self._state_rep: state_representation.StateRepresentation = state_representation.StateRepresentation(state_rep_args)
    self._static_embedder: static_environment_embedder.StaticEnvironmentEmbedder = static_environment_embedder.StaticEnvironmentEmbedder(self._state_rep, state_rep_args.get_property_embedding_size(), (not state_rep_args.learn_absence_embeddings()))
    self._dynamic_embedder: dynamic_environment_embedder.DynamicEnvironmentEmbedder = dynamic_environment_embedder.DynamicEnvironmentEmbedder(self._state_rep, state_rep_args.get_property_embedding_size(), (not state_rep_args.learn_absence_embeddings()))
    self._env_feature_channels = self._static_embedder.embedding_size()
    self._instruction_encoder: text_encoder.TextEncoder = text_encoder.TextEncoder(self._args.get_text_encoder_args(), input_vocabulary)
    text_feature_size = self._args.get_text_encoder_args().get_hidden_size()
    self._grounding_map_channels = 4
    self._initial_text_kernel_ll = nn.Linear(text_feature_size, (self._env_feature_channels * self._grounding_map_channels))
    self._intermediate_goal_ll: Optional[nn.Module] = None
    if (auxiliary.Auxiliary.INTERMEDIATE_GOALS in auxiliaries):
        self._intermediate_goal_ll = nn.Linear(self._grounding_map_channels, 1, bias=False)
    self._into_lingunet_transformer: map_transformer.MapTransformer = map_transformer.MapTransformer(source_map_size=environment_util.ENVIRONMENT_WIDTH, dest_map_size=environment_util.PADDED_WIDTH, world_size_px=environment_util.ENVIRONMENT_WIDTH, world_size_m=environment_util.ENVIRONMENT_WIDTH)
    self._after_lingunet_transformer: map_transformer.MapTransformer = map_transformer.MapTransformer(source_map_size=environment_util.PADDED_WIDTH, dest_map_size=environment_util.ENVIRONMENT_WIDTH, world_size_px=environment_util.PADDED_WIDTH, world_size_m=environment_util.PADDED_WIDTH)
    if (torch.cuda.device_count() >= 1):
        self._into_lingunet_transformer = self._into_lingunet_transformer.cuda(device=util.DEVICE)
        self._after_lingunet_transformer = self._after_lingunet_transformer.cuda(device=util.DEVICE)
    lingunet_out_channels: int = 0
    extra_head_channels: int = 0
    if (auxiliary.Auxiliary.FINAL_GOALS in auxiliaries):
        lingunet_out_channels += 1
    if (auxiliary.Auxiliary.TRAJECTORY in auxiliaries):
        lingunet_out_channels += 1
    if (auxiliary.Auxiliary.AVOID_LOCS in auxiliaries):
        lingunet_out_channels += 1
    if (auxiliary.Auxiliary.OBSTACLES in auxiliaries):
        lingunet_out_channels += 1
    self._lingunet = lingunet.LingUNet(self._args.get_state_encoder_args(), (self._env_feature_channels + self._grounding_map_channels), text_feature_size, lingunet_out_channels, self._args.get_dropout(), extra_head_channels=extra_head_channels, input_img_size=environment_util.PADDED_WIDTH, layer_single_preds=(auxiliary.Auxiliary.IMPLICIT in self._auxiliaries))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 54:------------------- similar code ------------------ index = 70, score = 5.0 
def __init__(self, config):
    super(AlbertForMultipleChoice, self).__init__(config)
    self.bert = AlbertModel(config)
    self.dropout = nn.Dropout((0.1 if (config.hidden_dropout_prob == 0) else config.hidden_dropout_prob))
    self.classifier = nn.Linear(config.hidden_size, 1)
    self.init_weights()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 55:------------------- similar code ------------------ index = 79, score = 5.0 
def __init__(self, config):
    super(AlbertPredictionHeadTransform, self).__init__()
    self.dense = nn.Linear(config.hidden_size, config.embedding_size)
    if (isinstance(config.hidden_act, str) or ((sys.version_info[0] == 2) and isinstance(config.hidden_act, unicode))):
        self.transform_act_fn = ACT2FN[config.hidden_act]
    else:
        self.transform_act_fn = config.hidden_act
    self.LayerNorm = AlbertLayerNorm(config.embedding_size, eps=config.layer_norm_eps)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 56:------------------- similar code ------------------ index = 75, score = 5.0 
def __init__(self, config):
    super(AlbertSelfOutput, self).__init__()
    self.dense = nn.Linear(config.hidden_size, config.hidden_size)
    self.LayerNorm = AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
    self.dropout = nn.Dropout(config.hidden_dropout_prob)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 57:------------------- similar code ------------------ index = 72, score = 5.0 
def __init__(self, input_size, output_size, pad_idx):
    super(CopyGenerator, self).__init__()
    self.linear = nn.Linear(input_size, output_size)
    self.linear_copy = nn.Linear(input_size, 1)
    self.pad_idx = pad_idx

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 58:------------------- similar code ------------------ index = 44, score = 5.0 
def __init__(self, config):
    super(BertLMPredictionHead, self).__init__()
    self.transform = BertPredictionHeadTransform(config)
    self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
    self.bias = nn.Parameter(torch.zeros(config.vocab_size))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 59:------------------- similar code ------------------ index = 52, score = 5.0 
def build(self):
    cfg = [256, 256, 256, 'M', 512, 512, 512, 1024, 1024]
    layers = []
    self.K = self.in_channels
    self.bn = nn.BatchNorm2d(self.K)
    if (self.classifier_type == 'cnn'):
        for v in cfg:
            if (v == 'M'):
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                conv2d = nn.Conv2d(self.in_channels, v, kernel_size=3, padding=1)
                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]
                self.in_channels = v
        layers += [nn.AdaptiveAvgPool2d(2)]
        self.features = nn.Sequential(*layers)
        self.classifier = nn.Linear((1024 * 4), 10)
    elif (self.classifier_type == 'mlp'):
        self.classifier = nn.Sequential(nn.Linear(((self.K * 8) * 8), 1024), nn.ReLU(), nn.Linear(1024, 1024), nn.ReLU(), nn.Linear(1024, 10))
        self.features = None
    elif (self.classifier_type == 'linear'):
        self.classifier = nn.Linear(((self.K * 8) * 8), 10)
        self.features = None

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:
 = nn.Linear

idx = 60:------------------- similar code ------------------ index = 51, score = 5.0 
def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):
    "\n        :param attention_dim: size of attention network\n        :param embed_dim: embedding size\n        :param decoder_dim: size of decoder's RNN\n        :param vocab_size: size of vocabulary\n        :param encoder_dim: feature size of encoded images\n        :param dropout: dropout\n        "
    super(DecoderWithAttention, self).__init__()
    self.encoder_dim = encoder_dim
    self.attention_dim = attention_dim
    self.embed_dim = embed_dim
    self.decoder_dim = decoder_dim
    self.vocab_size = vocab_size
    self.dropout = dropout
    self.attention = Attention(encoder_dim, decoder_dim, attention_dim)
    self.embedding = nn.Embedding(vocab_size, embed_dim)
    self.dropout = nn.Dropout(p=self.dropout)
    self.decode_step = nn.LSTMCell((embed_dim + encoder_dim), decoder_dim, bias=True)
    self.init_h = nn.Linear(encoder_dim, decoder_dim)
    self.init_c = nn.Linear(encoder_dim, decoder_dim)
    self.f_beta = nn.Linear(decoder_dim, encoder_dim)
    self.sigmoid = nn.Sigmoid()
    self.fc = nn.Linear(decoder_dim, vocab_size)
    self.init_weights()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 61:------------------- similar code ------------------ index = 20, score = 5.0 
def __init__(self, dim, seq_len, mem_len, cmem_len, cmem_ratio=4, heads=8, attn_dropout=0.0, dropout=0.0, reconstruction_attn_dropout=0.0):
    super().__init__()
    assert ((dim % heads) == 0), 'dimension must be divisible by the number of heads'
    self.heads = heads
    self.dim_head = (dim // heads)
    self.seq_len = seq_len
    self.mem_len = mem_len
    self.cmem_len = cmem_len
    self.cmem_ratio = cmem_ratio
    self.scale = (self.dim_head ** (- 0.5))
    self.compress_mem_fn = ConvCompress(dim, cmem_ratio)
    self.to_q = nn.Linear(dim, dim, bias=False)
    self.to_kv = nn.Linear(dim, (dim * 2), bias=False)
    self.to_out = nn.Linear(dim, dim)
    self.attn_dropout = nn.Dropout(attn_dropout)
    self.dropout = nn.Dropout(dropout)
    self.reconstruction_attn_dropout = nn.Dropout(reconstruction_attn_dropout)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 62:------------------- similar code ------------------ index = 19, score = 5.0 
def __init__(self, config):
    super(AlbertForMultiLable, self).__init__(config)
    self.bert = AlbertModel(config)
    self.dropout = nn.Dropout(config.hidden_dropout_prob)
    self.classifier = nn.Linear(config.hidden_size, config.num_labels)
    self.init_weights()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 63:------------------- similar code ------------------ index = 18, score = 5.0 
def __init__(self, config):
    super(AlbertPooler, self).__init__()
    self.dense = nn.Linear(config.hidden_size, config.hidden_size)
    self.activation = nn.Tanh()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 64:------------------- similar code ------------------ index = 17, score = 5.0 
def _init_modules(self):
    vgg = models.vgg16()
    if self.pretrained:
        print(('Loading pretrained weights from %s' % self.model_path))
        state_dict = torch.load(self.model_path)
        vgg.load_state_dict({k: v for (k, v) in state_dict.items() if (k in vgg.state_dict())})
    vgg.classifier = nn.Sequential(*list(vgg.classifier._modules.values())[:(- 1)])
    self.RCNN_base = nn.Sequential(*list(vgg.features._modules.values())[:(- 1)])
    for layer in range(10):
        for p in self.RCNN_base[layer].parameters():
            p.requires_grad = False
    self.RCNN_top = vgg.classifier
    self.RCNN_cls_score = nn.Linear(4096, self.n_classes)
    if self.class_agnostic:
        self.RCNN_bbox_pred = nn.Linear(4096, 4)
    else:
        self.RCNN_bbox_pred = nn.Linear(4096, (4 * self.n_classes))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = nn.Linear

idx = 65:------------------- similar code ------------------ index = 16, score = 5.0 
def __init__(self):
    super(Conv, self).__init__()
    self.conv1 = nn.Conv2d(1, 6, 5, 1, padding='same')
    self.conv2 = nn.Conv2d(6, 16, 5, 1, padding='same')
    self.fc1 = nn.Linear(784, 120)
    self.fc2 = nn.Linear(120, 84)
    self.fc3 = nn.Linear(84, 10)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = nn.Linear

idx = 66:------------------- similar code ------------------ index = 15, score = 5.0 
def __init__(self, config):
    super(BertForQuestionAnswering, self).__init__(config)
    self.num_labels = config.num_labels
    self.bert = BertModel(config)
    self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)
    self.init_weights()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 67:------------------- similar code ------------------ index = 14, score = 5.0 
def __init__(self, params):
    Model.check_parameters(params, {'name': 'AlexNet', 'input_shape': (3, 227, 227), 'num_classes': 1000, 'phase': 'training', 'dtype': 'float32'})
    Model.__init__(self, params)
    self.features = nn.Sequential(nn.Conv2d(3, 96, kernel_size=11, stride=4), nn.ReLU(inplace=True), nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2))
    self.classifier = nn.Sequential(nn.Linear(((256 * 6) * 6), 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, self.num_classes))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... (nn.Linear,,,,,,)

idx = 68:------------------- similar code ------------------ index = 13, score = 5.0 
def __init__(self, block, layers, num_classes=1000):
    self.inplanes = 128
    super(ResNet, self).__init__()
    self.conv1 = conv3x3(3, 64, stride=2)
    self.bn1 = SynchronizedBatchNorm2d(64)
    self.relu1 = nn.ReLU(inplace=True)
    self.conv2 = conv3x3(64, 64)
    self.bn2 = SynchronizedBatchNorm2d(64)
    self.relu2 = nn.ReLU(inplace=True)
    self.conv3 = conv3x3(64, 128)
    self.bn3 = SynchronizedBatchNorm2d(128)
    self.relu3 = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
    self.avgpool = nn.AvgPool2d(7, stride=1)
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = ((m.kernel_size[0] * m.kernel_size[1]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
        elif isinstance(m, SynchronizedBatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 69:------------------- similar code ------------------ index = 12, score = 5.0 
def __init__(self, config):
    super(BertForMultipleChoice, self).__init__(config)
    self.bert = BertModel(config)
    self.dropout = nn.Dropout(config.hidden_dropout_prob)
    self.classifier = nn.Linear(config.hidden_size, 1)
    self.init_weights()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 70:------------------- similar code ------------------ index = 11, score = 5.0 
def __init__(self, dim, mult=4, dropout=0.0, activation=None, glu=False):
    super().__init__()
    activation = default(activation, GELU)
    self.glu = glu
    self.w1 = nn.Linear(dim, ((dim * mult) * (2 if glu else 1)))
    self.act = activation()
    self.dropout = nn.Dropout(dropout)
    self.w2 = nn.Linear((dim * mult), dim)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 71:------------------- similar code ------------------ index = 10, score = 5.0 
def __init__(self, embeddings_size, decoder_size, attention_size, output_size):
    super(ContextGate, self).__init__()
    input_size = ((embeddings_size + decoder_size) + attention_size)
    self.gate = nn.Linear(input_size, output_size, bias=True)
    self.sig = nn.Sigmoid()
    self.source_proj = nn.Linear(attention_size, output_size)
    self.target_proj = nn.Linear((embeddings_size + decoder_size), output_size)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 72:------------------- similar code ------------------ index = 9, score = 5.0 
def __init__(self, config):
    super(AlbertEmbeddings, self).__init__()
    self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=0)
    self.word_embeddings_2 = nn.Linear(config.embedding_size, config.hidden_size, bias=False)
    self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
    self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
    self.LayerNorm = AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
    self.dropout = nn.Dropout(config.hidden_dropout_prob)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 73:------------------- similar code ------------------ index = 8, score = 5.0 
def __init__(self, local_rank, vocab, embed_dim, ff_embed_dim, num_heads, dropout, layers, smoothing_factor, approx):
    super(BIGLM, self).__init__()
    self.vocab = vocab
    self.embed_dim = embed_dim
    self.tok_embed = Embedding(self.vocab.size, embed_dim, self.vocab.padding_idx)
    self.pos_embed = LearnedPositionalEmbedding(embed_dim, device=local_rank)
    self.layers = nn.ModuleList()
    for i in range(layers):
        self.layers.append(TransformerLayer(embed_dim, ff_embed_dim, num_heads, dropout))
    self.emb_layer_norm = LayerNorm(embed_dim)
    self.one_more = nn.Linear(embed_dim, embed_dim)
    self.one_more_layer_norm = LayerNorm(embed_dim)
    self.out_proj = nn.Linear(embed_dim, self.vocab.size)
    self.attn_mask = SelfAttentionMask(device=local_rank)
    self.smoothing = LabelSmoothing(local_rank, self.vocab.size, self.vocab.padding_idx, smoothing_factor)
    self.dropout = dropout
    self.device = local_rank
    if (approx == 'none'):
        self.approx = None
    elif (approx == 'adaptive'):
        self.approx = nn.AdaptiveLogSoftmaxWithLoss(self.embed_dim, self.vocab.size, [10000, 20000, 200000])
    else:
        raise NotImplementedError(('%s has not been implemented' % approx))
    self.reset_parameters()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 74:------------------- similar code ------------------ index = 7, score = 5.0 
def __init__(self, spatial_scale, out_size, out_channels, no_trans, group_size=1, part_size=None, sample_per_part=4, trans_std=0.0, num_offset_fcs=3, deform_fc_channels=1024):
    super(DeformRoIPoolingPack, self).__init__(spatial_scale, out_size, out_channels, no_trans, group_size, part_size, sample_per_part, trans_std)
    self.num_offset_fcs = num_offset_fcs
    self.deform_fc_channels = deform_fc_channels
    if (not no_trans):
        seq = []
        ic = ((self.out_size[0] * self.out_size[1]) * self.out_channels)
        for i in range(self.num_offset_fcs):
            if (i < (self.num_offset_fcs - 1)):
                oc = self.deform_fc_channels
            else:
                oc = ((self.out_size[0] * self.out_size[1]) * 2)
            seq.append(nn.Linear(ic, oc))
            ic = oc
            if (i < (self.num_offset_fcs - 1)):
                seq.append(nn.ReLU(inplace=True))
        self.offset_fc = nn.Sequential(*seq)
        self.offset_fc[(- 1)].weight.data.zero_()
        self.offset_fc[(- 1)].bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
        for  ...  in:
             ... . ... (nn.Linear)

idx = 75:------------------- similar code ------------------ index = 6, score = 5.0 
def __init__(self, mode, cfg, logger_handle, **kwargs):
    fasterRCNNBase.__init__(self, FasterRCNNResNets.feature_stride, mode, cfg)
    self.logger_handle = logger_handle
    self.backbone_type = cfg.BACKBONE_TYPE
    self.pretrained_model_path = cfg.PRETRAINED_MODEL_PATH
    self.backbone = ResNets(resnet_type=self.backbone_type, pretrained=False)
    if (mode == 'TRAIN'):
        self.initializeBackbone()
    self.backbone.avgpool = None
    self.backbone.fc = None
    self.base_model = nn.Sequential(*[self.backbone.conv1, self.backbone.bn1, self.backbone.relu, self.backbone.maxpool, self.backbone.layer1, self.backbone.layer2, self.backbone.layer3])
    in_channels = (256 if (self.backbone_type in ['resnet18', 'resnet34']) else 1024)
    self.rpn_net = RegionProposalNet(in_channels=in_channels, feature_stride=self.feature_stride, mode=mode, cfg=cfg)
    self.build_proposal_target_layer = buildProposalTargetLayer(mode, cfg)
    self.top_model = nn.Sequential(*[self.backbone.layer4])
    in_features = (512 if (self.backbone_type in ['resnet18', 'resnet34']) else 2048)
    self.fc_cls = nn.Linear(in_features, self.num_classes)
    if self.is_class_agnostic:
        self.fc_reg = nn.Linear(in_features, 4)
    else:
        self.fc_reg = nn.Linear(in_features, (4 * self.num_classes))
    if (cfg.ADDED_MODULES_WEIGHT_INIT_METHOD and (mode == 'TRAIN')):
        init_methods = cfg.ADDED_MODULES_WEIGHT_INIT_METHOD
        self.rpn_net.initWeights(init_methods['rpn'])
        self.initializeAddedLayers(init_methods['rcnn'])
    if cfg.FIXED_FRONT_BLOCKS:
        for p in self.base_model[0].parameters():
            p.requires_grad = False
        for p in self.base_model[1].parameters():
            p.requires_grad = False
        for p in self.base_model[4].parameters():
            p.requires_grad = False
    self.base_model.apply(fasterRCNNBase.setBnFixed)
    self.top_model.apply(fasterRCNNBase.setBnFixed)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 76:------------------- similar code ------------------ index = 5, score = 5.0 
def build_base_model(model_opt, fields, gpu, checkpoint=None, gpu_id=None):
    "Build a model from opts.\n\n    Args:\n        model_opt: the option loaded from checkpoint. It's important that\n            the opts have been updated and validated. See\n            :class:`onmt.utils.parse.ArgumentParser`.\n        fields (dict[str, torchtext.data.Field]):\n            `Field` objects for the model.\n        gpu (bool): whether to use gpu.\n        checkpoint: the model gnerated by train phase, or a resumed snapshot\n                    model from a stopped training.\n        gpu_id (int or NoneType): Which GPU to use.\n\n    Returns:\n        the NMTModel.\n    "
    try:
        model_opt.attention_dropout
    except AttributeError:
        model_opt.attention_dropout = model_opt.dropout
    if ((model_opt.model_type == 'text') or (model_opt.model_type == 'vec')):
        src_field = fields['src']
        src_emb = build_embeddings(model_opt, src_field)
    else:
        src_emb = None
    encoder = build_encoder(model_opt, src_emb)
    tgt_field = fields['tgt']
    tgt_emb = build_embeddings(model_opt, tgt_field, for_encoder=False)
    if model_opt.share_embeddings:
        assert (src_field.base_field.vocab == tgt_field.base_field.vocab), 'preprocess with -share_vocab if you use share_embeddings'
        tgt_emb.word_lut.weight = src_emb.word_lut.weight
    decoder = build_decoder(model_opt, tgt_emb)
    if (gpu and (gpu_id is not None)):
        device = torch.device('cuda', gpu_id)
    elif (gpu and (not gpu_id)):
        device = torch.device('cuda')
    elif (not gpu):
        device = torch.device('cpu')
    model = onmt.models.NMTModel(encoder, decoder)
    if (not model_opt.copy_attn):
        if (model_opt.generator_function == 'sparsemax'):
            gen_func = onmt.modules.sparse_activations.LogSparsemax(dim=(- 1))
        else:
            gen_func = nn.LogSoftmax(dim=(- 1))
        generator = nn.Sequential(nn.Linear(model_opt.dec_rnn_size, len(fields['tgt'].base_field.vocab)), Cast(torch.float32), gen_func)
        if model_opt.share_decoder_embeddings:
            generator[0].weight = decoder.embeddings.word_lut.weight
    else:
        tgt_base_field = fields['tgt'].base_field
        vocab_size = len(tgt_base_field.vocab)
        pad_idx = tgt_base_field.vocab.stoi[tgt_base_field.pad_token]
        generator = CopyGenerator(model_opt.dec_rnn_size, vocab_size, pad_idx)
        if model_opt.share_decoder_embeddings:
            generator.linear.weight = decoder.embeddings.word_lut.weight
    if (checkpoint is not None):

        def fix_key(s):
            s = re.sub('(.*)\\.layer_norm((_\\d+)?)\\.b_2', '\\1.layer_norm\\2.bias', s)
            s = re.sub('(.*)\\.layer_norm((_\\d+)?)\\.a_2', '\\1.layer_norm\\2.weight', s)
            return s
        checkpoint['model'] = {fix_key(k): v for (k, v) in checkpoint['model'].items()}
        model.load_state_dict(checkpoint['model'], strict=False)
        generator.load_state_dict(checkpoint['generator'], strict=False)
    else:
        if (model_opt.param_init != 0.0):
            for p in model.parameters():
                p.data.uniform_((- model_opt.param_init), model_opt.param_init)
            for p in generator.parameters():
                p.data.uniform_((- model_opt.param_init), model_opt.param_init)
        if model_opt.param_init_glorot:
            for p in model.parameters():
                if (p.dim() > 1):
                    xavier_uniform_(p)
            for p in generator.parameters():
                if (p.dim() > 1):
                    xavier_uniform_(p)
        if hasattr(model.encoder, 'embeddings'):
            model.encoder.embeddings.load_pretrained_vectors(model_opt.pre_word_vecs_enc)
        if hasattr(model.decoder, 'embeddings'):
            model.decoder.embeddings.load_pretrained_vectors(model_opt.pre_word_vecs_dec)
    model.generator = generator
    model.to(device)
    if ((model_opt.model_dtype == 'fp16') and (model_opt.optim == 'fusedadam')):
        model.half()
    return model

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  =  ... . ... (nn.Linear,,  ... )

idx = 77:------------------- similar code ------------------ index = 4, score = 5.0 
def __init__(self, block, layers, num_classes=1000):
    self.inplanes = 128
    super(ResNet, self).__init__()
    self.conv1 = conv3x3(3, 64, stride=2)
    self.bn1 = SynchronizedBatchNorm2d(64)
    self.relu1 = nn.ReLU(inplace=True)
    self.conv2 = conv3x3(64, 64)
    self.bn2 = SynchronizedBatchNorm2d(64)
    self.relu2 = nn.ReLU(inplace=True)
    self.conv3 = conv3x3(64, 128)
    self.bn3 = SynchronizedBatchNorm2d(128)
    self.relu3 = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
    self.avgpool = nn.AvgPool2d(7, stride=1)
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = ((m.kernel_size[0] * m.kernel_size[1]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
        elif isinstance(m, SynchronizedBatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 78:------------------- similar code ------------------ index = 3, score = 5.0 
def __init__(self, fil_num, drop_rate):
    super(_CNN, self).__init__()
    self.block1 = ConvLayer(1, fil_num, 0.1, (7, 2, 0), (3, 2, 0))
    self.block2 = ConvLayer(fil_num, (2 * fil_num), 0.1, (4, 1, 0), (2, 2, 0))
    self.block3 = ConvLayer((2 * fil_num), (4 * fil_num), 0.1, (3, 1, 0), (2, 2, 0))
    self.block4 = ConvLayer((4 * fil_num), (8 * fil_num), 0.1, (3, 1, 0), (2, 1, 0))
    self.dense1 = nn.Sequential(nn.Dropout(drop_rate), nn.Linear(((((8 * fil_num) * 6) * 8) * 6), 30))
    self.dense2 = nn.Sequential(nn.LeakyReLU(), nn.Dropout(drop_rate), nn.Linear(30, 2))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... (, nn.Linear)

idx = 79:------------------- similar code ------------------ index = 1, score = 5.0 
def __init__(self, embed_dim, num_heads, dropout=0.0, weights_dropout=True):
    super(MultiheadAttention, self).__init__()
    self.embed_dim = embed_dim
    self.num_heads = num_heads
    self.dropout = dropout
    self.head_dim = (embed_dim // num_heads)
    assert ((self.head_dim * num_heads) == self.embed_dim), 'embed_dim must be divisible by num_heads'
    self.scaling = (self.head_dim ** (- 0.5))
    self.in_proj_weight = Parameter(torch.Tensor((3 * embed_dim), embed_dim))
    self.in_proj_bias = Parameter(torch.Tensor((3 * embed_dim)))
    self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)
    self.weights_dropout = weights_dropout
    self.reset_parameters()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 80:------------------- similar code ------------------ index = 22, score = 5.0 
def __init__(self, num_embeddings, embedding_dim=50, hidden_size=50, output_size=1, num_layers=1, dropout=0.2):
    super().__init__()
    self.emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)
    self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)
    self.linear = nn.Linear(hidden_size, output_size)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 81:------------------- similar code ------------------ index = 23, score = 5.0 
def __init__(self, params):
    Model.check_parameters(params, {'name': 'InceptionV3'})
    BaseInceptionModel.__init__(self, params)
    self.features = nn.Sequential(ConvModule(3, num_filters=32, kernel_size=3, stride=2, padding=0), ConvModule(32, num_filters=32, kernel_size=3, stride=1, padding=0), ConvModule(32, num_filters=64, kernel_size=3, stride=1, padding=1), nn.MaxPool2d(kernel_size=3, stride=2), ConvModule(64, num_filters=80, kernel_size=1, stride=1, padding=0), ConvModule(80, num_filters=192, kernel_size=3, stride=1, padding=0), nn.MaxPool2d(kernel_size=3, stride=2), self.module_a(192, index=0, n=32), self.module_a(256, index=1, n=64), self.module_a(288, index=2, n=64), self.module_b(288, index=0), self.module_c(768, index=0, n=128), self.module_c(768, index=1, n=160), self.module_c(768, index=2, n=160), self.module_c(768, index=3, n=192), self.module_d(768, index=0), self.module_e(1280, index=0, pooltype='avg'), self.module_e(2048, index=1, pooltype='max'), nn.AvgPool2d(kernel_size=8, stride=1))
    self.classifier = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(2048, self.num_classes))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... (, nn.Linear)

idx = 82:------------------- similar code ------------------ index = 24, score = 5.0 
def __init__(self, config):
    super(AlbertOutput, self).__init__()
    self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
    self.LayerNorm = AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
    self.dropout = nn.Dropout(config.hidden_dropout_prob)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 83:------------------- similar code ------------------ index = 37, score = 5.0 
def __init__(self, in_channels, k=2, n=4, num_classes=10, standard=False):
    super(Scattering2dResNet, self).__init__()
    self.inplanes = (16 * k)
    self.ichannels = (16 * k)
    if standard:
        self.init_conv = nn.Sequential(nn.Conv2d(3, self.ichannels, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(self.ichannels), nn.ReLU(True))
        self.layer1 = self._make_layer(BasicBlock, (16 * k), n)
        self.standard = True
    else:
        self.K = in_channels
        self.init_conv = nn.Sequential(nn.BatchNorm2d(in_channels, eps=1e-05, affine=False), nn.Conv2d(in_channels, self.ichannels, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(self.ichannels), nn.ReLU(True))
        self.standard = False
    self.layer2 = self._make_layer(BasicBlock, (32 * k), n)
    self.layer3 = self._make_layer(BasicBlock, (64 * k), n)
    self.avgpool = nn.AdaptiveAvgPool2d(2)
    self.fc = nn.Linear(((64 * k) * 4), num_classes)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 84:------------------- similar code ------------------ index = 50, score = 5.0 
def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu', attn=False, conv=False, convsz=1, shared_qk=False, sep=False):
    super(ConformerEncoderLayer, self).__init__()
    self.conv = conv
    if conv:
        assert ((convsz % 2) == 1)
        self.conv_layer = nn.Conv1d(d_model, d_model, convsz, padding=int(((convsz - 1) / 2)), groups=nhead)
    self.attn = attn
    if attn:
        self.self_attn = MultiheadSeparableAttention(d_model, nhead, dropout=dropout, shared_qk=shared_qk, sep=sep)
    self.linear1 = nn.Linear(d_model, dim_feedforward)
    self.dropout = nn.Dropout(dropout)
    self.linear2 = nn.Linear(dim_feedforward, d_model)
    self.norm1 = nn.LayerNorm(d_model)
    self.norm2 = nn.LayerNorm(d_model)
    self.dropout1 = nn.Dropout(dropout)
    self.dropout2 = nn.Dropout(dropout)
    self.activation = _get_activation_fn(activation)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 85:------------------- similar code ------------------ index = 49, score = 5.0 
def __init__(self, d_model, d_ff, dropout=0.1):
    super(PositionwiseFeedForward, self).__init__()
    self.w_1 = nn.Linear(d_model, d_ff)
    self.w_2 = nn.Linear(d_ff, d_model)
    self.layer_norm = nn.LayerNorm(d_model, eps=1e-06)
    self.dropout_1 = nn.Dropout(dropout)
    self.relu = nn.ReLU()
    self.dropout_2 = nn.Dropout(dropout)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 86:------------------- similar code ------------------ index = 48, score = 5.0 
def __init__(self, pretrained='imagenet', **kwargs):
    super(ScSeSenet154_9ch_Unet, self).__init__()
    encoder_filters = [128, 256, 512, 1024, 2048]
    decoder_filters = [96, 128, 160, 256, 512]
    self.conv6 = ConvRelu(encoder_filters[(- 1)], decoder_filters[(- 1)])
    self.conv6_2 = ConvRelu(((((decoder_filters[(- 1)] + encoder_filters[(- 2)]) + 2) + 27) + 2), decoder_filters[(- 1)])
    self.conv7 = ConvRelu(decoder_filters[(- 1)], decoder_filters[(- 2)])
    self.conv7_2 = ConvRelu(((((decoder_filters[(- 2)] + encoder_filters[(- 3)]) + 2) + 27) + 2), decoder_filters[(- 2)])
    self.conv8 = ConvRelu(decoder_filters[(- 2)], decoder_filters[(- 3)])
    self.conv8_2 = ConvRelu(((((decoder_filters[(- 3)] + encoder_filters[(- 4)]) + 2) + 27) + 2), decoder_filters[(- 3)])
    self.conv9 = ConvRelu(decoder_filters[(- 3)], decoder_filters[(- 4)])
    self.conv9_2 = ConvRelu(((((decoder_filters[(- 4)] + encoder_filters[(- 5)]) + 2) + 27) + 2), decoder_filters[(- 4)])
    self.conv10 = ConvRelu((((decoder_filters[(- 4)] + 2) + 2) + 27), decoder_filters[(- 5)])
    self.res = nn.Conv2d(((decoder_filters[(- 5)] + 2) + 2), 4, 1, stride=1, padding=0)
    self.off_nadir = nn.Sequential(nn.Linear(encoder_filters[(- 1)], 64), nn.ReLU(inplace=True), nn.Linear(64, 1))
    self._initialize_weights()
    encoder = senet154(pretrained=pretrained)
    conv1_new = nn.Conv2d(9, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    _w = encoder.layer0.conv1.state_dict()
    _w['weight'] = torch.cat([(0.8 * _w['weight']), (0.1 * _w['weight']), (0.1 * _w['weight'])], 1)
    conv1_new.load_state_dict(_w)
    self.conv1 = nn.Sequential(conv1_new, encoder.layer0.bn1, encoder.layer0.relu1, encoder.layer0.conv2, encoder.layer0.bn2, encoder.layer0.relu2, encoder.layer0.conv3, encoder.layer0.bn3, encoder.layer0.relu3)
    self.conv2 = nn.Sequential(encoder.pool, encoder.layer1)
    self.conv3 = encoder.layer2
    self.conv4 = encoder.layer3
    self.conv5 = encoder.layer4

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... (nn.Linear,,)

idx = 87:------------------- similar code ------------------ index = 47, score = 5.0 
def _init_modules(self):
    resnet = resnet101()
    if (self.pretrained == True):
        print(('Loading pretrained weights from %s' % self.model_path))
        state_dict = torch.load(self.model_path)
        resnet.load_state_dict({k: v for (k, v) in state_dict.items() if (k in resnet.state_dict())})
    self.RCNN_layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)
    self.RCNN_layer1 = nn.Sequential(resnet.layer1)
    self.RCNN_layer2 = nn.Sequential(resnet.layer2)
    self.RCNN_layer3 = nn.Sequential(resnet.layer3)
    self.RCNN_layer4 = nn.Sequential(resnet.layer4)
    self.RCNN_toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)
    self.RCNN_smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
    self.RCNN_smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
    self.RCNN_smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
    self.RCNN_latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)
    self.RCNN_latlayer2 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)
    self.RCNN_latlayer3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)
    self.RCNN_roi_feat_ds = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)
    self.RCNN_top = nn.Sequential(nn.Conv2d(256, 1024, kernel_size=cfg.POOLING_SIZE, stride=cfg.POOLING_SIZE, padding=0), nn.ReLU(True), nn.Conv2d(1024, 1024, kernel_size=1, stride=1, padding=0), nn.ReLU(True))
    self.RCNN_cls_score = nn.Linear(1024, self.n_classes)
    if self.class_agnostic:
        self.RCNN_bbox_pred = nn.Linear(1024, 4)
    else:
        self.RCNN_bbox_pred = nn.Linear(1024, (4 * self.n_classes))
    for p in self.RCNN_layer0[0].parameters():
        p.requires_grad = False
    for p in self.RCNN_layer0[1].parameters():
        p.requires_grad = False
    for p in self.RCNN_layer4.parameters():
        p.requires_grad = False
    for p in self.RCNN_layer3.parameters():
        p.requires_grad = False
    for p in self.RCNN_layer2.parameters():
        p.requires_grad = False
    for p in self.RCNN_layer1.parameters():
        p.requires_grad = False

    def set_bn_fix(m):
        classname = m.__class__.__name__
        if (classname.find('BatchNorm') != (- 1)):
            for p in m.parameters():
                p.requires_grad = False
    self.RCNN_layer0.apply(set_bn_fix)
    self.RCNN_layer1.apply(set_bn_fix)
    self.RCNN_layer2.apply(set_bn_fix)
    self.RCNN_layer3.apply(set_bn_fix)
    self.RCNN_layer4.apply(set_bn_fix)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = nn.Linear

idx = 88:------------------- similar code ------------------ index = 45, score = 5.0 
def __init__(self, block=Bottleneck, layers_scene=[3, 4, 6, 3, 2], layers_face=[3, 4, 6, 3, 2]):
    self.inplanes_scene = 64
    self.inplanes_face = 64
    super(ModelSpatial, self).__init__()
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.avgpool = nn.AvgPool2d(7, stride=1)
    self.conv1_scene = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1_scene = nn.BatchNorm2d(64)
    self.layer1_scene = self._make_layer_scene(block, 64, layers_scene[0])
    self.layer2_scene = self._make_layer_scene(block, 128, layers_scene[1], stride=2)
    self.layer3_scene = self._make_layer_scene(block, 256, layers_scene[2], stride=2)
    self.layer4_scene = self._make_layer_scene(block, 512, layers_scene[3], stride=2)
    self.layer5_scene = self._make_layer_scene(block, 256, layers_scene[4], stride=1)
    self.conv1_face = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1_face = nn.BatchNorm2d(64)
    self.layer1_face = self._make_layer_face(block, 64, layers_face[0])
    self.layer2_face = self._make_layer_face(block, 128, layers_face[1], stride=2)
    self.layer3_face = self._make_layer_face(block, 256, layers_face[2], stride=2)
    self.layer4_face = self._make_layer_face(block, 512, layers_face[3], stride=2)
    self.layer5_face = self._make_layer_face(block, 256, layers_face[4], stride=1)
    self.attn = nn.Linear(1808, ((1 * 7) * 7))
    self.compress_conv1 = nn.Conv2d(2048, 1024, kernel_size=1, stride=1, padding=0, bias=False)
    self.compress_bn1 = nn.BatchNorm2d(1024)
    self.compress_conv2 = nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0, bias=False)
    self.compress_bn2 = nn.BatchNorm2d(512)
    self.compress_conv1_inout = nn.Conv2d(2048, 512, kernel_size=1, stride=1, padding=0, bias=False)
    self.compress_bn1_inout = nn.BatchNorm2d(512)
    self.compress_conv2_inout = nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0, bias=False)
    self.compress_bn2_inout = nn.BatchNorm2d(1)
    self.fc_inout = nn.Linear(49, 1)
    self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2)
    self.deconv_bn1 = nn.BatchNorm2d(256)
    self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2)
    self.deconv_bn2 = nn.BatchNorm2d(128)
    self.deconv3 = nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2)
    self.deconv_bn3 = nn.BatchNorm2d(1)
    self.conv4 = nn.Conv2d(1, 1, kernel_size=1, stride=1)
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d)):
            n = ((m.kernel_size[0] * m.kernel_size[1]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 89:------------------- similar code ------------------ index = 96, score = 5.0 
def __init__(self, num_convs=0, num_fcs=0, conv_out_channels=1024, fc_out_channels=1024, conv_cfg=None, norm_cfg=dict(type='BN'), **kwargs):
    kwargs.setdefault('with_avg_pool', True)
    super(DoubleConvFCBBoxHead, self).__init__(**kwargs)
    assert self.with_avg_pool
    assert (num_convs > 0)
    assert (num_fcs > 0)
    self.num_convs = num_convs
    self.num_fcs = num_fcs
    self.conv_out_channels = conv_out_channels
    self.fc_out_channels = fc_out_channels
    self.conv_cfg = conv_cfg
    self.norm_cfg = norm_cfg
    self.res_block = BasicResBlock(self.in_channels, self.conv_out_channels)
    self.conv_branch = self._add_conv_branch()
    self.fc_branch = self._add_fc_branch()
    out_dim_reg = (4 if self.reg_class_agnostic else (4 * self.num_classes))
    self.fc_reg = nn.Linear(self.conv_out_channels, out_dim_reg)
    self.fc_cls = nn.Linear(self.fc_out_channels, self.num_classes)
    self.relu = nn.ReLU(inplace=True)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 90:------------------- similar code ------------------ index = 43, score = 5.0 
def __init__(self, config):
    super(AlbertSelfAttention, self).__init__()
    if ((config.hidden_size % config.num_attention_heads) != 0):
        raise ValueError(('The hidden size (%d) is not a multiple of the number of attention heads (%d)' % (config.hidden_size, config.num_attention_heads)))
    self.output_attentions = config.output_attentions
    self.num_attention_heads = config.num_attention_heads
    self.attention_head_size = int((config.hidden_size / config.num_attention_heads))
    self.all_head_size = (self.num_attention_heads * self.attention_head_size)
    self.query = nn.Linear(config.hidden_size, self.all_head_size)
    self.key = nn.Linear(config.hidden_size, self.all_head_size)
    self.value = nn.Linear(config.hidden_size, self.all_head_size)
    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 91:------------------- similar code ------------------ index = 38, score = 5.0 
def __init__(self, params):
    ''
    Model.check_parameters(params, {'name': 'GoogleNet', 'input_shape': (3, 224, 224), 'num_classes': 1000, 'phase': 'training', 'dtype': 'float32'})
    Model.__init__(self, params)
    self.features = nn.Sequential(ConvModule(self.input_shape[0], 64, kernel_size=7, stride=2, padding=3), nn.MaxPool2d(kernel_size=3, stride=2), nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2), ConvModule(64, 64, kernel_size=1, stride=1), ConvModule(64, 192, kernel_size=3, stride=1, padding=1), nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2), nn.MaxPool2d(kernel_size=3, stride=2), InceptionModule(192, num_1x1=64, num_3x3red=96, num_3x3=128, num_d5x5red=16, num_d5x5=32, proj=32), InceptionModule(256, num_1x1=128, num_3x3red=128, num_3x3=192, num_d5x5red=32, num_d5x5=96, proj=64), nn.MaxPool2d(kernel_size=3, stride=2), InceptionModule(480, num_1x1=192, num_3x3red=96, num_3x3=208, num_d5x5red=16, num_d5x5=48, proj=64), InceptionModule(512, num_1x1=160, num_3x3red=112, num_3x3=224, num_d5x5red=24, num_d5x5=64, proj=64), InceptionModule(512, num_1x1=128, num_3x3red=128, num_3x3=256, num_d5x5red=24, num_d5x5=64, proj=64), InceptionModule(512, num_1x1=112, num_3x3red=144, num_3x3=288, num_d5x5red=32, num_d5x5=64, proj=64), InceptionModule(528, num_1x1=256, num_3x3red=160, num_3x3=320, num_d5x5red=32, num_d5x5=128, proj=128), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), InceptionModule(832, num_1x1=256, num_3x3red=160, num_3x3=320, num_d5x5red=32, num_d5x5=128, proj=128), InceptionModule(832, num_1x1=384, num_3x3red=192, num_3x3=384, num_d5x5red=48, num_d5x5=128, proj=128), nn.AvgPool2d(kernel_size=7, stride=1))
    self.classifier = nn.Sequential(nn.Dropout(), nn.Linear(1024, self.num_classes))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... (, nn.Linear)

idx = 92:------------------- similar code ------------------ index = 35, score = 5.0 
def __init__(self, config):
    super(BertPreTrainingHeads, self).__init__()
    self.predictions = BertLMPredictionHead(config)
    self.seq_relationship = nn.Linear(config.hidden_size, 2)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 93:------------------- similar code ------------------ index = 25, score = 5.0 
def __init__(self, channel=4):
    super(GCN, self).__init__()
    self.channel = channel
    self.model = nn.Sequential(GConv(1, 10, 5, padding=2, stride=1, M=channel, nScale=1, bias=False, expand=True), nn.BatchNorm2d((10 * channel)), nn.ReLU(inplace=True), GConv(10, 20, 5, padding=2, stride=1, M=channel, nScale=2, bias=False), nn.BatchNorm2d((20 * channel)), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2), GConv(20, 40, 5, padding=0, stride=1, M=channel, nScale=3, bias=False), nn.BatchNorm2d((40 * channel)), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2), GConv(40, 80, 5, padding=0, stride=1, M=channel, nScale=4, bias=False), nn.BatchNorm2d((80 * channel)), nn.ReLU(inplace=True))
    self.fc1 = nn.Linear(80, 1024)
    self.relu = nn.ReLU(inplace=True)
    self.dropout = nn.Dropout(p=0.5)
    self.fc2 = nn.Linear(1024, 10)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 94:------------------- similar code ------------------ index = 34, score = 5.0 
def __init__(self):
    super(LSTM, self).__init__()
    self.recurrent = nn.LSTM(28, 30, batch_first=True)
    self.fc1 = nn.Linear(30, 10)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = nn.Linear

idx = 95:------------------- similar code ------------------ index = 33, score = 5.0 
def __init__(self, dim, heads=8, causal=False, compression_factor=3, dropout=0.0):
    super().__init__()
    assert ((dim % heads) == 0), 'dimension must be divisible by number of heads'
    self.heads = heads
    self.causal = causal
    self.compression_factor = compression_factor
    self.compress_fn = ConvCompress(dim, compression_factor, groups=heads)
    self.to_qkv = nn.Linear(dim, (dim * 3), bias=False)
    self.to_out = nn.Linear(dim, dim)
    self.dropout = nn.Dropout(dropout)
    self.null_k = nn.Parameter(torch.zeros(1, 1, dim))
    self.null_v = nn.Parameter(torch.zeros(1, 1, dim))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 96:------------------- similar code ------------------ index = 32, score = 5.0 
def __init__(self, config):
    super(AlbertForQuestionAnswering, self).__init__(config)
    self.num_labels = config.num_labels
    self.bert = AlbertModel(config)
    self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)
    self.init_weights()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 97:------------------- similar code ------------------ index = 31, score = 5.0 
def _add_fc_branch(self):
    'Add the fc branch which consists of a sequential of fc layers'
    branch_fcs = nn.ModuleList()
    for i in range(self.num_fcs):
        fc_in_channels = ((self.in_channels * self.roi_feat_area) if (i == 0) else self.fc_out_channels)
        branch_fcs.append(nn.Linear(fc_in_channels, self.fc_out_channels))
    return branch_fcs

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
         ... . ... (nn.Linear)

idx = 98:------------------- similar code ------------------ index = 29, score = 5.0 
def __init__(self, head_count, model_dim, dropout=0.1, max_relative_positions=0):
    assert ((model_dim % head_count) == 0)
    self.dim_per_head = (model_dim // head_count)
    self.model_dim = model_dim
    super(MultiHeadedAttention, self).__init__()
    self.head_count = head_count
    self.linear_keys = nn.Linear(model_dim, (head_count * self.dim_per_head))
    self.linear_values = nn.Linear(model_dim, (head_count * self.dim_per_head))
    self.linear_query = nn.Linear(model_dim, (head_count * self.dim_per_head))
    self.softmax = nn.Softmax(dim=(- 1))
    self.dropout = nn.Dropout(dropout)
    self.final_linear = nn.Linear(model_dim, model_dim)
    self.max_relative_positions = max_relative_positions
    if (max_relative_positions > 0):
        vocab_size = ((max_relative_positions * 2) + 1)
        self.relative_positions_embeddings = nn.Embedding(vocab_size, self.dim_per_head)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 99:------------------- similar code ------------------ index = 28, score = 5.0 
def __init__(self, config):
    super(PoolerAnswerClass, self).__init__()
    self.dense_0 = nn.Linear((config.hidden_size * 2), config.hidden_size)
    self.activation = nn.Tanh()
    self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

