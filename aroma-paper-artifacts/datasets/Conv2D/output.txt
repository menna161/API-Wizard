------------------------- example 1 ------------------------ 
@wraps(Conv2D)
def DarknetConv2D(*args, **kwargs):
    'Wrapper to set Darknet parameters for Convolution2D.'
    darknet_conv_kwargs = {'kernel_regularizer': l2(0.0005)}
    darknet_conv_kwargs['padding'] = ('valid' if (kwargs.get('strides') == (2, 2)) else 'same')
    darknet_conv_kwargs.update(kwargs)
    return Conv2D(*args, **darknet_conv_kwargs)

------------------------- example 2 ------------------------ 
def resnet_shortcut(l, n_out, stride, activation=tf.identity):
    n_in = l.get_shape().as_list()[1]
    if (n_in != n_out):
        return Conv2D('convshortcut', l, n_out, 1, strides=stride, activation=activation)
    else:
        return l

------------------------- example 3 ------------------------ 
@functools.wraps(Conv2D)
def DarknetConv2D(*args, **kwargs):
    'Wrapper to set Darknet weight regularizer for Convolution2D.'
    darknet_conv_kwargs = {'kernel_regularizer': l2(0.0005)}
    darknet_conv_kwargs.update(kwargs)
    return _DarknetConv2D(*args, **darknet_conv_kwargs)

------------------------- example 4 ------------------------ 
def layer(input_tensor):
    conv_params = get_conv_params()
    bn_params = get_bn_params()
    (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
    x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
    x = Activation('relu', name=(relu_name + '1'))(x)
    shortcut = x
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = Conv2D(filters, (3, 3), strides=strides, name=(conv_name + '1'), **conv_params)(x)
    x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '2'))(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = Conv2D(filters, (3, 3), name=(conv_name + '2'), **conv_params)(x)
    shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)
    x = Add()([x, shortcut])
    return x

------------------------- example 5 ------------------------ 
def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), use_bias=True, train_bn=True):
    "conv_block is the block that has a conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layres\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    "
    (nb_filter1, nb_filter2, nb_filter3) = filters
    conv_name_base = ((('res' + str(stage)) + block) + '_branch')
    bn_name_base = ((('bn' + str(stage)) + block) + '_branch')
    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides, name=(conv_name_base + '2a'), use_bias=use_bias)(input_tensor)
    x = BatchNorm(name=(bn_name_base + '2a'))(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=(conv_name_base + '2b'), use_bias=use_bias)(x)
    x = BatchNorm(name=(bn_name_base + '2b'))(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter3, (1, 1), name=(conv_name_base + '2c'), use_bias=use_bias)(x)
    x = BatchNorm(name=(bn_name_base + '2c'))(x, training=train_bn)
    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides, name=(conv_name_base + '1'), use_bias=use_bias)(input_tensor)
    shortcut = BatchNorm(name=(bn_name_base + '1'))(shortcut, training=train_bn)
    x = KL.Add()([x, shortcut])
    x = KL.Activation('relu', name=((('res' + str(stage)) + block) + '_out'))(x)
    return x

examples  ||  representativeness  ||  number of lines  || number of comments   ||  relevancy  
example1  ||          2           ||        7         ||         0        ||        0.42857142857142855         
example2  ||          2           ||        6         ||         0        ||        0.16666666666666666         
example3  ||          2           ||        6         ||         0        ||        0.5         
example4  ||          4           ||        16         ||         0        ||        0.1875         
example5  ||          4           ||        18         ||         0        ||        0.2222222222222222         

avg       ||          0.9427609427609427           ||        10.6         ||         0.0        ||         30.099206349206355        

idx = 0:------------------- similar code ------------------ index = 199, score = 2.0 
def __init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs):
    super(Conv2D, self).__init__(rank=2, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, data_format=data_format, dilation_rate=dilation_rate, activation=activations.get(activation), use_bias=use_bias, kernel_initializer=initializers.get(kernel_initializer), bias_initializer=initializers.get(bias_initializer), kernel_regularizer=regularizers.get(kernel_regularizer), bias_regularizer=regularizers.get(bias_regularizer), activity_regularizer=regularizers.get(activity_regularizer), kernel_constraint=constraints.get(kernel_constraint), bias_constraint=constraints.get(bias_constraint), **kwargs)

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ():
     ... (Conv2D,  ... )

idx = 1:------------------- similar code ------------------ index = 60, score = 2.0 
@wraps(Conv2D)
def DarknetConv2D(*args, **kwargs):
    'Wrapper to set Darknet parameters for Convolution2D.'
    darknet_conv_kwargs = {'kernel_regularizer': l2(0.0005)}
    darknet_conv_kwargs['padding'] = ('valid' if (kwargs.get('strides') == (2, 2)) else 'same')
    darknet_conv_kwargs.update(kwargs)
    return Conv2D(*args, **darknet_conv_kwargs)

------------------- similar code (pruned) ------------------ score = 0.5 
@ ... (Conv2D)

idx = 2:------------------- similar code ------------------ index = 108, score = 2.0 
def resnet_shortcut(l, n_out, stride, activation=tf.identity):
    n_in = l.get_shape().as_list()[1]
    if (n_in != n_out):
        return Conv2D('convshortcut', l, n_out, 1, strides=stride, activation=activation)
    else:
        return l

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ():
    if:
        return Conv2D

idx = 3:------------------- similar code ------------------ index = 239, score = 2.0 
@functools.wraps(Conv2D)
def DarknetConv2D(*args, **kwargs):
    'Wrapper to set Darknet weight regularizer for Convolution2D.'
    darknet_conv_kwargs = {'kernel_regularizer': l2(0.0005)}
    darknet_conv_kwargs.update(kwargs)
    return _DarknetConv2D(*args, **darknet_conv_kwargs)

------------------- similar code (pruned) ------------------ score = 0.5 
@(Conv2D)

idx = 4:------------------- similar code ------------------ index = 206, score = 2.0 
def resnet_shortcut(l, n_out, stride, activation=tf.identity):
    n_in = l.get_shape().as_list()[1]
    if (n_in != n_out):
        return Conv2D('convshortcut', l, n_out, 1, strides=stride, activation=activation)
    else:
        return l

------------------- similar code (pruned) ------------------ score = 0.5 
def  ... ():
    if:
        return Conv2D

idx = 5:------------------- similar code ------------------ index = 129, score = 2.0 
@wraps(Conv2D)
def DarknetConv2D(*args, **kwargs):
    'Wrapper to set Darknet parameters for Convolution2D.'
    darknet_conv_kwargs = {'kernel_regularizer': l2(0.0005)}
    darknet_conv_kwargs['padding'] = ('valid' if (kwargs.get('strides') == (2, 2)) else 'same')
    darknet_conv_kwargs.update(kwargs)
    return Conv2D(*args, **darknet_conv_kwargs)

------------------- similar code (pruned) ------------------ score = 0.5 
@ ... (Conv2D)

idx = 6:------------------- similar code ------------------ index = 76, score = 1.0 
def layer(input_tensor):
    conv_params = get_conv_params()
    bn_params = get_bn_params()
    (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
    x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
    x = Activation('relu', name=(relu_name + '1'))(x)
    shortcut = x
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = Conv2D(filters, (3, 3), strides=strides, name=(conv_name + '1'), **conv_params)(x)
    x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '2'))(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = Conv2D(filters, (3, 3), name=(conv_name + '2'), **conv_params)(x)
    shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)
    x = Add()([x, shortcut])
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 7:------------------- similar code ------------------ index = 77, score = 1.0 
def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), use_bias=True, train_bn=True):
    "conv_block is the block that has a conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layres\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    "
    (nb_filter1, nb_filter2, nb_filter3) = filters
    conv_name_base = ((('res' + str(stage)) + block) + '_branch')
    bn_name_base = ((('bn' + str(stage)) + block) + '_branch')
    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides, name=(conv_name_base + '2a'), use_bias=use_bias)(input_tensor)
    x = BatchNorm(name=(bn_name_base + '2a'))(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=(conv_name_base + '2b'), use_bias=use_bias)(x)
    x = BatchNorm(name=(bn_name_base + '2b'))(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter3, (1, 1), name=(conv_name_base + '2c'), use_bias=use_bias)(x)
    x = BatchNorm(name=(bn_name_base + '2c'))(x, training=train_bn)
    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides, name=(conv_name_base + '1'), use_bias=use_bias)(input_tensor)
    shortcut = BatchNorm(name=(bn_name_base + '1'))(shortcut, training=train_bn)
    x = KL.Add()([x, shortcut])
    x = KL.Activation('relu', name=((('res' + str(stage)) + block) + '_out'))(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .Conv2D

idx = 8:------------------- similar code ------------------ index = 73, score = 1.0 
def __init__(self):
    super(ExtraLayer, self).__init__()
    self.conv1 = tf.keras.layers.Conv2D(filters=256, kernel_size=1, strides=1, padding='same')
    self.conv2 = tf.keras.layers.Conv2D(filters=512, kernel_size=3, strides=2, padding='same')
    self.conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=1, strides=1, padding='same')
    self.conv4 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, padding='same')
    self.conv5 = tf.keras.layers.Conv2D(filters=128, kernel_size=1, strides=1, padding='same')
    self.conv6 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='valid')
    self.conv7 = tf.keras.layers.Conv2D(filters=128, kernel_size=1, strides=1, padding='same')
    self.conv8 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='valid')

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
 =  ... .Conv2D

idx = 9:------------------- similar code ------------------ index = 107, score = 1.0 
def layer(c, m=None):
    x = Conv2D(pyramid_filters, (1, 1))(c)
    if (m is not None):
        up = UpSampling2D((upsample_rate, upsample_rate))(m)
        x = Add()([x, up])
    p = Conv(segmentation_filters, (3, 3), padding='same', use_batchnorm=use_batchnorm)(x)
    p = Conv(segmentation_filters, (3, 3), padding='same', use_batchnorm=use_batchnorm)(p)
    m = x
    return (m, p)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 10:------------------- similar code ------------------ index = 105, score = 1.0 
def resden(x, fil, gr, beta, gamma_init, trainable):
    x1 = Conv2D(filters=gr, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(x)
    x1 = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(x1)
    x1 = LeakyReLU(alpha=0.2)(x1)
    x1 = Concatenate(axis=(- 1))([x, x1])
    x2 = Conv2D(filters=gr, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(x1)
    x2 = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(x2)
    x2 = LeakyReLU(alpha=0.2)(x2)
    x2 = Concatenate(axis=(- 1))([x1, x2])
    x3 = Conv2D(filters=gr, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(x2)
    x3 = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(x3)
    x3 = LeakyReLU(alpha=0.2)(x3)
    x3 = Concatenate(axis=(- 1))([x2, x3])
    x4 = Conv2D(filters=gr, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(x3)
    x4 = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(x4)
    x4 = LeakyReLU(alpha=0.2)(x4)
    x4 = Concatenate(axis=(- 1))([x3, x4])
    x5 = Conv2D(filters=fil, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(x4)
    x5 = Lambda((lambda x: (x * beta)))(x5)
    xout = Add()([x5, x])
    return xout

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 11:------------------- similar code ------------------ index = 102, score = 1.0 
def _wrapper(x):
    for _ in range(self._params['nb_dense_blocks']):
        for _ in range(self._params['layers_per_dense_block']):
            out_conv = keras.layers.Conv2D(filters=self._params['growth_rate'], kernel_size=(3, 3), padding='same', activation='relu')(x)
            x = keras.layers.Concatenate(axis=(- 1))([x, out_conv])
        scale_down_ratio = self._params['transition_scale_down_ratio']
        nb_filter = int((K.int_shape(x)[(- 1)] * scale_down_ratio))
        x = keras.layers.Conv2D(filters=nb_filter, kernel_size=(1, 1), padding='same', activation=None)(x)
        x = keras.layers.MaxPool2D(strides=(2, 2))(x)
    out_densenet = keras.layers.Flatten()(x)
    return out_densenet

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
    for  ...  in:
        for  ...  in:
             ...  =  ... .Conv2D

idx = 12:------------------- similar code ------------------ index = 101, score = 1.0 
def rpn_graph(feature_map, anchors_per_location, anchor_stride):
    'Builds the computation graph of Region Proposal Network.\n\n    feature_map: backbone features [batch, height, width, depth]\n    anchors_per_location: number of anchors per pixel in the feature map\n    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n                   every pixel in the feature map), or 2 (every other pixel).\n\n    Returns:\n        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n        rpn_probs: [batch, H, W, 2] Anchor classifier probabilities.\n        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n                  applied to anchors.\n    '
    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')(feature_map)
    x = KL.Conv2D((2 * anchors_per_location), (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared)
    rpn_class_logits = KL.Lambda((lambda t: tf.reshape(t, [tf.shape(t)[0], (- 1), 2])))(x)
    rpn_probs = KL.Activation('softmax', name='rpn_class_xxx')(rpn_class_logits)
    x = KL.Conv2D((anchors_per_location * 4), (1, 1), padding='valid', activation='linear', name='rpn_bbox_pred')(shared)
    rpn_bbox = KL.Lambda((lambda t: tf.reshape(t, [tf.shape(t)[0], (- 1), 4])))(x)
    return [rpn_class_logits, rpn_probs, rpn_bbox]

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .Conv2D

idx = 13:------------------- similar code ------------------ index = 100, score = 1.0 
def build_linknet(backbone, classes, skip_connection_layers, decoder_filters=(None, None, None, None, 16), upsample_rates=(2, 2, 2, 2, 2), n_upsample_blocks=5, upsample_kernel_size=(3, 3), upsample_layer='upsampling', activation='sigmoid', use_batchnorm=False):
    input = backbone.input
    x = backbone.output
    skip_connection_idx = [(get_layer_number(backbone, l) if isinstance(l, str) else l) for l in skip_connection_layers]
    for i in range(n_upsample_blocks):
        skip_connection = None
        if (i < len(skip_connection_idx)):
            skip_connection = backbone.layers[skip_connection_idx[i]].output
        upsample_rate = to_tuple(upsample_rates[i])
        print(upsample_rate)
        x = DecoderBlock(stage=i, filters=decoder_filters[i], kernel_size=upsample_kernel_size, upsample_rate=upsample_rate, use_batchnorm=use_batchnorm, upsample_layer=upsample_layer, skip=skip_connection)(x)
    x = Conv2D(classes, (3, 3), padding='same', name='final_conv')(x)
    x = Activation(activation, name=activation)(x)
    model = Model(input, x)
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 14:------------------- similar code ------------------ index = 99, score = 1.0 
def pyramid_block(pyramid_filters=256, segmentation_filters=128, upsample_rate=2, use_batchnorm=False):
    '\n    Pyramid block according to:\n        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n\n    This block generate `M` and `P` blocks.\n\n    Args:\n        pyramid_filters: integer, filters in `M` block of top-down FPN branch\n        segmentation_filters: integer, number of filters in segmentation head,\n            basically filters in convolution layers between `M` and `P` blocks\n        upsample_rate: integer, uspsample rate for `M` block of top-down FPN branch\n        use_batchnorm: bool, include batchnorm in convolution blocks\n\n    Returns:\n        Pyramid block function (as Keras layers functional API)\n    '

    def layer(c, m=None):
        x = Conv2D(pyramid_filters, (1, 1))(c)
        if (m is not None):
            up = UpSampling2D((upsample_rate, upsample_rate))(m)
            x = Add()([x, up])
        p = Conv(segmentation_filters, (3, 3), padding='same', use_batchnorm=use_batchnorm)(x)
        p = Conv(segmentation_filters, (3, 3), padding='same', use_batchnorm=use_batchnorm)(p)
        m = x
        return (m, p)
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ():
         ...  = Conv2D

idx = 15:------------------- similar code ------------------ index = 97, score = 1.0 
def det_conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), use_bias=True, train_bn=True):
    "conv_block is the block that has a conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    "
    (nb_filter1, nb_filter2, nb_filter3) = filters
    conv_name_base = ((('res' + str(stage)) + block) + '_branch')
    bn_name_base = ((('bn' + str(stage)) + block) + '_branch')
    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides, name=(conv_name_base + '2a'), use_bias=use_bias)(input_tensor)
    x = BatchNorm(axis=3, name=(bn_name_base + '2a'))(x)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=(conv_name_base + '2b'), use_bias=use_bias, dilation_rate=2)(x)
    x = BatchNorm(axis=3, name=(bn_name_base + '2b'))(x)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter3, (1, 1), name=(conv_name_base + '2c'), use_bias=use_bias)(x)
    x = BatchNorm(axis=3, name=(bn_name_base + '2c'))(x)
    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides, name=(conv_name_base + '1'), use_bias=use_bias)(input_tensor)
    shortcut = BatchNorm(axis=3, name=(bn_name_base + '1'))(shortcut)
    x = KL.Add()([x, shortcut])
    x = KL.Activation('relu', name=((('res' + str(stage)) + block) + '_out'))(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .Conv2D

idx = 16:------------------- similar code ------------------ index = 95, score = 1.0 
def fpn_classifier_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn=True):
    'Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from diffent layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    - image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layres\n\n    Returns:\n        logits: [N, NUM_CLASSES] classifier logits (before softmax)\n        probs: [N, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [N, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    '
    x = PyramidROIAlign([pool_size, pool_size], name='roi_align_classifier')(([rois, image_meta] + feature_maps))
    x = KL.TimeDistributed(KL.Conv2D(1024, (pool_size, pool_size), padding='valid'), name='mrcnn_class_conv1')(x)
    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.TimeDistributed(KL.Conv2D(1024, (1, 1)), name='mrcnn_class_conv2')(x)
    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)
    x = KL.Activation('relu')(x)
    shared = KL.Lambda((lambda x: K.squeeze(K.squeeze(x, 3), 2)), name='pool_squeeze')(x)
    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes), name='mrcnn_class_logits')(shared)
    mrcnn_probs = KL.TimeDistributed(KL.Activation('softmax'), name='mrcnn_class')(mrcnn_class_logits)
    x = KL.TimeDistributed(KL.Dense((num_classes * 4), activation='linear'), name='mrcnn_bbox_fc')(shared)
    s = K.int_shape(x)
    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name='mrcnn_bbox')(x)
    return (mrcnn_class_logits, mrcnn_probs, mrcnn_bbox)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... . ... ( ... .Conv2D,)

idx = 17:------------------- similar code ------------------ index = 94, score = 1.0 
def layer(x):
    x = Conv2D(filters, kernel_size, padding='same', name=conv_name, use_bias=(not use_batchnorm))(x)
    if use_batchnorm:
        x = BatchNormalization(name=bn_name)(x)
    x = Activation('relu', name=relu_name)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 18:------------------- similar code ------------------ index = 91, score = 1.0 
def fpn_classifier_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn=True):
    'Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from diffent layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    - image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layres\n\n    Returns:\n        logits: [N, NUM_CLASSES] classifier logits (before softmax)\n        probs: [N, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [N, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    '
    x = PyramidROIAlign([pool_size, pool_size], name='roi_align_classifier')(([rois, image_meta] + feature_maps))
    x = KL.TimeDistributed(KL.Conv2D(1024, (pool_size, pool_size), padding='valid'), name='mrcnn_class_conv1')(x)
    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.TimeDistributed(KL.Conv2D(1024, (1, 1)), name='mrcnn_class_conv2')(x)
    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)
    x = KL.Activation('relu')(x)
    shared = KL.Lambda((lambda x: K.squeeze(K.squeeze(x, 3), 2)), name='pool_squeeze')(x)
    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes), name='mrcnn_class_logits')(shared)
    mrcnn_probs = KL.TimeDistributed(KL.Activation('softmax'), name='mrcnn_class')(mrcnn_class_logits)
    x = KL.TimeDistributed(KL.Dense((num_classes * 4), activation='linear'), name='mrcnn_bbox_fc')(shared)
    s = K.int_shape(x)
    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name='mrcnn_bbox')(x)
    return (mrcnn_class_logits, mrcnn_probs, mrcnn_bbox)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... . ... ( ... .Conv2D,)

idx = 19:------------------- similar code ------------------ index = 111, score = 1.0 
def call(self, inputs):
    rejoin = (- 1)
    x = inputs
    for layer in self.conv:
        if isinstance(self.conv[layer], tf.keras.layers.Conv2D):
            block = int(layer.split('-')[1])
            if ((block > 0) and (self.shortcuts[(block - 1)] == 1)):
                rejoin = (block + 1)
                y = x
                count_downsampling = ((sum(self.apply_maxpools[block:(block + 2)]) + sum(self.strides[block:(block + 2)])) - 2)
                for _ in range(count_downsampling):
                    y = tf.keras.layers.AveragePooling2D(pool_size=(2, 2))(y)
                y = tf.pad(y, [[0, 0], [0, 0], [0, 0], [0, (self.out_channels[(block + 1)] - self.out_channels[(block - 1)])]], 'CONSTANT')
        if ((block == rejoin) and ('act' in layer)):
            x = tf.keras.layers.Add()([x, y])
        x = self.conv[layer](x)
    x = tf.keras.layers.Flatten()(x)
    for layer in self.mlp:
        x = self.mlp[layer](x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    for  ...  in:
        if  ... (,  ... .Conv2D):
idx = 20:------------------- similar code ------------------ index = 74, score = 1.0 
def __init__(self, y_dim: int, x_dim: int, vision_hidden_size: int, R: int, c_out: int, NUM_SYMBOLS: int, minimum_filters: int, **kwargs):
    super(ConvDiscriminator, self).__init__()
    self.res_preps = []
    self.residual_blocks_1 = []
    self.residual_blocks_2 = []
    self.maxpools = []
    for r in range(R):
        filters = (vision_hidden_size // (2 ** ((R - r) - 1)))
        filters = max([filters, minimum_filters])
        res_prep = tf.keras.layers.Conv2D(filters, kernel_size=1, strides=1, padding='same', **cnn_regularization)
        residual_block_1 = ResidualBlock(filters)
        residual_block_2 = ResidualBlock(filters)
        self.res_preps.append(res_prep)
        self.residual_blocks_1.append(residual_block_1)
        self.residual_blocks_2.append(residual_block_2)
        self.maxpools.append(tf.keras.layers.Conv2D(filters, kernel_size=3, strides=2, padding='same', **cnn_regularization))
    self.out_conv = tf.keras.layers.Conv2D(vision_hidden_size, kernel_size=1, strides=1, padding='same', **dense_regularization)
    self.pred = tf.keras.layers.Dense(NUM_SYMBOLS, **dense_regularization)
    self.gap = tf.keras.layers.GlobalAveragePooling2D()

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    for  ...  in:
         ...  =  ... .Conv2D

idx = 21:------------------- similar code ------------------ index = 88, score = 1.0 
def layer(x):
    x = Conv2D(filters, kernel_size, padding='same', name=conv_name, use_bias=(not use_batchnorm))(x)
    if use_batchnorm:
        x = BatchNormalization(name=bn_name)(x)
    x = Activation('relu', name=relu_name)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 22:------------------- similar code ------------------ index = 87, score = 1.0 
def __init__(self, config, image_size):
    super(decoder, self).__init__()
    self.num_hiddens = config['num_hiddens']
    self.num_residual_hiddens = config['num_residual_hiddens']
    self.num_residual_layers = config['num_residual_layers']
    self.conv1 = Conv2D(self.num_hiddens, kernel_size=(3, 3), strides=(1, 1), padding='same')
    self.residual_stack = residual(self.num_hiddens, self.num_residual_layers, self.num_residual_hiddens)
    self.flatten = Flatten()
    self.dense1 = Dense((((image_size[0] // 4) * (image_size[1] // 4)) * 128), activation='relu')
    self.reshape = Reshape(((image_size[0] // 4), (image_size[1] // 4), 128))
    self.upconv1 = Conv2DTranspose((self.num_hiddens // 2), kernel_size=(4, 4), strides=(2, 2), activation='relu', padding='same')
    self.upconv2 = Conv2DTranspose(image_size[(- 1)], kernel_size=(4, 4), strides=(2, 2), padding='same')

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
 = Conv2D

idx = 23:------------------- similar code ------------------ index = 83, score = 1.0 
def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), use_bias=True, train_bn=True):
    "conv_block is the block that has a conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layres\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    "
    (nb_filter1, nb_filter2, nb_filter3) = filters
    conv_name_base = ((('res' + str(stage)) + block) + '_branch')
    bn_name_base = ((('bn' + str(stage)) + block) + '_branch')
    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides, name=(conv_name_base + '2a'), use_bias=use_bias)(input_tensor)
    x = BatchNorm(name=(bn_name_base + '2a'))(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=(conv_name_base + '2b'), use_bias=use_bias)(x)
    x = BatchNorm(name=(bn_name_base + '2b'))(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter3, (1, 1), name=(conv_name_base + '2c'), use_bias=use_bias)(x)
    x = BatchNorm(name=(bn_name_base + '2c'))(x, training=train_bn)
    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides, name=(conv_name_base + '1'), use_bias=use_bias)(input_tensor)
    shortcut = BatchNorm(name=(bn_name_base + '1'))(shortcut, training=train_bn)
    x = KL.Add()([x, shortcut])
    x = KL.Activation('relu', name=((('res' + str(stage)) + block) + '_out'))(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .Conv2D

idx = 24:------------------- similar code ------------------ index = 79, score = 1.0 
def ConvRelu(filters, kernel_size, use_batchnorm=False, conv_name='conv', bn_name='bn', relu_name='relu'):

    def layer(x):
        x = Conv2D(filters, kernel_size, padding='same', name=conv_name, use_bias=(not use_batchnorm))(x)
        if use_batchnorm:
            x = BatchNormalization(name=bn_name)(x)
        x = Activation('relu', name=relu_name)(x)
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():

    def  ... ( ... ):
         ...  = Conv2D

idx = 25:------------------- similar code ------------------ index = 110, score = 1.0 
def __init__(self, restore=None, session=None, use_softmax=False):
    self.num_channels = 3
    self.image_size = 32
    self.num_labels = 10
    model = Sequential()
    model.add(Conv2D(64, (3, 3), input_shape=(32, 32, 3)))
    model.add(Activation('relu'))
    model.add(Conv2D(64, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(128, (3, 3)))
    model.add(Activation('relu'))
    model.add(Conv2D(128, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(256))
    model.add(Activation('relu'))
    model.add(Dense(256))
    model.add(Activation('relu'))
    model.add(Dense(10))
    if use_softmax:
        model.add(Activation('softmax'))
    if restore:
        model.load_weights(restore)
    self.model = model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ... . ... (Conv2D)

idx = 26:------------------- similar code ------------------ index = 251, score = 1.0 
def identity_block(filters, stage, block):
    "The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    "

    def layer(input_tensor):
        conv_params = get_conv_params()
        bn_params = get_bn_params()
        (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
        x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
        x = Activation('relu', name=(relu_name + '1'))(x)
        x = Conv2D(filters, (1, 1), name=(conv_name + '1'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '2'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = Conv2D(filters, (3, 3), name=(conv_name + '2'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '3'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '3'))(x)
        x = Conv2D((filters * 4), (1, 1), name=(conv_name + '3'), **conv_params)(x)
        x = Add()([x, input_tensor])
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 27:------------------- similar code ------------------ index = 112, score = 1.0 
def resnet_backbone(image, num_blocks, group_func, block_func):
    '\n    Sec 5.1: We adopt the initialization of [15] for all convolutional layers.\n    TensorFlow does not have the true "MSRA init". We use variance_scaling as an approximation.\n    '
    with argscope(Conv2D, use_bias=False, kernel_initializer=tf.variance_scaling_initializer(scale=2.0, mode='fan_out')):
        l = Conv2D('conv0', image, 64, 7, strides=2, activation=BNReLU)
        l = MaxPooling('pool0', l, pool_size=3, strides=2, padding='SAME')
        l = group_func('group0', l, block_func, 64, num_blocks[0], 1)
        l = group_func('group1', l, block_func, 128, num_blocks[1], 2)
        l = group_func('group2', l, block_func, 256, num_blocks[2], 2)
        l = group_func('group3', l, block_func, 512, num_blocks[3], 2)
        l = GlobalAvgPooling('gap', l)
        logits = FullyConnected('linear', l, 1000, kernel_initializer=tf.random_normal_initializer(stddev=0.01))
    '\n    Sec 5.1:\n    The 1000-way fully-connected layer is initialized by\n    drawing weights from a zero-mean Gaussian with standard\n    deviation of 0.01.\n    '
    return logits

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    with  ... (Conv2D,,):
idx = 28:------------------- similar code ------------------ index = 72, score = 1.0 
def generator(inp_shape, trainable=True):
    gamma_init = tf.random_normal_initializer(1.0, 0.02)
    fd = 512
    gr = 32
    nb = 12
    betad = 0.2
    betar = 0.2
    inp_real_imag = Input(inp_shape)
    lay_128dn = Conv2D(64, (4, 4), strides=(2, 2), padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(inp_real_imag)
    lay_128dn = LeakyReLU(alpha=0.2)(lay_128dn)
    lay_64dn = Conv2D(128, (4, 4), strides=(2, 2), padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(lay_128dn)
    lay_64dn = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(lay_64dn)
    lay_64dn = LeakyReLU(alpha=0.2)(lay_64dn)
    lay_32dn = Conv2D(256, (4, 4), strides=(2, 2), padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(lay_64dn)
    lay_32dn = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(lay_32dn)
    lay_32dn = LeakyReLU(alpha=0.2)(lay_32dn)
    lay_16dn = Conv2D(512, (4, 4), strides=(2, 2), padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(lay_32dn)
    lay_16dn = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(lay_16dn)
    lay_16dn = LeakyReLU(alpha=0.2)(lay_16dn)
    lay_8dn = Conv2D(512, (4, 4), strides=(2, 2), padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(lay_16dn)
    lay_8dn = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(lay_8dn)
    lay_8dn = LeakyReLU(alpha=0.2)(lay_8dn)
    xc1 = Conv2D(filters=fd, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(lay_8dn)
    xrrd = xc1
    for m in range(nb):
        xrrd = resresden(xrrd, fd, gr, betad, betar, gamma_init, trainable)
    xc2 = Conv2D(filters=fd, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(xrrd)
    lay_8upc = Add()([xc1, xc2])
    lay_16up = Conv2DTranspose(1024, (4, 4), strides=(2, 2), padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(lay_8upc)
    lay_16up = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(lay_16up)
    lay_16up = Activation('relu')(lay_16up)
    lay_16upc = Concatenate(axis=(- 1))([lay_16up, lay_16dn])
    lay_32up = Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(lay_16upc)
    lay_32up = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(lay_32up)
    lay_32up = Activation('relu')(lay_32up)
    lay_32upc = Concatenate(axis=(- 1))([lay_32up, lay_32dn])
    lay_64up = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(lay_32upc)
    lay_64up = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(lay_64up)
    lay_64up = Activation('relu')(lay_64up)
    lay_64upc = Concatenate(axis=(- 1))([lay_64up, lay_64dn])
    lay_128up = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(lay_64upc)
    lay_128up = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(lay_128up)
    lay_128up = Activation('relu')(lay_128up)
    lay_128upc = Concatenate(axis=(- 1))([lay_128up, lay_128dn])
    lay_256up = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(lay_128upc)
    lay_256up = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(lay_256up)
    lay_256up = Activation('relu')(lay_256up)
    out = Conv2D(1, (1, 1), strides=(1, 1), activation='tanh', padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(lay_256up)
    model = Model(inputs=inp_real_imag, outputs=out)
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 29:------------------- similar code ------------------ index = 149, score = 1.0 
def residual(name, x, chan):
    with tf.variable_scope(name):
        x = Conv2D('res1', x, chan, 3)
        x = BatchNorm('bn1', x)
        x = activation(x)
        x = Conv2D('res2', x, chan, 3)
        x = BatchNorm('bn2', x)
        x = activation(x)
        return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    with:
         ...  = Conv2D

idx = 30:------------------- similar code ------------------ index = 147, score = 1.0 
def layer(input_tensor):
    x = Conv2D(n_filters, kernel_size, use_bias=(not use_batchnorm), **kwargs)(input_tensor)
    if use_batchnorm:
        x = BatchNormalization()(x)
    x = Activation(activation)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 31:------------------- similar code ------------------ index = 145, score = 1.0 
def rpn_graph(feature_map, anchors_per_location, anchor_stride):
    'Builds the computation graph of Region Proposal Network.\n\n    feature_map: backbone features [batch, height, width, depth]\n    anchors_per_location: number of anchors per pixel in the feature map\n    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n                   every pixel in the feature map), or 2 (every other pixel).\n\n    Returns:\n        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n        rpn_probs: [batch, H, W, 2] Anchor classifier probabilities.\n        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n                  applied to anchors.\n    '
    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')(feature_map)
    x = KL.Conv2D((2 * anchors_per_location), (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared)
    rpn_class_logits = KL.Lambda((lambda t: tf.reshape(t, [tf.shape(t)[0], (- 1), 2])))(x)
    rpn_probs = KL.Activation('softmax', name='rpn_class_xxx')(rpn_class_logits)
    x = KL.Conv2D((anchors_per_location * 4), (1, 1), padding='valid', activation='linear', name='rpn_bbox_pred')(shared)
    rpn_bbox = KL.Lambda((lambda t: tf.reshape(t, [tf.shape(t)[0], (- 1), 4])))(x)
    return [rpn_class_logits, rpn_probs, rpn_bbox]

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .Conv2D

idx = 32:------------------- similar code ------------------ index = 144, score = 1.0 
def conv2d_bn(x, filters, kernel_size, strides=1, padding='same', activation='relu', use_bias=False, name=None):
    "Utility function to apply conv + BN.\n    # Arguments\n        x: input tensor.\n        filters: filters in `Conv2D`.\n        kernel_size: kernel size as in `Conv2D`.\n        strides: strides in `Conv2D`.\n        padding: padding mode in `Conv2D`.\n        activation: activation in `Conv2D`.\n        use_bias: whether to use a bias in `Conv2D`.\n        name: name of the ops; will become `name + '_ac'` for the activation\n            and `name + '_bn'` for the batch norm layer.\n    # Returns\n        Output tensor after applying `Conv2D` and `BatchNormalization`.\n    "
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias, name=name)(x)
    if (not use_bias):
        bn_axis = (1 if (K.image_data_format() == 'channels_first') else 3)
        bn_name = (None if (name is None) else (name + '_bn'))
        x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)
    if (activation is not None):
        ac_name = (None if (name is None) else (name + '_ac'))
        x = Activation(activation, name=ac_name)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 33:------------------- similar code ------------------ index = 143, score = 1.0 
def build_unet(backbone, classes, skip_connection_layers, decoder_filters=(256, 128, 64, 32, 16), upsample_rates=(2, 2, 2, 2, 2), n_upsample_blocks=5, block_type='upsampling', activation='tanh', use_batchnorm=True):
    input = backbone.input
    x = backbone.output
    if (block_type == 'transpose'):
        up_block = Transpose2D_block
    else:
        up_block = Upsample2D_block
    skip_connection_idx = [(get_layer_number(backbone, l) if isinstance(l, str) else l) for l in skip_connection_layers]
    for i in range(n_upsample_blocks):
        skip_connection = None
        if (i < len(skip_connection_idx)):
            skip_connection = backbone.layers[skip_connection_idx[i]].output
        upsample_rate = to_tuple(upsample_rates[i])
        x = up_block(decoder_filters[i], i, upsample_rate=upsample_rate, skip=skip_connection, use_batchnorm=use_batchnorm)(x)
    x = Conv2D(1, (3, 3), padding='same', name='final_conv')(x)
    x = Dense(1)(x)
    model = Model(input, x)
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 34:------------------- similar code ------------------ index = 142, score = 1.0 
def rpn_graph(feature_map, anchors_per_location, anchor_stride):
    'Builds the computation graph of Region Proposal Network.\n\n    feature_map: backbone features [batch, height, width, depth]\n    anchors_per_location: number of anchors per pixel in the feature map\n    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n                   every pixel in the feature map), or 2 (every other pixel).\n\n    Returns:\n        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n        rpn_probs: [batch, H, W, 2] Anchor classifier probabilities.\n        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n                  applied to anchors.\n    '
    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')(feature_map)
    x = KL.Conv2D((2 * anchors_per_location), (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared)
    rpn_class_logits = KL.Lambda((lambda t: tf.reshape(t, [tf.shape(t)[0], (- 1), 2])))(x)
    rpn_probs = KL.Activation('softmax', name='rpn_class_xxx')(rpn_class_logits)
    x = KL.Conv2D((anchors_per_location * 4), (1, 1), padding='valid', activation='linear', name='rpn_bbox_pred')(shared)
    rpn_bbox = KL.Lambda((lambda t: tf.reshape(t, [tf.shape(t)[0], (- 1), 4])))(x)
    return [rpn_class_logits, rpn_probs, rpn_bbox]

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .Conv2D

idx = 35:------------------- similar code ------------------ index = 141, score = 1.0 
def block_func(l, ch_out, stride):
    BN = (lambda x, name=None: BatchNorm('bn', x))
    shortcut = l
    l = Conv2D('conv1', l, ch_out, 1, strides=stride, activation=BNReLU)
    l = Conv2D('conv2', l, ch_out, 3, strides=1, activation=BNReLU)
    l = Conv2D('conv3', l, (ch_out * 4), 1, activation=BN)
    return tf.nn.relu((l + resnet_shortcut(shortcut, (ch_out * 4), stride, activation=BN)))

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 36:------------------- similar code ------------------ index = 137, score = 1.0 
def Conv(n_filters, kernel_size, activation='relu', use_batchnorm=False, **kwargs):
    'Extension of Conv2aaD layer with batchnorm'

    def layer(input_tensor):
        x = Conv2D(n_filters, kernel_size, use_bias=(not use_batchnorm), **kwargs)(input_tensor)
        if use_batchnorm:
            x = BatchNormalization()(x)
        x = Activation(activation)(x)
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 37:------------------- similar code ------------------ index = 136, score = 1.0 
def basic_conv_block(filters, stage, block, strides=(2, 2)):
    "The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    "

    def layer(input_tensor):
        conv_params = get_conv_params()
        bn_params = get_bn_params()
        (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
        x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
        x = Activation('relu', name=(relu_name + '1'))(x)
        shortcut = x
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = Conv2D(filters, (3, 3), strides=strides, name=(conv_name + '1'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '2'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = Conv2D(filters, (3, 3), name=(conv_name + '2'), **conv_params)(x)
        shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)
        x = Add()([x, shortcut])
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 38:------------------- similar code ------------------ index = 132, score = 1.0 
def resnet_graph(input_image, architecture, stage5=False, train_bn=True, dilation=[]):
    'Build a ResNet graph.\n        architecture: Can be resnet50 or resnet101\n        stage5: Boolean. If False, stage5 of the network is not created\n        train_bn: Boolean. Train or freeze Batch Norm layres\n    '
    assert (architecture in ['resnet50', 'resnet101'])
    x = KL.ZeroPadding2D((3, 3))(input_image)
    x = KL.Conv2D(64, (7, 7), strides=(1, 1), name='conv1', use_bias=True)(x)
    x = BatchNorm(name='bn_conv1')(x, training=train_bn)
    x = KL.Activation('relu')(x)
    C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    if (2 in dilation):
        x = det_conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), train_bn=train_bn)
        print('stage 2 dilated')
    else:
        x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), train_bn=train_bn)
    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_bn=train_bn)
    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_bn=train_bn)
    if (3 in dilation):
        x = det_conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn)
        print('stage 3 dilated')
    else:
        x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn)
    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_bn=train_bn)
    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_bn=train_bn)
    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_bn=train_bn)
    if (4 in dilation):
        x = det_conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn)
        print('stage 4 dilated')
    else:
        x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn)
    block_count = {'resnet50': 5, 'resnet101': 22}[architecture]
    for i in range(block_count):
        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr((98 + i)), train_bn=train_bn)
    C4 = x
    if stage5:
        if (5 in dilation):
            x = det_conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn)
            print('stage 5 dilated')
        else:
            x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn)
        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_bn=train_bn)
        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_bn=train_bn)
    else:
        C5 = None
    return [C1, C2, C3, C4, C5]

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .Conv2D

idx = 39:------------------- similar code ------------------ index = 128, score = 1.0 
def ConvRelu(filters, kernel_size, use_batchnorm=False, conv_name='conv', bn_name='bn', relu_name='relu'):

    def layer(x):
        x = Conv2D(filters, kernel_size, padding='same', name=conv_name, use_bias=(not use_batchnorm))(x)
        if use_batchnorm:
            x = BatchNormalization(name=bn_name)(x)
        x = Activation('relu', name=relu_name)(x)
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():

    def  ... ( ... ):
         ...  = Conv2D

idx = 40:------------------- similar code ------------------ index = 127, score = 1.0 
def fpn_classifier_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn=True):
    'Builds the computation graph of the feature pyramid network classifier\n    and regressor heads.\n\n    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized\n          coordinates.\n    feature_maps: List of feature maps from diffent layers of the pyramid,\n                  [P2, P3, P4, P5]. Each has a different resolution.\n    - image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n    pool_size: The width of the square feature map generated from ROI Pooling.\n    num_classes: number of classes, which determines the depth of the results\n    train_bn: Boolean. Train or freeze Batch Norm layres\n\n    Returns:\n        logits: [N, NUM_CLASSES] classifier logits (before softmax)\n        probs: [N, NUM_CLASSES] classifier probabilities\n        bbox_deltas: [N, (dy, dx, log(dh), log(dw))] Deltas to apply to\n                     proposal boxes\n    '
    x = PyramidROIAlign([pool_size, pool_size], name='roi_align_classifier')(([rois, image_meta] + feature_maps))
    x = KL.TimeDistributed(KL.Conv2D(1024, (pool_size, pool_size), padding='valid'), name='mrcnn_class_conv1')(x)
    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.TimeDistributed(KL.Conv2D(1024, (1, 1)), name='mrcnn_class_conv2')(x)
    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)
    x = KL.Activation('relu')(x)
    shared = KL.Lambda((lambda x: K.squeeze(K.squeeze(x, 3), 2)), name='pool_squeeze')(x)
    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes), name='mrcnn_class_logits')(shared)
    mrcnn_probs = KL.TimeDistributed(KL.Activation('softmax'), name='mrcnn_class')(mrcnn_class_logits)
    x = KL.TimeDistributed(KL.Dense((num_classes * 4), activation='linear'), name='mrcnn_bbox_fc')(shared)
    s = K.int_shape(x)
    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name='mrcnn_bbox')(x)
    return (mrcnn_class_logits, mrcnn_probs, mrcnn_bbox)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... . ... ( ... .Conv2D,)

idx = 41:------------------- similar code ------------------ index = 250, score = 1.0 
def bottom_up_agg(Ps):
    (P2, P3, P4, P5) = Ps
    N2 = P2
    N3 = KL.Add()([P3, KL.Conv2D(256, (3, 3), strides=(2, 2), padding='SAME')(N2)])
    N3 = KL.Conv2D(256, (3, 3), padding='SAME')(N3)
    N4 = KL.Add()([P4, KL.Conv2D(256, (3, 3), strides=(2, 2), padding='SAME')(N3)])
    N4 = KL.Conv2D(256, (3, 3), padding='SAME')(N4)
    N5 = KL.Add()([P5, KL.Conv2D(256, (3, 3), strides=(2, 2), padding='SAME')(N4)])
    N5 = KL.Conv2D(256, (3, 3), padding='SAME')(N5)
    return [N2, N3, N4, N5]

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  =  ... ([ ... ,  ... .Conv2D])

idx = 42:------------------- similar code ------------------ index = 124, score = 1.0 
def __init__(self, restore=None, session=None, use_softmax=False):
    self.num_channels = 1
    self.image_size = 28
    self.num_labels = 10
    model = Sequential()
    model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1)))
    model.add(Activation('relu'))
    model.add(Conv2D(32, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(64, (3, 3)))
    model.add(Activation('relu'))
    model.add(Conv2D(64, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(200))
    model.add(Activation('relu'))
    model.add(Dense(200))
    model.add(Activation('relu'))
    model.add(Dense(10))
    if use_softmax:
        model.add(Activation('softmax'))
    if restore:
        model.load_weights(restore)
    self.model = model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ... . ... (Conv2D)

idx = 43:------------------- similar code ------------------ index = 123, score = 1.0 
def resnet_graph(input_image, architecture, stage5=False, train_bn=True, dilation=[]):
    'Build a ResNet graph.\n        architecture: Can be resnet50 or resnet101\n        stage5: Boolean. If False, stage5 of the network is not created\n        train_bn: Boolean. Train or freeze Batch Norm layres\n    '
    assert (architecture in ['resnet50', 'resnet101'])
    x = KL.ZeroPadding2D((3, 3))(input_image)
    x = KL.Conv2D(64, (7, 7), strides=(1, 1), name='conv1', use_bias=True)(x)
    x = BatchNorm(name='bn_conv1')(x, training=train_bn)
    x = KL.Activation('relu')(x)
    C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    if (2 in dilation):
        x = det_conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), train_bn=train_bn)
        print('stage 2 dilated')
    else:
        x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), train_bn=train_bn)
    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_bn=train_bn)
    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_bn=train_bn)
    if (3 in dilation):
        x = det_conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn)
        print('stage 3 dilated')
    else:
        x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn)
    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_bn=train_bn)
    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_bn=train_bn)
    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_bn=train_bn)
    if (4 in dilation):
        x = det_conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn)
        print('stage 4 dilated')
    else:
        x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn)
    block_count = {'resnet50': 5, 'resnet101': 22}[architecture]
    for i in range(block_count):
        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr((98 + i)), train_bn=train_bn)
    C4 = x
    if stage5:
        if (5 in dilation):
            x = det_conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn)
            print('stage 5 dilated')
        else:
            x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn)
        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_bn=train_bn)
        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_bn=train_bn)
    else:
        C5 = None
    return [C1, C2, C3, C4, C5]

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .Conv2D

idx = 44:------------------- similar code ------------------ index = 122, score = 1.0 
def conv_block(filters, stage, block, strides=(2, 2)):
    "The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    "

    def layer(input_tensor):
        conv_params = get_conv_params()
        bn_params = get_bn_params()
        (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
        x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
        x = Activation('relu', name=(relu_name + '1'))(x)
        shortcut = x
        x = Conv2D(filters, (1, 1), name=(conv_name + '1'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '2'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = Conv2D(filters, (3, 3), strides=strides, name=(conv_name + '2'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '3'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '3'))(x)
        x = Conv2D((filters * 4), (1, 1), name=(conv_name + '3'), **conv_params)(x)
        shortcut = Conv2D((filters * 4), (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)
        x = Add()([x, shortcut])
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 45:------------------- similar code ------------------ index = 121, score = 1.0 
def discriminator(self):
    'Discriminator module for CycleGAN. Use it as a regular TensorFlow 2.0 Keras Model.\n\n        Return:\n            A tf.keras model  \n        '
    kernel_initializer = self.config['kernel_initializer']
    kernel_size = self.config['kernel_size']
    disc_channels = self.config['disc_channels']
    inputs = Input(shape=self.img_size)
    x = inputs
    down_stack = []
    for (i, channel) in enumerate(disc_channels[:(- 1)]):
        if (i == 0):
            down_stack.append(self._downsample(channel, kernel_size=kernel_size, kernel_initializer=kernel_initializer, batchnorm=False))
        else:
            down_stack.append(self._downsample(channel, kernel_size=kernel_size, kernel_initializer=kernel_initializer))
    down_stack.append(ZeroPadding2D())
    down_stack.append(Conv2D(disc_channels[(- 1)], kernel_size=kernel_size, strides=1, kernel_initializer=kernel_initializer, use_bias=False))
    down_stack.append(BatchNormalization())
    down_stack.append(LeakyReLU())
    down_stack.append(ZeroPadding2D())
    last = Conv2D(1, kernel_size=kernel_size, strides=1, kernel_initializer=kernel_initializer)
    for down in down_stack:
        x = down(x)
    out = last(x)
    model = Model(inputs=inputs, outputs=out)
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ... . ... (Conv2D)

idx = 46:------------------- similar code ------------------ index = 120, score = 1.0 
def basic_identity_block(filters, stage, block):
    "The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    "

    def layer(input_tensor):
        conv_params = get_conv_params()
        bn_params = get_bn_params()
        (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
        x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
        x = Activation('relu', name=(relu_name + '1'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = Conv2D(filters, (3, 3), name=(conv_name + '1'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '2'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = Conv2D(filters, (3, 3), name=(conv_name + '2'), **conv_params)(x)
        x = Add()([x, input_tensor])
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 47:------------------- similar code ------------------ index = 118, score = 1.0 
def identity_block(filters, stage, block):
    "The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    "

    def layer(input_tensor):
        conv_params = get_conv_params()
        bn_params = get_bn_params()
        (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
        x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
        x = Activation('relu', name=(relu_name + '1'))(x)
        x = Conv2D(filters, (1, 1), name=(conv_name + '1'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '2'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = Conv2D(filters, (3, 3), name=(conv_name + '2'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '3'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '3'))(x)
        x = Conv2D((filters * 4), (1, 1), name=(conv_name + '3'), **conv_params)(x)
        x = Add()([x, input_tensor])
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 48:------------------- similar code ------------------ index = 117, score = 1.0 
def build_psp(backbone, psp_layer, last_upsampling_factor, classes=21, activation='softmax', conv_filters=512, pooling_type='avg', dropout=None, final_interpolation='bilinear', use_batchnorm=True):
    input = backbone.input
    x = extract_outputs(backbone, [psp_layer])[0]
    x = PyramidPoolingModule(conv_filters=conv_filters, pooling_type=pooling_type, use_batchnorm=use_batchnorm)(x)
    x = Conv2DBlock(512, (1, 1), activation='relu', padding='same', use_batchnorm=use_batchnorm)(x)
    if (dropout is not None):
        x = SpatialDropout2D(dropout)(x)
    x = Conv2D(classes, (3, 3), padding='same', name='final_conv')(x)
    if (final_interpolation == 'bilinear'):
        x = ResizeImage(to_tuple(last_upsampling_factor))(x)
    elif (final_interpolation == 'duc'):
        x = DUC(to_tuple(last_upsampling_factor))(x)
    else:
        raise ValueError(('Unsupported interpolation type {}. '.format(final_interpolation) + 'Use `duc` or `bilinear`.'))
    x = Activation(activation, name=activation)(x)
    model = Model(input, x)
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 49:------------------- similar code ------------------ index = 116, score = 1.0 
def build_linknet(backbone, classes, skip_connection_layers, decoder_filters=(None, None, None, None, 16), upsample_rates=(2, 2, 2, 2, 2), n_upsample_blocks=5, upsample_kernel_size=(3, 3), upsample_layer='upsampling', activation='sigmoid', use_batchnorm=True):
    input = backbone.input
    x = backbone.output
    skip_connection_idx = [(get_layer_number(backbone, l) if isinstance(l, str) else l) for l in skip_connection_layers]
    for i in range(n_upsample_blocks):
        skip_connection = None
        if (i < len(skip_connection_idx)):
            skip_connection = backbone.layers[skip_connection_idx[i]].output
        upsample_rate = to_tuple(upsample_rates[i])
        x = DecoderBlock(stage=i, filters=decoder_filters[i], kernel_size=upsample_kernel_size, upsample_rate=upsample_rate, use_batchnorm=use_batchnorm, upsample_layer=upsample_layer, skip=skip_connection)(x)
    x = Conv2D(classes, (3, 3), padding='same', name='final_conv')(x)
    x = Activation(activation, name=activation)(x)
    model = Model(input, x)
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 50:------------------- similar code ------------------ index = 114, score = 1.0 
def layer(input_tensor):
    conv_params = get_conv_params()
    bn_params = get_bn_params()
    (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
    x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
    x = Activation('relu', name=(relu_name + '1'))(x)
    shortcut = x
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = Conv2D(filters, (3, 3), strides=strides, name=(conv_name + '1'), **conv_params)(x)
    x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '2'))(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = Conv2D(filters, (3, 3), name=(conv_name + '2'), **conv_params)(x)
    shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)
    x = Add()([x, shortcut])
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 51:------------------- similar code ------------------ index = 70, score = 1.0 
def build_unet(backbone, classes, skip_connection_layers, decoder_filters=(256, 128, 64, 32, 16), upsample_rates=(2, 2, 2, 2, 2), n_upsample_blocks=5, block_type='upsampling', activation='sigmoid', use_batchnorm=False):
    input = backbone.input
    x = backbone.output
    if (block_type == 'transpose'):
        up_block = Transpose2D_block
    else:
        up_block = Upsample2D_block
    skip_connection_idx = [(get_layer_number(backbone, l) if isinstance(l, str) else l) for l in skip_connection_layers]
    for i in range(n_upsample_blocks):
        skip_connection = None
        if (i < len(skip_connection_idx)):
            skip_connection = backbone.layers[skip_connection_idx[i]].output
        upsample_rate = to_tuple(upsample_rates[i])
        x = up_block(decoder_filters[i], i, upsample_rate=upsample_rate, skip=skip_connection, use_batchnorm=use_batchnorm)(x)
    x = Conv2D(classes, (3, 3), padding='same', name='final_conv')(x)
    x = Activation(activation, name=activation)(x)
    model = Model(input, x)
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 52:------------------- similar code ------------------ index = 71, score = 1.0 
def conv_block(filters, stage, block, strides=(2, 2)):
    'The conv block is the block that has conv layer at shortcut.\n    # Arguments\n        filters: integer, used for first and second conv layers, third conv layer double this value\n        strides: tuple of integers, strides for conv (3x3) layer in block\n        stage: integer, current stage label, used for generating layer names\n        block: integer, current block label, used for generating layer names\n    # Returns\n        Output layer for the block.\n    '

    def layer(input_tensor):
        conv_params = get_conv_params()
        bn_params = get_bn_params()
        (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
        x = Conv2D(filters, (1, 1), name=(conv_name + '1'), **conv_params)(input_tensor)
        x = BatchNormalization(name=(bn_name + '1'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '1'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = GroupConv2D(filters, (3, 3), conv_params, (conv_name + '2'), strides=strides)(x)
        x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '2'))(x)
        x = Conv2D((filters * 2), (1, 1), name=(conv_name + '3'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '3'), **bn_params)(x)
        shortcut = Conv2D((filters * 2), (1, 1), name=sc_name, strides=strides, **conv_params)(input_tensor)
        shortcut = BatchNormalization(name=(sc_name + '_bn'), **bn_params)(shortcut)
        x = Add()([x, shortcut])
        x = Activation('relu', name=relu_name)(x)
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 53:------------------- similar code ------------------ index = 15, score = 1.0 
def conv_block(filters, stage, block, strides=(2, 2)):
    'The conv block is the block that has conv layer at shortcut.\n    # Arguments\n        filters: integer, used for first and second conv layers, third conv layer double this value\n        strides: tuple of integers, strides for conv (3x3) layer in block\n        stage: integer, current stage label, used for generating layer names\n        block: integer, current block label, used for generating layer names\n    # Returns\n        Output layer for the block.\n    '

    def layer(input_tensor):
        conv_params = get_conv_params()
        bn_params = get_bn_params()
        (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
        x = Conv2D(filters, (1, 1), name=(conv_name + '1'), **conv_params)(input_tensor)
        x = BatchNormalization(name=(bn_name + '1'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '1'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = GroupConv2D(filters, (3, 3), conv_params, (conv_name + '2'), strides=strides)(x)
        x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '2'))(x)
        x = Conv2D((filters * 2), (1, 1), name=(conv_name + '3'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '3'), **bn_params)(x)
        shortcut = Conv2D((filters * 2), (1, 1), name=sc_name, strides=strides, **conv_params)(input_tensor)
        shortcut = BatchNormalization(name=(sc_name + '_bn'), **bn_params)(shortcut)
        x = Add()([x, shortcut])
        x = Activation('relu', name=relu_name)(x)
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 54:------------------- similar code ------------------ index = 27, score = 1.0 
def build_graph(self, image, label):
    image = (image / 255.0)
    num_blocks = [3, 4, 6, 3]
    with argscope([Conv2D, MaxPooling, BatchNorm, GlobalAvgPooling], data_format='channels_first'), argscope(Conv2D, use_bias=False):
        logits = LinearWrap(image).tf.pad([[0, 0], [3, 3], [3, 3], [0, 0]]).Conv2D('conv0', 64, 7, strides=2, activation=BNReLU, padding='VALID').MaxPooling('pool0', 3, strides=2, padding='SAME').apply(group_func, 'group0', block_func, 64, num_blocks[0], 1).apply(group_func, 'group1', block_func, 128, num_blocks[1], 2).apply(group_func, 'group2', block_func, 256, num_blocks[2], 2).apply(group_func, 'group3', block_func, 512, num_blocks[3], 2).GlobalAvgPooling('gap').FullyConnected('linear', 1000, activation=tf.identity)()
    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
    cost = tf.reduce_mean(cost, name='cost')
    return cost

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    with  ... ([Conv2D,  ... ,  ... ,  ... ],),:
idx = 55:------------------- similar code ------------------ index = 26, score = 1.0 
def basic_identity_block(filters, stage, block):
    "The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    "

    def layer(input_tensor):
        conv_params = get_conv_params()
        bn_params = get_bn_params()
        (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
        x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
        x = Activation('relu', name=(relu_name + '1'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = Conv2D(filters, (3, 3), name=(conv_name + '1'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '2'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = Conv2D(filters, (3, 3), name=(conv_name + '2'), **conv_params)(x)
        x = Add()([x, input_tensor])
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 56:------------------- similar code ------------------ index = 24, score = 1.0 
if (__name__ == '__main__'):
    import os
    import numpy as np
    import keras.optimizers
    from keras.datasets import mnist
    from keras.preprocessing.image import ImageDataGenerator
    GPU_COUNT = 2
    ROOT_DIR = os.path.abspath('../')
    MODEL_DIR = os.path.join(ROOT_DIR, 'logs')

    def build_model(x_train, num_classes):
        tf.reset_default_graph()
        inputs = KL.Input(shape=x_train.shape[1:], name='input_image')
        x = KL.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1')(inputs)
        x = KL.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2')(x)
        x = KL.MaxPooling2D(pool_size=(2, 2), name='pool1')(x)
        x = KL.Flatten(name='flat1')(x)
        x = KL.Dense(128, activation='relu', name='dense1')(x)
        x = KL.Dense(num_classes, activation='softmax', name='dense2')(x)
        return KM.Model(inputs, x, 'digit_classifier_model')
    ((x_train, y_train), (x_test, y_test)) = mnist.load_data()
    x_train = (np.expand_dims(x_train, (- 1)).astype('float32') / 255)
    x_test = (np.expand_dims(x_test, (- 1)).astype('float32') / 255)
    print('x_train shape:', x_train.shape)
    print('x_test shape:', x_test.shape)
    datagen = ImageDataGenerator()
    model = build_model(x_train, 10)
    model = ParallelModel(model, GPU_COUNT)
    optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, clipnorm=5.0)
    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    model.summary()
    model.fit_generator(datagen.flow(x_train, y_train, batch_size=64), steps_per_epoch=50, epochs=10, verbose=1, validation_data=(x_test, y_test), callbacks=[keras.callbacks.TensorBoard(log_dir=MODEL_DIR, write_graph=True)])

------------------- similar code (pruned) ------------------ score = 0.2 
if:
    def  ... ():
         ...  =  ... .Conv2D

idx = 57:------------------- similar code ------------------ index = 23, score = 1.0 
def build_resnet(repetitions=(2, 2, 2, 2), include_top=True, input_tensor=None, input_shape=None, classes=1000, block_type='usual'):
    '\n    TODO\n    '
    input_shape = _obtain_input_shape(input_shape, default_size=224, min_size=197, data_format='channels_last', require_flatten=include_top)
    if (input_tensor is None):
        img_input = Input(shape=input_shape, name='data')
    elif (not K.is_keras_tensor(input_tensor)):
        img_input = Input(tensor=input_tensor, shape=input_shape)
    else:
        img_input = input_tensor
    no_scale_bn_params = get_bn_params(scale=False)
    bn_params = get_bn_params()
    conv_params = get_conv_params()
    init_filters = 64
    if (block_type == 'basic'):
        conv_block = basic_conv_block
        identity_block = basic_identity_block
    else:
        conv_block = usual_conv_block
        identity_block = usual_identity_block
    x = BatchNormalization(name='bn_data', **no_scale_bn_params)(img_input)
    x = ZeroPadding2D(padding=(3, 3))(x)
    x = Conv2D(init_filters, (7, 7), strides=(2, 2), name='conv0', **conv_params)(x)
    x = BatchNormalization(name='bn0', **bn_params)(x)
    x = Activation('relu', name='relu0')(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x)
    for (stage, rep) in enumerate(repetitions):
        for block in range(rep):
            filters = (init_filters * (2 ** stage))
            if ((block == 0) and (stage == 0)):
                x = conv_block(filters, stage, block, strides=(1, 1))(x)
            elif (block == 0):
                x = conv_block(filters, stage, block, strides=(2, 2))(x)
            else:
                x = identity_block(filters, stage, block)(x)
    x = BatchNormalization(name='bn1', **bn_params)(x)
    x = Activation('relu', name='relu1')(x)
    if include_top:
        x = GlobalAveragePooling2D(name='pool1')(x)
        x = Dense(classes, name='fc1')(x)
        x = Activation('softmax', name='softmax')(x)
    if (input_tensor is not None):
        inputs = get_source_inputs(input_tensor)
    else:
        inputs = img_input
    model = Model(inputs, x)
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 58:------------------- similar code ------------------ index = 21, score = 1.0 
def build_psp(backbone, psp_layer, last_upsampling_factor, classes=21, activation='softmax', conv_filters=512, pooling_type='avg', dropout=None, final_interpolation='bilinear', use_batchnorm=True):
    input = backbone.input
    x = extract_outputs(backbone, [psp_layer])[0]
    x = PyramidPoolingModule(conv_filters=conv_filters, pooling_type=pooling_type, use_batchnorm=use_batchnorm)(x)
    x = Conv2DBlock(512, (1, 1), activation='relu', padding='same', use_batchnorm=use_batchnorm)(x)
    if (dropout is not None):
        x = SpatialDropout2D(dropout)(x)
    x = Conv2D(classes, (3, 3), padding='same', name='final_conv')(x)
    if (final_interpolation == 'bilinear'):
        x = ResizeImage(to_tuple(last_upsampling_factor))(x)
    elif (final_interpolation == 'duc'):
        x = DUC(to_tuple(last_upsampling_factor))(x)
    else:
        raise ValueError(('Unsupported interpolation type {}. '.format(final_interpolation) + 'Use `duc` or `bilinear`.'))
    x = Activation(activation, name=activation)(x)
    model = Model(input, x)
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 59:------------------- similar code ------------------ index = 20, score = 1.0 
def __init__(self, filters: int):
    super(ResidualBlock, self).__init__()
    self.first_conv = tf.keras.layers.Conv2D((filters // 4), kernel_size=1, strides=1, padding='same', **cnn_regularization)
    self.second_conv = tf.keras.layers.Conv2D((filters // 4), kernel_size=3, strides=1, padding='same', **cnn_regularization)
    self.third_conv = tf.keras.layers.Conv2D(filters, kernel_size=1, strides=1, padding='same', **cnn_regularization)
    self.batch_norms = [tf.keras.layers.BatchNormalization() for _ in range(3)]

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
 =  ... .Conv2D

idx = 60:------------------- similar code ------------------ index = 18, score = 1.0 
def layer(input_tensor):
    conv_params = get_conv_params()
    bn_params = get_bn_params()
    (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
    x = Conv2D(filters, (1, 1), name=(conv_name + '1'), **conv_params)(input_tensor)
    x = BatchNormalization(name=(bn_name + '1'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '1'))(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = GroupConv2D(filters, (3, 3), conv_params, (conv_name + '2'))(x)
    x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '2'))(x)
    x = Conv2D((filters * 2), (1, 1), name=(conv_name + '3'), **conv_params)(x)
    x = BatchNormalization(name=(bn_name + '3'), **bn_params)(x)
    x = Add()([x, input_tensor])
    x = Activation('relu', name=relu_name)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 61:------------------- similar code ------------------ index = 17, score = 1.0 
def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), use_bias=True, train_bn=True):
    "conv_block is the block that has a conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layres\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    "
    (nb_filter1, nb_filter2, nb_filter3) = filters
    conv_name_base = ((('res' + str(stage)) + block) + '_branch')
    bn_name_base = ((('bn' + str(stage)) + block) + '_branch')
    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides, name=(conv_name_base + '2a'), use_bias=use_bias)(input_tensor)
    x = BatchNorm(name=(bn_name_base + '2a'))(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=(conv_name_base + '2b'), use_bias=use_bias)(x)
    x = BatchNorm(name=(bn_name_base + '2b'))(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter3, (1, 1), name=(conv_name_base + '2c'), use_bias=use_bias)(x)
    x = BatchNorm(name=(bn_name_base + '2c'))(x, training=train_bn)
    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides, name=(conv_name_base + '1'), use_bias=use_bias)(input_tensor)
    shortcut = BatchNorm(name=(bn_name_base + '1'))(shortcut, training=train_bn)
    x = KL.Add()([x, shortcut])
    x = KL.Activation('relu', name=((('res' + str(stage)) + block) + '_out'))(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .Conv2D

idx = 62:------------------- similar code ------------------ index = 16, score = 1.0 
def ConvRelu(filters, kernel_size, use_batchnorm=False, conv_name='conv', bn_name='bn', relu_name='relu'):

    def layer(x):
        x = Conv2D(filters, kernel_size, padding='same', name=conv_name, use_bias=(not use_batchnorm))(x)
        if use_batchnorm:
            x = BatchNormalization(name=bn_name)(x)
        x = Activation('relu', name=relu_name)(x)
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():

    def  ... ( ... ):
         ...  = Conv2D

idx = 63:------------------- similar code ------------------ index = 13, score = 1.0 
def _get_outputs(self, inputs, input_seq_length, is_training):
    '\n\t\tCreate the variables and do the forward computation\n\n\t\tArgs:\n\t\t\tinputs: the inputs to the neural network, this is a list of\n\t\t\t\t[batch_size x time x ...] tensors\n\t\t\tinput_seq_length: The sequence lengths of the input utterances, this\n\t\t\t\tis a [batch_size] vector\n\t\t\tis_training: whether or not the network is in training mode\n\n\t\tReturns:\n\t\t\t- output, which is a [batch_size x time x ...] tensors\n\t\t'
    if ('filters' in self.conf):
        kernel_size = map(int, self.conf['filters'].split(' '))
    elif (('filter_size_t' in self.conf) and ('filter_size_f' in self.conf)):
        kernel_size_t = int(self.conf['filter_size_t'])
        kernel_size_f = int(self.conf['filter_size_f'])
        kernel_size = (kernel_size_t, kernel_size_f)
    else:
        raise ValueError('Kernel convolution size not specified.')
    f_stride = int(self.conf['f_stride'])
    t_stride = int(self.conf['t_stride'])
    num_layers = int(self.conf['num_layers'])
    num_filters_1st_layer = int(self.conf['num_filters_1st_layer'])
    if ('fac_per_layer' in self.conf):
        fac_per_layer = float(self.conf['fac_per_layer'])
    else:
        fac_per_layer = 1.0
    num_filters = [int(math.ceil((num_filters_1st_layer * (fac_per_layer ** l)))) for l in range(num_layers)]
    layer_norm = (self.conf['layer_norm'] == 'True')
    flat_freq = (self.conf['flat_freq'] == 'True')
    if ('activation_fn' in self.conf):
        if (self.conf['activation_fn'] == 'tanh'):
            activation_fn = tf.nn.tanh
        elif (self.conf['activation_fn'] == 'relu'):
            activation_fn = tf.nn.relu
        elif (self.conf['activation_fn'] == 'sigmoid'):
            activation_fn = tf.nn.sigmoid
        else:
            raise Exception(('Undefined activation function: %s' % self.conf['activation_fn']))
    else:
        activation_fn = tf.nn.relu
    cnn_layers = []
    for l in range(num_layers):
        num_filters_l = num_filters[l]
        cnn_layers.append(layer.Conv2D(num_filters=num_filters_l, kernel_size=kernel_size, strides=(t_stride, f_stride), padding='same', activation_fn=activation_fn, layer_norm=layer_norm))
    if (len(inputs) > 1):
        raise ('The implementation of DCNN expects 1 input and not %d' % len(inputs))
    else:
        inputs = inputs[0]
    if (num_layers == 0):
        output = inputs
        return output
    inputs = tf.expand_dims(inputs, (- 1))
    with tf.variable_scope(self.scope):
        if (is_training and (float(self.conf['input_noise']) > 0)):
            inputs = (inputs + tf.random_normal(tf.shape(inputs), stddev=float(self.conf['input_noise'])))
        logits = inputs
        with tf.variable_scope('cnn'):
            for l in range(num_layers):
                with tf.variable_scope(('layer_%s' % l)):
                    (logits, _) = cnn_layers[l](logits)
                    if (is_training and (float(self.conf['dropout']) < 1)):
                        raise Exception('have to check whether dropout is implemented correctly')
        if flat_freq:
            shapes = logits.get_shape().as_list()
            logits = tf.reshape(logits, [shapes[0], (- 1), (shapes[2] * shapes[3])])
        output = logits
    return output

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    for  ...  in:
         ... . ... ( ... .Conv2D)

idx = 64:------------------- similar code ------------------ index = 31, score = 1.0 
def layer(input_tensor):
    x = UpSampling2D(upsample_rate, name=up_name)(input_tensor)
    x = Conv2D(filters, kernel_size, padding='same', name=conv_name, **kwargs)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 65:------------------- similar code ------------------ index = 11, score = 1.0 
def __init__(self, use_bn=False):
    super(VGG, self).__init__()
    self.conv1 = VGG._make_conv_block(64, 3, 1, 'same', use_bn)
    self.conv2 = VGG._make_conv_block(64, 3, 1, 'same', use_bn)
    self.pool1 = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same')
    self.conv3 = VGG._make_conv_block(128, 3, 1, 'same', use_bn)
    self.conv4 = VGG._make_conv_block(128, 3, 1, 'same', use_bn)
    self.pool2 = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same')
    self.conv5 = VGG._make_conv_block(256, 3, 1, 'same', use_bn)
    self.conv6 = VGG._make_conv_block(256, 3, 1, 'same', use_bn)
    self.conv7 = VGG._make_conv_block(256, 3, 1, 'same', use_bn)
    self.pool3 = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same')
    self.conv8 = VGG._make_conv_block(512, 3, 1, 'same', use_bn)
    self.conv9 = VGG._make_conv_block(512, 3, 1, 'same', use_bn)
    self.conv10 = VGG._make_conv_block(512, 3, 1, 'same', use_bn)
    self.pool4 = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same')
    self.conv11 = VGG._make_conv_block(512, 3, 1, 'same', use_bn)
    self.conv12 = VGG._make_conv_block(512, 3, 1, 'same', use_bn)
    self.conv13 = VGG._make_conv_block(512, 3, 1, 'same', use_bn)
    self.pool5 = tf.keras.layers.MaxPool2D(pool_size=3, strides=1, padding='same')
    self.conv14 = tf.keras.layers.Conv2D(filters=1024, kernel_size=3, strides=1, padding='same', dilation_rate=6)
    self.conv15 = tf.keras.layers.Conv2D(filters=1024, kernel_size=1, strides=1, padding='same')

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
 =  ... .Conv2D

idx = 66:------------------- similar code ------------------ index = 10, score = 1.0 
def layer(input_tensor):
    conv_params = get_conv_params()
    bn_params = get_bn_params()
    (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
    x = Conv2D(filters, (1, 1), name=(conv_name + '1'), **conv_params)(input_tensor)
    x = BatchNormalization(name=(bn_name + '1'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '1'))(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = GroupConv2D(filters, (3, 3), conv_params, (conv_name + '2'))(x)
    x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '2'))(x)
    x = Conv2D((filters * 2), (1, 1), name=(conv_name + '3'), **conv_params)(x)
    x = BatchNormalization(name=(bn_name + '3'), **bn_params)(x)
    x = Add()([x, input_tensor])
    x = Activation('relu', name=relu_name)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 67:------------------- similar code ------------------ index = 9, score = 1.0 
def identity_block(filters, stage, block):
    'The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        filters: integer, used for first and second conv layers, third conv layer double this value\n        stage: integer, current stage label, used for generating layer names\n        block: integer, current block label, used for generating layer names\n    # Returns\n        Output layer for the block.\n    '

    def layer(input_tensor):
        conv_params = get_conv_params()
        bn_params = get_bn_params()
        (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
        x = Conv2D(filters, (1, 1), name=(conv_name + '1'), **conv_params)(input_tensor)
        x = BatchNormalization(name=(bn_name + '1'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '1'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = GroupConv2D(filters, (3, 3), conv_params, (conv_name + '2'))(x)
        x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '2'))(x)
        x = Conv2D((filters * 2), (1, 1), name=(conv_name + '3'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '3'), **bn_params)(x)
        x = Add()([x, input_tensor])
        x = Activation('relu', name=relu_name)(x)
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 68:------------------- similar code ------------------ index = 8, score = 1.0 
def __init__(self, config):
    super(encoder, self).__init__()
    self.num_hiddens = config['num_hiddens']
    self.num_residual_hiddens = config['num_residual_hiddens']
    self.num_residual_layers = config['num_residual_layers']
    self.conv1 = Conv2D((self.num_hiddens // 2), kernel_size=(4, 4), strides=(2, 2), activation='relu')
    self.conv2 = Conv2D(self.num_hiddens, kernel_size=(4, 4), strides=(2, 2), activation='relu')
    self.conv3 = Conv2D(self.num_hiddens, kernel_size=(3, 3), strides=(1, 1))
    self.residual_stack = residual(self.num_hiddens, self.num_residual_layers, self.num_residual_hiddens)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
 = Conv2D

idx = 69:------------------- similar code ------------------ index = 7, score = 1.0 
def __new__(self, input_shapes, optimizer, loss, weights=None):
    x1 = Input(input_shapes[0])
    x2 = Input(input_shapes[1])
    y1 = Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu')(x1)
    y1 = Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu')(y1)
    y1 = Conv2D(filters=1, kernel_size=(3, 3), padding='same', activation='relu')(y1)
    y1 = Flatten()(y1)
    y1 = Dense(units=512, activation='relu')(y1)
    y2 = Flatten()(x2)
    y2 = Dense(units=512, activation='relu')(y2)
    y = Concatenate()([y1, y2])
    y = Dense(units=1024, activation='relu')(y)
    y = Dropout(0.5)(y)
    y = Dense(units=1024, activation='relu')(y)
    y = Reshape(target_shape=(8, 8, 16))(y)
    y = UpSampling2D(size=(2, 2))(y)
    y = Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu')(y)
    y = UpSampling2D(size=(2, 2))(y)
    y = Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu')(y)
    y = UpSampling2D(size=(2, 2))(y)
    y = Conv2D(filters=1, kernel_size=(3, 3), padding='same', activation='relu')(y)
    model = Model(inputs=[x1, x2], outputs=y)
    model.compile(optimizer=optimizer, loss=loss)
    try:
        if (not (weights is None)):
            model.load_weights(weights)
    except:
        pass
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 70:------------------- similar code ------------------ index = 6, score = 1.0 
@classmethod
def _conv_block(cls, x, kernel_count: int, kernel_size: int, padding: str, activation: str) -> typing.Any:
    output = keras.layers.Conv2D(kernel_count, kernel_size, padding=padding, activation=activation)(x)
    return output

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... () ->:
     ...  =  ... .Conv2D

idx = 71:------------------- similar code ------------------ index = 5, score = 1.0 
def _main(args):
    config_path = os.path.expanduser(args.config_path)
    weights_path = os.path.expanduser(args.weights_path)
    assert config_path.endswith('.cfg'), '{} is not a .cfg file'.format(config_path)
    assert weights_path.endswith('.weights'), '{} is not a .weights file'.format(weights_path)
    output_path = os.path.expanduser(args.output_path)
    assert output_path.endswith('.h5'), 'output path {} is not a .h5 file'.format(output_path)
    output_root = os.path.splitext(output_path)[0]
    print('Loading weights.')
    weights_file = open(weights_path, 'rb')
    (major, minor, revision) = np.ndarray(shape=(3,), dtype='int32', buffer=weights_file.read(12))
    if ((((major * 10) + minor) >= 2) and (major < 1000) and (minor < 1000)):
        seen = np.ndarray(shape=(1,), dtype='int64', buffer=weights_file.read(8))
    else:
        seen = np.ndarray(shape=(1,), dtype='int32', buffer=weights_file.read(4))
    print('Weights Header: ', major, minor, revision, seen)
    print('Parsing Darknet config.')
    unique_config_file = unique_config_sections(config_path)
    cfg_parser = configparser.ConfigParser()
    cfg_parser.read_file(unique_config_file)
    print('Creating Keras model.')
    input_layer = Input(shape=(None, None, 3))
    prev_layer = input_layer
    all_layers = []
    weight_decay = (float(cfg_parser['net_0']['decay']) if ('net_0' in cfg_parser.sections()) else 0.0005)
    count = 0
    out_index = []
    for section in cfg_parser.sections():
        print('Parsing section {}'.format(section))
        if section.startswith('convolutional'):
            filters = int(cfg_parser[section]['filters'])
            size = int(cfg_parser[section]['size'])
            stride = int(cfg_parser[section]['stride'])
            pad = int(cfg_parser[section]['pad'])
            activation = cfg_parser[section]['activation']
            batch_normalize = ('batch_normalize' in cfg_parser[section])
            padding = ('same' if ((pad == 1) and (stride == 1)) else 'valid')
            prev_layer_shape = K.int_shape(prev_layer)
            weights_shape = (size, size, prev_layer_shape[(- 1)], filters)
            darknet_w_shape = (filters, weights_shape[2], size, size)
            weights_size = np.product(weights_shape)
            print('conv2d', ('bn' if batch_normalize else '  '), activation, weights_shape)
            conv_bias = np.ndarray(shape=(filters,), dtype='float32', buffer=weights_file.read((filters * 4)))
            count += filters
            if batch_normalize:
                bn_weights = np.ndarray(shape=(3, filters), dtype='float32', buffer=weights_file.read((filters * 12)))
                count += (3 * filters)
                bn_weight_list = [bn_weights[0], conv_bias, bn_weights[1], bn_weights[2]]
            conv_weights = np.ndarray(shape=darknet_w_shape, dtype='float32', buffer=weights_file.read((weights_size * 4)))
            count += weights_size
            conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])
            conv_weights = ([conv_weights] if batch_normalize else [conv_weights, conv_bias])
            act_fn = None
            if (activation == 'leaky'):
                pass
            elif (activation != 'linear'):
                raise ValueError('Unknown activation function `{}` in section {}'.format(activation, section))
            if (stride > 1):
                prev_layer = ZeroPadding2D(((1, 0), (1, 0)))(prev_layer)
            conv_layer = Conv2D(filters, (size, size), strides=(stride, stride), kernel_regularizer=l2(weight_decay), use_bias=(not batch_normalize), weights=conv_weights, activation=act_fn, padding=padding)(prev_layer)
            if batch_normalize:
                conv_layer = BatchNormalization(weights=bn_weight_list)(conv_layer)
            prev_layer = conv_layer
            if (activation == 'linear'):
                all_layers.append(prev_layer)
            elif (activation == 'leaky'):
                act_layer = LeakyReLU(alpha=0.1)(prev_layer)
                prev_layer = act_layer
                all_layers.append(act_layer)
        elif section.startswith('route'):
            ids = [int(i) for i in cfg_parser[section]['layers'].split(',')]
            layers = [all_layers[i] for i in ids]
            if (len(layers) > 1):
                print('Concatenating route layers:', layers)
                concatenate_layer = Concatenate()(layers)
                all_layers.append(concatenate_layer)
                prev_layer = concatenate_layer
            else:
                skip_layer = layers[0]
                all_layers.append(skip_layer)
                prev_layer = skip_layer
        elif section.startswith('maxpool'):
            size = int(cfg_parser[section]['size'])
            stride = int(cfg_parser[section]['stride'])
            all_layers.append(MaxPooling2D(pool_size=(size, size), strides=(stride, stride), padding='same')(prev_layer))
            prev_layer = all_layers[(- 1)]
        elif section.startswith('shortcut'):
            index = int(cfg_parser[section]['from'])
            activation = cfg_parser[section]['activation']
            assert (activation == 'linear'), 'Only linear activation supported.'
            all_layers.append(Add()([all_layers[index], prev_layer]))
            prev_layer = all_layers[(- 1)]
        elif section.startswith('upsample'):
            stride = int(cfg_parser[section]['stride'])
            assert (stride == 2), 'Only stride=2 supported.'
            all_layers.append(UpSampling2D(stride)(prev_layer))
            prev_layer = all_layers[(- 1)]
        elif section.startswith('yolo'):
            out_index.append((len(all_layers) - 1))
            all_layers.append(None)
            prev_layer = all_layers[(- 1)]
        elif section.startswith('net'):
            pass
        else:
            raise ValueError('Unsupported section header type: {}'.format(section))
    if (len(out_index) == 0):
        out_index.append((len(all_layers) - 1))
    model = Model(inputs=input_layer, outputs=[all_layers[i] for i in out_index])
    print(model.summary())
    if args.weights_only:
        model.save_weights('{}'.format(output_path))
        print('Saved Keras weights to {}'.format(output_path))
    else:
        model.save('{}'.format(output_path))
        print('Saved Keras model to {}'.format(output_path))
    remaining_weights = (len(weights_file.read()) / 4)
    weights_file.close()
    print('Read {} of {} from Darknet weights.'.format(count, (count + remaining_weights)))
    if (remaining_weights > 0):
        print('Warning: {} unused weights'.format(remaining_weights))
    if args.plot_model:
        plot(model, to_file='{}.png'.format(output_root), show_shapes=True)
        print('Saved model plot to {}.png'.format(output_root))

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
    for  ...  in:
        if:
             ...  = Conv2D

idx = 72:------------------- similar code ------------------ index = 2, score = 1.0 
def layer(input_tensor):
    conv_params = get_conv_params()
    bn_params = get_bn_params()
    (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
    x = Conv2D(filters, (1, 1), name=(conv_name + '1'), **conv_params)(input_tensor)
    x = BatchNormalization(name=(bn_name + '1'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '1'))(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = GroupConv2D(filters, (3, 3), conv_params, (conv_name + '2'), strides=strides)(x)
    x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '2'))(x)
    x = Conv2D((filters * 2), (1, 1), name=(conv_name + '3'), **conv_params)(x)
    x = BatchNormalization(name=(bn_name + '3'), **bn_params)(x)
    shortcut = Conv2D((filters * 2), (1, 1), name=sc_name, strides=strides, **conv_params)(input_tensor)
    shortcut = BatchNormalization(name=(sc_name + '_bn'), **bn_params)(shortcut)
    x = Add()([x, shortcut])
    x = Activation('relu', name=relu_name)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 73:------------------- similar code ------------------ index = 1, score = 1.0 
def ConvRelu(filters, kernel_size, use_batchnorm=False, conv_name='conv', bn_name='bn', relu_name='relu'):

    def layer(x):
        x = Conv2D(filters, kernel_size, padding='same', name=conv_name, use_bias=(not use_batchnorm))(x)
        if use_batchnorm:
            x = BatchNormalization(name=bn_name)(x)
        x = Activation('relu', name=relu_name)(x)
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():

    def  ... ( ... ):
         ...  = Conv2D

idx = 74:------------------- similar code ------------------ index = 28, score = 1.0 
def conv2d_bn(x, filters, kernel_size, strides=1, padding='same', activation='relu', use_bias=False, name=None):
    "Utility function to apply conv + BN.\n    # Arguments\n        x: input tensor.\n        filters: filters in `Conv2D`.\n        kernel_size: kernel size as in `Conv2D`.\n        strides: strides in `Conv2D`.\n        padding: padding mode in `Conv2D`.\n        activation: activation in `Conv2D`.\n        use_bias: whether to use a bias in `Conv2D`.\n        name: name of the ops; will become `name + '_ac'` for the activation\n            and `name + '_bn'` for the batch norm layer.\n    # Returns\n        Output tensor after applying `Conv2D` and `BatchNormalization`.\n    "
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias, name=name)(x)
    if (not use_bias):
        bn_axis = (1 if (K.image_data_format() == 'channels_first') else 3)
        bn_name = (None if (name is None) else (name + '_bn'))
        x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)
    if (activation is not None):
        ac_name = (None if (name is None) else (name + '_ac'))
        x = Activation(activation, name=ac_name)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 75:------------------- similar code ------------------ index = 33, score = 1.0 
def resnet_graph(input_image, architecture, stage5=False, train_bn=True):
    'Build a ResNet graph.\n        architecture: Can be resnet50 or resnet101\n        stage5: Boolean. If False, stage5 of the network is not created\n        train_bn: Boolean. Train or freeze Batch Norm layres\n    '
    assert (architecture in ['resnet50', 'resnet101'])
    x = KL.ZeroPadding2D((3, 3))(input_image)
    x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x)
    x = BatchNorm(name='bn_conv1')(x, training=train_bn)
    x = KL.Activation('relu')(x)
    C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), train_bn=train_bn)
    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_bn=train_bn)
    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_bn=train_bn)
    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn)
    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_bn=train_bn)
    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_bn=train_bn)
    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_bn=train_bn)
    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn)
    block_count = {'resnet50': 5, 'resnet101': 22}[architecture]
    for i in range(block_count):
        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr((98 + i)), train_bn=train_bn)
    C4 = x
    if stage5:
        x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn)
        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_bn=train_bn)
        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_bn=train_bn)
    else:
        C5 = None
    return [C1, C2, C3, C4, C5]

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .Conv2D

idx = 76:------------------- similar code ------------------ index = 152, score = 1.0 
def build_graph(image, label):
    if USE_FP16:
        image = tf.cast(image, tf.float16)

    def activation(x):
        return tf.nn.leaky_relu(x, alpha=0.1)

    def residual(name, x, chan):
        with tf.variable_scope(name):
            x = Conv2D('res1', x, chan, 3)
            x = BatchNorm('bn1', x)
            x = activation(x)
            x = Conv2D('res2', x, chan, 3)
            x = BatchNorm('bn2', x)
            x = activation(x)
            return x

    def fp16_getter(getter, *args, **kwargs):
        name = (args[0] if len(args) else kwargs['name'])
        if ((not USE_FP16) or ((not name.endswith('/W')) and (not name.endswith('/b')))):
            return getter(*args, **kwargs)
        elif (kwargs['dtype'] == tf.float16):
            kwargs['dtype'] = tf.float32
            ret = getter(*args, **kwargs)
            return tf.cast(ret, tf.float16)
        else:
            return getter(*args, **kwargs)
    with custom_getter_scope(fp16_getter), argscope(Conv2D, activation=tf.identity, use_bias=False), argscope([Conv2D, MaxPooling, BatchNorm], data_format=DATA_FORMAT), argscope(BatchNorm, momentum=0.8):
        with tf.variable_scope('prep'):
            l = Conv2D('conv', image, 64, 3)
            l = BatchNorm('bn', l)
            l = activation(l)
        with tf.variable_scope('layer1'):
            l = Conv2D('conv', l, 128, 3)
            l = MaxPooling('pool', l, 2)
            l = BatchNorm('bn', l)
            l = activation(l)
            l = (l + residual('res', l, 128))
        with tf.variable_scope('layer2'):
            l = Conv2D('conv', l, 256, 3)
            l = MaxPooling('pool', l, 2)
            l = BatchNorm('bn', l)
            l = activation(l)
        with tf.variable_scope('layer3'):
            l = Conv2D('conv', l, 512, 3)
            l = MaxPooling('pool', l, 2)
            l = BatchNorm('bn', l)
            l = activation(l)
            l = (l + residual('res', l, 512))
        l = tf.reduce_max(l, axis=([2, 3] if (DATA_FORMAT == 'NCHW') else [1, 2]))
        l = FullyConnected('fc', l, 10, use_bias=False)
        logits = tf.cast((l * 0.125), tf.float32, name='logits')
    cost = tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits)
    cost = tf.reduce_sum(cost)
    wd_cost = regularize_cost('.*', l2_regularizer((0.0005 * BATCH)), name='regularize_loss')
    correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(label, axis=1), name='correct')
    return tf.add_n([cost, wd_cost], name='cost')

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ():
        with:
             ...  = Conv2D

idx = 77:------------------- similar code ------------------ index = 51, score = 1.0 
def _create_densenet(self) -> typing.Callable:
    "\n        DenseNet is consisted of 'nb_dense_blocks' sets of Dense block\n        and Transition block pair.\n\n        :return: Wrapper Keras 'Layer' as DenseNet, tensor in tensor out.\n        "

    def _wrapper(x):
        for _ in range(self._params['nb_dense_blocks']):
            for _ in range(self._params['layers_per_dense_block']):
                out_conv = keras.layers.Conv2D(filters=self._params['growth_rate'], kernel_size=(3, 3), padding='same', activation='relu')(x)
                x = keras.layers.Concatenate(axis=(- 1))([x, out_conv])
            scale_down_ratio = self._params['transition_scale_down_ratio']
            nb_filter = int((K.int_shape(x)[(- 1)] * scale_down_ratio))
            x = keras.layers.Conv2D(filters=nb_filter, kernel_size=(1, 1), padding='same', activation=None)(x)
            x = keras.layers.MaxPool2D(strides=(2, 2))(x)
        out_densenet = keras.layers.Flatten()(x)
        return out_densenet
    return _wrapper

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ) ->:
    def  ... ( ... ):
        for  ...  in:
            for  ...  in:
                 ...  =  ... .Conv2D

idx = 78:------------------- similar code ------------------ index = 67, score = 1.0 
def layer(input_tensor):
    conv_params = get_conv_params()
    bn_params = get_bn_params()
    (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
    x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
    x = Activation('relu', name=(relu_name + '1'))(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = Conv2D(filters, (3, 3), name=(conv_name + '1'), **conv_params)(x)
    x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '2'))(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = Conv2D(filters, (3, 3), name=(conv_name + '2'), **conv_params)(x)
    x = Add()([x, input_tensor])
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 79:------------------- similar code ------------------ index = 65, score = 1.0 
def Conv2DBlock(n_filters, kernel_size, activation='relu', use_batchnorm=True, name='conv_block', **kwargs):
    'Extension of Conv2D layer with batchnorm'

    def layer(input_tensor):
        x = Conv2D(n_filters, kernel_size, use_bias=(not use_batchnorm), name=(name + '_conv'), **kwargs)(input_tensor)
        if use_batchnorm:
            x = BatchNormalization(name=(name + '_bn'))(x)
        x = Activation(activation, name=((name + '_') + activation))(x)
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 80:------------------- similar code ------------------ index = 58, score = 1.0 
def resden(x, fil, gr, beta, gamma_init, trainable):
    x1 = Conv2D(filters=gr, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(x)
    x1 = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(x1)
    x1 = LeakyReLU(alpha=0.2)(x1)
    x1 = Concatenate(axis=(- 1))([x, x1])
    x2 = Conv2D(filters=gr, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(x1)
    x2 = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(x2)
    x2 = LeakyReLU(alpha=0.2)(x2)
    x2 = Concatenate(axis=(- 1))([x1, x2])
    x3 = Conv2D(filters=gr, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(x2)
    x3 = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(x3)
    x3 = LeakyReLU(alpha=0.2)(x3)
    x3 = Concatenate(axis=(- 1))([x2, x3])
    x4 = Conv2D(filters=gr, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(x3)
    x4 = BatchNormalization(gamma_initializer=gamma_init, trainable=trainable)(x4)
    x4 = LeakyReLU(alpha=0.2)(x4)
    x4 = Concatenate(axis=(- 1))([x3, x4])
    x5 = Conv2D(filters=fil, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros')(x4)
    x5 = Lambda((lambda x: (x * beta)))(x5)
    xout = Add()([x5, x])
    return xout

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 81:------------------- similar code ------------------ index = 57, score = 1.0 
def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):
    super(residual, self).__init__()
    self.num_hiddens = num_hiddens
    self.num_residual_layers = num_residual_layers
    self.num_residual_hiddens = num_residual_hiddens
    self.relu = ReLU()
    self.conv1 = Conv2D(self.num_residual_hiddens, activation='relu', kernel_size=(3, 3), strides=(1, 1), padding='same')
    self.conv2 = Conv2D(self.num_hiddens, kernel_size=(1, 1), strides=(1, 1))

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
 = Conv2D

idx = 82:------------------- similar code ------------------ index = 56, score = 1.0 
def bottleneck(l, ch_out, stride, preact):
    ch_in = l.get_shape().as_list()[1]
    input = l
    l = Conv2D('conv1', l, ch_out, 1, strides=stride, activation=BNReLU)
    l = Conv2D('conv2', l, ch_out, 3, strides=1, activation=BNReLU)
    l = Conv2D('conv3', l, (ch_out * 4), 1, activation=tf.identity)
    l = BatchNorm('bn', l)
    ret = (l + shortcut(input, ch_in, (ch_out * 4), stride))
    return tf.nn.relu(ret)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  = Conv2D

idx = 83:------------------- similar code ------------------ index = 55, score = 1.0 
def layer(x):
    x = Conv2D(filters, kernel_size, padding='same', name=conv_name, use_bias=(not use_batchnorm))(x)
    if use_batchnorm:
        x = BatchNormalization(name=bn_name)(x)
    x = Activation('relu', name=relu_name)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 84:------------------- similar code ------------------ index = 54, score = 1.0 
def discriminator(self):
    'Discriminator module for Conditional GAN. Use it as a regular TensorFlow 2.0 Keras Model.\n\n        Return:\n            A tf.keras model  \n        '
    dropout_rate = self.config['dropout_rate']
    disc_channels = self.config['disc_channels']
    disc_layers = len(disc_channels)
    activation = self.config['activation']
    kernel_initializer = self.config['kernel_initializer']
    kernel_regularizer = self.config['kernel_regularizer']
    kernel_size = self.config['kernel_size']
    input_image = layers.Input(shape=self.image_size)
    input_label = layers.Input(shape=1)
    embedded_label = layers.Embedding(input_dim=self.n_classes, output_dim=self.embed_dim)(input_label)
    embedded_label = layers.Dense(units=(self.image_size[0] * self.image_size[1]), activation=activation)(embedded_label)
    embedded_label = layers.Reshape((self.image_size[0], self.image_size[1], 1))(embedded_label)
    x = layers.Concatenate()([input_image, embedded_label])
    for i in range(disc_layers):
        x = layers.Conv2D(filters=disc_channels[i], kernel_size=kernel_size, strides=(2, 2), padding='same', kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)
    x = layers.Flatten()(x)
    fe = layers.Dropout(dropout_rate)(x)
    out_layer = layers.Dense(1, activation='sigmoid')(fe)
    model = tf.keras.Model(inputs=[input_image, input_label], outputs=out_layer)
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
    for  ...  in:
         ...  =  ... .Conv2D

idx = 85:------------------- similar code ------------------ index = 53, score = 1.0 
def layer(input_tensor):
    conv_params = get_conv_params()
    bn_params = get_bn_params()
    (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
    x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
    x = Activation('relu', name=(relu_name + '1'))(x)
    x = Conv2D(filters, (1, 1), name=(conv_name + '1'), **conv_params)(x)
    x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '2'))(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = Conv2D(filters, (3, 3), name=(conv_name + '2'), **conv_params)(x)
    x = BatchNormalization(name=(bn_name + '3'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '3'))(x)
    x = Conv2D((filters * 4), (1, 1), name=(conv_name + '3'), **conv_params)(x)
    x = Add()([x, input_tensor])
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 86:------------------- similar code ------------------ index = 52, score = 1.0 
def __init__(self, filters, kernel_size=3, pad='same', spectral_norm=False):
    super(DiscOptResBlock, self).__init__(name='')
    if spectral_norm:
        self.conv1 = SpectralNormalization(tf.keras.layers.Conv2D(filters, kernel_size, padding=pad, kernel_initializer='glorot_uniform'))
        self.conv2 = SpectralNormalization(tf.keras.layers.Conv2D(filters, kernel_size, padding=pad, kernel_initializer='glorot_uniform'))
        self.shortcut_conv = SpectralNormalization(tf.keras.layers.Conv2D(filters, kernel_size=(1, 1), kernel_initializer='glorot_uniform', padding=pad))
    else:
        self.conv1 = tf.keras.layers.Conv2D(filters, kernel_size, padding=pad, kernel_initializer='glorot_uniform')
        self.conv2 = tf.keras.layers.Conv2D(filters, kernel_size, padding=pad, kernel_initializer='glorot_uniform')
        self.shortcut_conv = tf.keras.layers.Conv2D(filters, kernel_size=(1, 1), kernel_initializer='glorot_uniform', padding=pad)
    self.downsample_layer = tf.keras.layers.AvgPool2D((2, 2))

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    if  ... :
 =  ... ( ... .Conv2D)

idx = 87:------------------- similar code ------------------ index = 50, score = 1.0 
def build(self):
    'Build model structure.'
    inputs = self._make_inputs()
    (text_left, text_right) = inputs[0:2]
    (char_left, char_right) = inputs[2:4]
    (match_left, match_right) = inputs[4:6]
    left_embeddings = []
    right_embeddings = []
    word_embedding = self._make_embedding_layer()
    left_word_embedding = word_embedding(text_left)
    right_word_embedding = word_embedding(text_right)
    left_word_embedding = DecayingDropoutLayer(initial_keep_rate=self._params['dropout_initial_keep_rate'], decay_interval=self._params['dropout_decay_interval'], decay_rate=self._params['dropout_decay_rate'])(left_word_embedding)
    right_word_embedding = DecayingDropoutLayer(initial_keep_rate=self._params['dropout_initial_keep_rate'], decay_interval=self._params['dropout_decay_interval'], decay_rate=self._params['dropout_decay_rate'])(right_word_embedding)
    left_embeddings.append(left_word_embedding)
    right_embeddings.append(right_word_embedding)
    left_exact_match = keras.layers.Reshape(target_shape=(K.int_shape(match_left)[1], 1))(match_left)
    right_exact_match = keras.layers.Reshape(target_shape=(K.int_shape(match_left)[1], 1))(match_right)
    left_embeddings.append(left_exact_match)
    right_embeddings.append(right_exact_match)
    char_embedding = self._make_char_embedding_layer()
    char_embedding.build(input_shape=(None, None, K.int_shape(char_left)[(- 1)]))
    left_char_embedding = char_embedding(char_left)
    right_char_embedding = char_embedding(char_right)
    left_embeddings.append(left_char_embedding)
    right_embeddings.append(right_char_embedding)
    left_embedding = keras.layers.Concatenate()(left_embeddings)
    right_embedding = keras.layers.Concatenate()(right_embeddings)
    d = K.int_shape(left_embedding)[(- 1)]
    left_encoding = EncodingLayer(initial_keep_rate=self._params['dropout_initial_keep_rate'], decay_interval=self._params['dropout_decay_interval'], decay_rate=self._params['dropout_decay_rate'])(left_embedding)
    right_encoding = EncodingLayer(initial_keep_rate=self._params['dropout_initial_keep_rate'], decay_interval=self._params['dropout_decay_interval'], decay_rate=self._params['dropout_decay_rate'])(right_embedding)
    interaction = keras.layers.Lambda(self._make_interaction)([left_encoding, right_encoding])
    feature_extractor_input = keras.layers.Conv2D(filters=int((d * self._params['first_scale_down_ratio'])), kernel_size=(1, 1), activation=None)(interaction)
    feature_extractor = self._create_densenet()
    features = feature_extractor(feature_extractor_input)
    features = DecayingDropoutLayer(initial_keep_rate=self._params['dropout_initial_keep_rate'], decay_interval=self._params['dropout_decay_interval'], decay_rate=self._params['dropout_decay_rate'])(features)
    out = self._make_output_layer()(features)
    self._backend = keras.Model(inputs=inputs, outputs=out)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  =  ... .Conv2D

idx = 88:------------------- similar code ------------------ index = 34, score = 1.0 
def Conv2DBlock(n_filters, kernel_size, activation='relu', use_batchnorm=True, name='conv_block', **kwargs):
    'Extension of Conv2D layer with batchnorm'

    def layer(input_tensor):
        x = Conv2D(n_filters, kernel_size, use_bias=(not use_batchnorm), name=(name + '_conv'), **kwargs)(input_tensor)
        if use_batchnorm:
            x = BatchNormalization(name=(name + '_bn'))(x)
        x = Activation(activation, name=((name + '_') + activation))(x)
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 89:------------------- similar code ------------------ index = 49, score = 1.0 
@classmethod
def _conv_pool_block(cls, x, kernel_count: int, kernel_size: int, padding: str, activation: str, pool_size: int) -> typing.Any:
    output = keras.layers.Conv2D(kernel_count, kernel_size, padding=padding, activation=activation)(x)
    output = keras.layers.MaxPooling2D(pool_size=pool_size)(output)
    return output

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... () ->:
     ...  =  ... .Conv2D

idx = 90:------------------- similar code ------------------ index = 48, score = 1.0 
def identity_block(input_tensor, kernel_size, filters, stage, block, use_bias=True, train_bn=True):
    "The identity_block is the block that has no conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n        use_bias: Boolean. To use or not use a bias in conv layers.\n        train_bn: Boolean. Train or freeze Batch Norm layres\n    "
    (nb_filter1, nb_filter2, nb_filter3) = filters
    conv_name_base = ((('res' + str(stage)) + block) + '_branch')
    bn_name_base = ((('bn' + str(stage)) + block) + '_branch')
    x = KL.Conv2D(nb_filter1, (1, 1), name=(conv_name_base + '2a'), use_bias=use_bias)(input_tensor)
    x = BatchNorm(name=(bn_name_base + '2a'))(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=(conv_name_base + '2b'), use_bias=use_bias)(x)
    x = BatchNorm(name=(bn_name_base + '2b'))(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.Conv2D(nb_filter3, (1, 1), name=(conv_name_base + '2c'), use_bias=use_bias)(x)
    x = BatchNorm(name=(bn_name_base + '2c'))(x, training=train_bn)
    x = KL.Add()([x, input_tensor])
    x = KL.Activation('relu', name=((('res' + str(stage)) + block) + '_out'))(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ...  =  ... .Conv2D

idx = 91:------------------- similar code ------------------ index = 43, score = 1.0 
@staticmethod
def build(width, height, depth, classes):
    model = Sequential()
    input_shape = (height, width, depth)
    if (K.image_data_format() == 'channels_first'):
        input_shape = (depth, height, width)
    model.add(Conv2D(size, (3, 3), padding='same', input_shape=input_shape))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(3, 3)))
    model.add(Conv2D(size, (3, 3), padding='same', input_shape=input_shape))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(3, 3)))
    model.add(Conv2D(size, (3, 3), padding='same', input_shape=input_shape))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(3, 3)))
    model.add(Dropout(0.5))
    model.add(Conv2D(size, (3, 3), padding='same', input_shape=input_shape))
    model.add(Activation('relu'))
    model.add(Flatten())
    model.add(Activation('relu'))
    model.add(Dense(classes))
    model.add(Activation('softmax'))
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ... . ... (Conv2D)

idx = 92:------------------- similar code ------------------ index = 42, score = 1.0 
def conv_block(filters, stage, block, strides=(2, 2)):
    "The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    "

    def layer(input_tensor):
        conv_params = get_conv_params()
        bn_params = get_bn_params()
        (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
        x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
        x = Activation('relu', name=(relu_name + '1'))(x)
        shortcut = x
        x = Conv2D(filters, (1, 1), name=(conv_name + '1'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '2'))(x)
        x = ZeroPadding2D(padding=(1, 1))(x)
        x = Conv2D(filters, (3, 3), strides=strides, name=(conv_name + '2'), **conv_params)(x)
        x = BatchNormalization(name=(bn_name + '3'), **bn_params)(x)
        x = Activation('relu', name=(relu_name + '3'))(x)
        x = Conv2D((filters * 4), (1, 1), name=(conv_name + '3'), **conv_params)(x)
        shortcut = Conv2D((filters * 4), (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)
        x = Add()([x, shortcut])
        return x
    return layer

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    def  ... ( ... ):
         ...  = Conv2D

idx = 93:------------------- similar code ------------------ index = 41, score = 1.0 
def _downsample(self, filters, kernel_size, kernel_initializer, batchnorm=True):
    model = tf.keras.Sequential()
    model.add(Conv2D(filters, kernel_size=kernel_size, strides=2, kernel_initializer=kernel_initializer, padding='same', use_bias=False))
    if batchnorm:
        model.add(BatchNormalization())
    model.add(LeakyReLU())
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
     ... . ... (Conv2D)

idx = 94:------------------- similar code ------------------ index = 40, score = 1.0 
def make_default_model(self):
    '\n        Makes a CNN keras model with the default hyper parameters.\n        '
    self.model.add(Conv2D(8, (13, 13), input_shape=(self.input_shape[0], self.input_shape[1], 1)))
    self.model.add(BatchNormalization(axis=(- 1)))
    self.model.add(Activation('relu'))
    self.model.add(Conv2D(8, (13, 13)))
    self.model.add(BatchNormalization(axis=(- 1)))
    self.model.add(Activation('relu'))
    self.model.add(MaxPooling2D(pool_size=(2, 1)))
    self.model.add(Conv2D(8, (13, 13)))
    self.model.add(BatchNormalization(axis=(- 1)))
    self.model.add(Activation('relu'))
    self.model.add(Conv2D(8, (2, 2)))
    self.model.add(BatchNormalization(axis=(- 1)))
    self.model.add(Activation('relu'))
    self.model.add(MaxPooling2D(pool_size=(2, 1)))
    self.model.add(Flatten())
    self.model.add(Dense(64))
    self.model.add(BatchNormalization())
    self.model.add(Activation('relu'))
    self.model.add(Dropout(0.2))

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ... . ... (Conv2D)

idx = 95:------------------- similar code ------------------ index = 38, score = 1.0 
def layer(x):
    x = Conv2D(filters, kernel_size, padding='same', name=conv_name, use_bias=(not use_batchnorm))(x)
    if use_batchnorm:
        x = BatchNormalization(name=bn_name)(x)
    x = Activation('relu', name=relu_name)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 96:------------------- similar code ------------------ index = 36, score = 1.0 
def __init__(self, y_dim: int, x_dim: int, vision_hidden_size: int, R: int, c_out: int, Z_embed_num: int, minimum_filters: int, **kwargs):
    super(ConvGenerator, self).__init__()
    self.init_Z_embed = tf.keras.layers.Dense(vision_hidden_size, **dense_regularization)
    self.y_dim = y_dim
    self.x_dim = x_dim
    self.res_preps = []
    self.residual_blocks_1 = []
    self.residual_blocks_2 = []
    self.upsamples = []
    self.Z_embeds = []
    self.Z_norms = []
    self.cond_convs = []
    for r in range(R):
        filters = (vision_hidden_size // (2 ** (R - r)))
        filters = max([filters, minimum_filters])
        Z_filters = (filters // 4)
        res_prep = tf.keras.layers.Conv2D(filters, kernel_size=1, strides=1, padding='same', **cnn_regularization)
        residual_block_1 = ResidualBlock(filters)
        residual_block_2 = ResidualBlock(filters)
        upsample = tf.keras.layers.Conv2DTranspose(filters, kernel_size=3, strides=2, padding='same', **cnn_regularization)
        Z_embeds = ([tf.keras.layers.Dense(256, **dense_regularization) for _ in range((Z_embed_num - 1))] + [tf.keras.layers.Dense(Z_filters, **dense_regularization)])
        Z_norms = [tf.keras.layers.BatchNormalization() for _ in range(Z_embed_num)]
        cond_conv = tf.keras.layers.Conv2D(filters, kernel_size=1, strides=1, padding='same')
        self.res_preps = ([res_prep] + self.res_preps)
        self.residual_blocks_1 = ([residual_block_1] + self.residual_blocks_1)
        self.residual_blocks_2 = ([residual_block_2] + self.residual_blocks_2)
        self.upsamples = ([upsample] + self.upsamples)
        self.Z_embeds = ([Z_embeds] + self.Z_embeds)
        self.Z_norms = ([Z_norms] + self.Z_norms)
        self.cond_convs = ([cond_conv] + self.cond_convs)
    self.out_conv = tf.keras.layers.Conv2D(c_out, kernel_size=1, strides=1, padding='same', **cnn_regularization)

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ():
    for  ...  in:
         ...  =  ... .Conv2D

idx = 97:------------------- similar code ------------------ index = 35, score = 1.0 
def layer(x):
    x = Conv2D(filters, kernel_size, padding='same', name=conv_name, use_bias=(not use_batchnorm))(x)
    if use_batchnorm:
        x = BatchNormalization(name=bn_name)(x)
    x = Activation('relu', name=relu_name)(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

idx = 98:------------------- similar code ------------------ index = 150, score = 1.0 
def discriminator(self):
    'Discriminator module for Pix2Pix. Use it as a regular TensorFlow 2.0 Keras Model.\n\n        Return:\n            A tf.keras model  \n        '
    kernel_initializer = self.config['kernel_initializer']
    kernel_size = self.config['kernel_size']
    disc_channels = self.config['disc_channels']
    inputs = Input(shape=self.img_size)
    target = Input(shape=self.img_size)
    x = Concatenate()([inputs, target])
    down_stack = []
    for (i, channel) in enumerate(disc_channels[:(- 1)]):
        if (i == 0):
            down_stack.append(self._downsample(channel, kernel_size=kernel_size, kernel_initializer=kernel_initializer, batchnorm=False))
        else:
            down_stack.append(self._downsample(channel, kernel_size=kernel_size, kernel_initializer=kernel_initializer))
    down_stack.append(ZeroPadding2D())
    down_stack.append(Conv2D(disc_channels[(- 1)], kernel_size=kernel_size, strides=1, kernel_initializer=kernel_initializer, use_bias=False))
    down_stack.append(BatchNormalization())
    down_stack.append(LeakyReLU())
    down_stack.append(ZeroPadding2D())
    last = Conv2D(1, kernel_size=kernel_size, strides=1, kernel_initializer=kernel_initializer)
    for down in down_stack:
        x = down(x)
    out = last(x)
    model = Model(inputs=[inputs, target], outputs=out)
    return model

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ... . ... (Conv2D)

idx = 99:------------------- similar code ------------------ index = 0, score = 1.0 
def layer(input_tensor):
    conv_params = get_conv_params()
    bn_params = get_bn_params()
    (conv_name, bn_name, relu_name, sc_name) = handle_block_names(stage, block)
    x = BatchNormalization(name=(bn_name + '1'), **bn_params)(input_tensor)
    x = Activation('relu', name=(relu_name + '1'))(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = Conv2D(filters, (3, 3), name=(conv_name + '1'), **conv_params)(x)
    x = BatchNormalization(name=(bn_name + '2'), **bn_params)(x)
    x = Activation('relu', name=(relu_name + '2'))(x)
    x = ZeroPadding2D(padding=(1, 1))(x)
    x = Conv2D(filters, (3, 3), name=(conv_name + '2'), **conv_params)(x)
    x = Add()([x, input_tensor])
    return x

------------------- similar code (pruned) ------------------ score = 0.2 
def  ... ( ... ):
     ...  = Conv2D

