------------------------- example 1 ------------------------ 
def concat(self, inputs):
    return tf.concat(inputs, (- 1))

------------------------- example 2 ------------------------ 
def mean_image_summation(image, means=(_R_MEAN, _G_MEAN, _B_MEAN)):
    num_channels = 3
    channels = tf.split(image, num_channels, axis=2)
    for i in range(num_channels):
// your code ...
    return tf.concat(channels, axis=2)

------------------------- example 3 ------------------------ 
def _make_library(predicted_dict, predictor_fn, observed_dict, eval_batch_size, similarity_provider, name='library'):
    'Make idempotent [num_elements, library_entry_length] library Tensor.'

    def _get_library_shape(predicted_dict, observed_library):
        'Infer the shape of the library from the observed and predicted data.'
        if (observed_library is None):
            prediction_shape = util.get_static_shape_without_adding_ops(predicted_dict, predictor_fn)
            library_entry_length = prediction_shape[1]
            num_elements_observed = 0
        else:
            (num_elements_observed, library_entry_length) = observed_library.shape.as_list()
            assert (num_elements_observed is not None), 'batch_size must be statically inferrable for the observed data.'
        num_elements = num_elements_observed
        if (predicted_dict is not None):
            num_elements_predicted = tf.contrib.framework.nest.flatten(predicted_dict)[0].shape[0]
            assert (num_elements_predicted is not None), 'batch_size must be statically inferrable for the predicted data.'
// your code ...
        return [num_elements, library_entry_length]
    if (observed_dict is not None):
// your code ...
    if (predicted_dict is not None):
        library_shape = _get_library_shape(predicted_dict, observed_library)

        def make_value_op():
            return tf.get_local_variable(name=name, shape=library_shape, dtype=tf.float32, initializer=tf.zeros_initializer)

        def make_init_op(value_op):
            prediction = util.map_predictor(predicted_dict, predictor_fn, sub_batch_size=eval_batch_size)
            if (observed_dict is not None):
                library = tf.concat([prediction, observed_library], axis=0)
            else:
// your code ...
        full_library = util.value_op_with_initializer(make_value_op, make_init_op)
    else:
// your code ...

    def _get_ids_fingerprints_and_masses(data_dict):
        if (data_dict is None):
// your code ...
        if (ids.shape[0] == 0):
            return ([], [], [])
        fingerprints = data_dict[FP_NAME_FOR_JACCARD_SIMILARITY]
        masses = tf.squeeze(data_dict[fmap_constants.MOLECULE_WEIGHT], axis=1)
        return ([ids], [fingerprints], [masses])
    (predicted_ids, predicted_fingerprints, predicted_masses) = _get_ids_fingerprints_and_masses(predicted_dict)
// your code ...
    full_library_ids = tf.concat((predicted_ids + observed_ids), axis=0)
    full_fingerprints = tf.concat((predicted_fingerprints + observed_fingerprints), axis=0)
    full_masses = tf.concat((predicted_masses + observed_masses), axis=0)
    return (full_library, full_library_ids, full_fingerprints, full_masses)

------------------------- example 4 ------------------------ 
def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
    ' NOTE : y_pred must be cos similarity\n\n    Args:\n        y_true (tf.Tensor): shape [batch,ndim]\n        y_pred (tf.Tensor): shape [batch,ndim]\n\n    Returns:\n        tf.Tensor: loss\n    '
    idxs = tf.concat([self.batch_idxs, tf.cast(y_true, tf.int32)], 1)
    sp = tf.expand_dims(tf.gather_nd(y_pred, idxs), 1)
    mask = tf.logical_not(tf.scatter_nd(idxs, tf.ones(tf.shape(idxs)[0], tf.bool), tf.shape(y_pred)))
// your code ...
    alpha_p = tf.nn.relu((self.O_p - tf.stop_gradient(sp)))
    alpha_n = tf.nn.relu((tf.stop_gradient(sn) - self.O_n))
    r_sp_m = (alpha_p * (sp - self.Delta_p))
    r_sn_m = (alpha_n * (sn - self.Delta_n))
    _Z = tf.concat([r_sn_m, r_sp_m], 1)
    _Z = (_Z * self.gamma)
    logZ = tf.math.reduce_logsumexp(_Z, 1, keepdims=True)
    return (((- r_sp_m) * self.gamma) + logZ)

------------------------- example 5 ------------------------ 
def birnn(inputs, sequence_length, params):
    lstm_fw_cell = rnn.BasicLSTMCell(params.hidden_size)
// your code ...
    (outputs, states) = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, inputs, sequence_length=sequence_length, dtype=tf.float32)
    (states_fw, states_bw) = outputs
    return tf.concat([states_fw, states_bw], axis=2)

examples  ||  representativeness  ||  number of lines  || number of comments   ||  relevancy  
example1  ||          5           ||        2         ||         0        ||        0.5         
example2  ||          6           ||        5         ||         1        ||        0.4         
example3  ||          3           ||        36         ||         6        ||        0.1388888888888889         
example4  ||          2           ||        13         ||         1        ||        0.23076923076923078         
example5  ||          6           ||        5         ||         1        ||        0.2         

avg       ||          1.5017064846416384           ||        12.2         ||         1.8        ||         29.3931623931624        

idx = 0:------------------- similar code ------------------ index = 206, score = 7.0 
def concat(self, inputs):
    return tf.concat(inputs, (- 1))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 1:------------------- similar code ------------------ index = 89, score = 6.0 
def mean_image_summation(image, means=(_R_MEAN, _G_MEAN, _B_MEAN)):
    num_channels = 3
    channels = tf.split(image, num_channels, axis=2)
    for i in range(num_channels):
        channels[i] += means[i]
    return tf.concat(channels, axis=2)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 2:------------------- similar code ------------------ index = 154, score = 6.0 
def _make_library(predicted_dict, predictor_fn, observed_dict, eval_batch_size, similarity_provider, name='library'):
    'Make idempotent [num_elements, library_entry_length] library Tensor.'

    def _get_library_shape(predicted_dict, observed_library):
        'Infer the shape of the library from the observed and predicted data.'
        if (observed_library is None):
            prediction_shape = util.get_static_shape_without_adding_ops(predicted_dict, predictor_fn)
            library_entry_length = prediction_shape[1]
            num_elements_observed = 0
        else:
            (num_elements_observed, library_entry_length) = observed_library.shape.as_list()
            assert (num_elements_observed is not None), 'batch_size must be statically inferrable for the observed data.'
        num_elements = num_elements_observed
        if (predicted_dict is not None):
            num_elements_predicted = tf.contrib.framework.nest.flatten(predicted_dict)[0].shape[0]
            assert (num_elements_predicted is not None), 'batch_size must be statically inferrable for the predicted data.'
            num_elements += num_elements_predicted
        return [num_elements, library_entry_length]
    if (observed_dict is not None):
        observed_library = observed_dict[_KEY_FOR_LIBRARY_VECTORS]
    else:
        observed_library = None
    if (predicted_dict is not None):
        library_shape = _get_library_shape(predicted_dict, observed_library)

        def make_value_op():
            return tf.get_local_variable(name=name, shape=library_shape, dtype=tf.float32, initializer=tf.zeros_initializer)

        def make_init_op(value_op):
            prediction = util.map_predictor(predicted_dict, predictor_fn, sub_batch_size=eval_batch_size)
            if (observed_dict is not None):
                library = tf.concat([prediction, observed_library], axis=0)
            else:
                library = prediction
            normalized_library = similarity_provider.preprocess_library(library)
            return value_op.assign(normalized_library)
        full_library = util.value_op_with_initializer(make_value_op, make_init_op)
    else:
        full_library = similarity_provider.preprocess_library(observed_library)

    def _get_ids_fingerprints_and_masses(data_dict):
        if (data_dict is None):
            return ([], [], [])
        ids = data_dict[fmap_constants.INCHIKEY]
        if (ids.shape[0] == 0):
            return ([], [], [])
        fingerprints = data_dict[FP_NAME_FOR_JACCARD_SIMILARITY]
        masses = tf.squeeze(data_dict[fmap_constants.MOLECULE_WEIGHT], axis=1)
        return ([ids], [fingerprints], [masses])
    (predicted_ids, predicted_fingerprints, predicted_masses) = _get_ids_fingerprints_and_masses(predicted_dict)
    (observed_ids, observed_fingerprints, observed_masses) = _get_ids_fingerprints_and_masses(observed_dict)
    full_library_ids = tf.concat((predicted_ids + observed_ids), axis=0)
    full_fingerprints = tf.concat((predicted_fingerprints + observed_fingerprints), axis=0)
    full_masses = tf.concat((predicted_masses + observed_masses), axis=0)
    return (full_library, full_library_ids, full_fingerprints, full_masses)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        def  ... ():
            return tf

        def  ... ( ... ):
            if:
                 ...  =  ... .concat

idx = 3:------------------- similar code ------------------ index = 34, score = 6.0 
def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
    ' NOTE : y_pred must be cos similarity\n\n    Args:\n        y_true (tf.Tensor): shape [batch,ndim]\n        y_pred (tf.Tensor): shape [batch,ndim]\n\n    Returns:\n        tf.Tensor: loss\n    '
    idxs = tf.concat([self.batch_idxs, tf.cast(y_true, tf.int32)], 1)
    sp = tf.expand_dims(tf.gather_nd(y_pred, idxs), 1)
    mask = tf.logical_not(tf.scatter_nd(idxs, tf.ones(tf.shape(idxs)[0], tf.bool), tf.shape(y_pred)))
    sn = tf.reshape(tf.boolean_mask(y_pred, mask), (self.batch_size, (- 1)))
    alpha_p = tf.nn.relu((self.O_p - tf.stop_gradient(sp)))
    alpha_n = tf.nn.relu((tf.stop_gradient(sn) - self.O_n))
    r_sp_m = (alpha_p * (sp - self.Delta_p))
    r_sn_m = (alpha_n * (sn - self.Delta_n))
    _Z = tf.concat([r_sn_m, r_sp_m], 1)
    _Z = (_Z * self.gamma)
    logZ = tf.math.reduce_logsumexp(_Z, 1, keepdims=True)
    return (((- r_sp_m) * self.gamma) + logZ)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... () -> tf:
     ...  =  ... .concat

idx = 4:------------------- similar code ------------------ index = 158, score = 6.0 
def birnn(inputs, sequence_length, params):
    lstm_fw_cell = rnn.BasicLSTMCell(params.hidden_size)
    lstm_bw_cell = rnn.BasicLSTMCell(params.hidden_size)
    (outputs, states) = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, inputs, sequence_length=sequence_length, dtype=tf.float32)
    (states_fw, states_bw) = outputs
    return tf.concat([states_fw, states_bw], axis=2)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 5:------------------- similar code ------------------ index = 32, score = 6.0 
def _marshal(*rvs):
    'Args: a list of ed.RandomVariables each with vector or scalar event shape\n  (which must be staticly known), and all having the same batch shape.\n\n  Returns: a Tensor from concatenating their values along a single vector\n  dimension.\n  '
    vector_rvs = []
    for rv in rvs:
        v = rv.value
        if (v.shape.ndims == 0):
            vector_rvs.append([v])
        else:
            vector_rvs.append(v)
    print(vector_rvs)
    return tf.concat(vector_rvs, axis=(- 1))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 6:------------------- similar code ------------------ index = 30, score = 6.0 
def block_inception_b(inputs, scope=None, reuse=None):
    'Builds Inception-B block for Inception v4 network.'
    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding='SAME'):
        with tf.variable_scope(scope, 'BlockInceptionB', [inputs], reuse=reuse):
            with tf.variable_scope('Branch_0'):
                branch_0 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
            with tf.variable_scope('Branch_1'):
                branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
                branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope='Conv2d_0b_1x7')
                branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope='Conv2d_0c_7x1')
            with tf.variable_scope('Branch_2'):
                branch_2 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
                branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0b_7x1')
                branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope='Conv2d_0c_1x7')
                branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope='Conv2d_0d_7x1')
                branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope='Conv2d_0e_1x7')
            with tf.variable_scope('Branch_3'):
                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
                branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    with:
        with:
            return tf.concat

idx = 7:------------------- similar code ------------------ index = 168, score = 6.0 
def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
    idxs = tf.concat([self.batch_idxs, tf.cast(y_true, tf.int32)], 1)
    y_true_pred = tf.gather_nd(y_pred, idxs)
    y_true_pred = tf.expand_dims(y_true_pred, 1)
    y_true_pred_margin = (y_true_pred - self.margin)
    _Z = tf.concat([y_pred, y_true_pred_margin], 1)
    _Z = (_Z * self.scale)
    logZ = tf.math.reduce_logsumexp(_Z, 1, keepdims=True)
    logZ = (logZ + tf.math.log((1 - tf.math.exp(((self.scale * y_true_pred) - logZ)))))
    return (((- y_true_pred_margin) * self.scale) + logZ)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... () -> tf:
     ...  =  ... .concat

idx = 8:------------------- similar code ------------------ index = 119, score = 6.0 
def transformer(U, theta, out_size, name='SpatialTransformer', **kwargs):
    'Spatial Transformer Layer\n\n    Implements a spatial transformer layer as described in [1]_.\n    Based on [2]_ and edited by David Dao for Tensorflow.\n    Parameters\n    ----------\n    U : float\n        The output of a convolutional net should have the\n        shape [num_batch, height, width, num_channels].\n    theta: float\n        The output of the\n        localisation network should be [num_batch, 6].\n    out_size: tuple of two ints\n        The size of the output of the network (height, width)\n    References\n    ----------\n    .. [1]  Spatial Transformer Networks\n            Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n            Submitted on 5 Jun 2015\n    .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n    Notes\n    -----\n    To initialize the network to the identity transform init\n    ``theta`` to :\n        identity = np.array([[1., 0., 0.],\n                             [0., 1., 0.]])\n        identity = identity.flatten()\n        theta = tf.Variable(initial_value=identity)\n    '

    def _repeat(x, n_repeats):
        with tf.variable_scope('_repeat'):
            rep = tf.transpose(tf.expand_dims(tf.ones(shape=tf.stack([n_repeats])), 1), [1, 0])
            rep = tf.cast(rep, 'int32')
            x = tf.matmul(tf.reshape(x, ((- 1), 1)), rep)
            return tf.reshape(x, [(- 1)])

    def _interpolate(im, x, y, out_size):
        with tf.variable_scope('_interpolate'):
            num_batch = tf.shape(im)[0]
            height = tf.shape(im)[1]
            width = tf.shape(im)[2]
            channels = tf.shape(im)[3]
            x = tf.cast(x, 'float32')
            y = tf.cast(y, 'float32')
            height_f = tf.cast(height, 'float32')
            width_f = tf.cast(width, 'float32')
            out_height = out_size[0]
            out_width = out_size[1]
            zero = tf.zeros([], dtype='int32')
            max_y = tf.cast((tf.shape(im)[1] - 1), 'int32')
            max_x = tf.cast((tf.shape(im)[2] - 1), 'int32')
            x = (((x + 1.0) * (width_f - 1.001)) / 2.0)
            y = (((y + 1.0) * (height_f - 1.001)) / 2.0)
            x0 = tf.cast(tf.floor(x), 'int32')
            x1 = (x0 + 1)
            y0 = tf.cast(tf.floor(y), 'int32')
            y1 = (y0 + 1)
            x0 = tf.clip_by_value(x0, zero, max_x)
            x1 = tf.clip_by_value(x1, zero, max_x)
            y0 = tf.clip_by_value(y0, zero, max_y)
            y1 = tf.clip_by_value(y1, zero, max_y)
            dim2 = width
            dim1 = (width * height)
            base = _repeat((tf.range(num_batch) * dim1), (out_height * out_width))
            base_y0 = (base + (y0 * dim2))
            base_y1 = (base + (y1 * dim2))
            idx_a = (base_y0 + x0)
            idx_b = (base_y1 + x0)
            idx_c = (base_y0 + x1)
            idx_d = (base_y1 + x1)
            im_flat = tf.reshape(im, tf.stack([(- 1), channels]))
            im_flat = tf.cast(im_flat, 'float32')
            Ia = tf.gather(im_flat, idx_a)
            Ib = tf.gather(im_flat, idx_b)
            Ic = tf.gather(im_flat, idx_c)
            Id = tf.gather(im_flat, idx_d)
            x0_f = tf.cast(x0, 'float32')
            x1_f = tf.cast(x1, 'float32')
            y0_f = tf.cast(y0, 'float32')
            y1_f = tf.cast(y1, 'float32')
            wa = tf.expand_dims(((x1_f - x) * (y1_f - y)), 1)
            wb = tf.expand_dims(((x1_f - x) * (y - y0_f)), 1)
            wc = tf.expand_dims(((x - x0_f) * (y1_f - y)), 1)
            wd = tf.expand_dims(((x - x0_f) * (y - y0_f)), 1)
            output = tf.add_n([(wa * Ia), (wb * Ib), (wc * Ic), (wd * Id)])
            return output

    def _meshgrid(height, width):
        with tf.variable_scope('_meshgrid'):
            x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])), tf.transpose(tf.expand_dims(tf.linspace((- 1.0), 1.0, width), 1), [1, 0]))
            y_t = tf.matmul(tf.expand_dims(tf.linspace((- 1.0), 1.0, height), 1), tf.ones(shape=tf.stack([1, width])))
            x_t_flat = tf.reshape(x_t, (1, (- 1)))
            y_t_flat = tf.reshape(y_t, (1, (- 1)))
            ones = tf.ones_like(x_t_flat)
            grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones])
            return grid

    def _transform(theta, input_dim, out_size):
        with tf.variable_scope('_transform'):
            num_batch = tf.shape(input_dim)[0]
            num_channels = tf.shape(input_dim)[3]
            theta = tf.reshape(theta, ((- 1), 2, 3))
            theta = tf.cast(theta, 'float32')
            out_height = out_size[0]
            out_width = out_size[1]
            grid = _meshgrid(out_height, out_width)
            grid = tf.expand_dims(grid, 0)
            grid = tf.reshape(grid, [(- 1)])
            grid = tf.tile(grid, tf.stack([num_batch]))
            grid = tf.reshape(grid, tf.stack([num_batch, 3, (- 1)]))
            T_g = tf.matmul(theta, grid)
            x_s = tf.slice(T_g, [0, 0, 0], [(- 1), 1, (- 1)])
            y_s = tf.slice(T_g, [0, 1, 0], [(- 1), 1, (- 1)])
            x_s_flat = tf.reshape(x_s, [(- 1)])
            y_s_flat = tf.reshape(y_s, [(- 1)])
            input_transformed = _interpolate(input_dim, x_s_flat, y_s_flat, out_size)
            output = tf.reshape(input_transformed, tf.stack([num_batch, out_height, out_width, num_channels]))
            return output
    with tf.variable_scope(name):
        output = _transform(theta, U, out_size)
        return output

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    def  ... ():
        with:
            return tf

    def  ... ():
        with:
             ...  =  ... .concat

idx = 9:------------------- similar code ------------------ index = 68, score = 6.0 
def call(self, inputs: list, **kwargs) -> typing.Any:
    '\n        The computation logic of MatchingLayer.\n\n        :param inputs: two input tensors.\n        '
    x1 = inputs[0]
    x2 = inputs[1]
    if (self._matching_type == 'dot'):
        if self._normalize:
            x1 = tf.math.l2_normalize(x1, axis=2)
            x2 = tf.math.l2_normalize(x2, axis=2)
        return tf.expand_dims(tf.einsum('abd,acd->abc', x1, x2), 3)
    else:
        if (self._matching_type == 'mul'):

            def func(x, y):
                return (x * y)
        elif (self._matching_type == 'plus'):

            def func(x, y):
                return (x + y)
        elif (self._matching_type == 'minus'):

            def func(x, y):
                return (x - y)
        elif (self._matching_type == 'concat'):

            def func(x, y):
                return tf.concat([x, y], axis=3)
        else:
            raise ValueError(f'Invalid matching type.{self._matching_type} received.Mut be in `dot`, `mul`, `plus`, `minus` and `concat`.')
        x1_exp = tf.stack(([x1] * self._shape2[1]), 2)
        x2_exp = tf.stack(([x2] * self._shape1[1]), 1)
        return func(x1_exp, x2_exp)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... () ->:
    if:    else:
        if:        elif:

            def  ... ():
                return tf.concat

idx = 10:------------------- similar code ------------------ index = 113, score = 6.0 
def _ten_crop(image, crop_h, crop_w):

    def _crop(img, center_offset):
        img = tf.image.extract_glimpse([img], [crop_w, crop_h], offsets=tf.to_float([center_offset]), centered=False, normalized=False)
        return tf.squeeze(img, 0)

    def _crop5(img):
        im_shape = tf.shape(image)
        (height, width) = (im_shape[0], im_shape[1])
        (ch, cw) = (tf.to_int32((height / 2)), tf.to_int32((width / 2)))
        (hh, hw) = (tf.to_int32((crop_h / 2)), tf.to_int32((crop_w / 2)))
        ct = _crop(img, [ch, cw])
        lu = _crop(img, [hh, hw])
        ld = _crop(img, [(height - hh), hw])
        ru = _crop(img, [hh, (width - hw)])
        rd = _crop(img, [(height - hh), (width - hw)])
        org_images = tf.stack([lu, ru, ld, rd, ct])
        return (org_images, tf.stack([lu, ru, ld, rd, ct]))
    (lhs, aa_lhs) = _crop5(image)
    rhs = tf.image.flip_left_right(lhs)
    return tf.concat([lhs, rhs], axis=0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():

    return tf.concat

idx = 11:------------------- similar code ------------------ index = 118, score = 6.0 
def to_binary_tf(bar_or_track_bar, threshold=0.0, track_mode=False, melody=False):
    'Return the binarize tensor of the input tensor (be careful of the channel order!)'
    if track_mode:
        if melody:
            melody_is_max = tf.equal(bar_or_track_bar, tf.reduce_max(bar_or_track_bar, axis=2, keep_dims=True))
            melody_pass_threshold = (bar_or_track_bar > threshold)
            out_tensor = tf.logical_and(melody_is_max, melody_pass_threshold)
        else:
            out_tensor = (bar_or_track_bar > threshold)
        return out_tensor
    else:
        if (len(bar_or_track_bar.get_shape()) == 4):
            melody_track = tf.slice(bar_or_track_bar, [0, 0, 0, 0], [(- 1), (- 1), (- 1), 1])
            other_tracks = tf.slice(bar_or_track_bar, [0, 0, 0, 1], [(- 1), (- 1), (- 1), (- 1)])
        elif (len(bar_or_track_bar.get_shape()) == 5):
            melody_track = tf.slice(bar_or_track_bar, [0, 0, 0, 0, 0], [(- 1), (- 1), (- 1), (- 1), 1])
            other_tracks = tf.slice(bar_or_track_bar, [0, 0, 0, 0, 1], [(- 1), (- 1), (- 1), (- 1), (- 1)])
        melody_is_max = tf.equal(melody_track, tf.reduce_max(melody_track, axis=2, keep_dims=True))
        melody_pass_threshold = (melody_track > threshold)
        out_tensor_melody = tf.logical_and(melody_is_max, melody_pass_threshold)
        out_tensor_others = (other_tracks > threshold)
        if (len(bar_or_track_bar.get_shape()) == 4):
            return tf.concat([out_tensor_melody, out_tensor_others], 3)
        elif (len(bar_or_track_bar.get_shape()) == 5):
            return tf.concat([out_tensor_melody, out_tensor_others], 4)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if  ... :    else:
        if:
            return tf.concat

idx = 12:------------------- similar code ------------------ index = 121, score = 6.0 
def _ten_crop(image, crop_h, crop_w):

    def _crop(img, center_offset):
        img = tf.image.extract_glimpse([img], [crop_w, crop_h], offsets=tf.to_float([center_offset]), centered=False, normalized=False)
        return tf.squeeze(img, 0)

    def _crop5(img):
        im_shape = tf.shape(image)
        (height, width) = (im_shape[0], im_shape[1])
        (ch, cw) = (tf.to_int32((height / 2)), tf.to_int32((width / 2)))
        (hh, hw) = (tf.to_int32((crop_h / 2)), tf.to_int32((crop_w / 2)))
        ct = _crop(img, [ch, cw])
        lu = _crop(img, [hh, hw])
        ld = _crop(img, [(height - hh), hw])
        ru = _crop(img, [hh, (width - hw)])
        rd = _crop(img, [(height - hh), (width - hw)])
        return tf.stack([lu, ru, ld, rd, ct])
    lhs = _crop5(image)
    rhs = tf.image.flip_left_right(lhs)
    return tf.concat([lhs, rhs], axis=0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():

    return tf.concat

idx = 13:------------------- similar code ------------------ index = 59, score = 6.0 
def _mean_image_subtraction(image, means):
    "Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.\n  "
    num_channels = image.get_shape().as_list()[(- 1)]
    if (len(means) != num_channels):
        raise ValueError('len(means) must match the number of channels')
    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=image)
    for i in range(num_channels):
        channels[i] -= means[i]
    return tf.concat(axis=3, values=channels)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 14:------------------- similar code ------------------ index = 57, score = 6.0 
def attn(x, scope, n_state, *, past, hparams):
    assert (x.shape.ndims == 3)
    assert ((n_state % hparams.n_head) == 0)
    if (past is not None):
        assert (past.shape.ndims == 5)

    def split_heads(x):
        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])

    def merge_heads(x):
        return merge_states(tf.transpose(x, [0, 2, 1, 3]))

    def mask_attn_weights(w):
        (_, _, nd, ns) = shape_list(w)
        b = attention_mask(nd, ns, dtype=w.dtype)
        b = tf.reshape(b, [1, 1, nd, ns])
        w = ((w * b) - (tf.cast(10000000000.0, w.dtype) * (1 - b)))
        return w

    def multihead_attn(q, k, v):
        w = tf.matmul(q, k, transpose_b=True)
        w = (w * tf.rsqrt(tf.cast(v.shape[(- 1)].value, w.dtype)))
        w = mask_attn_weights(w)
        w = softmax(w)
        a = tf.matmul(w, v)
        return a
    with tf.variable_scope(scope):
        c = conv1d(x, 'c_attn', (n_state * 3))
        (q, k, v) = map(split_heads, tf.split(c, 3, axis=2))
        present = tf.stack([k, v], axis=1)
        if (past is not None):
            (pk, pv) = tf.unstack(past, axis=1)
            k = tf.concat([pk, k], axis=(- 2))
            v = tf.concat([pv, v], axis=(- 2))
        a = multihead_attn(q, k, v)
        a = merge_heads(a)
        a = conv1d(a, 'c_proj', n_state)
        return (a, present)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    def  ... ( ... ):
        return tf

    with:
        if:
             ...  =  ... .concat

idx = 15:------------------- similar code ------------------ index = 148, score = 6.0 
def block_reduction_a(inputs, scope=None, reuse=None):
    'Builds Reduction-A block for Inception v4 network.'
    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding='SAME'):
        with tf.variable_scope(scope, 'BlockReductionA', [inputs], reuse=reuse):
            with tf.variable_scope('Branch_0'):
                branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')
            with tf.variable_scope('Branch_1'):
                branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
                branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope='Conv2d_0b_3x3')
                branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')
            with tf.variable_scope('Branch_2'):
                branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID', scope='MaxPool_1a_3x3')
            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    with:
        with:
            return tf.concat

idx = 16:------------------- similar code ------------------ index = 88, score = 6.0 
def block_inception_a(inputs, scope=None, reuse=None):
    'Builds Inception-A block for Inception v4 network.'
    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding='SAME'):
        with tf.variable_scope(scope, 'BlockInceptionA', [inputs], reuse=reuse):
            with tf.variable_scope('Branch_0'):
                branch_0 = slim.conv2d(inputs, 96, [1, 1], scope='Conv2d_0a_1x1')
            with tf.variable_scope('Branch_1'):
                branch_1 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')
                branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='Conv2d_0b_3x3')
            with tf.variable_scope('Branch_2'):
                branch_2 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')
                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')
                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')
            with tf.variable_scope('Branch_3'):
                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
                branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope='Conv2d_0b_1x1')
            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    with:
        with:
            return tf.concat

idx = 17:------------------- similar code ------------------ index = 204, score = 6.0 
def _std_image_normalize(image, stds):
    "Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.\n  "
    num_channels = image.get_shape().as_list()[(- 1)]
    if (len(stds) != num_channels):
        raise ValueError('len(means) must match the number of channels')
    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=image)
    for i in range(num_channels):
        channels[i] /= stds[i]
    return tf.concat(axis=3, values=channels)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 18:------------------- similar code ------------------ index = 5, score = 6.0 
def mean_image_subtraction(images, means=(_R_MEAN, _G_MEAN, _B_MEAN)):
    num_channels = 3
    channels = tf.split(images, num_channels, axis=2)
    for i in range(num_channels):
        channels[i] -= means[i]
    return tf.concat(channels, axis=2)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 19:------------------- similar code ------------------ index = 197, score = 6.0 
def all_pairs_tf(a, b):
    '\n    Return a tensor of all pairs\n    a -- [batch_size1, dim]\n    b -- [batch_size2, dim]\n    '
    dim = tf.shape(a)[1]
    temp_a = (tf.expand_dims(a, axis=1) + tf.zeros(tf.shape(tf.expand_dims(b, axis=0)), dtype=b.dtype))
    temp_b = (tf.zeros(tf.expand_dims(a, axis=1), dtype=a.dtype) + tf.expand_dims(b, axis=0))
    return tf.concat((tf.reshape(temp_a, [(- 1), 1, dim]), tf.reshape(temp_b, [(- 1), 1, dim])), axis=1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 20:------------------- similar code ------------------ index = 198, score = 6.0 
def batch_mean_image_summation(images, means=(_R_MEAN, _G_MEAN, _B_MEAN)):
    if (images.get_shape().ndims != 4):
        raise ValueError('Input must be of size [batch, height, width, C>0')
    num_channels = images.get_shape().as_list()[(- 1)]
    if (len(means) != num_channels):
        raise ValueError('len(means) must match the number of channels')
    channels = tf.split(images, num_channels, axis=3)
    for i in range(num_channels):
        channels[i] += means[i]
    return tf.concat(channels, axis=3)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 21:------------------- similar code ------------------ index = 189, score = 6.0 
def _mean_image_subtraction(image, means):
    "Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.\n  "
    if (image.get_shape().ndims != 3):
        raise ValueError('Input must be of size [height, width, C>0]')
    num_channels = image.get_shape().as_list()[(- 1)]
    if (len(means) != num_channels):
        raise ValueError('len(means) must match the number of channels')
    channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)
    for i in range(num_channels):
        channels[i] -= means[i]
    return tf.concat(axis=2, values=channels)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 22:------------------- similar code ------------------ index = 13, score = 6.0 
def _std_image_normalize(image, stds):
    "Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.\n  "
    num_channels = image.get_shape().as_list()[(- 1)]
    if (len(stds) != num_channels):
        raise ValueError('len(means) must match the number of channels')
    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=image)
    for i in range(num_channels):
        channels[i] /= stds[i]
    return tf.concat(axis=3, values=channels)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 23:------------------- similar code ------------------ index = 203, score = 6.0 
def batch_mean_image_subtraction(images, means=(_R_MEAN, _G_MEAN, _B_MEAN)):
    if (images.get_shape().ndims != 4):
        raise ValueError('Input must be of size [batch, height, width, C>0')
    num_channels = images.get_shape().as_list()[(- 1)]
    if (len(means) != num_channels):
        raise ValueError('len(means) must match the number of channels')
    channels = tf.split(images, num_channels, axis=3)
    for i in range(num_channels):
        channels[i] -= means[i]
    return tf.concat(channels, axis=3)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 24:------------------- similar code ------------------ index = 177, score = 6.0 
def _mean_image_subtraction(image, means=(_R_MEAN, _G_MEAN, _B_MEAN)):
    if (image.get_shape().ndims != 3):
        raise ValueError('Input must be of size [height, width, C>0]')
    num_channels = image.get_shape().as_list()[(- 1)]
    if (len(means) != num_channels):
        raise ValueError('len(means) must match the number of channels')
    channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)
    for i in range(num_channels):
        channels[i] -= means[i]
    return tf.concat(axis=2, values=channels)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 25:------------------- similar code ------------------ index = 20, score = 6.0 
def features_from_state(self, state):
    return tf.concat([state['sample'], state['belief']], (- 1))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 26:------------------- similar code ------------------ index = 19, score = 6.0 
def block_reduction_b(inputs, scope=None, reuse=None):
    'Builds Reduction-B block for Inception v4 network.'
    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding='SAME'):
        with tf.variable_scope(scope, 'BlockReductionB', [inputs], reuse=reuse):
            with tf.variable_scope('Branch_0'):
                branch_0 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
                branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')
            with tf.variable_scope('Branch_1'):
                branch_1 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')
                branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope='Conv2d_0b_1x7')
                branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope='Conv2d_0c_7x1')
                branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')
            with tf.variable_scope('Branch_2'):
                branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID', scope='MaxPool_1a_3x3')
            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    with:
        with:
            return tf.concat

idx = 27:------------------- similar code ------------------ index = 152, score = 6.0 
def _add_thomson_constraint(self, filt, n_filt, model, power):
    filt = tf.reshape(filt, [(- 1), n_filt])
    if (model == 'half_mhe'):
        filt_neg = (filt * (- 1))
        filt = tf.concat((filt, filt_neg), axis=1)
        n_filt *= 2
    filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + 0.0001))
    norm_mat = tf.matmul(tf.transpose(filt_norm), filt_norm)
    inner_pro = tf.matmul(tf.transpose(filt), filt)
    inner_pro /= norm_mat
    if (power == '0'):
        cross_terms = (2.0 - (2.0 * inner_pro))
        final = (- tf.log((cross_terms + tf.diag(([1.0] * n_filt)))))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((1 * tf.reduce_sum(final)) / cnt)
    elif (power == '1'):
        cross_terms = ((2.0 - (2.0 * inner_pro)) + tf.diag(([1.0] * n_filt)))
        final = tf.pow(cross_terms, (tf.ones_like(cross_terms) * (- 0.5)))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((1 * tf.reduce_sum(final)) / cnt)
    elif (power == '2'):
        cross_terms = ((2.0 - (2.0 * inner_pro)) + tf.diag(([1.0] * n_filt)))
        final = tf.pow(cross_terms, (tf.ones_like(cross_terms) * (- 1)))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((1 * tf.reduce_sum(final)) / cnt)
    elif (power == 'a0'):
        acos = (tf.acos(inner_pro) / math.pi)
        acos += 0.0001
        final = (- tf.log(acos))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((1 * tf.reduce_sum(final)) / cnt)
    elif (power == 'a1'):
        acos = (tf.acos(inner_pro) / math.pi)
        acos += 0.0001
        final = tf.pow(acos, (tf.ones_like(acos) * (- 1)))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((0.1 * tf.reduce_sum(final)) / cnt)
    elif (power == 'a2'):
        acos = (tf.acos(inner_pro) / math.pi)
        acos += 0.0001
        final = tf.pow(acos, (tf.ones_like(acos) * (- 2)))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((0.1 * tf.reduce_sum(final)) / cnt)
    tf.add_to_collection('thomson_loss', loss)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = tf.concat

idx = 28:------------------- similar code ------------------ index = 45, score = 6.0 
def deconv2d(input_, output_shape, k_h=4, k_w=4, d_h=2, d_w=2, stddev=None, name='deconv2d', spectral_normed=False, update_collection=None, with_w=False, padding='SAME', mhe=False, net_type='g'):
    fan_in = ((k_h * k_w) * input_.get_shape().as_list()[(- 1)])
    fan_out = ((k_h * k_w) * output_shape[(- 1)])
    if (stddev is None):
        stddev = np.sqrt((2.0 / fan_in))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        w = tf.get_variable('w', [k_h, k_w, output_shape[(- 1)], input_.get_shape()[(- 1)]], initializer=tf.truncated_normal_initializer(stddev=stddev))
        if spectral_normed:
            deconv = tf.nn.conv2d_transpose(input_, spectral_normed_weight(w, update_collection=update_collection), output_shape=output_shape, strides=[1, d_h, d_w, 1], padding=padding)
        else:
            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1], padding=padding)
            if mhe:
                eps = 0.0001
                filt = w
                filt_num = input_.get_shape().as_list()[(- 1)]
                filt = tf.reshape(filt, [(- 1), filt_num])
                filt = tf.concat([filt, (- filt)], axis=0)
                filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
                filt /= filt_norm
                inner_pro = tf.matmul(tf.transpose(filt), filt)
                cross_terms = (2.0 - (2.0 * inner_pro))
                cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(filt_num)))
                loss = ((- 1e-07) * tf.reduce_mean(tf.log((cross_terms + eps))))
                if (net_type == 'g'):
                    tf.add_to_collection('g_mhe_loss', loss)
                else:
                    raise
        biases = tf.get_variable('b', [output_shape[(- 1)]], initializer=tf.constant_initializer(0))
        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())
        if with_w:
            return (deconv, w, biases)
        else:
            return deconv

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :        else:
            if  ... :
                 ...  = tf.concat

idx = 29:------------------- similar code ------------------ index = 142, score = 6.0 
def _leaky_routing(logits):
    leak_shape = cl.shape(logits)
    leak = tf.zeros((leak_shape[:(- 3)] + [1, 1, 1]))
    leaky_logits = tf.concat([leak, logits], axis=(- 3))
    leaky_routing = cl.softmax(leaky_logits, axis=(- 3))
    return tf.split(leaky_routing, [1, leak_shape[(- 3)]], axis=(- 3))[1]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = tf.concat

idx = 30:------------------- similar code ------------------ index = 43, score = 6.0 
def conv2d(input_, output_dim, k_h=4, k_w=4, d_h=2, d_w=2, stddev=None, name='conv2d', spectral_normed=False, update_collection=None, with_w=False, padding='SAME', mhe=False, net_type='d'):
    fan_in = ((k_h * k_w) * input_.get_shape().as_list()[(- 1)])
    fan_out = ((k_h * k_w) * output_dim)
    if (stddev is None):
        stddev = np.sqrt((2.0 / fan_in))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[(- 1)], output_dim], initializer=tf.truncated_normal_initializer(stddev=stddev))
        if spectral_normed:
            conv = tf.nn.conv2d(input_, spectral_normed_weight(w, update_collection=update_collection), strides=[1, d_h, d_w, 1], padding=padding)
        else:
            conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=padding)
        if mhe:
            print(('mhe on %s' % name))
            eps = 0.0001
            filt = w
            filt = tf.reshape(filt, [(- 1), output_dim])
            filt = tf.concat([filt, (- filt)], axis=0)
            filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
            filt /= filt_norm
            inner_pro = tf.matmul(tf.transpose(filt), filt)
            cross_terms = (2.0 - (2.0 * inner_pro))
            cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(output_dim)))
            loss = ((- 1e-06) * tf.reduce_mean(tf.log((cross_terms + eps))))
            if (net_type == 'g'):
                tf.add_to_collection('g_mhe_loss', loss)
            elif (net_type == 'd'):
                tf.add_to_collection('d_mhe_loss', loss)
            else:
                raise
        biases = tf.get_variable('b', [output_dim], initializer=tf.constant_initializer(0.0))
        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())
        if with_w:
            return (conv, w, biases)
        else:
            return conv

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :
             ...  = tf.concat

idx = 31:------------------- similar code ------------------ index = 51, score = 6.0 
def model_fn(features, labels, mode, params):
    is_training = (mode == tf.estimator.ModeKeys.TRAIN)
    is_validation = (mode == tf.estimator.ModeKeys.EVAL)
    is_prediction = (mode == tf.estimator.ModeKeys.PREDICT)
    embedding = Embedding(params.num_symbols, embedding_dim=params.embedding_dim)
    if params.use_accent_type:
        accent_embedding = Embedding(params.num_accent_type, embedding_dim=params.accent_type_embedding_dim, index_offset=params.accent_type_offset)
    encoder = encoder_factory(params, is_training)
    assert (params.decoder in ['DualSourceDecoder', 'DualSourceTransformerDecoder'])
    decoder = decoder_factory(params)
    assert (not (params.use_speaker_embedding and params.use_external_speaker_embedding))
    if params.use_speaker_embedding:
        speaker_embedding = Embedding(params.num_speakers, embedding_dim=params.speaker_embedding_dim, index_offset=params.speaker_embedding_offset)
    elif params.use_external_speaker_embedding:
        speaker_embedding = ExternalEmbedding(params.embedding_file, params.num_speakers, embedding_dim=params.speaker_embedding_dim, index_offset=params.speaker_embedding_offset)
    if (params.speaker_embedding_projection_out_dim > (- 1)):

        def _compose(f, g):
            return (lambda arg, *args, **kwargs: f(g(arg, *args, **kwargs)))
        resize = tf.layers.Dense(params.speaker_embedding_projection_out_dim, activation=tf.nn.relu)
        speaker_embedding = _compose(resize, speaker_embedding)
    if params.use_language_embedding:
        language_embedding = ExternalEmbedding(params.language_embedding_file, params.num_speakers, embedding_dim=params.language_embedding_dim, index_offset=params.speaker_embedding_offset)
    if (params.language_embedding_projection_out_dim > (- 1)):

        def _compose(f, g):
            return (lambda arg, *args, **kwargs: f(g(arg, *args, **kwargs)))
        resize = tf.layers.Dense(params.language_embedding_projection_out_dim, activation=tf.nn.relu)
        language_embedding = _compose(resize, language_embedding)
    if params.channel_id_to_postnet:
        channel_code = ExternalEmbedding(params.channel_id_file, params.num_speakers, embedding_dim=params.channel_id_dim, index_offset=params.speaker_embedding_offset)
    target = (labels.mel if (is_training or is_validation) else features.mel)
    x = params.speaker_for_synthesis
    if (x > (- 1)):
        speaker_embedding_output = speaker_embedding(x)
    else:
        speaker_embedding_output = (speaker_embedding(features.speaker_id) if (params.use_speaker_embedding or params.use_external_speaker_embedding) else None)
    if (x > (- 1)):
        language_embedding_output = language_embedding(x)
    else:
        language_embedding_output = (language_embedding(features.speaker_id) if params.use_language_embedding else None)
    channel_code_output = (channel_code(features.speaker_id) if params.channel_id_to_postnet else None)
    embedding_output = embedding(features.source)
    if params.language_embedd_to_input:
        language_embedd_input_projection_layer = tf.layers.Dense(params.embedding_dim)
        language_embedd_input_projected = language_embedd_input_projection_layer(language_embedding_output)
        expand_language_embedding_input = tf.tile(tf.expand_dims(language_embedd_input_projected, axis=1), [1, tf.shape(embedding_output)[1], 1])
        embedding_output = (embedding_output + expand_language_embedding_input)
    (encoder_lstm_output, encoder_self_attention_output, self_attention_alignment) = (encoder((embedding_output, accent_embedding(features.accent_type)), input_lengths=features.source_length) if params.use_accent_type else encoder(embedding_output, input_lengths=features.source_length))
    if params.speaker_embedd_to_decoder:
        expand_speaker_embedding_output = tf.tile(tf.expand_dims(speaker_embedding_output, axis=1), [1, tf.shape(encoder_lstm_output)[1], 1])
        encoder_lstm_output = tf.concat((encoder_lstm_output, expand_speaker_embedding_output), axis=(- 1))
        encoder_self_attention_output = tf.concat((encoder_self_attention_output, expand_speaker_embedding_output), axis=(- 1))
    if params.language_embedd_to_decoder:
        expand_language_embedding_output = tf.tile(tf.expand_dims(language_embedding_output, axis=1), [1, tf.shape(encoder_lstm_output)[1], 1])
        encoder_lstm_output = tf.concat((encoder_lstm_output, expand_language_embedding_output), axis=(- 1))
        encoder_self_attention_output = tf.concat((encoder_self_attention_output, expand_language_embedding_output), axis=(- 1))
    (attention1_fn, attention2_fn) = dual_source_attention_factory(params)
    (mel_output, stop_token, decoder_state) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=(is_validation or params.use_forced_alignment_mode), teacher_forcing=params.use_forced_alignment_mode, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=(labels.target_length if is_training else None), target=target, apply_dropout_on_inference=params.apply_dropout_on_inference)
    self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in self_attention_alignment]
    if ((params.decoder == 'DualSourceTransformerDecoder') and (not is_training)):
        decoder_rnn_state = decoder_state.rnn_state.rnn_state[0]
        alignment1 = tf.transpose(decoder_rnn_state.alignment_history[0].stack(), [1, 2, 0])
        alignment2 = tf.transpose(decoder_rnn_state.alignment_history[1].stack(), [1, 2, 0])
        decoder_self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in decoder_state.alignments]
    else:
        decoder_rnn_state = decoder_state[0]
        alignment1 = tf.transpose(decoder_rnn_state.alignment_history[0].stack(), [1, 2, 0])
        alignment2 = tf.transpose(decoder_rnn_state.alignment_history[1].stack(), [1, 2, 0])
        decoder_self_attention_alignment = []
    if params.use_forced_alignment_mode:
        (attention1_fn, attention2_fn) = force_alignment_dual_source_attention_factory(params)
        (mel_output, stop_token, decoder_state) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=True, teacher_forcing=False, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=(labels.target_length if is_training else None), target=target, teacher_alignments=(tf.transpose(alignment1, perm=[0, 2, 1]), tf.transpose(alignment2, perm=[0, 2, 1])), apply_dropout_on_inference=params.apply_dropout_on_inference)
        if ((params.decoder == 'DualSourceTransformerDecoder') and (not is_training)):
            alignment1 = tf.transpose(decoder_state.rnn_state.rnn_state[0].alignment_history[0].stack(), [1, 2, 0])
            alignment2 = tf.transpose(decoder_state.rnn_state.rnn_state[0].alignment_history[1].stack(), [1, 2, 0])
            decoder_self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in decoder_state.alignments]
        else:
            alignment1 = tf.transpose(decoder_state[0].alignment_history[0].stack(), [1, 2, 0])
            alignment2 = tf.transpose(decoder_state[0].alignment_history[1].stack(), [1, 2, 0])
            decoder_self_attention_alignment = []
    if params.use_postnet_v2:
        postnet = (MultiSpeakerPostNet(out_units=params.num_mels, speaker_embed=speaker_embedding_output, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate) if params.speaker_embedd_to_postnet else (ChannelEncoderPostNet(out_units=params.num_mels, channel_code=channel_code_output, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate) if params.channel_id_to_postnet else PostNetV2(out_units=params.num_mels, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate)))
        postnet_v2_mel_output = postnet(mel_output)
    global_step = tf.train.get_global_step()
    if (mode is not tf.estimator.ModeKeys.PREDICT):
        mel_loss = spec_loss(mel_output, labels.mel, labels.spec_loss_mask, params.spec_loss_type)
        done_loss = binary_loss(stop_token, labels.done, labels.binary_loss_mask)
        blacklist = ['embedding', 'bias', 'batch_normalization', 'output_projection_wrapper/kernel', 'lstm_cell', 'output_and_stop_token_wrapper/dense/', 'output_and_stop_token_wrapper/dense_1/', 'stop_token_projection/kernel']
        regularization_loss = (l2_regularization_loss(tf.trainable_variables(), params.l2_regularization_weight, blacklist) if params.use_l2_regularization else 0)
        postnet_v2_mel_loss = (spec_loss(postnet_v2_mel_output, labels.mel, labels.spec_loss_mask, params.spec_loss_type) if params.use_postnet_v2 else 0)
        loss = (((mel_loss + done_loss) + regularization_loss) + postnet_v2_mel_loss)
    if is_training:
        lr = (self.learning_rate_decay(params.initial_learning_rate, global_step, params.learning_rate_step_factor) if params.decay_learning_rate else tf.convert_to_tensor(params.initial_learning_rate))
        optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1=params.adam_beta1, beta2=params.adam_beta2, epsilon=params.adam_eps)
        (gradients, variables) = zip(*optimizer.compute_gradients(loss))
        (clipped_gradients, _) = tf.clip_by_global_norm(gradients, 1.0)
        self.add_training_stats(loss, mel_loss, done_loss, lr, postnet_v2_mel_loss, regularization_loss)
        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
            train_op = optimizer.apply_gradients(zip(clipped_gradients, variables), global_step=global_step)
            summary_writer = tf.summary.FileWriter(model_dir)
            alignment_saver = MetricsSaver(([alignment1, alignment2] + self_attention_alignment), global_step, mel_output, labels.mel, labels.target_length, features.id, features.text, params.alignment_save_steps, mode, summary_writer, save_training_time_metrics=params.save_training_time_metrics, keep_eval_results_max_epoch=params.keep_eval_results_max_epoch)
            hooks = [alignment_saver]
            if params.record_profile:
                profileHook = tf.train.ProfilerHook(save_steps=params.profile_steps, output_dir=model_dir, show_dataflow=True, show_memory=True)
                hooks.append(profileHook)
            return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_hooks=hooks)
    if is_validation:
        (attention1_fn, attention2_fn) = dual_source_attention_factory(params)
        (mel_output_with_teacher, stop_token_with_teacher, decoder_state_with_teacher) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=is_validation, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=labels.target_length, target=target, teacher_forcing=True, apply_dropout_on_inference=params.apply_dropout_on_inference)
        if params.use_postnet_v2:
            postnet_v2_mel_output_with_teacher = postnet(mel_output_with_teacher)
        mel_loss_with_teacher = spec_loss(mel_output_with_teacher, labels.mel, labels.spec_loss_mask, params.spec_loss_type)
        done_loss_with_teacher = binary_loss(stop_token_with_teacher, labels.done, labels.binary_loss_mask)
        postnet_v2_mel_loss_with_teacher = (spec_loss(postnet_v2_mel_output_with_teacher, labels.mel, labels.spec_loss_mask, params.spec_loss_type) if params.use_postnet_v2 else 0)
        loss_with_teacher = (((mel_loss_with_teacher + done_loss_with_teacher) + regularization_loss) + postnet_v2_mel_loss_with_teacher)
        eval_metric_ops = self.get_validation_metrics(mel_loss, done_loss, postnet_v2_mel_loss, loss_with_teacher, mel_loss_with_teacher, done_loss_with_teacher, postnet_v2_mel_loss_with_teacher, regularization_loss)
        summary_writer = tf.summary.FileWriter(model_dir)
        alignment_saver = MetricsSaver((([alignment1, alignment2] + self_attention_alignment) + decoder_self_attention_alignment), global_step, mel_output, labels.mel, labels.target_length, features.id, features.text, 1, mode, summary_writer, save_training_time_metrics=params.save_training_time_metrics, keep_eval_results_max_epoch=params.keep_eval_results_max_epoch)
        return tf.estimator.EstimatorSpec(mode, loss=loss, evaluation_hooks=[alignment_saver], eval_metric_ops=eval_metric_ops)
    if is_prediction:
        num_self_alignments = len(self_attention_alignment)
        num_decoder_self_alignments = len(decoder_self_attention_alignment)
        predictions = {'id': features.id, 'key': features.key, 'mel': mel_output, 'mel_postnet': (postnet_v2_mel_output if params.use_postnet_v2 else None), 'ground_truth_mel': features.mel, 'alignment': alignment1, 'alignment2': alignment2, 'alignment3': (decoder_self_attention_alignment[0] if (num_decoder_self_alignments >= 1) else None), 'alignment4': (decoder_self_attention_alignment[1] if (num_decoder_self_alignments >= 2) else None), 'alignment5': (self_attention_alignment[0] if (num_self_alignments >= 1) else None), 'alignment6': (self_attention_alignment[1] if (num_self_alignments >= 2) else None), 'alignment7': (self_attention_alignment[2] if (num_self_alignments >= 3) else None), 'alignment8': (self_attention_alignment[3] if (num_self_alignments >= 4) else None), 'source': features.source, 'text': features.text, 'accent_type': (features.accent_type if params.use_accent_type else None)}
        predictions = dict(filter((lambda xy: (xy[1] is not None)), predictions.items()))
        return tf.estimator.EstimatorSpec(mode, predictions=predictions)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = tf.concat

idx = 32:------------------- similar code ------------------ index = 161, score = 6.0 
def STEmbedding(SE, TE, T, D, bn, bn_decay, is_training):
    '\n    spatio-temporal embedding\n    SE:     [N, D]\n    TE:     [batch_size, P + Q, 2] (dayofweek, timeofday)\n    T:      num of time steps in one day\n    D:      output dims\n    retrun: [batch_size, P + Q, N, D]\n    '
    SE = tf.expand_dims(tf.expand_dims(SE, axis=0), axis=0)
    SE = FC(SE, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    dayofweek = tf.one_hot(TE[(..., 0)], depth=7)
    timeofday = tf.one_hot(TE[(..., 1)], depth=T)
    TE = tf.concat((dayofweek, timeofday), axis=(- 1))
    TE = tf.expand_dims(TE, axis=2)
    TE = FC(TE, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    return tf.add(SE, TE)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 33:------------------- similar code ------------------ index = 166, score = 6.0 
def reshape(tensor):
    'Reshape into batches of sub-batches.'
    pad_shape = tensor.shape.as_list()
    pad_shape[0] = pad_amount
    padding = tf.zeros(shape=pad_shape, dtype=tensor.dtype)
    tensor = tf.concat([tensor, padding], axis=0)
    if ((tensor.shape[0].value % sub_batch_size) != 0):
        raise ValueError(('Incorrent padding size: %d does not divide %d' % (sub_batch_size, tensor.shape[0].value)))
    shape = tensor.shape.as_list()
    output_shape = ([(- 1), sub_batch_size] + shape[1:])
    return tf.reshape(tensor, shape=output_shape)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = tf.concat

idx = 34:------------------- similar code ------------------ index = 47, score = 6.0 
def STEmbedding(SE, TE, T, D, bn, bn_decay, is_training):
    '\n    spatio-temporal embedding\n    SE:     [N, D]\n    TE:     [batch_size, P + Q, 2] (dayofweek, timeofday)\n    T:      num of time steps in one day\n    D:      output dims\n    retrun: [batch_size, P + Q, N, D]\n    '
    SE = tf.expand_dims(tf.expand_dims(SE, axis=0), axis=0)
    SE = FC(SE, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    dayofweek = tf.one_hot(TE[(..., 0)], depth=7)
    timeofday = tf.one_hot(TE[(..., 1)], depth=T)
    TE = tf.concat((dayofweek, timeofday), axis=(- 1))
    TE = tf.expand_dims(TE, axis=2)
    TE = FC(TE, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    return tf.add(SE, TE)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 35:------------------- similar code ------------------ index = 52, score = 6.0 
def conv2d(input_, output_dim, k_h=4, k_w=4, d_h=2, d_w=2, stddev=None, name='conv2d', spectral_normed=False, update_collection=None, with_w=False, padding='SAME', mhe=False, net_type='d'):
    fan_in = ((k_h * k_w) * input_.get_shape().as_list()[(- 1)])
    fan_out = ((k_h * k_w) * output_dim)
    if (stddev is None):
        stddev = np.sqrt((2.0 / fan_in))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[(- 1)], output_dim], initializer=tf.truncated_normal_initializer(stddev=stddev))
        if spectral_normed:
            conv = tf.nn.conv2d(input_, spectral_normed_weight(w, update_collection=update_collection), strides=[1, d_h, d_w, 1], padding=padding)
        else:
            conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=padding)
        if mhe:
            print(('mhe on %s' % name))
            eps = 0.0001
            filt = w
            filt = tf.reshape(filt, [(- 1), output_dim])
            filt = tf.concat([filt, (- filt)], axis=0)
            filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
            filt /= filt_norm
            inner_pro = tf.matmul(tf.transpose(filt), filt)
            cross_terms = (2.0 - (2.0 * inner_pro))
            cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(output_dim)))
            loss = ((- 1e-07) * tf.reduce_mean(tf.log((cross_terms + eps))))
            if (net_type == 'g'):
                tf.add_to_collection('g_mhe_loss', loss)
            elif (net_type == 'd'):
                tf.add_to_collection('d_mhe_loss', loss)
            else:
                raise
        biases = tf.get_variable('b', [output_dim], initializer=tf.constant_initializer(0.0))
        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())
        if with_w:
            return (conv, w, biases)
        else:
            return conv

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :
             ...  = tf.concat

idx = 36:------------------- similar code ------------------ index = 99, score = 6.0 
def deconv2d(input_, output_shape, k_h=4, k_w=4, d_h=2, d_w=2, stddev=None, name='deconv2d', spectral_normed=False, update_collection=None, with_w=False, padding='SAME', mhe=False, net_type='g'):
    fan_in = ((k_h * k_w) * input_.get_shape().as_list()[(- 1)])
    fan_out = ((k_h * k_w) * output_shape[(- 1)])
    if (stddev is None):
        stddev = np.sqrt((2.0 / fan_in))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        w = tf.get_variable('w', [k_h, k_w, output_shape[(- 1)], input_.get_shape()[(- 1)]], initializer=tf.truncated_normal_initializer(stddev=stddev))
        if spectral_normed:
            deconv = tf.nn.conv2d_transpose(input_, spectral_normed_weight(w, update_collection=update_collection), output_shape=output_shape, strides=[1, d_h, d_w, 1], padding=padding)
        else:
            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1], padding=padding)
            if mhe:
                eps = 0.0001
                filt = w
                filt_num = input_.get_shape().as_list()[(- 1)]
                filt = tf.reshape(filt, [(- 1), filt_num])
                filt = tf.concat([filt, (- filt)], axis=0)
                filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
                filt /= filt_norm
                inner_pro = tf.matmul(tf.transpose(filt), filt)
                cross_terms = (2.0 - (2.0 * inner_pro))
                cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(filt_num)))
                loss = ((- 1e-06) * tf.reduce_mean(tf.log((cross_terms + eps))))
                if (net_type == 'g'):
                    tf.add_to_collection('g_mhe_loss', loss)
                else:
                    raise
        biases = tf.get_variable('b', [output_shape[(- 1)]], initializer=tf.constant_initializer(0))
        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())
        if with_w:
            return (deconv, w, biases)
        else:
            return deconv

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :        else:
            if  ... :
                 ...  = tf.concat

idx = 37:------------------- similar code ------------------ index = 101, score = 6.0 
def __init__(self, params, model_dir=None, config=None, warm_start_from=None):

    def model_fn(features, labels, mode, params):
        is_training = (mode == tf.estimator.ModeKeys.TRAIN)
        is_validation = (mode == tf.estimator.ModeKeys.EVAL)
        is_prediction = (mode == tf.estimator.ModeKeys.PREDICT)
        embedding = Embedding(params.num_symbols, embedding_dim=params.embedding_dim)
        if params.use_accent_type:
            accent_embedding = Embedding(params.num_accent_type, embedding_dim=params.accent_type_embedding_dim, index_offset=params.accent_type_offset)
        encoder = encoder_factory(params, is_training)
        assert (params.decoder in ['DualSourceDecoder', 'DualSourceTransformerDecoder'])
        decoder = decoder_factory(params)
        assert (not (params.use_speaker_embedding and params.use_external_speaker_embedding))
        if params.use_speaker_embedding:
            speaker_embedding = Embedding(params.num_speakers, embedding_dim=params.speaker_embedding_dim, index_offset=params.speaker_embedding_offset)
        elif params.use_external_speaker_embedding:
            speaker_embedding = ExternalEmbedding(params.embedding_file, params.num_speakers, embedding_dim=params.speaker_embedding_dim, index_offset=params.speaker_embedding_offset)
        if (params.speaker_embedding_projection_out_dim > (- 1)):

            def _compose(f, g):
                return (lambda arg, *args, **kwargs: f(g(arg, *args, **kwargs)))
            resize = tf.layers.Dense(params.speaker_embedding_projection_out_dim, activation=tf.nn.relu)
            speaker_embedding = _compose(resize, speaker_embedding)
        if params.use_language_embedding:
            language_embedding = ExternalEmbedding(params.language_embedding_file, params.num_speakers, embedding_dim=params.language_embedding_dim, index_offset=params.speaker_embedding_offset)
        if (params.language_embedding_projection_out_dim > (- 1)):

            def _compose(f, g):
                return (lambda arg, *args, **kwargs: f(g(arg, *args, **kwargs)))
            resize = tf.layers.Dense(params.language_embedding_projection_out_dim, activation=tf.nn.relu)
            language_embedding = _compose(resize, language_embedding)
        if params.channel_id_to_postnet:
            channel_code = ExternalEmbedding(params.channel_id_file, params.num_speakers, embedding_dim=params.channel_id_dim, index_offset=params.speaker_embedding_offset)
        target = (labels.mel if (is_training or is_validation) else features.mel)
        x = params.speaker_for_synthesis
        if (x > (- 1)):
            speaker_embedding_output = speaker_embedding(x)
        else:
            speaker_embedding_output = (speaker_embedding(features.speaker_id) if (params.use_speaker_embedding or params.use_external_speaker_embedding) else None)
        if (x > (- 1)):
            language_embedding_output = language_embedding(x)
        else:
            language_embedding_output = (language_embedding(features.speaker_id) if params.use_language_embedding else None)
        channel_code_output = (channel_code(features.speaker_id) if params.channel_id_to_postnet else None)
        embedding_output = embedding(features.source)
        if params.language_embedd_to_input:
            language_embedd_input_projection_layer = tf.layers.Dense(params.embedding_dim)
            language_embedd_input_projected = language_embedd_input_projection_layer(language_embedding_output)
            expand_language_embedding_input = tf.tile(tf.expand_dims(language_embedd_input_projected, axis=1), [1, tf.shape(embedding_output)[1], 1])
            embedding_output = (embedding_output + expand_language_embedding_input)
        (encoder_lstm_output, encoder_self_attention_output, self_attention_alignment) = (encoder((embedding_output, accent_embedding(features.accent_type)), input_lengths=features.source_length) if params.use_accent_type else encoder(embedding_output, input_lengths=features.source_length))
        if params.speaker_embedd_to_decoder:
            expand_speaker_embedding_output = tf.tile(tf.expand_dims(speaker_embedding_output, axis=1), [1, tf.shape(encoder_lstm_output)[1], 1])
            encoder_lstm_output = tf.concat((encoder_lstm_output, expand_speaker_embedding_output), axis=(- 1))
            encoder_self_attention_output = tf.concat((encoder_self_attention_output, expand_speaker_embedding_output), axis=(- 1))
        if params.language_embedd_to_decoder:
            expand_language_embedding_output = tf.tile(tf.expand_dims(language_embedding_output, axis=1), [1, tf.shape(encoder_lstm_output)[1], 1])
            encoder_lstm_output = tf.concat((encoder_lstm_output, expand_language_embedding_output), axis=(- 1))
            encoder_self_attention_output = tf.concat((encoder_self_attention_output, expand_language_embedding_output), axis=(- 1))
        (attention1_fn, attention2_fn) = dual_source_attention_factory(params)
        (mel_output, stop_token, decoder_state) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=(is_validation or params.use_forced_alignment_mode), teacher_forcing=params.use_forced_alignment_mode, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=(labels.target_length if is_training else None), target=target, apply_dropout_on_inference=params.apply_dropout_on_inference)
        self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in self_attention_alignment]
        if ((params.decoder == 'DualSourceTransformerDecoder') and (not is_training)):
            decoder_rnn_state = decoder_state.rnn_state.rnn_state[0]
            alignment1 = tf.transpose(decoder_rnn_state.alignment_history[0].stack(), [1, 2, 0])
            alignment2 = tf.transpose(decoder_rnn_state.alignment_history[1].stack(), [1, 2, 0])
            decoder_self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in decoder_state.alignments]
        else:
            decoder_rnn_state = decoder_state[0]
            alignment1 = tf.transpose(decoder_rnn_state.alignment_history[0].stack(), [1, 2, 0])
            alignment2 = tf.transpose(decoder_rnn_state.alignment_history[1].stack(), [1, 2, 0])
            decoder_self_attention_alignment = []
        if params.use_forced_alignment_mode:
            (attention1_fn, attention2_fn) = force_alignment_dual_source_attention_factory(params)
            (mel_output, stop_token, decoder_state) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=True, teacher_forcing=False, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=(labels.target_length if is_training else None), target=target, teacher_alignments=(tf.transpose(alignment1, perm=[0, 2, 1]), tf.transpose(alignment2, perm=[0, 2, 1])), apply_dropout_on_inference=params.apply_dropout_on_inference)
            if ((params.decoder == 'DualSourceTransformerDecoder') and (not is_training)):
                alignment1 = tf.transpose(decoder_state.rnn_state.rnn_state[0].alignment_history[0].stack(), [1, 2, 0])
                alignment2 = tf.transpose(decoder_state.rnn_state.rnn_state[0].alignment_history[1].stack(), [1, 2, 0])
                decoder_self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in decoder_state.alignments]
            else:
                alignment1 = tf.transpose(decoder_state[0].alignment_history[0].stack(), [1, 2, 0])
                alignment2 = tf.transpose(decoder_state[0].alignment_history[1].stack(), [1, 2, 0])
                decoder_self_attention_alignment = []
        if params.use_postnet_v2:
            postnet = (MultiSpeakerPostNet(out_units=params.num_mels, speaker_embed=speaker_embedding_output, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate) if params.speaker_embedd_to_postnet else (ChannelEncoderPostNet(out_units=params.num_mels, channel_code=channel_code_output, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate) if params.channel_id_to_postnet else PostNetV2(out_units=params.num_mels, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate)))
            postnet_v2_mel_output = postnet(mel_output)
        global_step = tf.train.get_global_step()
        if (mode is not tf.estimator.ModeKeys.PREDICT):
            mel_loss = spec_loss(mel_output, labels.mel, labels.spec_loss_mask, params.spec_loss_type)
            done_loss = binary_loss(stop_token, labels.done, labels.binary_loss_mask)
            blacklist = ['embedding', 'bias', 'batch_normalization', 'output_projection_wrapper/kernel', 'lstm_cell', 'output_and_stop_token_wrapper/dense/', 'output_and_stop_token_wrapper/dense_1/', 'stop_token_projection/kernel']
            regularization_loss = (l2_regularization_loss(tf.trainable_variables(), params.l2_regularization_weight, blacklist) if params.use_l2_regularization else 0)
            postnet_v2_mel_loss = (spec_loss(postnet_v2_mel_output, labels.mel, labels.spec_loss_mask, params.spec_loss_type) if params.use_postnet_v2 else 0)
            loss = (((mel_loss + done_loss) + regularization_loss) + postnet_v2_mel_loss)
        if is_training:
            lr = (self.learning_rate_decay(params.initial_learning_rate, global_step, params.learning_rate_step_factor) if params.decay_learning_rate else tf.convert_to_tensor(params.initial_learning_rate))
            optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1=params.adam_beta1, beta2=params.adam_beta2, epsilon=params.adam_eps)
            (gradients, variables) = zip(*optimizer.compute_gradients(loss))
            (clipped_gradients, _) = tf.clip_by_global_norm(gradients, 1.0)
            self.add_training_stats(loss, mel_loss, done_loss, lr, postnet_v2_mel_loss, regularization_loss)
            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
                train_op = optimizer.apply_gradients(zip(clipped_gradients, variables), global_step=global_step)
                summary_writer = tf.summary.FileWriter(model_dir)
                alignment_saver = MetricsSaver(([alignment1, alignment2] + self_attention_alignment), global_step, mel_output, labels.mel, labels.target_length, features.id, features.text, params.alignment_save_steps, mode, summary_writer, save_training_time_metrics=params.save_training_time_metrics, keep_eval_results_max_epoch=params.keep_eval_results_max_epoch)
                hooks = [alignment_saver]
                if params.record_profile:
                    profileHook = tf.train.ProfilerHook(save_steps=params.profile_steps, output_dir=model_dir, show_dataflow=True, show_memory=True)
                    hooks.append(profileHook)
                return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_hooks=hooks)
        if is_validation:
            (attention1_fn, attention2_fn) = dual_source_attention_factory(params)
            (mel_output_with_teacher, stop_token_with_teacher, decoder_state_with_teacher) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=is_validation, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=labels.target_length, target=target, teacher_forcing=True, apply_dropout_on_inference=params.apply_dropout_on_inference)
            if params.use_postnet_v2:
                postnet_v2_mel_output_with_teacher = postnet(mel_output_with_teacher)
            mel_loss_with_teacher = spec_loss(mel_output_with_teacher, labels.mel, labels.spec_loss_mask, params.spec_loss_type)
            done_loss_with_teacher = binary_loss(stop_token_with_teacher, labels.done, labels.binary_loss_mask)
            postnet_v2_mel_loss_with_teacher = (spec_loss(postnet_v2_mel_output_with_teacher, labels.mel, labels.spec_loss_mask, params.spec_loss_type) if params.use_postnet_v2 else 0)
            loss_with_teacher = (((mel_loss_with_teacher + done_loss_with_teacher) + regularization_loss) + postnet_v2_mel_loss_with_teacher)
            eval_metric_ops = self.get_validation_metrics(mel_loss, done_loss, postnet_v2_mel_loss, loss_with_teacher, mel_loss_with_teacher, done_loss_with_teacher, postnet_v2_mel_loss_with_teacher, regularization_loss)
            summary_writer = tf.summary.FileWriter(model_dir)
            alignment_saver = MetricsSaver((([alignment1, alignment2] + self_attention_alignment) + decoder_self_attention_alignment), global_step, mel_output, labels.mel, labels.target_length, features.id, features.text, 1, mode, summary_writer, save_training_time_metrics=params.save_training_time_metrics, keep_eval_results_max_epoch=params.keep_eval_results_max_epoch)
            return tf.estimator.EstimatorSpec(mode, loss=loss, evaluation_hooks=[alignment_saver], eval_metric_ops=eval_metric_ops)
        if is_prediction:
            num_self_alignments = len(self_attention_alignment)
            num_decoder_self_alignments = len(decoder_self_attention_alignment)
            predictions = {'id': features.id, 'key': features.key, 'mel': mel_output, 'mel_postnet': (postnet_v2_mel_output if params.use_postnet_v2 else None), 'ground_truth_mel': features.mel, 'alignment': alignment1, 'alignment2': alignment2, 'alignment3': (decoder_self_attention_alignment[0] if (num_decoder_self_alignments >= 1) else None), 'alignment4': (decoder_self_attention_alignment[1] if (num_decoder_self_alignments >= 2) else None), 'alignment5': (self_attention_alignment[0] if (num_self_alignments >= 1) else None), 'alignment6': (self_attention_alignment[1] if (num_self_alignments >= 2) else None), 'alignment7': (self_attention_alignment[2] if (num_self_alignments >= 3) else None), 'alignment8': (self_attention_alignment[3] if (num_self_alignments >= 4) else None), 'source': features.source, 'text': features.text, 'accent_type': (features.accent_type if params.use_accent_type else None)}
            predictions = dict(filter((lambda xy: (xy[1] is not None)), predictions.items()))
            return tf.estimator.EstimatorSpec(mode, predictions=predictions)
    super(DualSourceSelfAttentionTacotronModel, self).__init__(model_fn=model_fn, model_dir=model_dir, config=config, params=params, warm_start_from=warm_start_from)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():

    def  ... ():
        if:
             ...  = tf.concat

idx = 38:------------------- similar code ------------------ index = 115, score = 6.0 
def split_heads(inputs, num_heads, name=None):
    ' Split heads\n    :param inputs: A tensor with shape [batch, ..., channels]\n    :param num_heads: An integer\n    :param name: An optional string\n    :returns: A tensor with shape [batch, heads, ..., channels / heads]\n    '
    with tf.name_scope(name, default_name='split_heads', values=[inputs]):
        x = inputs
        n = num_heads
        old_shape = x.get_shape().dims
        ndims = x.shape.ndims
        last = old_shape[(- 1)]
        new_shape = ((old_shape[:(- 1)] + [n]) + [((last // n) if last else None)])
        ret = tf.reshape(x, tf.concat([tf.shape(x)[:(- 1)], [n, (- 1)]], 0))
        ret.set_shape(new_shape)
        perm = (([0, (ndims - 1)] + [i for i in range(1, (ndims - 1))]) + [ndims])
        return tf.transpose(ret, perm)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  =  ... . ... ( ... , tf.concat)

idx = 39:------------------- similar code ------------------ index = 64, score = 6.0 
def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):
    'Return a 3x3 transformmatrix which transforms indicies of original images\n  '
    rotation = ((math.pi * rotation) / 180.0)
    shear = ((math.pi * shear) / 180.0)
    c1 = tf.math.cos(rotation)
    s1 = tf.math.sin(rotation)
    rotation_matrix = tf.reshape(tf.concat([c1, s1, [0], (- s1), c1, [0], [0], [0], [1]], axis=0), [3, 3])
    c2 = tf.math.cos(shear)
    s2 = tf.math.sin(shear)
    shear_matrix = tf.reshape(tf.concat([[1], s2, [0], [0], c2, [0], [0], [0], [1]], axis=0), [3, 3])
    zoom_matrix = tf.reshape(tf.concat([([1] / height_zoom), [0], [0], [0], ([1] / width_zoom), [0], [0], [0], [1]], axis=0), [3, 3])
    shift_matrix = tf.reshape(tf.concat([[1], [0], height_shift, [0], [1], width_shift, [0], [0], [1]], axis=0), [3, 3])
    return tf.matmul(tf.matmul(rotation_matrix, shear_matrix), tf.matmul(zoom_matrix, shift_matrix))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... (tf.concat,)

idx = 40:------------------- similar code ------------------ index = 56, score = 6.0 
def generator_unet(image, options, reuse=False, name='generator'):
    dropout_rate = (0.5 if options.is_training else 1.0)
    with tf.variable_scope(name):
        if reuse:
            tf.get_variable_scope().reuse_variables()
        else:
            assert (tf.get_variable_scope().reuse is False)
        e1 = instance_norm(conv2d(image, options.gf_dim, name='g_e1_conv'))
        e2 = instance_norm(conv2d(lrelu(e1), (options.gf_dim * 2), name='g_e2_conv'), 'g_bn_e2')
        e3 = instance_norm(conv2d(lrelu(e2), (options.gf_dim * 4), s=3, name='g_e3_conv'), 'g_bn_e3')
        e4 = instance_norm(conv2d(lrelu(e3), (options.gf_dim * 8), s=[2, 1], name='g_e4_conv'), 'g_bn_e4')
        e5 = instance_norm(conv2d(lrelu(e4), (options.gf_dim * 8), s=[2, 1], name='g_e5_conv'), 'g_bn_e5')
        e6 = instance_norm(conv2d(lrelu(e5), (options.gf_dim * 8), s=[2, 7], name='g_e6_conv'), 'g_bn_e6')
        e7 = instance_norm(conv2d(lrelu(e6), (options.gf_dim * 8), s=[2, 1], name='g_e7_conv'), 'g_bn_e7')
        e8 = instance_norm(conv2d(lrelu(e7), (options.gf_dim * 8), s=[2, 1], name='g_e8_conv'), 'g_bn_e8')
        d1 = deconv2d(tf.nn.relu(e8), (options.gf_dim * 8), s=[2, 1], name='g_d1')
        d1 = tf.nn.dropout(d1, dropout_rate)
        d1 = tf.concat([instance_norm(d1, 'g_bn_d1'), e7], 3)
        d2 = deconv2d(tf.nn.relu(d1), (options.gf_dim * 8), s=[2, 1], name='g_d2')
        d2 = tf.nn.dropout(d2, dropout_rate)
        d2 = tf.concat([instance_norm(d2, 'g_bn_d2'), e6], 3)
        d3 = deconv2d(tf.nn.relu(d2), (options.gf_dim * 8), s=[2, 7], name='g_d3')
        d3 = tf.nn.dropout(d3, dropout_rate)
        d3 = tf.concat([instance_norm(d3, 'g_bn_d3'), e5], 3)
        d4 = deconv2d(tf.nn.relu(d3), (options.gf_dim * 8), s=[2, 1], name='g_d4')
        d4 = tf.concat([instance_norm(d4, 'g_bn_d4'), e4], 3)
        d5 = deconv2d(tf.nn.relu(d4), (options.gf_dim * 4), s=[2, 1], name='g_d5')
        d5 = tf.concat([instance_norm(d5, 'g_bn_d5'), e3], 3)
        d6 = deconv2d(tf.nn.relu(d5), (options.gf_dim * 2), s=3, name='g_d6')
        d6 = tf.concat([instance_norm(d6, 'g_bn_d6'), e2], 3)
        d7 = deconv2d(tf.nn.relu(d6), options.gf_dim, name='g_d7')
        d7 = tf.concat([instance_norm(d7, 'g_bn_d7'), e1], 3)
        d8 = deconv2d(tf.nn.relu(d7), options.output_c_dim, name='g_d8')
        return tf.nn.tanh(d8)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = tf.concat

idx = 41:------------------- similar code ------------------ index = 122, score = 6.0 
def tf_batch_histogram(values, value_range, axis, nbins=100, dtype=tf.int32, use_map=True):
    '\n    Computes histogram with fixed width considering batch dimensions\n    :param values: Numeric `Tensor` containing the values for histogram computation.\n    :param value_range: Shape [2] `Tensor` of same `dtype` as `values`. values <= value_range[0] will be mapped to\n    hist[0], values >= value_range[1] will be mapped to hist[-1].\n    :param axis: Number of batch dimensions. First axis to apply histogram computation to.\n    :param nbins: Scalar `int32 Tensor`. Number of histogram bins.\n    :param dtype: dtype for returned histogram, can be either tf.int32 or tf.int64.\n    :return: histogram with batch dimensions.\n    '
    values_shape = tf.shape(values)
    batch_dim = values_shape[:axis]
    rest_dim = values_shape[axis:]
    num_batch = tf.reduce_prod(batch_dim)
    if use_map:
        values_reshaped = tf.reshape(values, tf.concat([[num_batch], rest_dim], 0))
        hist = tf.map_fn((lambda x: tf.histogram_fixed_width(x, value_range, nbins=nbins, dtype=dtype)), values_reshaped, dtype=dtype, parallel_iterations=64)
    else:
        values_float = tf.cast(values, tf.float32)
        value_range_float = tf.cast(value_range, tf.float32)
        values_norm = ((values_float - value_range_float[0]) / (value_range_float[1] - value_range_float[0]))
        values_clip1 = tf.maximum(values_norm, (0.5 / tf.cast(nbins, tf.float32)))
        values_clip2 = tf.minimum(values_clip1, (1.0 - (0.5 / tf.cast(nbins, tf.float32))))
        values_shift = (values_clip2 + tf.reshape(tf.range(tf.cast(num_batch, tf.float32), dtype=tf.float32), tf.concat([batch_dim, tf.ones(tf.size(rest_dim), tf.int32)], 0)))
        hist = tf.histogram_fixed_width(values_shift, [0.0, tf.cast(num_batch, tf.float32)], nbins=(num_batch * nbins), dtype=dtype)
    return tf.reshape(hist, tf.concat([batch_dim, [nbins]], 0))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if  ... :
         ...  =  ... . ... ( ... , tf.concat)

idx = 42:------------------- similar code ------------------ index = 63, score = 6.0 
def linear(input_, output_size, name='linear', spectral_normed=False, update_collection=None, stddev=None, bias_start=0.0, with_biases=False, with_w=False, mhe=False, net_type='d'):
    shape = input_.get_shape().as_list()
    if (stddev is None):
        stddev = np.sqrt((1.0 / shape[1]))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        weight = tf.get_variable('w', [shape[1], output_size], tf.float32, tf.truncated_normal_initializer(stddev=stddev))
        if with_biases:
            bias = tf.get_variable('b', [output_size], initializer=tf.constant_initializer(bias_start))
        if spectral_normed:
            mul = tf.matmul(input_, spectral_normed_weight(weight, update_collection=update_collection))
        else:
            mul = tf.matmul(input_, weight)
        if mhe:
            eps = 0.0001
            filt = weight
            filt_num = filt.get_shape().as_list()[(- 1)]
            filt = tf.concat([filt, (- filt)], axis=0)
            filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
            filt /= filt_norm
            inner_pro = tf.matmul(tf.transpose(filt), filt)
            cross_terms = (2.0 - (2.0 * inner_pro))
            cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(filt_num)))
            loss = ((- 1e-06) * tf.reduce_mean(tf.log((cross_terms + eps))))
            if (net_type == 'g'):
                tf.add_to_collection('g_mhe_loss', loss)
            elif (net_type == 'd'):
                tf.add_to_collection('d_mhe_loss', loss)
            else:
                raise
        if with_w:
            if with_biases:
                return ((mul + bias), weight, bias)
            else:
                return (mul, weight, None)
        elif with_biases:
            return (mul + bias)
        else:
            return mul

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :
             ...  = tf.concat

idx = 43:------------------- similar code ------------------ index = 61, score = 6.0 
def build(self):
    'Build model.'
    (input_left, input_right) = self._make_inputs()
    len_left = input_left.shape[1]
    len_right = input_right.shape[1]
    embedding = self._make_embedding_layer()
    embed_left = embedding(input_left)
    embed_right = embedding(input_right)
    lstm_left = keras.layers.LSTM(self._params['lstm_num_units'], return_sequences=True, name='lstm_left')
    lstm_right = keras.layers.LSTM(self._params['lstm_num_units'], return_sequences=True, name='lstm_right')
    encoded_left = lstm_left(embed_left)
    encoded_right = lstm_right(embed_right)

    def attention(tensors):
        'Attention layer.'
        (left, right) = tensors
        tensor_left = tf.expand_dims(left, axis=2)
        tensor_right = tf.expand_dims(right, axis=1)
        tensor_left = K.repeat_elements(tensor_left, len_right, 2)
        tensor_right = K.repeat_elements(tensor_right, len_left, 1)
        tensor_merged = tf.concat([tensor_left, tensor_right], axis=(- 1))
        middle_output = keras.layers.Dense(self._params['fc_num_units'], activation='tanh')(tensor_merged)
        attn_scores = keras.layers.Dense(1)(middle_output)
        attn_scores = tf.squeeze(attn_scores, axis=3)
        exp_attn_scores = tf.math.exp((attn_scores - tf.reduce_max(attn_scores, axis=(- 1), keepdims=True)))
        exp_sum = tf.reduce_sum(exp_attn_scores, axis=(- 1), keepdims=True)
        attention_weights = (exp_attn_scores / exp_sum)
        return K.batch_dot(attention_weights, right)
    attn_layer = keras.layers.Lambda(attention)
    left_attn_vec = attn_layer([encoded_left, encoded_right])
    concat = keras.layers.Concatenate(axis=1)([left_attn_vec, encoded_right])
    lstm_merge = keras.layers.LSTM((self._params['lstm_num_units'] * 2), return_sequences=False, name='lstm_merge')
    merged = lstm_merge(concat)
    dropout = keras.layers.Dropout(rate=self._params['dropout_rate'])(merged)
    phi = keras.layers.Dense(self._params['fc_num_units'], activation='tanh')(dropout)
    inputs = [input_left, input_right]
    out = self._make_output_layer()(phi)
    self._backend = keras.Model(inputs=inputs, outputs=[out])

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    def  ... ( ... ):
         ...  = tf.concat

idx = 44:------------------- similar code ------------------ index = 27, score = 6.0 
def build_net(self, batch_data, config, summary=True, reuse=False):
    self.config = config
    batch_pos = ((batch_data / 127.5) - 1.0)
    if (config.mask_type == 'rect'):
        bbox = random_bbox(config)
        mask = bbox2mask(bbox, config, name='mask_c')
    else:
        mask = free_form_mask_tf(parts=8, im_size=(config.img_shapes[0], config.img_shapes[1]), maxBrushWidth=20, maxLength=80, maxVertex=16)
    batch_incomplete = (batch_pos * (1.0 - mask))
    mask_priority = priority_loss_mask(mask)
    (x_coarse, x_fine, layout, orth_loss) = self.build_generator(batch_incomplete, mask, reuse=reuse)
    losses = {}
    losses['orth_loss'] = orth_loss
    if summary:
        tf.summary.scalar('losses/orth_loss', losses['orth_loss'])
    if (config.pretrain_network is True):
        batch_predicted = x_fine
    else:
        batch_predicted = x_fine
    batch_complete = ((batch_predicted * mask) + (batch_incomplete * (1.0 - mask)))
    if (config.mask_type == 'rect'):
        local_patch_batch_pos = local_patch(batch_pos, bbox)
        local_patch_batch_complete = local_patch(batch_complete, bbox)
        local_patch_mask = local_patch(mask, bbox)
        local_patch_batch_pred = local_patch(batch_predicted, bbox)
        mask_priority = local_patch(mask_priority, bbox)
        local_patch_x_coarse = local_patch(x_coarse, bbox)
        local_patch_x_fine = local_patch(x_fine, bbox)
    else:
        local_patch_batch_pos = batch_pos
        local_patch_batch_complete = batch_complete
        local_patch_batch_pred = batch_predicted
        local_patch_x_coarse = x_coarse
        local_patch_x_fine = x_fine
    if config.pretrain_network:
        print('Pretrain the whole net with only reconstruction loss.')
    ID_MRF_loss = 0
    if ((config.pretrain_network is False) and (config.mrf_alpha != 0)):
        config.feat_style_layers = {'conv3_2': 1.0, 'conv4_2': 1.0}
        config.feat_content_layers = {'conv4_2': 1.0}
        config.mrf_style_w = 1.0
        config.mrf_content_w = 1.0
        ID_MRF_loss = id_mrf_reg(local_patch_batch_pred, local_patch_batch_pos, config)
        losses['ID_MRF_loss'] = ID_MRF_loss
        tf.summary.scalar('losses/ID_MRF_loss', losses['ID_MRF_loss'])
    pretrain_l1_alpha = config.pretrain_l1_alpha
    losses['l1_loss'] = (pretrain_l1_alpha * tf.reduce_mean((tf.abs((local_patch_batch_pos - local_patch_x_coarse)) * mask_priority)))
    if (not config.pretrain_network):
        losses['l1_loss'] += (pretrain_l1_alpha * tf.reduce_mean((tf.abs((local_patch_batch_pos - local_patch_x_fine)) * mask_priority)))
        losses['l1_loss'] += tf.reduce_mean((ID_MRF_loss * config.mrf_alpha))
    losses['ae_loss'] = (pretrain_l1_alpha * tf.reduce_mean((tf.abs((batch_pos - x_coarse)) * (1.0 - mask))))
    if (not config.pretrain_network):
        losses['ae_loss'] += (pretrain_l1_alpha * tf.reduce_mean((tf.abs((batch_pos - x_fine)) * (1.0 - mask))))
    losses['ae_loss'] /= tf.reduce_mean((1.0 - mask))
    if summary:
        viz_img = tf.concat([batch_pos, batch_incomplete, x_coarse, batch_predicted, batch_complete], axis=2)
        tf.summary.image('gt__degraded__coarse-predicted__predicted__completed', f2uint(viz_img))
        tf.summary.scalar('losses/l1_loss', losses['l1_loss'])
        tf.summary.scalar('losses/ae_loss', losses['ae_loss'])
    batch_pos_neg = tf.concat([batch_pos, batch_complete], axis=0)
    if (config.mask_type == 'rect'):
        local_patch_batch_pos_neg = tf.concat([local_patch_batch_pos, local_patch_batch_complete], 0)
        (pos_neg_local, pos_neg_global) = self.wgan_discriminator(local_patch_batch_pos_neg, batch_pos_neg, config.d_cnum, reuse=reuse)
    else:
        (pos_neg_local, pos_neg_global, mask_local) = self.wgan_mask_discriminator(batch_pos_neg, mask, config.d_cnum, reuse=reuse)
    (pos_local, neg_local) = tf.split(pos_neg_local, 2)
    (pos_global, neg_global) = tf.split(pos_neg_global, 2)
    global_wgan_loss_alpha = 1.0
    (g_loss_local, d_loss_local) = gan_wgan_loss(pos_local, neg_local, name='gan/local_gan')
    (g_loss_global, d_loss_global) = gan_wgan_loss(pos_global, neg_global, name='gan/global_gan')
    losses['g_loss'] = ((global_wgan_loss_alpha * g_loss_global) + g_loss_local)
    losses['d_loss'] = (d_loss_global + d_loss_local)
    interpolates_global = random_interpolates(batch_pos, batch_complete)
    if (config.mask_type == 'rect'):
        interpolates_local = random_interpolates(local_patch_batch_pos, local_patch_batch_complete)
        (dout_local, dout_global) = self.wgan_discriminator(interpolates_local, interpolates_global, config.d_cnum, reuse=True)
    else:
        interpolates_local = interpolates_global
        (dout_local, dout_global, _) = self.wgan_mask_discriminator(interpolates_global, mask, config.d_cnum, reuse=True)
    if (config.mask_type == 'rect'):
        penalty_local = gradients_penalty(interpolates_local, dout_local, mask=local_patch_mask)
    else:
        penalty_local = gradients_penalty(interpolates_local, dout_local, mask=mask)
    penalty_global = gradients_penalty(interpolates_global, dout_global, mask=mask)
    losses['gp_loss'] = (config.wgan_gp_lambda * (penalty_local + penalty_global))
    losses['d_loss'] = (losses['d_loss'] + losses['gp_loss'])
    if (summary and (not config.pretrain_network)):
        tf.summary.scalar('convergence/d_loss', losses['d_loss'])
        tf.summary.scalar('convergence/local_d_loss', d_loss_local)
        tf.summary.scalar('convergence/global_d_loss', d_loss_global)
        tf.summary.scalar('gan_wgan_loss/gp_loss', losses['gp_loss'])
        tf.summary.scalar('gan_wgan_loss/gp_penalty_local', penalty_local)
        tf.summary.scalar('gan_wgan_loss/gp_penalty_global', penalty_global)
    if config.pretrain_network:
        losses['g_loss'] = 0
    else:
        losses['g_loss'] = (config.gan_loss_alpha * losses['g_loss'])
        losses['g_loss'] += (config.orth_loss_alpha * losses['orth_loss'])
    losses['g_loss'] += (config.l1_loss_alpha * losses['l1_loss'])
    print(('Set L1_LOSS_ALPHA to %f' % config.l1_loss_alpha))
    print(('Set GAN_LOSS_ALPHA to %f' % config.gan_loss_alpha))
    losses['g_loss'] += (config.ae_loss_alpha * losses['ae_loss'])
    print(('Set AE_LOSS_ALPHA to %f' % config.ae_loss_alpha))
    tf.summary.scalar('losses/g_loss', losses['g_loss'])
    g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'inpaint_net')
    d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')
    return (g_vars, d_vars, losses)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if  ... :
         ...  = tf.concat

idx = 45:------------------- similar code ------------------ index = 195, score = 6.0 
def build_lut(histo, step):
    lut = ((tf.cumsum(histo) + (step // 2)) // step)
    lut = tf.concat([[0], lut[:(- 1)]], 0)
    return tf.clip_by_value(lut, 0, 255)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 46:------------------- similar code ------------------ index = 188, score = 6.0 
def train():
    with tf.Graph().as_default():
        with tf.device('/cpu:0'):
            (pointclouds_pl, labels_pl) = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)
            is_training_pl = tf.placeholder(tf.bool, shape=())
            batch = tf.get_variable('batch', [], initializer=tf.constant_initializer(0), trainable=False)
            bn_decay = get_bn_decay(batch)
            tf.summary.scalar('bn_decay', bn_decay)
            learning_rate = get_learning_rate(batch)
            tf.summary.scalar('learning_rate', learning_rate)
            if (OPTIMIZER == 'momentum'):
                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)
            elif (OPTIMIZER == 'adam'):
                optimizer = tf.train.AdamOptimizer(learning_rate)
            MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)
            tower_grads = []
            pred_gpu = []
            total_loss_gpu = []
            for i in range(NUM_GPUS):
                with tf.variable_scope(tf.get_variable_scope(), reuse=True):
                    with tf.device(('/gpu:%d' % i)), tf.name_scope(('gpu_%d' % i)) as scope:
                        pc_batch = tf.slice(pointclouds_pl, [(i * DEVICE_BATCH_SIZE), 0, 0], [DEVICE_BATCH_SIZE, (- 1), (- 1)])
                        label_batch = tf.slice(labels_pl, [(i * DEVICE_BATCH_SIZE)], [DEVICE_BATCH_SIZE])
                        (pred, end_points) = MODEL.get_model(pc_batch, is_training=is_training_pl, bn_decay=bn_decay)
                        MODEL.get_loss(pred, label_batch, end_points)
                        losses = tf.get_collection('losses', scope)
                        total_loss = tf.add_n(losses, name='total_loss')
                        for l in (losses + [total_loss]):
                            tf.summary.scalar(l.op.name, l)
                        grads = optimizer.compute_gradients(total_loss)
                        tower_grads.append(grads)
                        pred_gpu.append(pred)
                        total_loss_gpu.append(total_loss)
            pred = tf.concat(pred_gpu, 0)
            total_loss = tf.reduce_mean(total_loss_gpu)
            grads = average_gradients(tower_grads)
            train_op = optimizer.apply_gradients(grads, global_step=batch)
            correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))
            accuracy = (tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE))
            tf.summary.scalar('accuracy', accuracy)
        saver = tf.train.Saver()
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        config.allow_soft_placement = True
        config.log_device_placement = False
        sess = tf.Session(config=config)
        merged = tf.summary.merge_all()
        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph)
        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph)
        init = tf.global_variables_initializer()
        sess.run(init)
        ops = {'pointclouds_pl': pointclouds_pl, 'labels_pl': labels_pl, 'is_training_pl': is_training_pl, 'pred': pred, 'loss': total_loss, 'train_op': train_op, 'merged': merged, 'step': batch, 'end_points': end_points}
        best_acc = (- 1)
        for epoch in range(MAX_EPOCH):
            log_string(('**** EPOCH %03d ****' % epoch))
            sys.stdout.flush()
            train_one_epoch(sess, ops, train_writer)
            eval_one_epoch(sess, ops, test_writer)
            if ((epoch % 10) == 0):
                save_path = saver.save(sess, os.path.join(LOG_DIR, 'model.ckpt'))
                log_string(('Model saved in file: %s' % save_path))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        with:
             ...  = tf.concat

idx = 47:------------------- similar code ------------------ index = 182, score = 6.0 
def linear(input_, output_size, name='linear', spectral_normed=False, update_collection=None, stddev=None, bias_start=0.0, with_biases=False, with_w=False, mhe=False, net_type='d'):
    shape = input_.get_shape().as_list()
    if (stddev is None):
        stddev = np.sqrt((1.0 / shape[1]))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        weight = tf.get_variable('w', [shape[1], output_size], tf.float32, tf.truncated_normal_initializer(stddev=stddev))
        if with_biases:
            bias = tf.get_variable('b', [output_size], initializer=tf.constant_initializer(bias_start))
        if spectral_normed:
            mul = tf.matmul(input_, spectral_normed_weight(weight, update_collection=update_collection))
        else:
            mul = tf.matmul(input_, weight)
        if mhe:
            eps = 0.0001
            filt = weight
            filt_num = filt.get_shape().as_list()[(- 1)]
            filt = tf.concat([filt, (- filt)], axis=0)
            filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
            filt /= filt_norm
            inner_pro = tf.matmul(tf.transpose(filt), filt)
            cross_terms = (2.0 - (2.0 * inner_pro))
            cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(filt_num)))
            loss = ((- 1e-07) * tf.reduce_mean(tf.log((cross_terms + eps))))
            if (net_type == 'g'):
                tf.add_to_collection('g_mhe_loss', loss)
            elif (net_type == 'd'):
                tf.add_to_collection('d_mhe_loss', loss)
            else:
                raise
        if with_w:
            if with_biases:
                return ((mul + bias), weight, bias)
            else:
                return (mul, weight, None)
        elif with_biases:
            return (mul + bias)
        else:
            return mul

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :
             ...  = tf.concat

idx = 48:------------------- similar code ------------------ index = 62, score = 5.0 
def make_init_op(value_op):
    prediction = util.map_predictor(predicted_dict, predictor_fn, sub_batch_size=eval_batch_size)
    if (observed_dict is not None):
        library = tf.concat([prediction, observed_library], axis=0)
    else:
        library = prediction
    normalized_library = similarity_provider.preprocess_library(library)
    return value_op.assign(normalized_library)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:
         ...  = tf.concat

idx = 49:------------------- similar code ------------------ index = 87, score = 5.0 
def __call__(self, inputs, sequence_length, scope=None):
    '\n\t\tCreate the variables and do the forward computation\n\n\t\tArgs:\n\t\t\tinputs: the input to the layer as a\n\t\t\t\t[batch_size, max_length, dim] tensor\n\t\t\tsequence_length: the length of the input sequences as a\n\t\t\t\t[batch_size] tensor\n\t\t\tscope: The variable scope sets the namespace under which\n\t\t\t\tthe variables created during this call will be stored.\n\n\t\tReturns:\n\t\t\tthe output of the layer\n\t\t'
    with tf.variable_scope((scope or type(self).__name__)):
        gru_cell_fw = rnn_cell.LeakGRUCell(num_units=self.num_units, leak_factor=self.leak_factor, activation=self.activation_fn, reuse=tf.get_variable_scope().reuse)
        gru_cell_bw = rnn_cell.LeakGRUCell(num_units=self.num_units, leak_factor=self.leak_factor, activation=self.activation_fn, reuse=tf.get_variable_scope().reuse)
        (outputs_tupple, _) = bidirectional_dynamic_rnn(gru_cell_fw, gru_cell_bw, inputs, dtype=tf.float32, sequence_length=sequence_length)
        outputs = tf.concat(outputs_tupple, 2)
        return outputs

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = tf.concat

idx = 50:------------------- similar code ------------------ index = 86, score = 5.0 
def _meshgrid(height, width):
    with tf.variable_scope('_meshgrid'):
        x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])), tf.transpose(tf.expand_dims(tf.linspace((- 1.0), 1.0, width), 1), [1, 0]))
        y_t = tf.matmul(tf.expand_dims(tf.linspace((- 1.0), 1.0, height), 1), tf.ones(shape=tf.stack([1, width])))
        x_t_flat = tf.reshape(x_t, (1, (- 1)))
        y_t_flat = tf.reshape(y_t, (1, (- 1)))
        ones = tf.ones_like(x_t_flat)
        grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones])
        return grid

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = tf.concat

idx = 51:------------------- similar code ------------------ index = 4, score = 5.0 
def build_graph(self, x):
    (output, attention_feature_map) = self.self_attention_autoencoder(x)
    output = utils.batch_mean_image_subtraction(output)
    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))
    if (self.recons_weight > 0.0):
        recons_loss = tf.losses.mean_squared_error(x, output, weights=self.recons_weight, scope='recons_loss')
        self.recons_loss = recons_loss
        self.total_loss += recons_loss
        summaries.add(tf.summary.scalar('losses/recons_loss', recons_loss))
    if (self.perceptual_weight > 0.0):
        input_features = utils.extract_image_features(x, True)
        output_features = utils.extract_image_features(output, True)
        perceptual_loss = 0.0
        for layer in self.perceptual_loss_layers:
            input_perceptual_features = input_features[('vgg_19/' + layer)]
            output_perceptual_features = output_features[('vgg_19/' + layer)]
            perceptual_loss += tf.losses.mean_squared_error(input_perceptual_features, output_perceptual_features, weights=self.perceptual_weight, scope=layer)
        self.perceptual_loss = perceptual_loss
        self.total_loss += perceptual_loss
        summaries.add(tf.summary.scalar('losses/perceptual_loss', perceptual_loss))
    if (self.tv_weight > 0.0):
        tv_loss = utils.compute_total_variation_loss_l1(output, self.tv_weight)
        self.tv_loss = tv_loss
        self.total_loss += tv_loss
        summaries.add(tf.summary.scalar('losses/tv_loss', tv_loss))
    if (self.attention_weight > 0.0):
        atten_l1_loss = (self.attention_weight * tf.norm(attention_feature_map, 1))
        self.attention_l1_loss = atten_l1_loss
        self.total_loss += atten_l1_loss
        summaries.add(tf.summary.scalar('losses/attention_l1_loss', atten_l1_loss))
    summaries.add(tf.summary.scalar('losses/total_loss', self.total_loss))
    image_tiles = tf.concat([x, output], axis=2)
    image_tiles = utils.batch_mean_image_summation(image_tiles)
    image_tiles = tf.cast(tf.clip_by_value(image_tiles, 0.0, 255.0), tf.uint8)
    summaries.add(tf.summary.image('image_comparison', image_tiles, max_outputs=8))
    self.summaries = summaries
    return self.total_loss

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 52:------------------- similar code ------------------ index = 85, score = 5.0 
def call(self, inputs: list, **kwargs) -> typing.Any:
    '\n        The computation logic of DynamicPoolingLayer.\n\n        :param inputs: two input tensors.\n        '
    self._validate_dpool_size()
    (x, dpool_index) = inputs
    dpool_shape = tf.shape(dpool_index)
    batch_index_one = tf.expand_dims(tf.expand_dims(tf.range(dpool_shape[0]), axis=(- 1)), axis=(- 1))
    batch_index = tf.expand_dims(tf.tile(batch_index_one, [1, self._msize1, self._msize2]), axis=(- 1))
    dpool_index_ex = tf.concat([batch_index, dpool_index], axis=3)
    x_expand = tf.gather_nd(x, dpool_index_ex)
    stride1 = (self._msize1 // self._psize1)
    stride2 = (self._msize2 // self._psize2)
    x_pool = tf.nn.max_pool(x_expand, [1, stride1, stride2, 1], [1, stride1, stride2, 1], 'VALID')
    return x_pool

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... () ->:
     ...  = tf.concat

idx = 53:------------------- similar code ------------------ index = 84, score = 5.0 
def call(self, inputs, input_lengths=None, **kwargs):
    (input, accent_type) = inputs
    prenet_output = reduce((lambda acc, pn: pn(acc)), self.prenets, input)
    accent_type_prenet_output = reduce((lambda acc, pn: pn(acc)), self.accent_type_prenets, accent_type)
    concatenated = tf.concat([prenet_output, accent_type_prenet_output], axis=(- 1))
    cbhg_output = self.cbhg(concatenated, input_lengths=input_lengths)
    return cbhg_output

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 54:------------------- similar code ------------------ index = 83, score = 5.0 
def get_model(point_cloud, is_training, num_class, bn_decay=None):
    ' Semantic segmentation PointNet, input is BxNx4, output Bxnum_class '
    batch_size = point_cloud.get_shape()[0].value
    num_point = point_cloud.get_shape()[1].value
    end_points = {}
    l0_xyz = tf.slice(point_cloud, [0, 0, 0], [(- 1), (- 1), 3])
    l0_points = tf.slice(point_cloud, [0, 0, 3], [(- 1), (- 1), 1])
    end_points['l0_xyz'] = l0_xyz
    (l1_xyz, l1_points, l1_indices) = pointnet_sa_module(l0_xyz, l0_points, npoint=1024, radius=0.1, nsample=32, mlp=[32, 32, 64], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer1')
    (l2_xyz, l2_points, l2_indices) = pointnet_sa_module(l1_xyz, l1_points, npoint=256, radius=0.2, nsample=32, mlp=[64, 64, 128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer2')
    (l3_xyz, l3_points, l3_indices) = pointnet_sa_module(l2_xyz, l2_points, npoint=64, radius=0.4, nsample=32, mlp=[128, 128, 256], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer3')
    (l4_xyz, l4_points, l4_indices) = pointnet_sa_module(l3_xyz, l3_points, npoint=16, radius=0.8, nsample=32, mlp=[256, 256, 512], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer4')
    l3_points = pointnet_fp_module(l3_xyz, l4_xyz, l3_points, l4_points, [256, 256], is_training, bn_decay, scope='fa_layer1')
    l2_points = pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256, 256], is_training, bn_decay, scope='fa_layer2')
    l1_points = pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256, 128], is_training, bn_decay, scope='fa_layer3')
    l0_points = pointnet_fp_module(l0_xyz, l1_xyz, tf.concat([l0_xyz, l0_points], axis=(- 1)), l1_points, [128, 128, 128], is_training, bn_decay, scope='fa_layer4')
    net = tf_util.conv1d(l0_points, 128, 1, padding='VALID', bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay)
    end_points['feats'] = net
    net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp1')
    net = tf_util.conv1d(net, num_class, 1, padding='VALID', activation_fn=None, scope='fc2')
    return (net, end_points)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... ( ... ,  ... , tf.concat,  ... ,,  ... ,  ... ,)

idx = 55:------------------- similar code ------------------ index = 82, score = 5.0 
def add_embedding(self):
    with tf.device('/cpu:0'), tf.name_scope('word_embedding'):
        W = tf.Variable(self.pretrained_embedding, trainable=False, dtype=tf.float32, name='W')
        self.embedded_words = tf.nn.embedding_lookup(W, self.input_words)
        self.embedded_mentions = tf.nn.embedding_lookup(W, self.input_mentions)
        self.mention_embedding = tf.divide(tf.reduce_sum(tf.nn.embedding_lookup(W, self.mention), axis=1), self.mentionlen)
    with tf.device('/cpu:0'), tf.name_scope('position_embedding'):
        W = tf.Variable(self.wpe, trainable=False, dtype=tf.float32, name='W')
        self.wpe_chars = tf.nn.embedding_lookup(W, self.input_positions)
    self.input_sentences = tf.concat([self.embedded_words, self.wpe_chars], 2)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = tf.concat

idx = 56:------------------- similar code ------------------ index = 3, score = 5.0 
def call(self, inputs, state):
    "Perform a step of attention-wrapped RNN.\n        - Step 1: Mix the `inputs` and previous step's `attention` output via\n          `cell_input_fn`.\n        - Step 2: Call the wrapped `cell` with this input and its previous state.\n        - Step 3: Score the cell's output with `attention_mechanism`.\n        - Step 4: Calculate the alignments by passing the score through the\n          `normalizer`.\n        - Step 5: Calculate the context vector as the inner product between the\n          alignments and the attention_mechanism's values (memory).\n        - Step 6: Calculate the attention output by concatenating the cell output\n          and context through the attention layer (a linear layer with\n          `attention_layer_size` outputs).\n        Args:\n          inputs: (Possibly nested tuple of) Tensor, the input at this time step.\n          state: An instance of `AttentionWrapperState` containing\n            tensors from the previous time step.\n        Returns:\n          A tuple `(attention_or_cell_output, next_state)`, where:\n          - `attention_or_cell_output` depending on `output_attention`.\n          - `next_state` is an instance of `AttentionWrapperState`\n             containing the state calculated at this time step.\n        Raises:\n          TypeError: If `state` is not an instance of `AttentionWrapperState`.\n        "
    if (not isinstance(state, AttentionWrapperState)):
        raise TypeError(('Expected state to be instance of AttentionWrapperState. Received type %s instead.' % type(state)))
    cell_inputs = self._cell_input_fn(inputs, state.attention)
    cell_state = state.cell_state
    (cell_output, next_cell_state) = self._cell(cell_inputs, cell_state)
    cell_batch_size = (cell_output.shape[0].value or tf.shape(cell_output)[0])
    error_message = (('When applying AttentionWrapper %s: ' % self.name) + 'Non-matching batch sizes between the memory (encoder output) and the query (decoder output).  Are you using the BeamSearchDecoder?  You may need to tile your memory input via the tf.contrib.seq2seq.tile_batch function with argument multiple=beam_width.')
    with tf.control_dependencies(self._batch_size_checks(cell_batch_size, error_message)):
        cell_output = tf.identity(cell_output, name='checked_cell_output')
    if self._is_multi:
        previous_attention_state = state.attention_state
        previous_alignment_history = state.alignment_history
    else:
        previous_attention_state = [state.attention_state]
        previous_alignment_history = [state.alignment_history]
    all_alignments = []
    all_attentions = []
    all_attention_states = []
    maybe_all_histories = []
    for (i, attention_mechanism) in enumerate(self._attention_mechanisms):
        (attention, alignments, next_attention_state) = _compute_attention(attention_mechanism, cell_output, previous_attention_state[i], (self._attention_layers[i] if self._attention_layers else None), self.is_manual_attention, self.manual_alignments, state.time)
        alignment_history = (previous_alignment_history[i].write(state.time, alignments) if self._alignment_history else ())
        all_attention_states.append(next_attention_state)
        all_alignments.append(alignments)
        all_attentions.append(attention)
        maybe_all_histories.append(alignment_history)
    attention = tf.concat(all_attentions, 1)
    next_state = AttentionWrapperState(time=(state.time + 1), cell_state=next_cell_state, attention=attention, attention_state=self._item_or_tuple(all_attention_states), alignments=self._item_or_tuple(all_alignments), alignment_history=self._item_or_tuple(maybe_all_histories))
    if self._output_attention:
        return (attention, next_state)
    else:
        return (cell_output, next_state)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 57:------------------- similar code ------------------ index = 65, score = 5.0 
def build_features(self, features, embedding_suffix=''):
    '\n        categorical feature id starts from -1 (as missing)\n        '
    if self.flags.wide_cols:
        ev_list = [self.EmbeddingDict[key](features[key]) for key in self.CATEGORY_FEATURES if (key in self.flags.wide_cols)]
        fv_list = [self.build_dense_layer(features[key]) for key in self.NUMERICAL_FEATURES if (key in self.flags.wide_cols)]
        wide_list = self.concat((fv_list + ev_list))
    else:
        ev_list = [self.EmbeddingDict[key](features[key]) for key in self.CATEGORY_FEATURES]
        fv_list = [self.build_dense_layer(features[key]) for key in self.NUMERICAL_FEATURES]
        wide_list = self.concat((fv_list + ev_list))
    if self.flags.deep_cols:
        ev_list = [self.EmbeddingDict[key](features[key]) for key in self.CATEGORY_FEATURES if (key in self.flags.deep_cols)]
        fv_list = [self.build_dense_layer(features[key]) for key in self.NUMERICAL_FEATURES if (key in self.flags.deep_cols)]
        deep_list = self.concat((fv_list + ev_list))
    else:
        ev_list = [self.EmbeddingDict[key](features[key]) for key in self.CATEGORY_FEATURES]
        fv_list = [self.build_dense_layer(features[key]) for key in self.NUMERICAL_FEATURES]
        deep_list = self.concat((fv_list + ev_list))
    return (tf.concat(wide_list, (- 1)), tf.concat(deep_list, (- 1)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    return (tf.concat,)

idx = 58:------------------- similar code ------------------ index = 66, score = 5.0 
def matmul_v1(a, b, transpose_a=False, transpose_b=False, name=None):
    name = ('matmul' if (name is None) else name)
    with tf.name_scope(name):
        rank_a = len(a.shape)
        rank_b = len(b.shape)
        if ((rank_a < 2) or (rank_b < 2)):
            raise TypeError('Rank must be greater than 2')
        perm_a = [i for i in range((rank_a - 2))]
        perm_b = [i for i in range((rank_b - 2))]
        if transpose_a:
            perm = (([(rank_a - 1)] + perm_a) + [(rank_a - 2)])
        else:
            perm = (([(rank_a - 2)] + perm_a) + [(rank_a - 1)])
        a = tf.transpose(a, perm=perm)
        if transpose_b:
            perm = (([(rank_b - 2)] + perm_b) + [(rank_b - 1)])
        else:
            perm = (([(rank_b - 1)] + perm_b) + [(rank_b - 2)])
        b = tf.transpose(b, perm=perm)
        C = []
        for i in range(a.get_shape()[0].value):
            B = []
            for j in range(b.get_shape()[0].value):
                k = tf.reduce_sum((a[i] * b[j]), axis=(- 1), keepdims=True)
                B.append(k)
            C.append(tf.expand_dims(tf.concat(B, axis=(- 1)), axis=(- 2)))
        C = tf.concat(C, axis=(- 2))
        return C

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        for  ...  in:
             ... . ... ( ... . ... (tf.concat,))

idx = 59:------------------- similar code ------------------ index = 81, score = 5.0 
def linear(inputs, output_size, bias, concat=True, dtype=None, scope=None, trainable=True):
    "\n    Linear layer\n    :param inputs: A Tensor or a list of Tensors with shape [batch, input_size]\n    :param output_size: An integer specify the output size\n    :param bias: a boolean value indicate whether to use bias term\n    :param concat: a boolean value indicate whether to concatenate all inputs\n    :param dtype: an instance of tf.DType, the default value is ``tf.float32''\n    :param scope: the scope of this layer, the default value is ``linear''\n    :returns: a Tensor with shape [batch, output_size]\n    :raises RuntimeError: raises ``RuntimeError'' when input sizes do not\n                          compatible with each other\n    "
    with tf.variable_scope(scope, default_name='linear', values=[inputs]):
        if (not isinstance(inputs, (list, tuple))):
            inputs = [inputs]
        input_size = [item.get_shape()[(- 1)].value for item in inputs]
        if (len(inputs) != len(input_size)):
            raise RuntimeError('inputs and input_size unmatched!')
        output_shape = tf.concat([tf.shape(inputs[0])[:(- 1)], [output_size]], axis=0)
        inputs = [tf.reshape(inp, [(- 1), inp.shape[(- 1)].value]) for inp in inputs]
        results = []
        if concat:
            input_size = sum(input_size)
            inputs = tf.concat(inputs, 1)
            shape = [input_size, output_size]
            matrix = tf.get_variable('matrix', shape, dtype=dtype, trainable=trainable)
            results.append(tf.matmul(inputs, matrix))
        else:
            for i in range(len(input_size)):
                shape = [input_size[i], output_size]
                name = ('matrix_%d' % i)
                matrix = tf.get_variable(name, shape, dtype=dtype, trainable=trainable)
                results.append(tf.matmul(inputs[i], matrix))
        output = tf.add_n(results)
        if bias:
            shape = [output_size]
            bias = tf.get_variable('bias', shape, dtype=dtype, trainable=trainable)
            output = tf.nn.bias_add(output, bias)
        output = tf.reshape(output, output_shape)
        return output

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = tf.concat

idx = 60:------------------- similar code ------------------ index = 67, score = 5.0 
def __call__(self, inputs, sequence_length, scope=None):
    '\n\t\tCreate the variables and do the forward computation\n\n\t\tArgs:\n\t\t\tinputs: the input to the layer as a\n\t\t\t\t[batch_size, max_length, dim] tensor\n\t\t\tsequence_length: the length of the input sequences as a\n\t\t\t\t[batch_size] tensor\n\t\t\tscope: The variable scope sets the namespace under which\n\t\t\t\tthe variables created during this call will be stored.\n\n\t\tReturns:\n\t\t\tthe output of the layer\n\t\t'
    with tf.variable_scope((scope or type(self).__name__)):
        if self.use_output_matrix:
            lstm_cell_type = rnn_cell.LSTMCapsuleCellRecOnlyVoteOutputMatrix
        else:
            lstm_cell_type = rnn_cell.LSTMCapsuleCellRecOnlyVote
        lstm_cell_fw = lstm_cell_type(num_capsules=self.num_capsules, capsule_dim=self.capsule_dim, routing_iters=self.routing_iters, activation=self._activation, input_probability_fn=self.input_probability_fn, recurrent_probability_fn=self.recurrent_probability_fn, logits_prior=self.logits_prior, accumulate_input_logits=self.accumulate_input_logits, accumulate_state_logits=self.accumulate_state_logits, gates_fc=self.gates_fc, reuse=tf.get_variable_scope().reuse)
        lstm_cell_bw = lstm_cell_type(num_capsules=self.num_capsules, capsule_dim=self.capsule_dim, routing_iters=self.routing_iters, activation=self._activation, input_probability_fn=self.input_probability_fn, recurrent_probability_fn=self.recurrent_probability_fn, logits_prior=self.logits_prior, accumulate_input_logits=self.accumulate_input_logits, accumulate_state_logits=self.accumulate_state_logits, gates_fc=self.gates_fc, reuse=tf.get_variable_scope().reuse)
        (outputs_tupple, _) = bidirectional_dynamic_rnn(lstm_cell_fw, lstm_cell_bw, inputs, dtype=tf.float32, sequence_length=sequence_length)
        outputs = tf.concat(outputs_tupple, 2)
        return outputs

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = tf.concat

idx = 61:------------------- similar code ------------------ index = 1, score = 5.0 
def inception_module(self, name, cols, input_layer=None, in_size=None):
    if (input_layer is None):
        input_layer = self.top_layer
    if (in_size is None):
        in_size = self.top_size
    name += str(self.counts[name])
    self.counts[name] += 1
    with tf.variable_scope(name):
        col_layers = []
        col_layer_sizes = []
        for (c, col) in enumerate(cols):
            col_layers.append([])
            col_layer_sizes.append([])
            for (l, layer) in enumerate(col):
                (ltype, args) = (layer[0], layer[1:])
                kwargs = ({'input_layer': input_layer, 'num_channels_in': in_size} if (l == 0) else {})
                if (ltype == 'conv'):
                    self.conv(*args, **kwargs)
                elif (ltype == 'mpool'):
                    self.mpool(*args, **kwargs)
                elif (ltype == 'apool'):
                    self.apool(*args, **kwargs)
                elif (ltype == 'share'):
                    self.top_layer = col_layers[(c - 1)][l]
                    self.top_size = col_layer_sizes[(c - 1)][l]
                else:
                    raise KeyError(("Invalid layer type for inception module: '%s'" % ltype))
                col_layers[c].append(self.top_layer)
                col_layer_sizes[c].append(self.top_size)
        catdim = (3 if (self.data_format == 'NHWC') else 1)
        self.top_layer = tf.concat([layers[(- 1)] for layers in col_layers], catdim)
        self.top_size = sum([sizes[(- 1)] for sizes in col_layer_sizes])
        return self.top_layer

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
 = tf.concat

idx = 62:------------------- similar code ------------------ index = 79, score = 5.0 
def tf_generateSpecularRendering(batchSize, surfaceArray, targets, outputs):
    currentViewDir = tf_generate_normalized_random_direction(batchSize)
    currentLightDir = (currentViewDir * tf.expand_dims([(- 1.0), (- 1.0), 1.0], axis=0))
    currentShift = tf.concat([tf.random_uniform([batchSize, 2], (- 1.0), 1.0), (tf.zeros([batchSize, 1], dtype=tf.float32) + 0.0001)], axis=(- 1))
    currentViewPos = (tf.multiply(currentViewDir, tf_generate_distance(batchSize)) + currentShift)
    currentLightPos = (tf.multiply(currentLightDir, tf_generate_distance(batchSize)) + currentShift)
    currentViewPos = tf.expand_dims(currentViewPos, axis=1)
    currentViewPos = tf.expand_dims(currentViewPos, axis=1)
    currentLightPos = tf.expand_dims(currentLightPos, axis=1)
    currentLightPos = tf.expand_dims(currentLightPos, axis=1)
    wo = (currentViewPos - surfaceArray)
    wi = (currentLightPos - surfaceArray)
    renderedSpecular = tf_Render(targets, wi, wo, includeDiffuse=a.includeDiffuse)
    renderedSpecularOutputs = tf_Render(outputs, wi, wo, includeDiffuse=a.includeDiffuse)
    return [renderedSpecular, renderedSpecularOutputs]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 63:------------------- similar code ------------------ index = 69, score = 5.0 
def __call__(self, inputs, sequence_length, scope=None):
    '\n\t\tCreate the variables and do the forward computation\n\n\t\tArgs:\n\t\t\tinputs: the input to the layer as a\n\t\t\t\t[batch_size, max_length, dim] tensor\n\t\t\tsequence_length: the length of the input sequences as a\n\t\t\t\t[batch_size] tensor\n\t\t\tscope: The variable scope sets the namespace under which\n\t\t\t\tthe variables created during this call will be stored.\n\n\t\tReturns:\n\t\t\tthe output of the layer\n\t\t'
    with tf.variable_scope((scope or type(self).__name__)):
        lstm_cell_fw = rnn_cell.LayerNormIZNotRecLeakLSTMCell(num_units=self.num_units, leak_factor=self.leak_factor, layer_norm=self.layer_norm, dropout_keep_prob=self.recurrent_dropout, reuse=tf.get_variable_scope().reuse)
        lstm_cell_bw = rnn_cell.LayerNormIZNotRecLeakLSTMCell(self.num_units, leak_factor=self.leak_factor, layer_norm=self.layer_norm, dropout_keep_prob=self.recurrent_dropout, reuse=tf.get_variable_scope().reuse)
        (outputs_tupple, _) = bidirectional_dynamic_rnn(lstm_cell_fw, lstm_cell_bw, inputs, dtype=tf.float32, sequence_length=sequence_length)
        outputs = tf.concat(outputs_tupple, 2)
        return outputs

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = tf.concat

idx = 64:------------------- similar code ------------------ index = 70, score = 5.0 
if (__name__ == '__main__'):
    batch_size = 8
    nclass = 10
    y_true = tf.random.uniform((batch_size,), 0, nclass, dtype=tf.int32)
    y_pred = tf.random.uniform((batch_size, nclass), (- 1), 1, dtype=tf.float32)
    batch_idxs = tf.expand_dims(tf.range(0, batch_size, dtype=tf.int32), 1)
    idxs = tf.concat([batch_idxs, tf.cast(tf.expand_dims(y_true, (- 1)), tf.int32)], 1)
    mask = tf.logical_not(tf.scatter_nd(idxs, tf.ones(tf.shape(idxs)[0], tf.bool), tf.shape(y_pred)))
    sp = tf.expand_dims(tf.gather_nd(y_pred, idxs), 1)
    sn = tf.reshape(tf.boolean_mask(y_pred, mask), (batch_size, (- 1)))
    circleloss = CircleLoss()
    sparsecircleloss = SparseCircleLoss(batch_size=batch_size)
    paircircleloss = PairCircleLoss()
    print('circle loss:\n', circleloss.call(tf.one_hot(y_true, nclass, dtype=tf.float32), y_pred).numpy())
    print('sparse circle loss:\n', sparsecircleloss.call(tf.expand_dims(y_true, (- 1)), y_pred).numpy().ravel())
    print('pair circle loss:\n', paircircleloss.call(sp, sn).numpy().ravel())

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
     ...  = tf.concat

idx = 65:------------------- similar code ------------------ index = 71, score = 5.0 
def tf_batch_gather(params, indices, axis, name=None):
    '\n    Extension of the batch_gather function in tensorflow\n    (see https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/ops/array_ops.py\n    or https://www.tensorflow.org/api_docs/python/tf/batch_gather)\n    Gather slices from `params` according to `indices` with leading batch dims.\n    This operation assumes that the leading dimensions of `indices` are dense,\n    and the gathers on the axis corresponding to the last dimension of `indices`.\n    More concretely it computes:\n    `result[i1, ..., in, j1, ..., jm, k1, ...., kl] = params[i1, ..., in, indices[i1, ..., in, j1, ..., jm], k1, ..., kl]`\n    Therefore `params` should be a Tensor of shape [A1, ..., AN, C0, B1, ..., BM],\n    `indices` should be a Tensor of shape [A1, ..., AN, C1, ..., CK] and `result` will be\n    a Tensor of size `[A1, ..., AN, C1, ..., CK, B1, ..., BM]`.\n    In the case in which indices is a 1D tensor, this operation is equivalent to\n    `tf.gather`.\n    See also `tf.gather` and `tf.gather_nd`.\n    Args:\n      params: A `Tensor`. The tensor from which to gather values.\n      indices: A `Tensor`. Must be one of the following types: int32, int64. Index\n          tensor. Must be in range `[0, params.shape[axis]`, where `axis` is the\n          last dimension of `indices` itself.\n      axis: A `Tensor`. Must be one of the following types: int32, int64. The axis\n            in `params` to gather `indices` from.\n      name: A name for the operation (optional).\n    Returns:\n      A Tensor. Has the same type as `params`.\n    Raises:\n      ValueError: if `indices` has an unknown shape.\n    '
    with ops.name_scope(name):
        indices = ops.convert_to_tensor(indices, name='indices')
        params = ops.convert_to_tensor(params, name='params')
        indices_shape = tf.shape(indices)
        params_shape = tf.shape(params)
        ndims = indices.shape.ndims
        if (ndims is None):
            raise ValueError('batch_gather does not allow indices with unknown shape.')
        batch_indices = indices
        indices_dtype = indices.dtype.base_dtype
        accum_dim_value = tf.ones((), dtype=indices_dtype)
        casted_params_shape = gen_math_ops.cast(params_shape, indices_dtype)
        for dim in range(axis, 0, (- 1)):
            dim_value = casted_params_shape[(dim - 1)]
            accum_dim_value *= casted_params_shape[dim]
            start = tf.zeros((), dtype=indices_dtype)
            step = tf.ones((), dtype=indices_dtype)
            dim_indices = gen_math_ops._range(start, dim_value, step)
            dim_indices *= accum_dim_value
            dim_shape = tf.stack(((([1] * (dim - 1)) + [dim_value]) + ([1] * (ndims - dim))), axis=0)
            batch_indices += tf.reshape(dim_indices, dim_shape)
        flat_inner_shape_indices = gen_math_ops.prod(indices_shape[:(axis + 1)], [0], False)
        flat_indices = tf.reshape(batch_indices, tf.concat([[flat_inner_shape_indices], indices_shape[(axis + 1):]], axis=0))
        outer_shape = params_shape[(axis + 1):]
        flat_inner_shape_params = gen_math_ops.prod(params_shape[:(axis + 1)], [0], False)
        flat_params = tf.reshape(params, tf.concat([[flat_inner_shape_params], outer_shape], axis=0))
        flat_result = tf.gather(flat_params, flat_indices)
        result = tf.reshape(flat_result, tf.concat([indices_shape, outer_shape], axis=0))
        final_shape = indices.get_shape()[:axis].merge_with(params.get_shape()[:axis])
        final_shape = final_shape.concatenate(indices.get_shape()[axis:])
        final_shape = final_shape.concatenate(params.get_shape()[(axis + 1):])
        result.set_shape(final_shape)
        return result

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  =  ... . ... ( ... , tf.concat)

idx = 66:------------------- similar code ------------------ index = 72, score = 5.0 
def _multi_perspective_match(mp_dim, reps_rt, att_lt, with_cosine=True, with_mp_cosine=True):
    "\n    The core function of zhiguowang's implementation.\n\n    reference:\n    https://github.com/zhiguowang/BiMPM/blob/master/src/match_utils.py#L207-L223\n    :param mp_dim: about 20\n    :param reps_rt: [batch, len_rt, dim]\n    :param att_lt: [batch, len_rt, dim]\n    :param with_cosine: True\n    :param with_mp_cosine: True\n    :return: [batch, len, 1 + mp_dim]\n    "
    shape_rt = tf.shape(reps_rt)
    batch_size = shape_rt[0]
    len_lt = shape_rt[1]
    match_dim = 0
    match_result_list = []
    if with_cosine:
        cosine_tensor = _cosine_distance(reps_rt, att_lt, False)
        cosine_tensor = tf.reshape(cosine_tensor, [batch_size, len_lt, 1])
        match_result_list.append(cosine_tensor)
        match_dim += 1
    if with_mp_cosine:
        mp_cosine_layer = MpCosineLayer(mp_dim)
        mp_cosine_tensor = mp_cosine_layer([reps_rt, att_lt])
        mp_cosine_tensor = tf.reshape(mp_cosine_tensor, [batch_size, len_lt, mp_dim])
        match_result_list.append(mp_cosine_tensor)
        match_dim += mp_cosine_layer.mp_dim
    match_result = tf.concat(match_result_list, 2)
    return (match_result, match_dim)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 67:------------------- similar code ------------------ index = 60, score = 5.0 
def __call__(self, inputs, state, scope=None):
    'call wrapped cell with constant scope'
    (_, new_state) = self._cell(inputs, state, scope)
    output = tf.concat(nest.flatten(new_state), axis=1)
    return (output, new_state)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 68:------------------- similar code ------------------ index = 73, score = 5.0 
def _encoder(cell_fw, cell_bw, inputs, sequence_length, dtype=None, scope=None):
    with tf.variable_scope((scope or 'encoder'), values=[inputs, sequence_length]):
        inputs_fw = inputs
        inputs_bw = tf.reverse_sequence(inputs, sequence_length, batch_axis=0, seq_axis=1)
        with tf.variable_scope('forward'):
            (output_fw, state_fw) = _gru_encoder(cell_fw, inputs_fw, sequence_length, None, dtype=dtype)
        with tf.variable_scope('backward'):
            (output_bw, state_bw) = _gru_encoder(cell_bw, inputs_bw, sequence_length, None, dtype=dtype)
            output_bw = tf.reverse_sequence(output_bw, sequence_length, batch_axis=0, seq_axis=1)
        results = {'annotation': tf.concat([output_fw, output_bw], axis=2), 'outputs': {'forward': output_fw, 'backward': output_bw}, 'final_states': {'forward': state_fw, 'backward': state_bw}}
        return results

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = { ... : tf.concat,  ... :,  ... :}

idx = 69:------------------- similar code ------------------ index = 74, score = 5.0 
def bending_energy(dvf, voxel_size=None):
    '\n    Bending Energy in TensorFlow:\n    :param dvf: with shape of (batch_size, dim0, dim1, dim2, 3)\n    :param voxel_size: physical voxel spacing in mm\n    :return: 3D bending energy\n    '
    if (voxel_size is None):
        voxel_size = [1, 1, 1]
    (indices_x, indices_y, indices_z) = tf.meshgrid(tf.range(0, dvf.get_shape()[1]), tf.range(0, dvf.get_shape()[2]), tf.range(0, dvf.get_shape()[3]), indexing='ij')
    dvf_tensor = tf.concat(([(tf.expand_dims(indices_x, (- 1)) * voxel_size[0]), (tf.expand_dims(indices_y, (- 1)) * voxel_size[1]), tf.expand_dims(indices_z, (- 1))] * voxel_size[2]), axis=(- 1))
    dvf_tensor = tf.expand_dims(dvf_tensor, axis=0)
    dvf_tensor = tf.tile(dvf_tensor, [tf.shape(dvf)[0], 1, 1, 1, 1])
    dvf_tensor = tf.to_float(dvf_tensor)
    dvf_tensor = tf.add(dvf_tensor, dvf)
    dvf_grad_dim0 = (diff(dvf_tensor, axis=1) / voxel_size[0])
    dvf_grad_dim1 = (diff(dvf_tensor, axis=2) / voxel_size[1])
    dvf_grad_dim2 = (diff(dvf_tensor, axis=3) / voxel_size[2])
    dvf_grad_dim0_dim0 = (diff(dvf_grad_dim0, axis=1) / voxel_size[0])
    dvf_grad_dim0_dim1 = (diff(dvf_grad_dim0, axis=2) / voxel_size[1])
    dvf_grad_dim0_dim2 = (diff(dvf_grad_dim0, axis=3) / voxel_size[2])
    dvf_grad_dim1_dim1 = (diff(dvf_grad_dim1, axis=2) / voxel_size[1])
    dvf_grad_dim1_dim2 = (diff(dvf_grad_dim1, axis=3) / voxel_size[2])
    dvf_grad_dim2_dim2 = (diff(dvf_grad_dim2, axis=3) / voxel_size[2])
    dvf_grad_dim0_dim0 = tf.pad(dvf_grad_dim0_dim0, ([0, 0], [0, 2], [0, 0], [0, 0], [0, 0]))
    dvf_grad_dim0_dim1 = tf.pad(dvf_grad_dim0_dim1, ([0, 0], [0, 1], [0, 1], [0, 0], [0, 0]))
    dvf_grad_dim0_dim2 = tf.pad(dvf_grad_dim0_dim2, ([0, 0], [0, 1], [0, 0], [0, 1], [0, 0]))
    dvf_grad_dim1_dim1 = tf.pad(dvf_grad_dim1_dim1, ([0, 0], [0, 0], [0, 2], [0, 0], [0, 0]))
    dvf_grad_dim1_dim2 = tf.pad(dvf_grad_dim1_dim2, ([0, 0], [0, 0], [0, 1], [0, 1], [0, 0]))
    dvf_grad_dim2_dim2 = tf.pad(dvf_grad_dim2_dim2, ([0, 0], [0, 0], [0, 0], [0, 2], [0, 0]))
    smoothness = tf.reduce_mean((((((tf.square(dvf_grad_dim0_dim0) + (2 * tf.square(dvf_grad_dim0_dim1))) + (2 * tf.square(dvf_grad_dim0_dim2))) + tf.square(dvf_grad_dim1_dim1)) + (2 * tf.square(dvf_grad_dim1_dim2))) + tf.square(dvf_grad_dim2_dim2)))
    return smoothness

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 70:------------------- similar code ------------------ index = 75, score = 5.0 
def oldcall(self, targets, logits, seq_length):
    multi_targets = targets['multi_targets']
    nr_act_spk = multi_targets.get_shape()[(- 1)]
    logits = logits['act_logit']
    logits = tf.squeeze(logits, axis=(- 1))
    nr_spk = logits.get_shape()[1]
    batch_size = logits.get_shape()[0]
    if (self.lossconf['activation'] == 'sigmoid'):
        logits = tf.sigmoid(logits)
    else:
        raise BaseException('Other activations not yet implemented')
    if (self.lossconf['av_time'] == 'True'):
        logits = tf.reduce_mean(logits, 2)
    targets = tf.concat([tf.ones([batch_size, nr_act_spk]), tf.zeros([batch_size, (nr_spk - nr_act_spk)])], (- 1))
    loss = tf.reduce_sum(tf.square((logits - targets)))
    norm = tf.to_float((batch_size * nr_spk))
    return (loss, norm)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 71:------------------- similar code ------------------ index = 76, score = 5.0 
def build_encoder(self):
    with tf.name_scope('encode'):
        for layer in range(self.num_layers):
            with tf.variable_scope('encoder_{}'.format((layer + 1))):
                cell_fw = tf.contrib.rnn.LayerNormBasicLSTMCell(self.lstm_hidden_units)
                cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob=self.keep_prob)
                cell_bw = tf.contrib.rnn.LayerNormBasicLSTMCell(self.lstm_hidden_units)
                cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob=self.keep_prob)
                (self.enc_output, self.enc_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, self.enc_embed_input, self.source_sentence_length, dtype=tf.float32)
        self.h_N = tf.concat([self.enc_state[0][1], self.enc_state[1][1]], axis=(- 1), name='h_N')
        self.enc_outputs = tf.concat([self.enc_output[0], self.enc_output[1]], axis=(- 1), name='encoder_outputs')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    with:
 = tf.concat

idx = 72:------------------- similar code ------------------ index = 77, score = 5.0 
def call(self, inputs):
    nf = inputs['node_features']
    feature_reps = []
    for (name, layer) in self.nf_w.items():
        nf_rep = layer(nf[name])
        nf_rep = tf.nn.swish(nf_rep)
        feature_reps.append(nf_rep)
    feature_reps = tf.concat(feature_reps, axis=(- 1))
    x = self.w(feature_reps)
    x = self.layer_norm_1(x)
    pre_linear_x = x
    x = tf.nn.swish(x)
    x = self.w_out(x)
    x = self.layer_norm_2(x)
    x = (pre_linear_x + x)
    x = tf.nn.swish(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 73:------------------- similar code ------------------ index = 78, score = 5.0 
def pointnet_sa_module(xyz, points, npoint, radius, nsample, mlp, mlp2, group_all, is_training, bn_decay, scope, bn=True, pooling='max', knn=False, use_xyz=True, use_nchw=False):
    ' PointNet Set Abstraction (SA) Module\n        Input:\n            xyz: (batch_size, ndataset, 3) TF tensor\n            points: (batch_size, ndataset, channel) TF tensor\n            npoint: int32 -- #points sampled in farthest point sampling\n            radius: float32 -- search radius in local region\n            nsample: int32 -- how many points in each local region\n            mlp: list of int32 -- output size for MLP on each point\n            mlp2: list of int32 -- output size for MLP on each region\n            group_all: bool -- group all points into one PC if set true, OVERRIDE\n                npoint, radius and nsample settings\n            use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n            use_nchw: bool, if True, use NCHW data format for conv2d, which is usually faster than NHWC format\n        Return:\n            new_xyz: (batch_size, npoint, 3) TF tensor\n            new_points: (batch_size, npoint, mlp[-1] or mlp2[-1]) TF tensor\n            idx: (batch_size, npoint, nsample) int32 -- indices for local regions\n    '
    data_format = ('NCHW' if use_nchw else 'NHWC')
    with tf.variable_scope(scope) as sc:
        if group_all:
            nsample = xyz.get_shape()[1].value
            (new_xyz, new_points, idx, grouped_xyz) = sample_and_group_all(xyz, points, use_xyz)
        else:
            (new_xyz, new_points, idx, grouped_xyz) = sample_and_group(npoint, radius, nsample, xyz, points, knn, use_xyz)
        if use_nchw:
            new_points = tf.transpose(new_points, [0, 3, 1, 2])
        for (i, num_out_channel) in enumerate(mlp):
            new_points = tf_util.conv2d(new_points, num_out_channel, [1, 1], padding='VALID', stride=[1, 1], bn=bn, is_training=is_training, scope=('conv%d' % i), bn_decay=bn_decay, data_format=data_format)
        if use_nchw:
            new_points = tf.transpose(new_points, [0, 2, 3, 1])
        if (pooling == 'max'):
            new_points = tf.reduce_max(new_points, axis=[2], keep_dims=True, name='maxpool')
        elif (pooling == 'avg'):
            new_points = tf.reduce_mean(new_points, axis=[2], keep_dims=True, name='avgpool')
        elif (pooling == 'weighted_avg'):
            with tf.variable_scope('weighted_avg'):
                dists = tf.norm(grouped_xyz, axis=(- 1), ord=2, keep_dims=True)
                exp_dists = tf.exp(((- dists) * 5))
                weights = (exp_dists / tf.reduce_sum(exp_dists, axis=2, keep_dims=True))
                new_points *= weights
                new_points = tf.reduce_sum(new_points, axis=2, keep_dims=True)
        elif (pooling == 'max_and_avg'):
            max_points = tf.reduce_max(new_points, axis=[2], keep_dims=True, name='maxpool')
            avg_points = tf.reduce_mean(new_points, axis=[2], keep_dims=True, name='avgpool')
            new_points = tf.concat([avg_points, max_points], axis=(- 1))
        if (mlp2 is not None):
            if use_nchw:
                new_points = tf.transpose(new_points, [0, 3, 1, 2])
            for (i, num_out_channel) in enumerate(mlp2):
                new_points = tf_util.conv2d(new_points, num_out_channel, [1, 1], padding='VALID', stride=[1, 1], bn=bn, is_training=is_training, scope=('conv_post_%d' % i), bn_decay=bn_decay, data_format=data_format)
            if use_nchw:
                new_points = tf.transpose(new_points, [0, 2, 3, 1])
        new_points = tf.squeeze(new_points, [2])
        return (new_xyz, new_points, idx)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if:        elif:
             ...  = tf.concat

idx = 74:------------------- similar code ------------------ index = 80, score = 5.0 
def generator(self, inputs_condition):
    inputs = inputs_condition
    with tf.variable_scope('generator'):
        inputs1 = leaky_relu(conv2d('conv1', inputs, 64, 5, 2))
        inputs2 = leaky_relu(instanceNorm('in1', conv2d('conv2', inputs1, 128, 5, 2)))
        inputs3 = leaky_relu(instanceNorm('in2', conv2d('conv3', inputs2, 256, 5, 2)))
        inputs4 = leaky_relu(instanceNorm('in3', conv2d('conv4', inputs3, 512, 5, 2)))
        inputs5 = leaky_relu(instanceNorm('in4', conv2d('conv5', inputs4, 512, 5, 2)))
        inputs6 = leaky_relu(instanceNorm('in5', conv2d('conv6', inputs5, 512, 5, 2)))
        inputs7 = leaky_relu(instanceNorm('in6', conv2d('conv7', inputs6, 512, 5, 2)))
        inputs8 = leaky_relu(instanceNorm('in7', conv2d('conv8', inputs7, 512, 5, 2)))
        outputs1 = tf.nn.relu(tf.concat([tf.nn.dropout(instanceNorm('in9', deconv2d('dconv1', inputs8, 512, 5, 2)), 0.5), inputs7], axis=3))
        outputs2 = tf.nn.relu(tf.concat([tf.nn.dropout(instanceNorm('in10', deconv2d('dconv2', outputs1, 512, 5, 2)), 0.5), inputs6], axis=3))
        outputs3 = tf.nn.relu(tf.concat([tf.nn.dropout(instanceNorm('in11', deconv2d('dconv3', outputs2, 512, 5, 2)), 0.5), inputs5], axis=3))
        outputs4 = tf.nn.relu(tf.concat([instanceNorm('in12', deconv2d('dconv4', outputs3, 512, 5, 2)), inputs4], axis=3))
        outputs5 = tf.nn.relu(tf.concat([instanceNorm('in13', deconv2d('dconv5', outputs4, 256, 5, 2)), inputs3], axis=3))
        outputs6 = tf.nn.relu(tf.concat([instanceNorm('in14', deconv2d('dconv6', outputs5, 128, 5, 2)), inputs2], axis=3))
        outputs7 = tf.nn.relu(tf.concat([instanceNorm('in15', deconv2d('dconv7', outputs6, 64, 5, 2)), inputs1], axis=3))
        outputs8 = tf.nn.tanh(deconv2d('dconv8', outputs7, 3, 5, 2))
        return outputs8

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  =  ... . ... (tf.concat)

idx = 75:------------------- similar code ------------------ index = 6, score = 5.0 
def pointnet_sa_module_msg(xyz, points, npoint, radius_list, nsample_list, mlp_list, is_training, bn_decay, scope, bn=True, use_xyz=True, use_nchw=False):
    ' PointNet Set Abstraction (SA) module with Multi-Scale Grouping (MSG)\n        Input:\n            xyz: (batch_size, ndataset, 3) TF tensor\n            points: (batch_size, ndataset, channel) TF tensor\n            npoint: int32 -- #points sampled in farthest point sampling\n            radius: list of float32 -- search radius in local region\n            nsample: list of int32 -- how many points in each local region\n            mlp: list of list of int32 -- output size for MLP on each point\n            use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n            use_nchw: bool, if True, use NCHW data format for conv2d, which is usually faster than NHWC format\n        Return:\n            new_xyz: (batch_size, npoint, 3) TF tensor\n            new_points: (batch_size, npoint, \\sum_k{mlp[k][-1]}) TF tensor\n    '
    data_format = ('NCHW' if use_nchw else 'NHWC')
    with tf.variable_scope(scope) as sc:
        new_xyz = gather_point(xyz, farthest_point_sample(npoint, xyz))
        new_points_list = []
        for i in range(len(radius_list)):
            radius = radius_list[i]
            nsample = nsample_list[i]
            (idx, pts_cnt) = query_ball_point(radius, nsample, xyz, new_xyz)
            grouped_xyz = group_point(xyz, idx)
            grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1, 1, nsample, 1])
            if (points is not None):
                grouped_points = group_point(points, idx)
                if use_xyz:
                    grouped_points = tf.concat([grouped_points, grouped_xyz], axis=(- 1))
            else:
                grouped_points = grouped_xyz
            if use_nchw:
                grouped_points = tf.transpose(grouped_points, [0, 3, 1, 2])
            for (j, num_out_channel) in enumerate(mlp_list[i]):
                grouped_points = tf_util.conv2d(grouped_points, num_out_channel, [1, 1], padding='VALID', stride=[1, 1], bn=bn, is_training=is_training, scope=('conv%d_%d' % (i, j)), bn_decay=bn_decay)
            if use_nchw:
                grouped_points = tf.transpose(grouped_points, [0, 2, 3, 1])
            new_points = tf.reduce_max(grouped_points, axis=[2])
            new_points_list.append(new_points)
        new_points_concat = tf.concat(new_points_list, axis=(- 1))
        return (new_xyz, new_points_concat)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        for  ...  in:
            if:
                if  ... :
                     ...  = tf.concat

idx = 76:------------------- similar code ------------------ index = 25, score = 5.0 
def __call__(self, targets, logits, seq_length):
    multi_targets = targets['multi_targets']
    nr_act_spk = multi_targets.get_shape()[(- 1)]
    logits = logits['act_logit']
    logits = tf.squeeze(logits, axis=(- 1))
    nr_spk = logits.get_shape()[1]
    batch_size = logits.get_shape()[0]
    if (self.lossconf['activation'] == 'sigmoid'):
        logits = tf.sigmoid(logits)
    else:
        raise BaseException('Other activations not yet implemented')
    if (len(logits.get_shape()) != 3):
        raise BaseException('Hardcoded some stuff for 3 dimensions')
    second_dim = logits.get_shape()[1]
    seq_length = seq_length['features']
    max_len = tf.shape(logits)[(- 1)]
    tmp = []
    for utt_ind in range(batch_size):
        tmp.append(tf.expand_dims(tf.concat([tf.ones([second_dim, seq_length[utt_ind]]), tf.zeros([second_dim, (max_len - seq_length[utt_ind])])], (- 1)), 0))
    seq_length_mask = tf.concat(tmp, 0)
    logits = (logits * seq_length_mask)
    if (self.lossconf['av_time'] == 'True'):
        logits = tf.reduce_sum(logits, 2)
        logits = tf.divide(logits, tf.expand_dims(tf.to_float(seq_length), (- 1)))
    targets = tf.concat([tf.ones([batch_size, nr_act_spk]), tf.zeros([batch_size, (nr_spk - nr_act_spk)])], (- 1))
    loss = tf.reduce_sum(tf.square((logits - targets)))
    norm = tf.to_float((batch_size * nr_spk))
    return (loss, norm)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ... . ... ( ... . ... (tf.concat,  ... ))

idx = 77:------------------- similar code ------------------ index = 7, score = 5.0 
def conv(attr_hs, attr_as, attr_vs, dim, feature_map_size=2, kernel_size=[2, 4], activation=tf.nn.tanh, layer_num=2):
    attr_as = tf.reshape(attr_as, [(- 1), 1, dim])
    attr_vs = tf.reshape(attr_vs, [(- 1), 1, dim])
    input_avs = tf.concat([attr_as, attr_vs], 1)
    input_shape = input_avs.shape.as_list()
    input_layer = tf.reshape(input_avs, [(- 1), input_shape[1], input_shape[2], 1])
    _conv = input_layer
    _conv = tf.layers.batch_normalization(_conv, 2)
    for i in range(layer_num):
        _conv = tf.layers.conv2d(inputs=_conv, filters=feature_map_size, kernel_size=kernel_size, strides=[1, 1], padding='same', activation=activation)
    _conv = tf.nn.l2_normalize(_conv, 2)
    _shape = _conv.shape.as_list()
    _flat = tf.reshape(_conv, [(- 1), ((_shape[1] * _shape[2]) * _shape[3])])
    dense = tf.layers.dense(inputs=_flat, units=dim, activation=activation)
    dense = tf.nn.l2_normalize(dense)
    score = (- tf.reduce_sum(tf.square((attr_hs - dense)), 1))
    return score

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 78:------------------- similar code ------------------ index = 58, score = 5.0 
def parse_example_proto(example_serialized, ret_dict=False):
    feature_map = {'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''), 'image/filename': tf.FixedLenFeature([], dtype=tf.string, default_value=''), 'image/class/label': tf.FixedLenFeature([], dtype=tf.int64, default_value=(- 1)), 'image/logit': tf.VarLenFeature(dtype=tf.float32)}
    sparse_float32 = tf.VarLenFeature(dtype=tf.float32)
    feature_map.update({k: sparse_float32 for k in ['image/object/bbox/xmin', 'image/object/bbox/ymin', 'image/object/bbox/xmax', 'image/object/bbox/ymax']})
    features = tf.parse_single_example(example_serialized, feature_map)
    label = tf.cast(features['image/class/label'], dtype=tf.int32)
    xmin = tf.expand_dims(features['image/object/bbox/xmin'].values, 0)
    ymin = tf.expand_dims(features['image/object/bbox/ymin'].values, 0)
    xmax = tf.expand_dims(features['image/object/bbox/xmax'].values, 0)
    ymax = tf.expand_dims(features['image/object/bbox/ymax'].values, 0)
    bbox = tf.concat([ymin, xmin, ymax, xmax], 0)
    bbox = tf.expand_dims(bbox, 0)
    bbox = tf.transpose(bbox, [0, 2, 1])
    if ret_dict:
        return features
    else:
        return (features['image/encoded'], label, bbox, features['image/filename'], features['image/logit'].values)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 79:------------------- similar code ------------------ index = 38, score = 5.0 
def build(self, input_shape):
    '\n        initialize embedding_weights, where\n        id 0 is reserved for UNK, and its embedding fix to all zeros\n        '
    with tf.compat.v1.variable_scope(self.scope_name):
        unknown_id = tf.Variable(tf.zeros_initializer()([1, self.output_dim]), name='-'.join([self.layer_name, 'unknown']), trainable=False)
        normal_ids = tf.compat.v1.get_variable('-'.join([self.layer_name, 'normal']), [(self.input_dim - 1), self.output_dim], initializer=(tf.random_uniform_initializer(minval=(- self.initial_range), maxval=self.initial_range) if self.initial_range else None))
    self.embeddings = tf.concat([unknown_id, normal_ids], axis=0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = tf.concat

idx = 80:------------------- similar code ------------------ index = 16, score = 5.0 
def _get_outputs(self, inputs, input_seq_length=None, is_training=None):
    '\n\t\tpermutes and stacks the inputs\n\n\t\tArgs:\n\t\t\tinputs: the inputs to concatenate, this is a list of\n\t\t\t\t[batch_size x time x ...] tensors and/or [batch_size x ...] tensors\n\t\t\tinput_seq_length: None\n\t\t\tis_training: None\n\n\t\tReturns:\n\t\t\t- outputs, the reshaped input\n\t\t'
    permute_dim = int(self.conf['permute_dim'])
    stack_dim = int(self.conf['stack_dim'])
    if (len(inputs) > 1):
        raise ('The implementation of PermuteStacker expects 1 input and not %d' % len(inputs))
    else:
        input = inputs[0]
    with tf.variable_scope(self.scope):
        permute_dim_size = input.get_shape()[permute_dim]
        permutations = list(itertools.permutations(range(permute_dim_size), permute_dim_size))
        all_inp_perms = [tf.gather(input, perm, axis=permute_dim) for perm in permutations]
        output = tf.concat(all_inp_perms, axis=stack_dim)
    return output

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = tf.concat

idx = 81:------------------- similar code ------------------ index = 37, score = 5.0 
def mixup(x, y, alpha=0.2, keep_batch_size=True, y_t=None):
    dist = tf.contrib.distributions.Beta(alpha, alpha)
    (_, h, w, c) = x.get_shape().as_list()
    batch_size = tf.shape(x)[0]
    num_class = y.get_shape().as_list()[1]
    lam1 = dist.sample([(batch_size // 2)])
    if (x.dtype == tf.float16):
        lam1 = tf.cast(lam1, dtype=tf.float16)
        y = tf.cast(y, dtype=tf.float16)
        if (y_t is not None):
            y_t = tf.cast(y_t, dtype=tf.float16)
    (x1, x2) = tf.split(x, 2, axis=0)
    (y1, y2) = tf.split(y, 2, axis=0)
    lam1_x = tf.tile(tf.reshape(lam1, [(batch_size // 2), 1, 1, 1]), [1, h, w, c])
    lam1_y = tf.tile(tf.reshape(lam1, [(batch_size // 2), 1]), [1, num_class])
    mixed_sx1 = ((lam1_x * x1) + ((1.0 - lam1_x) * x2))
    mixed_sy1 = ((lam1_y * y1) + ((1.0 - lam1_y) * y2))
    mixed_sx1 = tf.stop_gradient(mixed_sx1)
    mixed_sy1 = tf.stop_gradient(mixed_sy1)
    if (y_t is not None):
        (y1_t, y2_t) = tf.split(y_t, 2, axis=0)
        mixed_sy1_t = ((lam1_y * y1_t) + ((1.0 - lam1_y) * y2_t))
        mixed_sy1_t = tf.stop_gradient(mixed_sy1_t)
    else:
        mixed_sy1_t = None
    if keep_batch_size:
        lam2 = dist.sample([(batch_size // 2)])
        if (x.dtype == tf.float16):
            lam2 = tf.cast(lam2, dtype=tf.float16)
        lam2_x = tf.tile(tf.reshape(lam2, [(batch_size // 2), 1, 1, 1]), [1, h, w, c])
        lam2_y = tf.tile(tf.reshape(lam2, [(batch_size // 2), 1]), [1, num_class])
        x3 = tf.reverse(x2, [0])
        y3 = tf.reverse(y2, [0])
        mixed_sx2 = ((lam2_x * x1) + ((1.0 - lam2_x) * x3))
        mixed_sy2 = ((lam2_y * y1) + ((1.0 - lam2_y) * y3))
        mixed_sx2 = tf.stop_gradient(mixed_sx2)
        mixed_sy2 = tf.stop_gradient(mixed_sy2)
        mixed_sx1 = tf.concat([mixed_sx1, mixed_sx2], axis=0)
        mixed_sy1 = tf.concat([mixed_sy1, mixed_sy2], axis=0)
        if (y_t is not None):
            y3_t = tf.reverse(y2_t, [0])
            mixed_sy2_t = ((lam2_y * y1) + ((1.0 - lam2_y) * y3_t))
            mixed_sy2_t = tf.stop_gradient(mixed_sy2_t)
            mixed_sy1_t = tf.concat([mixed_sy1_t, mixed_sy2_t], axis=0)
    return (mixed_sx1, mixed_sy1, mixed_sy1_t)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if  ... :
         ...  = tf.concat

idx = 82:------------------- similar code ------------------ index = 17, score = 5.0 
def add_embedding(self):
    with tf.device('/cpu:0'), tf.name_scope('word_embedding'):
        W = tf.Variable(self.pretrained_embedding, trainable=False, dtype=tf.float32, name='W')
        self.embedded_words = tf.nn.embedding_lookup(W, self.input_words_flatten)
    with tf.device('/cpu:0'), tf.name_scope('position_embedding'):
        W = tf.Variable(self.wpe, trainable=False, dtype=tf.float32, name='W')
        self.wpe_chars = tf.nn.embedding_lookup(W, self.input_positions_flatten)
    self.input_sentences = tf.concat(([self.embedded_words] + tf.unstack(self.wpe_chars, axis=1)), 2)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = tf.concat

idx = 83:------------------- similar code ------------------ index = 36, score = 5.0 
def transformAttention(X, STE_P, STE_Q, K, d, bn, bn_decay, is_training):
    '\n    transform attention mechanism\n    X:      [batch_size, P, N, D]\n    STE_P:  [batch_size, P, N, D]\n    STE_Q:  [batch_size, Q, N, D]\n    K:      number of attention heads\n    d:      dimension of each attention outputs\n    return: [batch_size, Q, N, D]\n    '
    D = (K * d)
    query = FC(STE_Q, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    key = FC(STE_P, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    value = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    query = tf.concat(tf.split(query, K, axis=(- 1)), axis=0)
    key = tf.concat(tf.split(key, K, axis=(- 1)), axis=0)
    value = tf.concat(tf.split(value, K, axis=(- 1)), axis=0)
    query = tf.transpose(query, perm=(0, 2, 1, 3))
    key = tf.transpose(key, perm=(0, 2, 3, 1))
    value = tf.transpose(value, perm=(0, 2, 1, 3))
    attention = tf.matmul(query, key)
    attention /= (d ** 0.5)
    attention = tf.nn.softmax(attention, axis=(- 1))
    X = tf.matmul(attention, value)
    X = tf.transpose(X, perm=(0, 2, 1, 3))
    X = tf.concat(tf.split(X, K, axis=0), axis=(- 1))
    X = FC(X, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    return X

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 84:------------------- similar code ------------------ index = 35, score = 5.0 
def tf_generate_normalized_random_direction(batchSize, lowEps=0.001, highEps=0.05):
    r1 = tf.random_uniform([batchSize, 1], (0.0 + lowEps), (1.0 - highEps), dtype=tf.float32)
    r2 = tf.random_uniform([batchSize, 1], 0.0, 1.0, dtype=tf.float32)
    r = tf.sqrt(r1)
    phi = ((2 * math.pi) * r2)
    x = (r * tf.cos(phi))
    y = (r * tf.sin(phi))
    z = tf.sqrt((1.0 - tf.square(r)))
    finalVec = tf.concat([x, y, z], axis=(- 1))
    return finalVec

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 85:------------------- similar code ------------------ index = 18, score = 5.0 
def build_network(self, features, is_training=None):
    '\n\n        :param features:\n        :param is_training:\n        :return:\n        '
    (ev_list, fv_list) = features
    deep_out = self.mlp_block(self.concat((ev_list + fv_list)))
    fwbi = self.fwbi(ev_list)
    fwbi_fc_32 = self.fwbi_fc_32(fwbi)
    fwbi_bn = self.fwbi_bn(fwbi_fc_32)
    fwbi_out = self.fwbi_drop(fwbi_bn)
    logit = tf.concat(values=[deep_out, fwbi_out], axis=1)
    return logit

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 86:------------------- similar code ------------------ index = 33, score = 5.0 
def pointnet_fp_module(xyz1, xyz2, points1, points2, mlp, is_training, bn_decay, scope, bn=True):
    ' PointNet Feature Propogation (FP) Module\n        Input:                                                                                                      \n            xyz1: (batch_size, ndataset1, 3) TF tensor                                                              \n            xyz2: (batch_size, ndataset2, 3) TF tensor, sparser than xyz1                                           \n            points1: (batch_size, ndataset1, nchannel1) TF tensor                                                   \n            points2: (batch_size, ndataset2, nchannel2) TF tensor\n            mlp: list of int32 -- output size for MLP on each point                                                 \n        Return:\n            new_points: (batch_size, ndataset1, mlp[-1]) TF tensor\n    '
    with tf.variable_scope(scope) as sc:
        (dist, idx) = three_nn(xyz1, xyz2)
        dist = tf.maximum(dist, 1e-10)
        norm = tf.reduce_sum((1.0 / dist), axis=2, keep_dims=True)
        norm = tf.tile(norm, [1, 1, 3])
        weight = ((1.0 / dist) / norm)
        interpolated_points = three_interpolate(points2, idx, weight)
        if (points1 is not None):
            new_points1 = tf.concat(axis=2, values=[interpolated_points, points1])
        else:
            new_points1 = interpolated_points
        new_points1 = tf.expand_dims(new_points1, 2)
        for (i, num_out_channel) in enumerate(mlp):
            new_points1 = tf_util.conv2d(new_points1, num_out_channel, [1, 1], padding='VALID', stride=[1, 1], bn=bn, is_training=is_training, scope=('conv_%d' % i), bn_decay=bn_decay)
        new_points1 = tf.squeeze(new_points1, [2])
        return new_points1

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if:
             ...  = tf.concat

idx = 87:------------------- similar code ------------------ index = 31, score = 5.0 
def __call__(self, inputs, sequence_length, scope=None):
    '\n\t\tCreate the variables and do the forward computation\n\n\t\tArgs:\n\t\t\tinputs: the input to the layer as a\n\t\t\t\t[batch_size, max_length, dim] tensor\n\t\t\tsequence_length: the length of the input sequences as a\n\t\t\t\t[batch_size] tensor\n\t\t\tscope: The variable scope sets the namespace under which\n\t\t\t\tthe variables created during this call will be stored.\n\n\t\tReturns:\n\t\t\tthe output of the layer\n\t\t'
    with tf.variable_scope((scope or type(self).__name__)):
        if self.linear_out_flag:
            lstm_cell_type = rnn_cell.LayerNormBasicLSTMCellLineairOut
        else:
            lstm_cell_type = tf.contrib.rnn.LayerNormBasicLSTMCell
        lstm_cell_fw = lstm_cell_type(num_units=self.num_units, activation=self.activation_fn, layer_norm=self.layer_norm, dropout_keep_prob=self.recurrent_dropout, reuse=tf.get_variable_scope().reuse)
        lstm_cell_bw = lstm_cell_type(num_units=self.num_units, activation=self.activation_fn, layer_norm=self.layer_norm, dropout_keep_prob=self.recurrent_dropout, reuse=tf.get_variable_scope().reuse)
        if (not self.separate_directions):
            (outputs_tupple, _) = bidirectional_dynamic_rnn(lstm_cell_fw, lstm_cell_bw, inputs, dtype=tf.float32, sequence_length=sequence_length)
            outputs = tf.concat(outputs_tupple, 2)
        else:
            (outputs, _) = rnn.bidirectional_dynamic_rnn_2inputs(lstm_cell_fw, lstm_cell_bw, inputs[0], inputs[1], dtype=tf.float32, sequence_length=sequence_length)
        return outputs

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if:
             ...  = tf.concat

idx = 88:------------------- similar code ------------------ index = 21, score = 5.0 
def __call__(self, inputs, sequence_length, scope=None):
    '\n\t\tCreate the variables and do the forward computation\n\n\t\tArgs:\n\t\t\tinputs: the input to the layer as a\n\t\t\t\t[batch_size, max_length, dim] tensor\n\t\t\tsequence_length: the length of the input sequences as a\n\t\t\t\t[batch_size] tensor\n\t\t\tscope: The variable scope sets the namespace under which\n\t\t\t\tthe variables created during this call will be stored.\n\n\t\tReturns:\n\t\t\tthe output of the layer\n\t\t'
    with tf.variable_scope((scope or type(self).__name__)):
        gru_cell_fw = tf.contrib.rnn.GRUCell(num_units=self.num_units, activation=self.activation_fn, reuse=tf.get_variable_scope().reuse)
        gru_cell_bw = tf.contrib.rnn.GRUCell(num_units=self.num_units, activation=self.activation_fn, reuse=tf.get_variable_scope().reuse)
        (outputs_tupple, _) = bidirectional_dynamic_rnn(gru_cell_fw, gru_cell_bw, inputs, dtype=tf.float32, sequence_length=sequence_length)
        outputs = tf.concat(outputs_tupple, 2)
        return outputs

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = tf.concat

idx = 89:------------------- similar code ------------------ index = 29, score = 5.0 
def _call(self, objects, background, is_training, appearance_only=False, mask_only=False):
    ' If mask_only==True, then we ignore the provided background, using a black blackground instead,\n            and also ignore the computed appearance, using all-white appearances instead.\n\n        '
    if (not self.initialized):
        self.image_depth = tf_shape(background)[(- 1)]
    single = False
    if isinstance(objects, dict):
        single = True
        objects = [objects]
    _object_maps = []
    _scales = []
    _offsets = []
    _appearance = []
    for (i, obj) in enumerate(objects):
        anchor_box = self.anchor_boxes[i]
        object_shape = self.object_shapes[i]
        object_decoder = self.maybe_build_subnet('object_decoder_for_flight_{}'.format(i), builder_name='build_object_decoder')
        appearance_logit = apply_object_wise(object_decoder, obj.attr, output_size=(object_shape + ((self.image_depth + 1),)), is_training=is_training)
        appearance_logit = (appearance_logit * (([self.color_logit_scale] * self.image_depth) + [self.alpha_logit_scale]))
        appearance_logit = (appearance_logit + (([0.0] * self.image_depth) + [self.alpha_logit_bias]))
        appearance = tf.nn.sigmoid(tf.clip_by_value(appearance_logit, (- 10.0), 10.0))
        _appearance.append(appearance)
        if appearance_only:
            continue
        (batch_size, *obj_leading_shape, _, _, _) = tf_shape(appearance)
        n_objects = np.prod(obj_leading_shape)
        appearance = tf.reshape(appearance, (batch_size, n_objects, *object_shape, (self.image_depth + 1)))
        (obj_colors, obj_alpha) = tf.split(appearance, [self.image_depth, 1], axis=(- 1))
        if mask_only:
            obj_colors = tf.ones_like(obj_colors)
        obj_alpha *= tf.reshape(obj.obj, (batch_size, n_objects, 1, 1, 1))
        z = tf.reshape(obj.z, (batch_size, n_objects, 1, 1, 1))
        obj_importance = tf.maximum(((obj_alpha * z) / self.importance_temp), 0.01)
        object_maps = tf.concat([obj_colors, obj_alpha, obj_importance], axis=(- 1))
        (*_, image_height, image_width, _) = tf_shape(background)
        (yt, xt, ys, xs) = coords_to_image_space(obj.yt, obj.xt, obj.ys, obj.xs, (image_height, image_width), anchor_box, top_left=True)
        scales = tf.concat([ys, xs], axis=(- 1))
        scales = tf.reshape(scales, (batch_size, n_objects, 2))
        offsets = tf.concat([yt, xt], axis=(- 1))
        offsets = tf.reshape(offsets, (batch_size, n_objects, 2))
        _object_maps.append(object_maps)
        _scales.append(scales)
        _offsets.append(offsets)
    if single:
        _appearance = _appearance[0]
    if appearance_only:
        return dict(appearance=_appearance)
    if mask_only:
        background = tf.zeros_like(background)
    output = render_sprites.render_sprites(_object_maps, _scales, _offsets, background)
    return dict(appearance=_appearance, output=output)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
         ...  = tf.concat

idx = 90:------------------- similar code ------------------ index = 28, score = 5.0 
def lambda_return(reward, value, bootstrap, pcont, lambda_, axis, stop_gradient=True):
    assert (reward.shape.ndims == value.shape.ndims), (reward.shape, value.shape)
    dims = list(range(reward.shape.ndims))
    dims = ((([axis] + dims[1:axis]) + [0]) + dims[(axis + 1):])
    if isinstance(pcont, (int, float)):
        pcont = (pcont * tf.ones_like(reward))
    reward = tf.transpose(reward, dims)
    value = tf.transpose(value, dims)
    pcont = tf.transpose(pcont, dims)
    if (bootstrap is None):
        bootstrap = tf.zeros_like(value[(- 1)])
    next_values = tf.concat([value[1:], bootstrap[None]], 0)
    inputs = (reward + ((pcont * next_values) * (1 - lambda_)))
    return_ = tf.scan(fn=(lambda agg, cur: (cur[0] + ((cur[1] * lambda_) * agg))), elems=(inputs, pcont), initializer=bootstrap, back_prop=(not stop_gradient), reverse=True)
    return_ = tf.transpose(return_, dims)
    if stop_gradient:
        return_ = tf.stop_gradient(return_)
    return return_

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 91:------------------- similar code ------------------ index = 22, score = 5.0 
def initialize(self, name=None):
    'Initialize the decoder.\n        Args:\n          name: Name scope for any created operations.\n        Returns:\n          `(finished, first_inputs, initial_state)`.\n        '
    return ((self._helper.initialize()[0], tf.concat([self._helper.initialize()[1], self._latent_vector], axis=(- 1))) + (self._initial_state,))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    return ((, tf.concat) +)

idx = 92:------------------- similar code ------------------ index = 23, score = 5.0 
def get_inference_input_ctx(inputs, ctxs, params):
    with tf.device('/cpu:0'):
        dataset = tf.data.Dataset.from_tensor_slices(tf.constant(inputs))
        dataset = dataset.map((lambda x: tf.string_split([x]).values), num_parallel_calls=params.num_threads)
        dataset = dataset.map((lambda x: tf.concat([x, [tf.constant(params.eos)]], axis=0)), num_parallel_calls=params.num_threads)
        dataset = dataset.map((lambda x: {'source': x, 'source_length': tf.shape(x)[0]}), num_parallel_calls=params.num_threads)
        dataset = dataset.padded_batch((params.decode_batch_size * len(params.device_list)), {'source': [tf.Dimension(None)], 'source_length': []}, {'source': params.pad, 'source_length': 0})
        iterator = dataset.make_one_shot_iterator()
        features = iterator.get_next()
        src_table = tf.contrib.lookup.index_table_from_tensor(tf.constant(params.vocabulary['source']), default_value=params.mapping['source'][params.unk])
        features['source'] = src_table.lookup(features['source'])
        return features

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  =  ... . ... ((lambda  ... : tf.concat),)

idx = 93:------------------- similar code ------------------ index = 24, score = 5.0 
def build_encoder(self):
    with tf.name_scope('encode'):
        for layer in range(self.num_layers):
            with tf.variable_scope('encoder_{}'.format((layer + 1))):
                cell_fw = tf.contrib.rnn.LayerNormBasicLSTMCell(self.lstm_hidden_units)
                cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob=self.keep_prob)
                cell_bw = tf.contrib.rnn.LayerNormBasicLSTMCell(self.lstm_hidden_units)
                cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob=self.keep_prob)
                (self.enc_output, self.enc_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, self.enc_embed_input, self.source_sentence_length, dtype=tf.float32)
        self.h_N = tf.concat([self.enc_state[0][1], self.enc_state[1][1]], axis=(- 1), name='h_N')
        self.enc_outputs = tf.concat([self.enc_output[0], self.enc_output[1]], axis=(- 1), name='encoder_outputs')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    with:
 = tf.concat

idx = 94:------------------- similar code ------------------ index = 15, score = 5.0 
def build_math(self):
    if (self.math_input_network is None):
        self.math_input_network = cfg.build_math_input(scope='math_input_network')
        if ('math' in self.fixed_weights):
            self.math_input_network.fix_variables()
    if (self.math_network is None):
        self.math_network = cfg.build_math_network(scope='math_network')
        if ('math' in self.fixed_weights):
            self.math_network.fix_variables()
    (math_rep, mask) = self.build_math_representation()
    if (self.max_possible_objects is not None):
        (math_rep, _, mask) = apply_mask_and_group_at_front(math_rep, mask)
        n_pad = (self.max_possible_objects - tf.shape(math_rep)[1])
        mask = tf.cast(mask, tf.float32)
        batch_size = tf.shape(math_rep)[0]
        A = math_rep.shape[2]
        math_rep = tf.pad(math_rep, [(0, 0), (0, n_pad), (0, 0)])
        math_rep = tf.reshape(math_rep, (batch_size, self.max_possible_objects, A))
        mask = tf.pad(mask, [(0, 0), (0, n_pad)])
        mask = tf.reshape(mask, (batch_size, self.max_possible_objects, 1))
    mask_shape = tf.concat([tf.shape(math_rep)[:(- 1)], [1]], axis=0)
    mask = tf.reshape(mask, mask_shape)
    math_rep = tf.concat([mask, math_rep], axis=(- 1))
    logits = self.math_network(math_rep, cfg.n_classes, self.is_training)
    self._tensors['prediction'] = tf.nn.softmax(logits)
    recorded_tensors = self.recorded_tensors
    if (self.math_weight is not None):
        self.record_tensors(raw_loss_math=tf.nn.softmax_cross_entropy_with_logits_v2(labels=self._tensors['targets'], logits=logits))
        self.losses['math'] = (self.math_weight * recorded_tensors['raw_loss_math'])
    self.record_tensors(math_accuracy=tf.equal(tf.argmax(logits, axis=1), tf.argmax(self._tensors['targets'], axis=1)), math_1norm=tf.abs((tf.argmax(logits, axis=1) - tf.argmax(self._tensors['targets'], axis=1))), math_correct_prob=tf.reduce_sum((tf.nn.softmax(logits) * self._tensors['targets']), axis=1))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = tf.concat

idx = 95:------------------- similar code ------------------ index = 40, score = 5.0 
def call(self, inputs, state):
    (output, res_state) = self._cell(inputs, state)
    if (self._embed_to_concat is not None):
        tensors = [output, res_state.attention, self._embed_to_concat]
        return (tf.concat(tensors, axis=(- 1)), res_state)
    else:
        return (tf.concat([output, res_state.attention], axis=(- 1)), res_state)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
        return (tf.concat,  ... )

idx = 96:------------------- similar code ------------------ index = 145, score = 6.0 
def block_inception_c(inputs, scope=None, reuse=None):
    'Builds Inception-C block for Inception v4 network.'
    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding='SAME'):
        with tf.variable_scope(scope, 'BlockInceptionC', [inputs], reuse=reuse):
            with tf.variable_scope('Branch_0'):
                branch_0 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')
            with tf.variable_scope('Branch_1'):
                branch_1 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
                branch_1 = tf.concat(axis=3, values=[slim.conv2d(branch_1, 256, [1, 3], scope='Conv2d_0b_1x3'), slim.conv2d(branch_1, 256, [3, 1], scope='Conv2d_0c_3x1')])
            with tf.variable_scope('Branch_2'):
                branch_2 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
                branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope='Conv2d_0b_3x1')
                branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope='Conv2d_0c_1x3')
                branch_2 = tf.concat(axis=3, values=[slim.conv2d(branch_2, 256, [1, 3], scope='Conv2d_0d_1x3'), slim.conv2d(branch_2, 256, [3, 1], scope='Conv2d_0e_3x1')])
            with tf.variable_scope('Branch_3'):
                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
                branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope='Conv2d_0b_1x1')
            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    with:
        with:
            with:
                 ...  =  ... .concat
            return tf

idx = 97:------------------- similar code ------------------ index = 39, score = 6.0 
def call(self, inputs, input_lengths=None, **kwargs):
    conv_outputs = tf.concat([conv1d(inputs) for conv1d in self.convolution_banks], axis=(- 1))
    maxpool_output = self.maxpool(conv_outputs)
    proj1_output = self.projection1(maxpool_output)
    proj2_output = self.projection2(proj1_output)
    highway_input = (proj2_output + inputs)
    if (highway_input.shape[2] != (self.out_units // 2)):
        highway_input = self.adjustment_layer(highway_input)
    highway_output = reduce((lambda acc, hw: hw(acc)), self.highway_nets, highway_input)
    (outputs, states) = tf.nn.bidirectional_dynamic_rnn(ZoneoutLSTMCell((self.out_units // 2), self._is_training, zoneout_factor_cell=self._zoneout_factor_cell, zoneout_factor_output=self._zoneout_factor_output, lstm_impl=self._lstm_impl, dtype=self.dtype), ZoneoutLSTMCell((self.out_units // 2), self._is_training, zoneout_factor_cell=self._zoneout_factor_cell, zoneout_factor_output=self._zoneout_factor_output, lstm_impl=self._lstm_impl, dtype=self.dtype), highway_output, sequence_length=input_lengths, dtype=highway_output.dtype)
    return tf.concat(outputs, axis=(- 1))

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
     ...  =  ... .concat
    return tf

idx = 98:------------------- similar code ------------------ index = 200, score = 6.0 
def cbhg(inputs, input_lengths, is_training, bank_size, bank_channel_size, maxpool_width, highway_depth, rnn_size, proj_sizes, proj_width, scope, before_highway=None, encoder_rnn_init_state=None):
    batch_size = tf.shape(inputs)[0]
    with tf.variable_scope(scope):
        with tf.variable_scope('conv_bank'):
            conv_fn = (lambda k: conv1d(inputs, k, bank_channel_size, tf.nn.relu, is_training, ('conv1d_%d' % k)))
            conv_outputs = tf.concat([conv_fn(k) for k in range(1, (bank_size + 1))], axis=(- 1))
        maxpool_output = tf.layers.max_pooling1d(conv_outputs, pool_size=maxpool_width, strides=1, padding='same')
        proj_out = maxpool_output
        for (idx, proj_size) in enumerate(proj_sizes):
            activation_fn = (None if (idx == (len(proj_sizes) - 1)) else tf.nn.relu)
            proj_out = conv1d(proj_out, proj_width, proj_size, activation_fn, is_training, 'proj_{}'.format((idx + 1)))
        if (before_highway is not None):
            expanded_before_highway = tf.expand_dims(before_highway, [1])
            tiled_before_highway = tf.tile(expanded_before_highway, [1, tf.shape(proj_out)[1], 1])
            highway_input = ((proj_out + inputs) + tiled_before_highway)
        else:
            highway_input = (proj_out + inputs)
        if (highway_input.shape[2] != rnn_size):
            highway_input = tf.layers.dense(highway_input, rnn_size)
        for idx in range(highway_depth):
            highway_input = highwaynet(highway_input, ('highway_%d' % (idx + 1)))
        rnn_input = highway_input
        if (encoder_rnn_init_state is not None):
            (initial_state_fw, initial_state_bw) = tf.split(encoder_rnn_init_state, 2, 1)
        else:
            (initial_state_fw, initial_state_bw) = (None, None)
        (cell_fw, cell_bw) = (GRUCell(rnn_size), GRUCell(rnn_size))
        (outputs, states) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, rnn_input, sequence_length=input_lengths, initial_state_fw=initial_state_fw, initial_state_bw=initial_state_bw, dtype=tf.float32)
        return tf.concat(outputs, axis=2)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    with:
        with:
             ...  =  ... .concat
        return tf

idx = 99:------------------- similar code ------------------ index = 2, score = 6.0 
def call(self, raw_inputs, **kwargs):
    outputs = []
    with tf.name_scope('deep'):
        output = tf.concat(raw_inputs, (- 1))
        for (i, hs) in enumerate(self.hidden):
            output = CustomDropout(0.1)(output)
            output = DeepBlock(hidden=hs, activation=self.activation, prefix='deep_{}'.format(i), sparse=self.sparse)(output)
            outputs.append(output)
        '\n            H: column wise matrix of each deep layer\n            '
        H = tf.stack(outputs, axis=2)
        "\n            S = H' * H\n            "
        S = tf.matmul(tf.transpose(H, perm=[0, 2, 1]), H)
        '\n            Column wise softmax as attention\n            '
        attention = tf.nn.softmax(S, axis=1)
        '\n            G = H * A\n            '
        G = tf.matmul(H, attention)
        '\n            Sum over deep layers\n            '
        G = tf.reduce_sum(G, axis=(- 1))
        if self.concat_last_deep:
            return tf.concat([outputs[(- 1)], G], axis=(- 1))
        else:
            return G

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    with:
         ...  =  ... .concat
        if:
            return tf

