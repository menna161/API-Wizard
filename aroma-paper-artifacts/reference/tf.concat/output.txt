------------------------- example 1 ------------------------ 
def concat(self, inputs):
    return tf.concat(inputs, (- 1))

------------------------- example 2 ------------------------ 
def block_reduction_b(inputs, scope=None, reuse=None):
    'Builds Reduction-B block for Inception v4 network.'
    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding='SAME'):
        with tf.variable_scope(scope, 'BlockReductionB', [inputs], reuse=reuse):
            with tf.variable_scope('Branch_0'):
                branch_0 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
                branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')
            with tf.variable_scope('Branch_1'):
// your code ...

            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])

------------------------- example 3 ------------------------ 
def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
    idxs = tf.concat([self.batch_idxs, tf.cast(y_true, tf.int32)], 1)
    y_true_pred = tf.gather_nd(y_pred, idxs)
    y_true_pred = tf.expand_dims(y_true_pred, 1)
    y_true_pred_margin = (y_true_pred - self.margin)
    _Z = tf.concat([y_pred, y_true_pred_margin], 1)
    _Z = (_Z * self.scale)
    logZ = tf.math.reduce_logsumexp(_Z, 1, keepdims=True)
    logZ = (logZ + tf.math.log((1 - tf.math.exp(((self.scale * y_true_pred) - logZ)))))
    return (((- y_true_pred_margin) * self.scale) + logZ)

------------------------- example 4 ------------------------ 
def _std_image_normalize(image, stds):
    "Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.\n  "
    num_channels = image.get_shape().as_list()[(- 1)]
    if (len(stds) != num_channels):
// your code ...

    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=image)
    for i in range(num_channels):
// your code ...

    return tf.concat(axis=3, values=channels)

------------------------- example 5 ------------------------ 
def attn(x, scope, n_state, *, past, hparams):
    assert (x.shape.ndims == 3)
    assert ((n_state % hparams.n_head) == 0)
    if (past is not None):
        assert (past.shape.ndims == 5)

    def split_heads(x):
        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])

    def merge_heads(x):
        return merge_states(tf.transpose(x, [0, 2, 1, 3]))

    def mask_attn_weights(w):
        (_, _, nd, ns) = shape_list(w)
        b = attention_mask(nd, ns, dtype=w.dtype)
        b = tf.reshape(b, [1, 1, nd, ns])
        w = ((w * b) - (tf.cast(10000000000.0, w.dtype) * (1 - b)))
        return w

    def multihead_attn(q, k, v):
        w = tf.matmul(q, k, transpose_b=True)
        w = (w * tf.rsqrt(tf.cast(v.shape[(- 1)].value, w.dtype)))
        w = mask_attn_weights(w)
// your code ...

        a = tf.matmul(w, v)
        return a
    with tf.variable_scope(scope):
        c = conv1d(x, 'c_attn', (n_state * 3))
        (q, k, v) = map(split_heads, tf.split(c, 3, axis=2))
        present = tf.stack([k, v], axis=1)
        if (past is not None):
            (pk, pv) = tf.unstack(past, axis=1)
            k = tf.concat([pk, k], axis=(- 2))
            v = tf.concat([pv, v], axis=(- 2))
        a = multihead_attn(q, k, v)
// your code ...


examples  ||  representativeness  ||  number of lines  || number of comments 
example1  ||          4           ||        2         ||         0        
example2  ||          4           ||        10         ||         1        
example3  ||          2           ||        10         ||         0        
example4  ||          3           ||        9         ||         2        
example5  ||          2           ||        28         ||         2        


idx = 0:------------------- similar code ------------------ index = 52, score = 7.0 
def concat(self, inputs):
    return tf.concat(inputs, (- 1))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 1:------------------- similar code ------------------ index = 48, score = 6.0 
def block_reduction_b(inputs, scope=None, reuse=None):
    'Builds Reduction-B block for Inception v4 network.'
    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding='SAME'):
        with tf.variable_scope(scope, 'BlockReductionB', [inputs], reuse=reuse):
            with tf.variable_scope('Branch_0'):
                branch_0 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
                branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')
            with tf.variable_scope('Branch_1'):
                branch_1 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')
                branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope='Conv2d_0b_1x7')
                branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope='Conv2d_0c_7x1')
                branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')
            with tf.variable_scope('Branch_2'):
                branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID', scope='MaxPool_1a_3x3')
            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    with:
        with:
            return tf.concat

idx = 2:------------------- similar code ------------------ index = 66, score = 6.0 
def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
    idxs = tf.concat([self.batch_idxs, tf.cast(y_true, tf.int32)], 1)
    y_true_pred = tf.gather_nd(y_pred, idxs)
    y_true_pred = tf.expand_dims(y_true_pred, 1)
    y_true_pred_margin = (y_true_pred - self.margin)
    _Z = tf.concat([y_pred, y_true_pred_margin], 1)
    _Z = (_Z * self.scale)
    logZ = tf.math.reduce_logsumexp(_Z, 1, keepdims=True)
    logZ = (logZ + tf.math.log((1 - tf.math.exp(((self.scale * y_true_pred) - logZ)))))
    return (((- y_true_pred_margin) * self.scale) + logZ)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... () -> tf:
     ...  =  ... .concat

idx = 3:------------------- similar code ------------------ index = 53, score = 6.0 
def _std_image_normalize(image, stds):
    "Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.\n  "
    num_channels = image.get_shape().as_list()[(- 1)]
    if (len(stds) != num_channels):
        raise ValueError('len(means) must match the number of channels')
    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=image)
    for i in range(num_channels):
        channels[i] /= stds[i]
    return tf.concat(axis=3, values=channels)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 4:------------------- similar code ------------------ index = 47, score = 6.0 
def attn(x, scope, n_state, *, past, hparams):
    assert (x.shape.ndims == 3)
    assert ((n_state % hparams.n_head) == 0)
    if (past is not None):
        assert (past.shape.ndims == 5)

    def split_heads(x):
        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])

    def merge_heads(x):
        return merge_states(tf.transpose(x, [0, 2, 1, 3]))

    def mask_attn_weights(w):
        (_, _, nd, ns) = shape_list(w)
        b = attention_mask(nd, ns, dtype=w.dtype)
        b = tf.reshape(b, [1, 1, nd, ns])
        w = ((w * b) - (tf.cast(10000000000.0, w.dtype) * (1 - b)))
        return w

    def multihead_attn(q, k, v):
        w = tf.matmul(q, k, transpose_b=True)
        w = (w * tf.rsqrt(tf.cast(v.shape[(- 1)].value, w.dtype)))
        w = mask_attn_weights(w)
        w = softmax(w)
        a = tf.matmul(w, v)
        return a
    with tf.variable_scope(scope):
        c = conv1d(x, 'c_attn', (n_state * 3))
        (q, k, v) = map(split_heads, tf.split(c, 3, axis=2))
        present = tf.stack([k, v], axis=1)
        if (past is not None):
            (pk, pv) = tf.unstack(past, axis=1)
            k = tf.concat([pk, k], axis=(- 2))
            v = tf.concat([pv, v], axis=(- 2))
        a = multihead_attn(q, k, v)
        a = merge_heads(a)
        a = conv1d(a, 'c_proj', n_state)
        return (a, present)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    def  ... ( ... ):
        return tf

    with:
        if:
             ...  =  ... .concat

idx = 5:------------------- similar code ------------------ index = 20, score = 6.0 
def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
    ' NOTE : y_pred must be cos similarity\n\n    Args:\n        y_true (tf.Tensor): shape [batch,ndim]\n        y_pred (tf.Tensor): shape [batch,ndim]\n\n    Returns:\n        tf.Tensor: loss\n    '
    idxs = tf.concat([self.batch_idxs, tf.cast(y_true, tf.int32)], 1)
    sp = tf.expand_dims(tf.gather_nd(y_pred, idxs), 1)
    mask = tf.logical_not(tf.scatter_nd(idxs, tf.ones(tf.shape(idxs)[0], tf.bool), tf.shape(y_pred)))
    sn = tf.reshape(tf.boolean_mask(y_pred, mask), (self.batch_size, (- 1)))
    alpha_p = tf.nn.relu((self.O_p - tf.stop_gradient(sp)))
    alpha_n = tf.nn.relu((tf.stop_gradient(sn) - self.O_n))
    r_sp_m = (alpha_p * (sp - self.Delta_p))
    r_sn_m = (alpha_n * (sn - self.Delta_n))
    _Z = tf.concat([r_sn_m, r_sp_m], 1)
    _Z = (_Z * self.gamma)
    logZ = tf.math.reduce_logsumexp(_Z, 1, keepdims=True)
    return (((- r_sp_m) * self.gamma) + logZ)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... () -> tf:
     ...  =  ... .concat

idx = 6:------------------- similar code ------------------ index = 19, score = 6.0 
def _mean_image_subtraction(image, means):
    "Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.\n  "
    if (image.get_shape().ndims != 3):
        raise ValueError('Input must be of size [height, width, C>0]')
    num_channels = image.get_shape().as_list()[(- 1)]
    if (len(means) != num_channels):
        raise ValueError('len(means) must match the number of channels')
    channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)
    for i in range(num_channels):
        channels[i] -= means[i]
    return tf.concat(axis=2, values=channels)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 7:------------------- similar code ------------------ index = 17, score = 6.0 
def transformer(U, theta, out_size, name='SpatialTransformer', **kwargs):
    'Spatial Transformer Layer\n\n    Implements a spatial transformer layer as described in [1]_.\n    Based on [2]_ and edited by David Dao for Tensorflow.\n    Parameters\n    ----------\n    U : float\n        The output of a convolutional net should have the\n        shape [num_batch, height, width, num_channels].\n    theta: float\n        The output of the\n        localisation network should be [num_batch, 6].\n    out_size: tuple of two ints\n        The size of the output of the network (height, width)\n    References\n    ----------\n    .. [1]  Spatial Transformer Networks\n            Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n            Submitted on 5 Jun 2015\n    .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n    Notes\n    -----\n    To initialize the network to the identity transform init\n    ``theta`` to :\n        identity = np.array([[1., 0., 0.],\n                             [0., 1., 0.]])\n        identity = identity.flatten()\n        theta = tf.Variable(initial_value=identity)\n    '

    def _repeat(x, n_repeats):
        with tf.variable_scope('_repeat'):
            rep = tf.transpose(tf.expand_dims(tf.ones(shape=tf.stack([n_repeats])), 1), [1, 0])
            rep = tf.cast(rep, 'int32')
            x = tf.matmul(tf.reshape(x, ((- 1), 1)), rep)
            return tf.reshape(x, [(- 1)])

    def _interpolate(im, x, y, out_size):
        with tf.variable_scope('_interpolate'):
            num_batch = tf.shape(im)[0]
            height = tf.shape(im)[1]
            width = tf.shape(im)[2]
            channels = tf.shape(im)[3]
            x = tf.cast(x, 'float32')
            y = tf.cast(y, 'float32')
            height_f = tf.cast(height, 'float32')
            width_f = tf.cast(width, 'float32')
            out_height = out_size[0]
            out_width = out_size[1]
            zero = tf.zeros([], dtype='int32')
            max_y = tf.cast((tf.shape(im)[1] - 1), 'int32')
            max_x = tf.cast((tf.shape(im)[2] - 1), 'int32')
            x = (((x + 1.0) * (width_f - 1.001)) / 2.0)
            y = (((y + 1.0) * (height_f - 1.001)) / 2.0)
            x0 = tf.cast(tf.floor(x), 'int32')
            x1 = (x0 + 1)
            y0 = tf.cast(tf.floor(y), 'int32')
            y1 = (y0 + 1)
            x0 = tf.clip_by_value(x0, zero, max_x)
            x1 = tf.clip_by_value(x1, zero, max_x)
            y0 = tf.clip_by_value(y0, zero, max_y)
            y1 = tf.clip_by_value(y1, zero, max_y)
            dim2 = width
            dim1 = (width * height)
            base = _repeat((tf.range(num_batch) * dim1), (out_height * out_width))
            base_y0 = (base + (y0 * dim2))
            base_y1 = (base + (y1 * dim2))
            idx_a = (base_y0 + x0)
            idx_b = (base_y1 + x0)
            idx_c = (base_y0 + x1)
            idx_d = (base_y1 + x1)
            im_flat = tf.reshape(im, tf.stack([(- 1), channels]))
            im_flat = tf.cast(im_flat, 'float32')
            Ia = tf.gather(im_flat, idx_a)
            Ib = tf.gather(im_flat, idx_b)
            Ic = tf.gather(im_flat, idx_c)
            Id = tf.gather(im_flat, idx_d)
            x0_f = tf.cast(x0, 'float32')
            x1_f = tf.cast(x1, 'float32')
            y0_f = tf.cast(y0, 'float32')
            y1_f = tf.cast(y1, 'float32')
            wa = tf.expand_dims(((x1_f - x) * (y1_f - y)), 1)
            wb = tf.expand_dims(((x1_f - x) * (y - y0_f)), 1)
            wc = tf.expand_dims(((x - x0_f) * (y1_f - y)), 1)
            wd = tf.expand_dims(((x - x0_f) * (y - y0_f)), 1)
            output = tf.add_n([(wa * Ia), (wb * Ib), (wc * Ic), (wd * Id)])
            return output

    def _meshgrid(height, width):
        with tf.variable_scope('_meshgrid'):
            x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])), tf.transpose(tf.expand_dims(tf.linspace((- 1.0), 1.0, width), 1), [1, 0]))
            y_t = tf.matmul(tf.expand_dims(tf.linspace((- 1.0), 1.0, height), 1), tf.ones(shape=tf.stack([1, width])))
            x_t_flat = tf.reshape(x_t, (1, (- 1)))
            y_t_flat = tf.reshape(y_t, (1, (- 1)))
            ones = tf.ones_like(x_t_flat)
            grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones])
            return grid

    def _transform(theta, input_dim, out_size):
        with tf.variable_scope('_transform'):
            num_batch = tf.shape(input_dim)[0]
            num_channels = tf.shape(input_dim)[3]
            theta = tf.reshape(theta, ((- 1), 2, 3))
            theta = tf.cast(theta, 'float32')
            out_height = out_size[0]
            out_width = out_size[1]
            grid = _meshgrid(out_height, out_width)
            grid = tf.expand_dims(grid, 0)
            grid = tf.reshape(grid, [(- 1)])
            grid = tf.tile(grid, tf.stack([num_batch]))
            grid = tf.reshape(grid, tf.stack([num_batch, 3, (- 1)]))
            T_g = tf.matmul(theta, grid)
            x_s = tf.slice(T_g, [0, 0, 0], [(- 1), 1, (- 1)])
            y_s = tf.slice(T_g, [0, 1, 0], [(- 1), 1, (- 1)])
            x_s_flat = tf.reshape(x_s, [(- 1)])
            y_s_flat = tf.reshape(y_s, [(- 1)])
            input_transformed = _interpolate(input_dim, x_s_flat, y_s_flat, out_size)
            output = tf.reshape(input_transformed, tf.stack([num_batch, out_height, out_width, num_channels]))
            return output
    with tf.variable_scope(name):
        output = _transform(theta, U, out_size)
        return output

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    def  ... ():
        with:
            return tf

    def  ... ():
        with:
             ...  =  ... .concat

idx = 8:------------------- similar code ------------------ index = 15, score = 6.0 
def block_inception_a(inputs, scope=None, reuse=None):
    'Builds Inception-A block for Inception v4 network.'
    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding='SAME'):
        with tf.variable_scope(scope, 'BlockInceptionA', [inputs], reuse=reuse):
            with tf.variable_scope('Branch_0'):
                branch_0 = slim.conv2d(inputs, 96, [1, 1], scope='Conv2d_0a_1x1')
            with tf.variable_scope('Branch_1'):
                branch_1 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')
                branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='Conv2d_0b_3x3')
            with tf.variable_scope('Branch_2'):
                branch_2 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')
                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')
                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')
            with tf.variable_scope('Branch_3'):
                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
                branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope='Conv2d_0b_1x1')
            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    with:
        with:
            return tf.concat

idx = 9:------------------- similar code ------------------ index = 14, score = 6.0 
def _marshal(*rvs):
    'Args: a list of ed.RandomVariables each with vector or scalar event shape\n  (which must be staticly known), and all having the same batch shape.\n\n  Returns: a Tensor from concatenating their values along a single vector\n  dimension.\n  '
    vector_rvs = []
    for rv in rvs:
        v = rv.value
        if (v.shape.ndims == 0):
            vector_rvs.append([v])
        else:
            vector_rvs.append(v)
    print(vector_rvs)
    return tf.concat(vector_rvs, axis=(- 1))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 10:------------------- similar code ------------------ index = 12, score = 6.0 
def _mean_image_subtraction(image, means):
    "Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.\n  "
    num_channels = image.get_shape().as_list()[(- 1)]
    if (len(means) != num_channels):
        raise ValueError('len(means) must match the number of channels')
    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=image)
    for i in range(num_channels):
        channels[i] -= means[i]
    return tf.concat(axis=3, values=channels)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 11:------------------- similar code ------------------ index = 0, score = 6.0 
def call(self, inputs: list, **kwargs) -> typing.Any:
    '\n        The computation logic of MatchingLayer.\n\n        :param inputs: two input tensors.\n        '
    x1 = inputs[0]
    x2 = inputs[1]
    if (self._matching_type == 'dot'):
        if self._normalize:
            x1 = tf.math.l2_normalize(x1, axis=2)
            x2 = tf.math.l2_normalize(x2, axis=2)
        return tf.expand_dims(tf.einsum('abd,acd->abc', x1, x2), 3)
    else:
        if (self._matching_type == 'mul'):

            def func(x, y):
                return (x * y)
        elif (self._matching_type == 'plus'):

            def func(x, y):
                return (x + y)
        elif (self._matching_type == 'minus'):

            def func(x, y):
                return (x - y)
        elif (self._matching_type == 'concat'):

            def func(x, y):
                return tf.concat([x, y], axis=3)
        else:
            raise ValueError(f'Invalid matching type.{self._matching_type} received.Mut be in `dot`, `mul`, `plus`, `minus` and `concat`.')
        x1_exp = tf.stack(([x1] * self._shape2[1]), 2)
        x2_exp = tf.stack(([x2] * self._shape1[1]), 1)
        return func(x1_exp, x2_exp)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... () ->:
    if:    else:
        if:        elif:

            def  ... ():
                return tf.concat

idx = 12:------------------- similar code ------------------ index = 92, score = 6.0 
def block_inception_b(inputs, scope=None, reuse=None):
    'Builds Inception-B block for Inception v4 network.'
    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding='SAME'):
        with tf.variable_scope(scope, 'BlockInceptionB', [inputs], reuse=reuse):
            with tf.variable_scope('Branch_0'):
                branch_0 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
            with tf.variable_scope('Branch_1'):
                branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
                branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope='Conv2d_0b_1x7')
                branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope='Conv2d_0c_7x1')
            with tf.variable_scope('Branch_2'):
                branch_2 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
                branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0b_7x1')
                branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope='Conv2d_0c_1x7')
                branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope='Conv2d_0d_7x1')
                branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope='Conv2d_0e_1x7')
            with tf.variable_scope('Branch_3'):
                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
                branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    with:
        with:
            return tf.concat

idx = 13:------------------- similar code ------------------ index = 70, score = 6.0 
def block_reduction_a(inputs, scope=None, reuse=None):
    'Builds Reduction-A block for Inception v4 network.'
    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding='SAME'):
        with tf.variable_scope(scope, 'BlockReductionA', [inputs], reuse=reuse):
            with tf.variable_scope('Branch_0'):
                branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')
            with tf.variable_scope('Branch_1'):
                branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
                branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope='Conv2d_0b_3x3')
                branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')
            with tf.variable_scope('Branch_2'):
                branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID', scope='MaxPool_1a_3x3')
            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    with:
        with:
            return tf.concat

idx = 14:------------------- similar code ------------------ index = 90, score = 6.0 
def _std_image_normalize(image, stds):
    "Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.\n  "
    num_channels = image.get_shape().as_list()[(- 1)]
    if (len(stds) != num_channels):
        raise ValueError('len(means) must match the number of channels')
    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=image)
    for i in range(num_channels):
        channels[i] /= stds[i]
    return tf.concat(axis=3, values=channels)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 15:------------------- similar code ------------------ index = 89, score = 6.0 
def all_pairs_tf(a, b):
    '\n    Return a tensor of all pairs\n    a -- [batch_size1, dim]\n    b -- [batch_size2, dim]\n    '
    dim = tf.shape(a)[1]
    temp_a = (tf.expand_dims(a, axis=1) + tf.zeros(tf.shape(tf.expand_dims(b, axis=0)), dtype=b.dtype))
    temp_b = (tf.zeros(tf.expand_dims(a, axis=1), dtype=a.dtype) + tf.expand_dims(b, axis=0))
    return tf.concat((tf.reshape(temp_a, [(- 1), 1, dim]), tf.reshape(temp_b, [(- 1), 1, dim])), axis=1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return tf.concat

idx = 16:------------------- similar code ------------------ index = 58, score = 6.0 
def conv2d(input_, output_dim, k_h=4, k_w=4, d_h=2, d_w=2, stddev=None, name='conv2d', spectral_normed=False, update_collection=None, with_w=False, padding='SAME', mhe=False, net_type='d'):
    fan_in = ((k_h * k_w) * input_.get_shape().as_list()[(- 1)])
    fan_out = ((k_h * k_w) * output_dim)
    if (stddev is None):
        stddev = np.sqrt((2.0 / fan_in))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[(- 1)], output_dim], initializer=tf.truncated_normal_initializer(stddev=stddev))
        if spectral_normed:
            conv = tf.nn.conv2d(input_, spectral_normed_weight(w, update_collection=update_collection), strides=[1, d_h, d_w, 1], padding=padding)
        else:
            conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=padding)
        if mhe:
            print(('mhe on %s' % name))
            eps = 0.0001
            filt = w
            filt = tf.reshape(filt, [(- 1), output_dim])
            filt = tf.concat([filt, (- filt)], axis=0)
            filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
            filt /= filt_norm
            inner_pro = tf.matmul(tf.transpose(filt), filt)
            cross_terms = (2.0 - (2.0 * inner_pro))
            cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(output_dim)))
            loss = ((- 1e-07) * tf.reduce_mean(tf.log((cross_terms + eps))))
            if (net_type == 'g'):
                tf.add_to_collection('g_mhe_loss', loss)
            elif (net_type == 'd'):
                tf.add_to_collection('d_mhe_loss', loss)
            else:
                raise
        biases = tf.get_variable('b', [output_dim], initializer=tf.constant_initializer(0.0))
        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())
        if with_w:
            return (conv, w, biases)
        else:
            return conv

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :
             ...  = tf.concat

idx = 17:------------------- similar code ------------------ index = 56, score = 6.0 
def __init__(self, params, model_dir=None, config=None, warm_start_from=None):

    def model_fn(features, labels, mode, params):
        is_training = (mode == tf.estimator.ModeKeys.TRAIN)
        is_validation = (mode == tf.estimator.ModeKeys.EVAL)
        is_prediction = (mode == tf.estimator.ModeKeys.PREDICT)
        embedding = Embedding(params.num_symbols, embedding_dim=params.embedding_dim)
        if params.use_accent_type:
            accent_embedding = Embedding(params.num_accent_type, embedding_dim=params.accent_type_embedding_dim, index_offset=params.accent_type_offset)
        encoder = encoder_factory(params, is_training)
        assert (params.decoder in ['DualSourceDecoder', 'DualSourceTransformerDecoder'])
        decoder = decoder_factory(params)
        assert (not (params.use_speaker_embedding and params.use_external_speaker_embedding))
        if params.use_speaker_embedding:
            speaker_embedding = Embedding(params.num_speakers, embedding_dim=params.speaker_embedding_dim, index_offset=params.speaker_embedding_offset)
        elif params.use_external_speaker_embedding:
            speaker_embedding = ExternalEmbedding(params.embedding_file, params.num_speakers, embedding_dim=params.speaker_embedding_dim, index_offset=params.speaker_embedding_offset)
        if (params.speaker_embedding_projection_out_dim > (- 1)):

            def _compose(f, g):
                return (lambda arg, *args, **kwargs: f(g(arg, *args, **kwargs)))
            resize = tf.layers.Dense(params.speaker_embedding_projection_out_dim, activation=tf.nn.relu)
            speaker_embedding = _compose(resize, speaker_embedding)
        if params.use_language_embedding:
            language_embedding = ExternalEmbedding(params.language_embedding_file, params.num_speakers, embedding_dim=params.language_embedding_dim, index_offset=params.speaker_embedding_offset)
        if (params.language_embedding_projection_out_dim > (- 1)):

            def _compose(f, g):
                return (lambda arg, *args, **kwargs: f(g(arg, *args, **kwargs)))
            resize = tf.layers.Dense(params.language_embedding_projection_out_dim, activation=tf.nn.relu)
            language_embedding = _compose(resize, language_embedding)
        if params.channel_id_to_postnet:
            channel_code = ExternalEmbedding(params.channel_id_file, params.num_speakers, embedding_dim=params.channel_id_dim, index_offset=params.speaker_embedding_offset)
        target = (labels.mel if (is_training or is_validation) else features.mel)
        x = params.speaker_for_synthesis
        if (x > (- 1)):
            speaker_embedding_output = speaker_embedding(x)
        else:
            speaker_embedding_output = (speaker_embedding(features.speaker_id) if (params.use_speaker_embedding or params.use_external_speaker_embedding) else None)
        if (x > (- 1)):
            language_embedding_output = language_embedding(x)
        else:
            language_embedding_output = (language_embedding(features.speaker_id) if params.use_language_embedding else None)
        channel_code_output = (channel_code(features.speaker_id) if params.channel_id_to_postnet else None)
        embedding_output = embedding(features.source)
        if params.language_embedd_to_input:
            language_embedd_input_projection_layer = tf.layers.Dense(params.embedding_dim)
            language_embedd_input_projected = language_embedd_input_projection_layer(language_embedding_output)
            expand_language_embedding_input = tf.tile(tf.expand_dims(language_embedd_input_projected, axis=1), [1, tf.shape(embedding_output)[1], 1])
            embedding_output = (embedding_output + expand_language_embedding_input)
        (encoder_lstm_output, encoder_self_attention_output, self_attention_alignment) = (encoder((embedding_output, accent_embedding(features.accent_type)), input_lengths=features.source_length) if params.use_accent_type else encoder(embedding_output, input_lengths=features.source_length))
        if params.speaker_embedd_to_decoder:
            expand_speaker_embedding_output = tf.tile(tf.expand_dims(speaker_embedding_output, axis=1), [1, tf.shape(encoder_lstm_output)[1], 1])
            encoder_lstm_output = tf.concat((encoder_lstm_output, expand_speaker_embedding_output), axis=(- 1))
            encoder_self_attention_output = tf.concat((encoder_self_attention_output, expand_speaker_embedding_output), axis=(- 1))
        if params.language_embedd_to_decoder:
            expand_language_embedding_output = tf.tile(tf.expand_dims(language_embedding_output, axis=1), [1, tf.shape(encoder_lstm_output)[1], 1])
            encoder_lstm_output = tf.concat((encoder_lstm_output, expand_language_embedding_output), axis=(- 1))
            encoder_self_attention_output = tf.concat((encoder_self_attention_output, expand_language_embedding_output), axis=(- 1))
        (attention1_fn, attention2_fn) = dual_source_attention_factory(params)
        (mel_output, stop_token, decoder_state) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=(is_validation or params.use_forced_alignment_mode), teacher_forcing=params.use_forced_alignment_mode, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=(labels.target_length if is_training else None), target=target, apply_dropout_on_inference=params.apply_dropout_on_inference)
        self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in self_attention_alignment]
        if ((params.decoder == 'DualSourceTransformerDecoder') and (not is_training)):
            decoder_rnn_state = decoder_state.rnn_state.rnn_state[0]
            alignment1 = tf.transpose(decoder_rnn_state.alignment_history[0].stack(), [1, 2, 0])
            alignment2 = tf.transpose(decoder_rnn_state.alignment_history[1].stack(), [1, 2, 0])
            decoder_self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in decoder_state.alignments]
        else:
            decoder_rnn_state = decoder_state[0]
            alignment1 = tf.transpose(decoder_rnn_state.alignment_history[0].stack(), [1, 2, 0])
            alignment2 = tf.transpose(decoder_rnn_state.alignment_history[1].stack(), [1, 2, 0])
            decoder_self_attention_alignment = []
        if params.use_forced_alignment_mode:
            (attention1_fn, attention2_fn) = force_alignment_dual_source_attention_factory(params)
            (mel_output, stop_token, decoder_state) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=True, teacher_forcing=False, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=(labels.target_length if is_training else None), target=target, teacher_alignments=(tf.transpose(alignment1, perm=[0, 2, 1]), tf.transpose(alignment2, perm=[0, 2, 1])), apply_dropout_on_inference=params.apply_dropout_on_inference)
            if ((params.decoder == 'DualSourceTransformerDecoder') and (not is_training)):
                alignment1 = tf.transpose(decoder_state.rnn_state.rnn_state[0].alignment_history[0].stack(), [1, 2, 0])
                alignment2 = tf.transpose(decoder_state.rnn_state.rnn_state[0].alignment_history[1].stack(), [1, 2, 0])
                decoder_self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in decoder_state.alignments]
            else:
                alignment1 = tf.transpose(decoder_state[0].alignment_history[0].stack(), [1, 2, 0])
                alignment2 = tf.transpose(decoder_state[0].alignment_history[1].stack(), [1, 2, 0])
                decoder_self_attention_alignment = []
        if params.use_postnet_v2:
            postnet = (MultiSpeakerPostNet(out_units=params.num_mels, speaker_embed=speaker_embedding_output, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate) if params.speaker_embedd_to_postnet else (ChannelEncoderPostNet(out_units=params.num_mels, channel_code=channel_code_output, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate) if params.channel_id_to_postnet else PostNetV2(out_units=params.num_mels, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate)))
            postnet_v2_mel_output = postnet(mel_output)
        global_step = tf.train.get_global_step()
        if (mode is not tf.estimator.ModeKeys.PREDICT):
            mel_loss = spec_loss(mel_output, labels.mel, labels.spec_loss_mask, params.spec_loss_type)
            done_loss = binary_loss(stop_token, labels.done, labels.binary_loss_mask)
            blacklist = ['embedding', 'bias', 'batch_normalization', 'output_projection_wrapper/kernel', 'lstm_cell', 'output_and_stop_token_wrapper/dense/', 'output_and_stop_token_wrapper/dense_1/', 'stop_token_projection/kernel']
            regularization_loss = (l2_regularization_loss(tf.trainable_variables(), params.l2_regularization_weight, blacklist) if params.use_l2_regularization else 0)
            postnet_v2_mel_loss = (spec_loss(postnet_v2_mel_output, labels.mel, labels.spec_loss_mask, params.spec_loss_type) if params.use_postnet_v2 else 0)
            loss = (((mel_loss + done_loss) + regularization_loss) + postnet_v2_mel_loss)
        if is_training:
            lr = (self.learning_rate_decay(params.initial_learning_rate, global_step, params.learning_rate_step_factor) if params.decay_learning_rate else tf.convert_to_tensor(params.initial_learning_rate))
            optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1=params.adam_beta1, beta2=params.adam_beta2, epsilon=params.adam_eps)
            (gradients, variables) = zip(*optimizer.compute_gradients(loss))
            (clipped_gradients, _) = tf.clip_by_global_norm(gradients, 1.0)
            self.add_training_stats(loss, mel_loss, done_loss, lr, postnet_v2_mel_loss, regularization_loss)
            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
                train_op = optimizer.apply_gradients(zip(clipped_gradients, variables), global_step=global_step)
                summary_writer = tf.summary.FileWriter(model_dir)
                alignment_saver = MetricsSaver(([alignment1, alignment2] + self_attention_alignment), global_step, mel_output, labels.mel, labels.target_length, features.id, features.text, params.alignment_save_steps, mode, summary_writer, save_training_time_metrics=params.save_training_time_metrics, keep_eval_results_max_epoch=params.keep_eval_results_max_epoch)
                hooks = [alignment_saver]
                if params.record_profile:
                    profileHook = tf.train.ProfilerHook(save_steps=params.profile_steps, output_dir=model_dir, show_dataflow=True, show_memory=True)
                    hooks.append(profileHook)
                return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_hooks=hooks)
        if is_validation:
            (attention1_fn, attention2_fn) = dual_source_attention_factory(params)
            (mel_output_with_teacher, stop_token_with_teacher, decoder_state_with_teacher) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=is_validation, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=labels.target_length, target=target, teacher_forcing=True, apply_dropout_on_inference=params.apply_dropout_on_inference)
            if params.use_postnet_v2:
                postnet_v2_mel_output_with_teacher = postnet(mel_output_with_teacher)
            mel_loss_with_teacher = spec_loss(mel_output_with_teacher, labels.mel, labels.spec_loss_mask, params.spec_loss_type)
            done_loss_with_teacher = binary_loss(stop_token_with_teacher, labels.done, labels.binary_loss_mask)
            postnet_v2_mel_loss_with_teacher = (spec_loss(postnet_v2_mel_output_with_teacher, labels.mel, labels.spec_loss_mask, params.spec_loss_type) if params.use_postnet_v2 else 0)
            loss_with_teacher = (((mel_loss_with_teacher + done_loss_with_teacher) + regularization_loss) + postnet_v2_mel_loss_with_teacher)
            eval_metric_ops = self.get_validation_metrics(mel_loss, done_loss, postnet_v2_mel_loss, loss_with_teacher, mel_loss_with_teacher, done_loss_with_teacher, postnet_v2_mel_loss_with_teacher, regularization_loss)
            summary_writer = tf.summary.FileWriter(model_dir)
            alignment_saver = MetricsSaver((([alignment1, alignment2] + self_attention_alignment) + decoder_self_attention_alignment), global_step, mel_output, labels.mel, labels.target_length, features.id, features.text, 1, mode, summary_writer, save_training_time_metrics=params.save_training_time_metrics, keep_eval_results_max_epoch=params.keep_eval_results_max_epoch)
            return tf.estimator.EstimatorSpec(mode, loss=loss, evaluation_hooks=[alignment_saver], eval_metric_ops=eval_metric_ops)
        if is_prediction:
            num_self_alignments = len(self_attention_alignment)
            num_decoder_self_alignments = len(decoder_self_attention_alignment)
            predictions = {'id': features.id, 'key': features.key, 'mel': mel_output, 'mel_postnet': (postnet_v2_mel_output if params.use_postnet_v2 else None), 'ground_truth_mel': features.mel, 'alignment': alignment1, 'alignment2': alignment2, 'alignment3': (decoder_self_attention_alignment[0] if (num_decoder_self_alignments >= 1) else None), 'alignment4': (decoder_self_attention_alignment[1] if (num_decoder_self_alignments >= 2) else None), 'alignment5': (self_attention_alignment[0] if (num_self_alignments >= 1) else None), 'alignment6': (self_attention_alignment[1] if (num_self_alignments >= 2) else None), 'alignment7': (self_attention_alignment[2] if (num_self_alignments >= 3) else None), 'alignment8': (self_attention_alignment[3] if (num_self_alignments >= 4) else None), 'source': features.source, 'text': features.text, 'accent_type': (features.accent_type if params.use_accent_type else None)}
            predictions = dict(filter((lambda xy: (xy[1] is not None)), predictions.items()))
            return tf.estimator.EstimatorSpec(mode, predictions=predictions)
    super(DualSourceSelfAttentionTacotronModel, self).__init__(model_fn=model_fn, model_dir=model_dir, config=config, params=params, warm_start_from=warm_start_from)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():

    def  ... ():
        if:
             ...  = tf.concat

idx = 18:------------------- similar code ------------------ index = 55, score = 6.0 
def STEmbedding(SE, TE, T, D, bn, bn_decay, is_training):
    '\n    spatio-temporal embedding\n    SE:     [N, D]\n    TE:     [batch_size, P + Q, 2] (dayofweek, timeofday)\n    T:      num of time steps in one day\n    D:      output dims\n    retrun: [batch_size, P + Q, N, D]\n    '
    SE = tf.expand_dims(tf.expand_dims(SE, axis=0), axis=0)
    SE = FC(SE, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    dayofweek = tf.one_hot(TE[(..., 0)], depth=7)
    timeofday = tf.one_hot(TE[(..., 1)], depth=T)
    TE = tf.concat((dayofweek, timeofday), axis=(- 1))
    TE = tf.expand_dims(TE, axis=2)
    TE = FC(TE, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    return tf.add(SE, TE)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 19:------------------- similar code ------------------ index = 54, score = 6.0 
def linear(input_, output_size, name='linear', spectral_normed=False, update_collection=None, stddev=None, bias_start=0.0, with_biases=False, with_w=False, mhe=False, net_type='d'):
    shape = input_.get_shape().as_list()
    if (stddev is None):
        stddev = np.sqrt((1.0 / shape[1]))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        weight = tf.get_variable('w', [shape[1], output_size], tf.float32, tf.truncated_normal_initializer(stddev=stddev))
        if with_biases:
            bias = tf.get_variable('b', [output_size], initializer=tf.constant_initializer(bias_start))
        if spectral_normed:
            mul = tf.matmul(input_, spectral_normed_weight(weight, update_collection=update_collection))
        else:
            mul = tf.matmul(input_, weight)
        if mhe:
            eps = 0.0001
            filt = weight
            filt_num = filt.get_shape().as_list()[(- 1)]
            filt = tf.concat([filt, (- filt)], axis=0)
            filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
            filt /= filt_norm
            inner_pro = tf.matmul(tf.transpose(filt), filt)
            cross_terms = (2.0 - (2.0 * inner_pro))
            cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(filt_num)))
            loss = ((- 1e-06) * tf.reduce_mean(tf.log((cross_terms + eps))))
            if (net_type == 'g'):
                tf.add_to_collection('g_mhe_loss', loss)
            elif (net_type == 'd'):
                tf.add_to_collection('d_mhe_loss', loss)
            else:
                raise
        if with_w:
            if with_biases:
                return ((mul + bias), weight, bias)
            else:
                return (mul, weight, None)
        elif with_biases:
            return (mul + bias)
        else:
            return mul

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :
             ...  = tf.concat

idx = 20:------------------- similar code ------------------ index = 51, score = 6.0 
def _add_thomson_constraint(self, filt, n_filt, model, power):
    filt = tf.reshape(filt, [(- 1), n_filt])
    if (model == 'half_mhe'):
        filt_neg = (filt * (- 1))
        filt = tf.concat((filt, filt_neg), axis=1)
        n_filt *= 2
    filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + 0.0001))
    norm_mat = tf.matmul(tf.transpose(filt_norm), filt_norm)
    inner_pro = tf.matmul(tf.transpose(filt), filt)
    inner_pro /= norm_mat
    if (power == '0'):
        cross_terms = (2.0 - (2.0 * inner_pro))
        final = (- tf.log((cross_terms + tf.diag(([1.0] * n_filt)))))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((1 * tf.reduce_sum(final)) / cnt)
    elif (power == '1'):
        cross_terms = ((2.0 - (2.0 * inner_pro)) + tf.diag(([1.0] * n_filt)))
        final = tf.pow(cross_terms, (tf.ones_like(cross_terms) * (- 0.5)))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((1 * tf.reduce_sum(final)) / cnt)
    elif (power == '2'):
        cross_terms = ((2.0 - (2.0 * inner_pro)) + tf.diag(([1.0] * n_filt)))
        final = tf.pow(cross_terms, (tf.ones_like(cross_terms) * (- 1)))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((1 * tf.reduce_sum(final)) / cnt)
    elif (power == 'a0'):
        acos = (tf.acos(inner_pro) / math.pi)
        acos += 0.0001
        final = (- tf.log(acos))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((1 * tf.reduce_sum(final)) / cnt)
    elif (power == 'a1'):
        acos = (tf.acos(inner_pro) / math.pi)
        acos += 0.0001
        final = tf.pow(acos, (tf.ones_like(acos) * (- 1)))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((0.1 * tf.reduce_sum(final)) / cnt)
    elif (power == 'a2'):
        acos = (tf.acos(inner_pro) / math.pi)
        acos += 0.0001
        final = tf.pow(acos, (tf.ones_like(acos) * (- 2)))
        final -= tf.matrix_band_part(final, (- 1), 0)
        cnt = ((n_filt * (n_filt - 1)) / 2.0)
        loss = ((0.1 * tf.reduce_sum(final)) / cnt)
    tf.add_to_collection('thomson_loss', loss)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = tf.concat

idx = 21:------------------- similar code ------------------ index = 45, score = 6.0 
def linear(input_, output_size, name='linear', spectral_normed=False, update_collection=None, stddev=None, bias_start=0.0, with_biases=False, with_w=False, mhe=False, net_type='d'):
    shape = input_.get_shape().as_list()
    if (stddev is None):
        stddev = np.sqrt((1.0 / shape[1]))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        weight = tf.get_variable('w', [shape[1], output_size], tf.float32, tf.truncated_normal_initializer(stddev=stddev))
        if with_biases:
            bias = tf.get_variable('b', [output_size], initializer=tf.constant_initializer(bias_start))
        if spectral_normed:
            mul = tf.matmul(input_, spectral_normed_weight(weight, update_collection=update_collection))
        else:
            mul = tf.matmul(input_, weight)
        if mhe:
            eps = 0.0001
            filt = weight
            filt_num = filt.get_shape().as_list()[(- 1)]
            filt = tf.concat([filt, (- filt)], axis=0)
            filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
            filt /= filt_norm
            inner_pro = tf.matmul(tf.transpose(filt), filt)
            cross_terms = (2.0 - (2.0 * inner_pro))
            cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(filt_num)))
            loss = ((- 1e-07) * tf.reduce_mean(tf.log((cross_terms + eps))))
            if (net_type == 'g'):
                tf.add_to_collection('g_mhe_loss', loss)
            elif (net_type == 'd'):
                tf.add_to_collection('d_mhe_loss', loss)
            else:
                raise
        if with_w:
            if with_biases:
                return ((mul + bias), weight, bias)
            else:
                return (mul, weight, None)
        elif with_biases:
            return (mul + bias)
        else:
            return mul

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :
             ...  = tf.concat

idx = 22:------------------- similar code ------------------ index = 40, score = 6.0 
def STEmbedding(SE, TE, T, D, bn, bn_decay, is_training):
    '\n    spatio-temporal embedding\n    SE:     [N, D]\n    TE:     [batch_size, P + Q, 2] (dayofweek, timeofday)\n    T:      num of time steps in one day\n    D:      output dims\n    retrun: [batch_size, P + Q, N, D]\n    '
    SE = tf.expand_dims(tf.expand_dims(SE, axis=0), axis=0)
    SE = FC(SE, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    dayofweek = tf.one_hot(TE[(..., 0)], depth=7)
    timeofday = tf.one_hot(TE[(..., 1)], depth=T)
    TE = tf.concat((dayofweek, timeofday), axis=(- 1))
    TE = tf.expand_dims(TE, axis=2)
    TE = FC(TE, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    return tf.add(SE, TE)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 23:------------------- similar code ------------------ index = 37, score = 6.0 
def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):
    'Return a 3x3 transformmatrix which transforms indicies of original images\n  '
    rotation = ((math.pi * rotation) / 180.0)
    shear = ((math.pi * shear) / 180.0)
    c1 = tf.math.cos(rotation)
    s1 = tf.math.sin(rotation)
    rotation_matrix = tf.reshape(tf.concat([c1, s1, [0], (- s1), c1, [0], [0], [0], [1]], axis=0), [3, 3])
    c2 = tf.math.cos(shear)
    s2 = tf.math.sin(shear)
    shear_matrix = tf.reshape(tf.concat([[1], s2, [0], [0], c2, [0], [0], [0], [1]], axis=0), [3, 3])
    zoom_matrix = tf.reshape(tf.concat([([1] / height_zoom), [0], [0], [0], ([1] / width_zoom), [0], [0], [0], [1]], axis=0), [3, 3])
    shift_matrix = tf.reshape(tf.concat([[1], [0], height_shift, [0], [1], width_shift, [0], [0], [1]], axis=0), [3, 3])
    return tf.matmul(tf.matmul(rotation_matrix, shear_matrix), tf.matmul(zoom_matrix, shift_matrix))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... (tf.concat,)

idx = 24:------------------- similar code ------------------ index = 36, score = 6.0 
def train():
    with tf.Graph().as_default():
        with tf.device('/cpu:0'):
            (pointclouds_pl, labels_pl) = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)
            is_training_pl = tf.placeholder(tf.bool, shape=())
            batch = tf.get_variable('batch', [], initializer=tf.constant_initializer(0), trainable=False)
            bn_decay = get_bn_decay(batch)
            tf.summary.scalar('bn_decay', bn_decay)
            learning_rate = get_learning_rate(batch)
            tf.summary.scalar('learning_rate', learning_rate)
            if (OPTIMIZER == 'momentum'):
                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)
            elif (OPTIMIZER == 'adam'):
                optimizer = tf.train.AdamOptimizer(learning_rate)
            MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)
            tower_grads = []
            pred_gpu = []
            total_loss_gpu = []
            for i in range(NUM_GPUS):
                with tf.variable_scope(tf.get_variable_scope(), reuse=True):
                    with tf.device(('/gpu:%d' % i)), tf.name_scope(('gpu_%d' % i)) as scope:
                        pc_batch = tf.slice(pointclouds_pl, [(i * DEVICE_BATCH_SIZE), 0, 0], [DEVICE_BATCH_SIZE, (- 1), (- 1)])
                        label_batch = tf.slice(labels_pl, [(i * DEVICE_BATCH_SIZE)], [DEVICE_BATCH_SIZE])
                        (pred, end_points) = MODEL.get_model(pc_batch, is_training=is_training_pl, bn_decay=bn_decay)
                        MODEL.get_loss(pred, label_batch, end_points)
                        losses = tf.get_collection('losses', scope)
                        total_loss = tf.add_n(losses, name='total_loss')
                        for l in (losses + [total_loss]):
                            tf.summary.scalar(l.op.name, l)
                        grads = optimizer.compute_gradients(total_loss)
                        tower_grads.append(grads)
                        pred_gpu.append(pred)
                        total_loss_gpu.append(total_loss)
            pred = tf.concat(pred_gpu, 0)
            total_loss = tf.reduce_mean(total_loss_gpu)
            grads = average_gradients(tower_grads)
            train_op = optimizer.apply_gradients(grads, global_step=batch)
            correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))
            accuracy = (tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE))
            tf.summary.scalar('accuracy', accuracy)
        saver = tf.train.Saver()
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        config.allow_soft_placement = True
        config.log_device_placement = False
        sess = tf.Session(config=config)
        merged = tf.summary.merge_all()
        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph)
        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph)
        init = tf.global_variables_initializer()
        sess.run(init)
        ops = {'pointclouds_pl': pointclouds_pl, 'labels_pl': labels_pl, 'is_training_pl': is_training_pl, 'pred': pred, 'loss': total_loss, 'train_op': train_op, 'merged': merged, 'step': batch, 'end_points': end_points}
        best_acc = (- 1)
        for epoch in range(MAX_EPOCH):
            log_string(('**** EPOCH %03d ****' % epoch))
            sys.stdout.flush()
            train_one_epoch(sess, ops, train_writer)
            eval_one_epoch(sess, ops, test_writer)
            if ((epoch % 10) == 0):
                save_path = saver.save(sess, os.path.join(LOG_DIR, 'model.ckpt'))
                log_string(('Model saved in file: %s' % save_path))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        with:
             ...  = tf.concat

idx = 25:------------------- similar code ------------------ index = 34, score = 6.0 
def deconv2d(input_, output_shape, k_h=4, k_w=4, d_h=2, d_w=2, stddev=None, name='deconv2d', spectral_normed=False, update_collection=None, with_w=False, padding='SAME', mhe=False, net_type='g'):
    fan_in = ((k_h * k_w) * input_.get_shape().as_list()[(- 1)])
    fan_out = ((k_h * k_w) * output_shape[(- 1)])
    if (stddev is None):
        stddev = np.sqrt((2.0 / fan_in))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        w = tf.get_variable('w', [k_h, k_w, output_shape[(- 1)], input_.get_shape()[(- 1)]], initializer=tf.truncated_normal_initializer(stddev=stddev))
        if spectral_normed:
            deconv = tf.nn.conv2d_transpose(input_, spectral_normed_weight(w, update_collection=update_collection), output_shape=output_shape, strides=[1, d_h, d_w, 1], padding=padding)
        else:
            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1], padding=padding)
            if mhe:
                eps = 0.0001
                filt = w
                filt_num = input_.get_shape().as_list()[(- 1)]
                filt = tf.reshape(filt, [(- 1), filt_num])
                filt = tf.concat([filt, (- filt)], axis=0)
                filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
                filt /= filt_norm
                inner_pro = tf.matmul(tf.transpose(filt), filt)
                cross_terms = (2.0 - (2.0 * inner_pro))
                cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(filt_num)))
                loss = ((- 1e-07) * tf.reduce_mean(tf.log((cross_terms + eps))))
                if (net_type == 'g'):
                    tf.add_to_collection('g_mhe_loss', loss)
                else:
                    raise
        biases = tf.get_variable('b', [output_shape[(- 1)]], initializer=tf.constant_initializer(0))
        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())
        if with_w:
            return (deconv, w, biases)
        else:
            return deconv

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :        else:
            if  ... :
                 ...  = tf.concat

idx = 26:------------------- similar code ------------------ index = 30, score = 6.0 
def model_fn(features, labels, mode, params):
    is_training = (mode == tf.estimator.ModeKeys.TRAIN)
    is_validation = (mode == tf.estimator.ModeKeys.EVAL)
    is_prediction = (mode == tf.estimator.ModeKeys.PREDICT)
    embedding = Embedding(params.num_symbols, embedding_dim=params.embedding_dim)
    if params.use_accent_type:
        accent_embedding = Embedding(params.num_accent_type, embedding_dim=params.accent_type_embedding_dim, index_offset=params.accent_type_offset)
    encoder = encoder_factory(params, is_training)
    assert (params.decoder in ['DualSourceDecoder', 'DualSourceTransformerDecoder'])
    decoder = decoder_factory(params)
    assert (not (params.use_speaker_embedding and params.use_external_speaker_embedding))
    if params.use_speaker_embedding:
        speaker_embedding = Embedding(params.num_speakers, embedding_dim=params.speaker_embedding_dim, index_offset=params.speaker_embedding_offset)
    elif params.use_external_speaker_embedding:
        speaker_embedding = ExternalEmbedding(params.embedding_file, params.num_speakers, embedding_dim=params.speaker_embedding_dim, index_offset=params.speaker_embedding_offset)
    if (params.speaker_embedding_projection_out_dim > (- 1)):

        def _compose(f, g):
            return (lambda arg, *args, **kwargs: f(g(arg, *args, **kwargs)))
        resize = tf.layers.Dense(params.speaker_embedding_projection_out_dim, activation=tf.nn.relu)
        speaker_embedding = _compose(resize, speaker_embedding)
    if params.use_language_embedding:
        language_embedding = ExternalEmbedding(params.language_embedding_file, params.num_speakers, embedding_dim=params.language_embedding_dim, index_offset=params.speaker_embedding_offset)
    if (params.language_embedding_projection_out_dim > (- 1)):

        def _compose(f, g):
            return (lambda arg, *args, **kwargs: f(g(arg, *args, **kwargs)))
        resize = tf.layers.Dense(params.language_embedding_projection_out_dim, activation=tf.nn.relu)
        language_embedding = _compose(resize, language_embedding)
    if params.channel_id_to_postnet:
        channel_code = ExternalEmbedding(params.channel_id_file, params.num_speakers, embedding_dim=params.channel_id_dim, index_offset=params.speaker_embedding_offset)
    target = (labels.mel if (is_training or is_validation) else features.mel)
    x = params.speaker_for_synthesis
    if (x > (- 1)):
        speaker_embedding_output = speaker_embedding(x)
    else:
        speaker_embedding_output = (speaker_embedding(features.speaker_id) if (params.use_speaker_embedding or params.use_external_speaker_embedding) else None)
    if (x > (- 1)):
        language_embedding_output = language_embedding(x)
    else:
        language_embedding_output = (language_embedding(features.speaker_id) if params.use_language_embedding else None)
    channel_code_output = (channel_code(features.speaker_id) if params.channel_id_to_postnet else None)
    embedding_output = embedding(features.source)
    if params.language_embedd_to_input:
        language_embedd_input_projection_layer = tf.layers.Dense(params.embedding_dim)
        language_embedd_input_projected = language_embedd_input_projection_layer(language_embedding_output)
        expand_language_embedding_input = tf.tile(tf.expand_dims(language_embedd_input_projected, axis=1), [1, tf.shape(embedding_output)[1], 1])
        embedding_output = (embedding_output + expand_language_embedding_input)
    (encoder_lstm_output, encoder_self_attention_output, self_attention_alignment) = (encoder((embedding_output, accent_embedding(features.accent_type)), input_lengths=features.source_length) if params.use_accent_type else encoder(embedding_output, input_lengths=features.source_length))
    if params.speaker_embedd_to_decoder:
        expand_speaker_embedding_output = tf.tile(tf.expand_dims(speaker_embedding_output, axis=1), [1, tf.shape(encoder_lstm_output)[1], 1])
        encoder_lstm_output = tf.concat((encoder_lstm_output, expand_speaker_embedding_output), axis=(- 1))
        encoder_self_attention_output = tf.concat((encoder_self_attention_output, expand_speaker_embedding_output), axis=(- 1))
    if params.language_embedd_to_decoder:
        expand_language_embedding_output = tf.tile(tf.expand_dims(language_embedding_output, axis=1), [1, tf.shape(encoder_lstm_output)[1], 1])
        encoder_lstm_output = tf.concat((encoder_lstm_output, expand_language_embedding_output), axis=(- 1))
        encoder_self_attention_output = tf.concat((encoder_self_attention_output, expand_language_embedding_output), axis=(- 1))
    (attention1_fn, attention2_fn) = dual_source_attention_factory(params)
    (mel_output, stop_token, decoder_state) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=(is_validation or params.use_forced_alignment_mode), teacher_forcing=params.use_forced_alignment_mode, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=(labels.target_length if is_training else None), target=target, apply_dropout_on_inference=params.apply_dropout_on_inference)
    self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in self_attention_alignment]
    if ((params.decoder == 'DualSourceTransformerDecoder') and (not is_training)):
        decoder_rnn_state = decoder_state.rnn_state.rnn_state[0]
        alignment1 = tf.transpose(decoder_rnn_state.alignment_history[0].stack(), [1, 2, 0])
        alignment2 = tf.transpose(decoder_rnn_state.alignment_history[1].stack(), [1, 2, 0])
        decoder_self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in decoder_state.alignments]
    else:
        decoder_rnn_state = decoder_state[0]
        alignment1 = tf.transpose(decoder_rnn_state.alignment_history[0].stack(), [1, 2, 0])
        alignment2 = tf.transpose(decoder_rnn_state.alignment_history[1].stack(), [1, 2, 0])
        decoder_self_attention_alignment = []
    if params.use_forced_alignment_mode:
        (attention1_fn, attention2_fn) = force_alignment_dual_source_attention_factory(params)
        (mel_output, stop_token, decoder_state) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=True, teacher_forcing=False, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=(labels.target_length if is_training else None), target=target, teacher_alignments=(tf.transpose(alignment1, perm=[0, 2, 1]), tf.transpose(alignment2, perm=[0, 2, 1])), apply_dropout_on_inference=params.apply_dropout_on_inference)
        if ((params.decoder == 'DualSourceTransformerDecoder') and (not is_training)):
            alignment1 = tf.transpose(decoder_state.rnn_state.rnn_state[0].alignment_history[0].stack(), [1, 2, 0])
            alignment2 = tf.transpose(decoder_state.rnn_state.rnn_state[0].alignment_history[1].stack(), [1, 2, 0])
            decoder_self_attention_alignment = [tf.transpose(a, perm=[0, 2, 1]) for a in decoder_state.alignments]
        else:
            alignment1 = tf.transpose(decoder_state[0].alignment_history[0].stack(), [1, 2, 0])
            alignment2 = tf.transpose(decoder_state[0].alignment_history[1].stack(), [1, 2, 0])
            decoder_self_attention_alignment = []
    if params.use_postnet_v2:
        postnet = (MultiSpeakerPostNet(out_units=params.num_mels, speaker_embed=speaker_embedding_output, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate) if params.speaker_embedd_to_postnet else (ChannelEncoderPostNet(out_units=params.num_mels, channel_code=channel_code_output, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate) if params.channel_id_to_postnet else PostNetV2(out_units=params.num_mels, num_postnet_layers=params.num_postnet_v2_layers, kernel_size=params.postnet_v2_kernel_size, out_channels=params.postnet_v2_out_channels, is_training=is_training, drop_rate=params.postnet_v2_drop_rate)))
        postnet_v2_mel_output = postnet(mel_output)
    global_step = tf.train.get_global_step()
    if (mode is not tf.estimator.ModeKeys.PREDICT):
        mel_loss = spec_loss(mel_output, labels.mel, labels.spec_loss_mask, params.spec_loss_type)
        done_loss = binary_loss(stop_token, labels.done, labels.binary_loss_mask)
        blacklist = ['embedding', 'bias', 'batch_normalization', 'output_projection_wrapper/kernel', 'lstm_cell', 'output_and_stop_token_wrapper/dense/', 'output_and_stop_token_wrapper/dense_1/', 'stop_token_projection/kernel']
        regularization_loss = (l2_regularization_loss(tf.trainable_variables(), params.l2_regularization_weight, blacklist) if params.use_l2_regularization else 0)
        postnet_v2_mel_loss = (spec_loss(postnet_v2_mel_output, labels.mel, labels.spec_loss_mask, params.spec_loss_type) if params.use_postnet_v2 else 0)
        loss = (((mel_loss + done_loss) + regularization_loss) + postnet_v2_mel_loss)
    if is_training:
        lr = (self.learning_rate_decay(params.initial_learning_rate, global_step, params.learning_rate_step_factor) if params.decay_learning_rate else tf.convert_to_tensor(params.initial_learning_rate))
        optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1=params.adam_beta1, beta2=params.adam_beta2, epsilon=params.adam_eps)
        (gradients, variables) = zip(*optimizer.compute_gradients(loss))
        (clipped_gradients, _) = tf.clip_by_global_norm(gradients, 1.0)
        self.add_training_stats(loss, mel_loss, done_loss, lr, postnet_v2_mel_loss, regularization_loss)
        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
            train_op = optimizer.apply_gradients(zip(clipped_gradients, variables), global_step=global_step)
            summary_writer = tf.summary.FileWriter(model_dir)
            alignment_saver = MetricsSaver(([alignment1, alignment2] + self_attention_alignment), global_step, mel_output, labels.mel, labels.target_length, features.id, features.text, params.alignment_save_steps, mode, summary_writer, save_training_time_metrics=params.save_training_time_metrics, keep_eval_results_max_epoch=params.keep_eval_results_max_epoch)
            hooks = [alignment_saver]
            if params.record_profile:
                profileHook = tf.train.ProfilerHook(save_steps=params.profile_steps, output_dir=model_dir, show_dataflow=True, show_memory=True)
                hooks.append(profileHook)
            return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_hooks=hooks)
    if is_validation:
        (attention1_fn, attention2_fn) = dual_source_attention_factory(params)
        (mel_output_with_teacher, stop_token_with_teacher, decoder_state_with_teacher) = decoder((encoder_lstm_output, encoder_self_attention_output), attention1_fn=attention1_fn, attention2_fn=attention2_fn, speaker_embed=(speaker_embedding_output if params.speaker_embedd_to_prenet else None), is_training=is_training, is_validation=is_validation, memory_sequence_length=features.source_length, memory2_sequence_length=features.source_length, target_sequence_length=labels.target_length, target=target, teacher_forcing=True, apply_dropout_on_inference=params.apply_dropout_on_inference)
        if params.use_postnet_v2:
            postnet_v2_mel_output_with_teacher = postnet(mel_output_with_teacher)
        mel_loss_with_teacher = spec_loss(mel_output_with_teacher, labels.mel, labels.spec_loss_mask, params.spec_loss_type)
        done_loss_with_teacher = binary_loss(stop_token_with_teacher, labels.done, labels.binary_loss_mask)
        postnet_v2_mel_loss_with_teacher = (spec_loss(postnet_v2_mel_output_with_teacher, labels.mel, labels.spec_loss_mask, params.spec_loss_type) if params.use_postnet_v2 else 0)
        loss_with_teacher = (((mel_loss_with_teacher + done_loss_with_teacher) + regularization_loss) + postnet_v2_mel_loss_with_teacher)
        eval_metric_ops = self.get_validation_metrics(mel_loss, done_loss, postnet_v2_mel_loss, loss_with_teacher, mel_loss_with_teacher, done_loss_with_teacher, postnet_v2_mel_loss_with_teacher, regularization_loss)
        summary_writer = tf.summary.FileWriter(model_dir)
        alignment_saver = MetricsSaver((([alignment1, alignment2] + self_attention_alignment) + decoder_self_attention_alignment), global_step, mel_output, labels.mel, labels.target_length, features.id, features.text, 1, mode, summary_writer, save_training_time_metrics=params.save_training_time_metrics, keep_eval_results_max_epoch=params.keep_eval_results_max_epoch)
        return tf.estimator.EstimatorSpec(mode, loss=loss, evaluation_hooks=[alignment_saver], eval_metric_ops=eval_metric_ops)
    if is_prediction:
        num_self_alignments = len(self_attention_alignment)
        num_decoder_self_alignments = len(decoder_self_attention_alignment)
        predictions = {'id': features.id, 'key': features.key, 'mel': mel_output, 'mel_postnet': (postnet_v2_mel_output if params.use_postnet_v2 else None), 'ground_truth_mel': features.mel, 'alignment': alignment1, 'alignment2': alignment2, 'alignment3': (decoder_self_attention_alignment[0] if (num_decoder_self_alignments >= 1) else None), 'alignment4': (decoder_self_attention_alignment[1] if (num_decoder_self_alignments >= 2) else None), 'alignment5': (self_attention_alignment[0] if (num_self_alignments >= 1) else None), 'alignment6': (self_attention_alignment[1] if (num_self_alignments >= 2) else None), 'alignment7': (self_attention_alignment[2] if (num_self_alignments >= 3) else None), 'alignment8': (self_attention_alignment[3] if (num_self_alignments >= 4) else None), 'source': features.source, 'text': features.text, 'accent_type': (features.accent_type if params.use_accent_type else None)}
        predictions = dict(filter((lambda xy: (xy[1] is not None)), predictions.items()))
        return tf.estimator.EstimatorSpec(mode, predictions=predictions)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = tf.concat

idx = 27:------------------- similar code ------------------ index = 24, score = 6.0 
def deconv2d(input_, output_shape, k_h=4, k_w=4, d_h=2, d_w=2, stddev=None, name='deconv2d', spectral_normed=False, update_collection=None, with_w=False, padding='SAME', mhe=False, net_type='g'):
    fan_in = ((k_h * k_w) * input_.get_shape().as_list()[(- 1)])
    fan_out = ((k_h * k_w) * output_shape[(- 1)])
    if (stddev is None):
        stddev = np.sqrt((2.0 / fan_in))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        w = tf.get_variable('w', [k_h, k_w, output_shape[(- 1)], input_.get_shape()[(- 1)]], initializer=tf.truncated_normal_initializer(stddev=stddev))
        if spectral_normed:
            deconv = tf.nn.conv2d_transpose(input_, spectral_normed_weight(w, update_collection=update_collection), output_shape=output_shape, strides=[1, d_h, d_w, 1], padding=padding)
        else:
            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1], padding=padding)
            if mhe:
                eps = 0.0001
                filt = w
                filt_num = input_.get_shape().as_list()[(- 1)]
                filt = tf.reshape(filt, [(- 1), filt_num])
                filt = tf.concat([filt, (- filt)], axis=0)
                filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
                filt /= filt_norm
                inner_pro = tf.matmul(tf.transpose(filt), filt)
                cross_terms = (2.0 - (2.0 * inner_pro))
                cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(filt_num)))
                loss = ((- 1e-06) * tf.reduce_mean(tf.log((cross_terms + eps))))
                if (net_type == 'g'):
                    tf.add_to_collection('g_mhe_loss', loss)
                else:
                    raise
        biases = tf.get_variable('b', [output_shape[(- 1)]], initializer=tf.constant_initializer(0))
        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())
        if with_w:
            return (deconv, w, biases)
        else:
            return deconv

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :        else:
            if  ... :
                 ...  = tf.concat

idx = 28:------------------- similar code ------------------ index = 79, score = 6.0 
def conv2d(input_, output_dim, k_h=4, k_w=4, d_h=2, d_w=2, stddev=None, name='conv2d', spectral_normed=False, update_collection=None, with_w=False, padding='SAME', mhe=False, net_type='d'):
    fan_in = ((k_h * k_w) * input_.get_shape().as_list()[(- 1)])
    fan_out = ((k_h * k_w) * output_dim)
    if (stddev is None):
        stddev = np.sqrt((2.0 / fan_in))
    with tf.variable_scope(name) as scope:
        if scope_has_variables(scope):
            scope.reuse_variables()
        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[(- 1)], output_dim], initializer=tf.truncated_normal_initializer(stddev=stddev))
        if spectral_normed:
            conv = tf.nn.conv2d(input_, spectral_normed_weight(w, update_collection=update_collection), strides=[1, d_h, d_w, 1], padding=padding)
        else:
            conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=padding)
        if mhe:
            print(('mhe on %s' % name))
            eps = 0.0001
            filt = w
            filt = tf.reshape(filt, [(- 1), output_dim])
            filt = tf.concat([filt, (- filt)], axis=0)
            filt_norm = tf.sqrt((tf.reduce_sum((filt * filt), [0], keep_dims=True) + eps))
            filt /= filt_norm
            inner_pro = tf.matmul(tf.transpose(filt), filt)
            cross_terms = (2.0 - (2.0 * inner_pro))
            cross_terms = (tf.matrix_band_part(cross_terms, 0, (- 1)) * (1.0 - np.eye(output_dim)))
            loss = ((- 1e-06) * tf.reduce_mean(tf.log((cross_terms + eps))))
            if (net_type == 'g'):
                tf.add_to_collection('g_mhe_loss', loss)
            elif (net_type == 'd'):
                tf.add_to_collection('d_mhe_loss', loss)
            else:
                raise
        biases = tf.get_variable('b', [output_dim], initializer=tf.constant_initializer(0.0))
        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())
        if with_w:
            return (conv, w, biases)
        else:
            return conv

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if  ... :
             ...  = tf.concat

idx = 29:------------------- similar code ------------------ index = 82, score = 6.0 
def build(self):
    'Build model.'
    (input_left, input_right) = self._make_inputs()
    len_left = input_left.shape[1]
    len_right = input_right.shape[1]
    embedding = self._make_embedding_layer()
    embed_left = embedding(input_left)
    embed_right = embedding(input_right)
    lstm_left = keras.layers.LSTM(self._params['lstm_num_units'], return_sequences=True, name='lstm_left')
    lstm_right = keras.layers.LSTM(self._params['lstm_num_units'], return_sequences=True, name='lstm_right')
    encoded_left = lstm_left(embed_left)
    encoded_right = lstm_right(embed_right)

    def attention(tensors):
        'Attention layer.'
        (left, right) = tensors
        tensor_left = tf.expand_dims(left, axis=2)
        tensor_right = tf.expand_dims(right, axis=1)
        tensor_left = K.repeat_elements(tensor_left, len_right, 2)
        tensor_right = K.repeat_elements(tensor_right, len_left, 1)
        tensor_merged = tf.concat([tensor_left, tensor_right], axis=(- 1))
        middle_output = keras.layers.Dense(self._params['fc_num_units'], activation='tanh')(tensor_merged)
        attn_scores = keras.layers.Dense(1)(middle_output)
        attn_scores = tf.squeeze(attn_scores, axis=3)
        exp_attn_scores = tf.math.exp((attn_scores - tf.reduce_max(attn_scores, axis=(- 1), keepdims=True)))
        exp_sum = tf.reduce_sum(exp_attn_scores, axis=(- 1), keepdims=True)
        attention_weights = (exp_attn_scores / exp_sum)
        return K.batch_dot(attention_weights, right)
    attn_layer = keras.layers.Lambda(attention)
    left_attn_vec = attn_layer([encoded_left, encoded_right])
    concat = keras.layers.Concatenate(axis=1)([left_attn_vec, encoded_right])
    lstm_merge = keras.layers.LSTM((self._params['lstm_num_units'] * 2), return_sequences=False, name='lstm_merge')
    merged = lstm_merge(concat)
    dropout = keras.layers.Dropout(rate=self._params['dropout_rate'])(merged)
    phi = keras.layers.Dense(self._params['fc_num_units'], activation='tanh')(dropout)
    inputs = [input_left, input_right]
    out = self._make_output_layer()(phi)
    self._backend = keras.Model(inputs=inputs, outputs=[out])

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    def  ... ( ... ):
         ...  = tf.concat

idx = 30:------------------- similar code ------------------ index = 80, score = 6.0 
def tf_batch_histogram(values, value_range, axis, nbins=100, dtype=tf.int32, use_map=True):
    '\n    Computes histogram with fixed width considering batch dimensions\n    :param values: Numeric `Tensor` containing the values for histogram computation.\n    :param value_range: Shape [2] `Tensor` of same `dtype` as `values`. values <= value_range[0] will be mapped to\n    hist[0], values >= value_range[1] will be mapped to hist[-1].\n    :param axis: Number of batch dimensions. First axis to apply histogram computation to.\n    :param nbins: Scalar `int32 Tensor`. Number of histogram bins.\n    :param dtype: dtype for returned histogram, can be either tf.int32 or tf.int64.\n    :return: histogram with batch dimensions.\n    '
    values_shape = tf.shape(values)
    batch_dim = values_shape[:axis]
    rest_dim = values_shape[axis:]
    num_batch = tf.reduce_prod(batch_dim)
    if use_map:
        values_reshaped = tf.reshape(values, tf.concat([[num_batch], rest_dim], 0))
        hist = tf.map_fn((lambda x: tf.histogram_fixed_width(x, value_range, nbins=nbins, dtype=dtype)), values_reshaped, dtype=dtype, parallel_iterations=64)
    else:
        values_float = tf.cast(values, tf.float32)
        value_range_float = tf.cast(value_range, tf.float32)
        values_norm = ((values_float - value_range_float[0]) / (value_range_float[1] - value_range_float[0]))
        values_clip1 = tf.maximum(values_norm, (0.5 / tf.cast(nbins, tf.float32)))
        values_clip2 = tf.minimum(values_clip1, (1.0 - (0.5 / tf.cast(nbins, tf.float32))))
        values_shift = (values_clip2 + tf.reshape(tf.range(tf.cast(num_batch, tf.float32), dtype=tf.float32), tf.concat([batch_dim, tf.ones(tf.size(rest_dim), tf.int32)], 0)))
        hist = tf.histogram_fixed_width(values_shift, [0.0, tf.cast(num_batch, tf.float32)], nbins=(num_batch * nbins), dtype=dtype)
    return tf.reshape(hist, tf.concat([batch_dim, [nbins]], 0))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if  ... :
         ...  =  ... . ... ( ... , tf.concat)

idx = 31:------------------- similar code ------------------ index = 22, score = 5.0 
@slim.add_arg_scope
def _conv_block(inputs, num_filters, data_format='NHWC', scope=None, outputs_collections=None):
    with tf.variable_scope(scope, 'conv_blockx', [inputs]) as sc:
        net = inputs
        net = _conv(net, (num_filters * 4), 1, scope='x1')
        net = _conv(net, num_filters, 3, scope='x2')
        if (data_format == 'NHWC'):
            net = tf.concat([inputs, net], axis=3)
        else:
            net = tf.concat([inputs, net], axis=1)
        net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)
    return net

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if:
             ...  = tf.concat

idx = 32:------------------- similar code ------------------ index = 18, score = 5.0 
@staticmethod
def _prepare_target(target, hparams):

    def convert(target: PreprocessedMelData):
        r = hparams.outputs_per_step
        mel_normalized = ((target.mel - np.array(hparams.average_mel_level_db, dtype=np.float32)) / np.array(hparams.stddev_mel_level_db, dtype=np.float32))
        mel_with_silence = tf.pad(mel_normalized, paddings=[[r, r], [0, 0]], constant_values=hparams.silence_mel_level_db)
        target_length = (target.target_length + (2 * r))
        padded_target_length = (((target_length // r) + 1) * r)

        def padding_function(t):
            tail_padding = (padded_target_length - target_length)
            padding_shape = tf.sparse_tensor_to_dense(tf.SparseTensor(indices=[(0, 1)], values=tf.expand_dims(tail_padding, axis=0), dense_shape=(2, 2)))
            return (lambda : tf.pad(t, paddings=padding_shape, constant_values=hparams.silence_mel_level_db))
        no_padding_condition = tf.equal(tf.to_int64(0), (target_length % r))
        mel = tf.cond(no_padding_condition, (lambda : mel_with_silence), padding_function(mel_with_silence))
        mel.set_shape((None, hparams.num_mels))
        padded_target_length = tf.cond(no_padding_condition, (lambda : target_length), (lambda : padded_target_length))
        done = tf.concat([tf.zeros(((padded_target_length // r) - 1), dtype=tf.float32), tf.ones(1, dtype=tf.float32)], axis=0)
        spec_loss_mask = tf.ones(shape=padded_target_length, dtype=tf.float32)
        binary_loss_mask = tf.ones(shape=(padded_target_length // r), dtype=tf.float32)
        return MelData(target.id, target.key, mel, target.mel_width, padded_target_length, done, spec_loss_mask, binary_loss_mask)
    return DatasetSource._decode_target(target).map((lambda inputs: convert(inputs)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():

    def  ... ():
         ...  = tf.concat

idx = 33:------------------- similar code ------------------ index = 21, score = 5.0 
def build_network(self, features, is_training=None):
    '\n\n        :param features:\n        :param is_training:\n        :return:\n        '
    deep_part = self.deep_block(features, is_training=is_training)
    attention_part = self.attention_block(features, is_training=is_training)
    logit = tf.concat([attention_part, deep_part], (- 1))
    return logit

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 34:------------------- similar code ------------------ index = 88, score = 5.0 
def get_model(point_cloud, is_training, num_class, bn_decay=None):
    ' Semantic segmentation PointNet, input is BxNx5, output Bxnum_class '
    batch_size = point_cloud.get_shape()[0].value
    num_point = point_cloud.get_shape()[1].value
    end_points = {}
    l0_xyz = tf.slice(point_cloud, [0, 0, 0], [(- 1), (- 1), 3])
    l0_points = tf.slice(point_cloud, [0, 0, 3], [(- 1), (- 1), 2])
    end_points['l0_xyz'] = l0_xyz
    (l1_xyz, l1_points, l1_indices) = pointnet_sa_module(l0_xyz, l0_points, npoint=1024, radius=0.1, nsample=32, mlp=[32, 32, 64], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer1')
    (l2_xyz, l2_points, l2_indices) = pointnet_sa_module(l1_xyz, l1_points, npoint=256, radius=0.2, nsample=32, mlp=[64, 64, 128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer2')
    (l3_xyz, l3_points, l3_indices) = pointnet_sa_module(l2_xyz, l2_points, npoint=64, radius=0.4, nsample=32, mlp=[128, 128, 256], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer3')
    (l4_xyz, l4_points, l4_indices) = pointnet_sa_module(l3_xyz, l3_points, npoint=16, radius=0.8, nsample=32, mlp=[256, 256, 512], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer4')
    l3_points = pointnet_fp_module(l3_xyz, l4_xyz, l3_points, l4_points, [256, 256], is_training, bn_decay, scope='fa_layer1')
    l2_points = pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256, 256], is_training, bn_decay, scope='fa_layer2')
    l1_points = pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256, 128], is_training, bn_decay, scope='fa_layer3')
    l0_points = pointnet_fp_module(l0_xyz, l1_xyz, tf.concat([l0_xyz, l0_points], axis=(- 1)), l1_points, [128, 128, 128], is_training, bn_decay, scope='fa_layer4')
    net = tf_util.conv1d(l0_points, 128, 1, padding='VALID', bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay)
    end_points['feats'] = net
    net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp1')
    net = tf_util.conv1d(net, num_class, 1, padding='VALID', activation_fn=None, scope='fc2')
    return (net, end_points)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... ( ... ,  ... , tf.concat,  ... ,,  ... ,  ... ,)

idx = 35:------------------- similar code ------------------ index = 23, score = 5.0 
def build_model(self):
    self.ema = tf.train.ExponentialMovingAverage(decay=self.ema_decay)
    if (self.phase == 'train'):
        ' Input Image'
        img_class = Image_data(self.img_height, self.img_width, self.img_ch, self.dataset_path, self.label_list, self.augment_flag)
        img_class.preprocess()
        dataset_num = len(img_class.image)
        print('Dataset number : ', dataset_num)
        self.lr = tf.placeholder(tf.float32, name='learning_rate')
        self.ds_weight_placeholder = tf.placeholder(tf.float32, name='ds_weight')
        img_and_label = tf.data.Dataset.from_tensor_slices((img_class.image, img_class.label))
        gpu_device = '/gpu:0'
        img_and_label = img_and_label.apply(shuffle_and_repeat(dataset_num)).apply(map_and_batch(img_class.image_processing, (self.batch_size * self.gpu_num), num_parallel_batches=16, drop_remainder=True)).apply(prefetch_to_device(gpu_device, None))
        img_and_label_iterator = img_and_label.make_one_shot_iterator()
        (self.x_real, label_org) = img_and_label_iterator.get_next()
        label_trg = tf.random_uniform(shape=tf.shape(label_org), minval=0, maxval=self.c_dim, dtype=tf.int32)
        ' split '
        x_real_gpu_split = tf.split(self.x_real, num_or_size_splits=self.gpu_num, axis=0)
        label_org_gpu_split = tf.split(label_org, num_or_size_splits=self.gpu_num, axis=0)
        label_trg_gpu_split = tf.split(label_trg, num_or_size_splits=self.gpu_num, axis=0)
        g_adv_loss_per_gpu = []
        g_sty_recon_loss_per_gpu = []
        g_sty_diverse_loss_per_gpu = []
        g_cyc_loss_per_gpu = []
        g_loss_per_gpu = []
        d_adv_loss_per_gpu = []
        d_loss_per_gpu = []
        for gpu_id in range(self.gpu_num):
            with tf.device(tf.DeviceSpec(device_type='GPU', device_index=gpu_id)):
                with tf.variable_scope(tf.get_variable_scope(), reuse=(gpu_id > 0)):
                    x_real_split = tf.split(x_real_gpu_split[gpu_id], num_or_size_splits=self.batch_size, axis=0)
                    label_org_split = tf.split(label_org_gpu_split[gpu_id], num_or_size_splits=self.batch_size, axis=0)
                    label_trg_split = tf.split(label_trg_gpu_split[gpu_id], num_or_size_splits=self.batch_size, axis=0)
                    g_adv_loss = None
                    g_sty_recon_loss = None
                    g_sty_diverse_loss = None
                    g_cyc_loss = None
                    d_adv_loss = None
                    d_simple_gp = None
                    d_gp = None
                    for each_bs in range(self.batch_size):
                        ' Define Generator, Discriminator '
                        x_real_each = x_real_split[each_bs]
                        label_org_each = tf.squeeze(label_org_split[each_bs], axis=[0, 1])
                        label_trg_each = tf.squeeze(label_trg_split[each_bs], axis=[0, 1])
                        random_style_code = tf.random_normal(shape=[1, self.style_dim])
                        random_style_code_1 = tf.random_normal(shape=[1, self.style_dim])
                        random_style_code_2 = tf.random_normal(shape=[1, self.style_dim])
                        random_style = tf.gather(self.mapping_network(random_style_code), label_trg_each)
                        random_style_1 = tf.gather(self.mapping_network(random_style_code_1), label_trg_each)
                        random_style_2 = tf.gather(self.mapping_network(random_style_code_2), label_trg_each)
                        x_fake = self.generator(x_real_each, random_style)
                        x_fake_1 = self.generator(x_real_each, random_style_1)
                        x_fake_2 = self.generator(x_real_each, random_style_2)
                        x_real_each_style = tf.gather(self.style_encoder(x_real_each), label_org_each)
                        x_fake_style = tf.gather(self.style_encoder(x_fake), label_trg_each)
                        x_cycle = self.generator(x_fake, x_real_each_style)
                        real_logit = tf.gather(self.discriminator(x_real_each), label_org_each)
                        fake_logit = tf.gather(self.discriminator(x_fake), label_trg_each)
                        ' Define loss '
                        if (self.gan_type.__contains__('wgan') or (self.gan_type == 'dragan')):
                            GP = self.gradient_panalty(real=x_real_each, fake=x_fake, real_label=label_org_each)
                        else:
                            GP = tf.constant([0], tf.float32)
                        if (each_bs == 0):
                            g_adv_loss = (self.adv_weight * generator_loss(self.gan_type, fake_logit))
                            g_sty_recon_loss = (self.sty_weight * L1_loss(random_style, x_fake_style))
                            g_sty_diverse_loss = (self.ds_weight_placeholder * L1_loss(x_fake_1, x_fake_2))
                            g_cyc_loss = (self.cyc_weight * L1_loss(x_real_each, x_cycle))
                            d_adv_loss = (self.adv_weight * discriminator_loss(self.gan_type, real_logit, fake_logit))
                            d_simple_gp = (self.adv_weight * simple_gp(real_logit, fake_logit, x_real_each, x_fake, r1_gamma=self.r1_weight, r2_gamma=0.0))
                            d_gp = (self.adv_weight * GP)
                        else:
                            g_adv_loss = tf.concat([g_adv_loss, (self.adv_weight * generator_loss(self.gan_type, fake_logit))], axis=0)
                            g_sty_recon_loss = tf.concat([g_sty_recon_loss, (self.sty_weight * L1_loss(random_style, x_fake_style))], axis=0)
                            g_sty_diverse_loss = tf.concat([g_sty_diverse_loss, (self.ds_weight_placeholder * L1_loss(x_fake_1, x_fake_2))], axis=0)
                            g_cyc_loss = tf.concat([g_cyc_loss, (self.cyc_weight * L1_loss(x_real_each, x_cycle))], axis=0)
                            d_adv_loss = tf.concat([d_adv_loss, (self.adv_weight * discriminator_loss(self.gan_type, real_logit, fake_logit))], axis=0)
                            d_simple_gp = tf.concat([d_simple_gp, (self.adv_weight * simple_gp(real_logit, fake_logit, x_real_each, x_fake, r1_gamma=self.r1_weight, r2_gamma=0.0))], axis=0)
                            d_gp = tf.concat([d_gp, (self.adv_weight * GP)], axis=0)
                    g_adv_loss = tf.reduce_mean(g_adv_loss)
                    g_sty_recon_loss = tf.reduce_mean(g_sty_recon_loss)
                    g_sty_diverse_loss = tf.reduce_mean(g_sty_diverse_loss)
                    g_cyc_loss = tf.reduce_mean(g_cyc_loss)
                    d_adv_loss = tf.reduce_mean(d_adv_loss)
                    d_simple_gp = tf.reduce_mean(tf.reduce_sum(d_simple_gp, axis=[1, 2, 3]))
                    d_gp = tf.reduce_mean(d_gp)
                    g_loss = (((g_adv_loss + g_sty_recon_loss) - g_sty_diverse_loss) + g_cyc_loss)
                    d_loss = ((d_adv_loss + d_simple_gp) + d_gp)
                    g_adv_loss_per_gpu.append(g_adv_loss)
                    g_sty_recon_loss_per_gpu.append(g_sty_recon_loss)
                    g_sty_diverse_loss_per_gpu.append(g_sty_diverse_loss)
                    g_cyc_loss_per_gpu.append(g_cyc_loss)
                    d_adv_loss_per_gpu.append(d_adv_loss)
                    g_loss_per_gpu.append(g_loss)
                    d_loss_per_gpu.append(d_loss)
        g_adv_loss = tf.reduce_mean(g_adv_loss_per_gpu)
        g_sty_recon_loss = tf.reduce_mean(g_sty_recon_loss_per_gpu)
        g_sty_diverse_loss = tf.reduce_mean(g_sty_diverse_loss_per_gpu)
        g_cyc_loss = tf.reduce_mean(g_cyc_loss_per_gpu)
        self.g_loss = tf.reduce_mean(g_loss_per_gpu)
        d_adv_loss = tf.reduce_mean(d_adv_loss_per_gpu)
        self.d_loss = tf.reduce_mean(d_loss_per_gpu)
        ' Training '
        t_vars = tf.trainable_variables()
        G_vars = [var for var in t_vars if ('generator' in var.name)]
        E_vars = [var for var in t_vars if ('encoder' in var.name)]
        F_vars = [var for var in t_vars if ('mapping' in var.name)]
        D_vars = [var for var in t_vars if ('discriminator' in var.name)]
        if (self.gpu_num == 1):
            prev_g_optimizer = tf.train.AdamOptimizer(self.lr, beta1=0, beta2=0.99).minimize(self.g_loss, var_list=G_vars)
            prev_e_optimizer = tf.train.AdamOptimizer(self.lr, beta1=0, beta2=0.99).minimize(self.g_loss, var_list=E_vars)
            prev_f_optimizer = tf.train.AdamOptimizer((self.lr * 0.01), beta1=0, beta2=0.99).minimize(self.g_loss, var_list=F_vars)
            self.d_optimizer = tf.train.AdamOptimizer(self.lr, beta1=0, beta2=0.99).minimize(self.d_loss, var_list=D_vars)
        else:
            prev_g_optimizer = tf.train.AdamOptimizer(self.lr, beta1=0, beta2=0.99).minimize(self.g_loss, var_list=G_vars, colocate_gradients_with_ops=True)
            prev_e_optimizer = tf.train.AdamOptimizer(self.lr, beta1=0, beta2=0.99).minimize(self.g_loss, var_list=E_vars, colocate_gradients_with_ops=True)
            prev_f_optimizer = tf.train.AdamOptimizer((self.lr * 0.01), beta1=0, beta2=0.99).minimize(self.g_loss, var_list=F_vars, colocate_gradients_with_ops=True)
            self.d_optimizer = tf.train.AdamOptimizer(self.lr, beta1=0, beta2=0.99).minimize(self.d_loss, var_list=D_vars, colocate_gradients_with_ops=True)
        with tf.control_dependencies([prev_g_optimizer, prev_e_optimizer, prev_f_optimizer]):
            self.g_optimizer = self.ema.apply(G_vars)
            self.e_optimizer = self.ema.apply(E_vars)
            self.f_optimizer = self.ema.apply(F_vars)
        '" Summary '
        self.Generator_loss = tf.summary.scalar('g_loss', self.g_loss)
        self.Discriminator_loss = tf.summary.scalar('d_loss', self.d_loss)
        self.g_adv_loss = tf.summary.scalar('g_adv_loss', g_adv_loss)
        self.g_sty_recon_loss = tf.summary.scalar('g_sty_recon_loss', g_sty_recon_loss)
        self.g_sty_diverse_loss = tf.summary.scalar('g_sty_diverse_loss', g_sty_diverse_loss)
        self.g_cyc_loss = tf.summary.scalar('g_cyc_loss', g_cyc_loss)
        self.d_adv_loss = tf.summary.scalar('d_adv_loss', d_adv_loss)
        g_summary_list = [self.Generator_loss, self.g_adv_loss, self.g_sty_recon_loss, self.g_sty_diverse_loss, self.g_cyc_loss]
        d_summary_list = [self.Discriminator_loss, self.d_adv_loss]
        self.g_summary_loss = tf.summary.merge(g_summary_list)
        self.d_summary_loss = tf.summary.merge(d_summary_list)
        ' Result Image '

        def return_g_images(generator, image, code):
            x = generator(image, code)
            return x
        self.x_fake_list = []
        first_x_real = tf.expand_dims(self.x_real[0], axis=0)
        label_fix_list = tf.constant([idx for idx in range(self.c_dim)])
        for _ in range(self.num_style):
            random_style_code = tf.truncated_normal(shape=[1, self.style_dim])
            self.x_fake_list.append(tf.map_fn((lambda c: return_g_images(self.generator, first_x_real, tf.gather(self.mapping_network(random_style_code), c))), label_fix_list, dtype=tf.float32))
    elif (self.phase == 'refer_test'):
        ' Test '

        def return_g_images(generator, image, code):
            x = generator(image, code)
            return x
        self.custom_image = tf.placeholder(tf.float32, [1, self.img_height, self.img_width, self.img_ch], name='custom_image')
        self.refer_image = tf.placeholder(tf.float32, [1, self.img_height, self.img_width, self.img_ch], name='refer_image')
        label_fix_list = tf.constant([idx for idx in range(self.c_dim)])
        self.refer_fake_image = tf.map_fn((lambda c: return_g_images(self.generator, self.custom_image, tf.gather(self.style_encoder(self.refer_image), c))), label_fix_list, dtype=tf.float32)
    else:
        ' Test '

        def return_g_images(generator, image, code):
            x = generator(image, code)
            return x
        self.custom_image = tf.placeholder(tf.float32, [1, self.img_height, self.img_width, self.img_ch], name='custom_image')
        label_fix_list = tf.constant([idx for idx in range(self.c_dim)])
        random_style_code = tf.truncated_normal(shape=[1, self.style_dim])
        self.custom_fake_image = tf.map_fn((lambda c: return_g_images(self.generator, self.custom_image, tf.gather(self.mapping_network(random_style_code), c))), label_fix_list, dtype=tf.float32)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:
        for  ...  in:
            with:
                with:
                    for  ...  in:
                        if:                        else:
                             ...  = tf.concat

idx = 36:------------------- similar code ------------------ index = 25, score = 5.0 
def build_network(self, features, is_training=None):
    '\n        TODO\n\n        :param features:\n        :param is_training:\n        :return:\n        '
    (ev_list, sparse_ev_list, fv_list) = features
    din_part = self.din_block(sparse_ev_list[self.flags.candidate_item], sparse_ev_list[self.flags.history_item], is_training)
    deep_part = self.mlp_block(self.concat((ev_list + fv_list)))
    logit = tf.concat(values=[din_part, deep_part], axis=1)
    return logit

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 37:------------------- similar code ------------------ index = 26, score = 5.0 
def temporalAttention(X, STE, K, d, bn, bn_decay, is_training, mask=True):
    '\n    temporal attention mechanism\n    X:      [batch_size, num_step, N, D]\n    STE:    [batch_size, num_step, N, D]\n    K:      number of attention heads\n    d:      dimension of each attention outputs\n    return: [batch_size, num_step, N, D]\n    '
    D = (K * d)
    X = tf.concat((X, STE), axis=(- 1))
    query = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    key = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    value = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    query = tf.concat(tf.split(query, K, axis=(- 1)), axis=0)
    key = tf.concat(tf.split(key, K, axis=(- 1)), axis=0)
    value = tf.concat(tf.split(value, K, axis=(- 1)), axis=0)
    query = tf.transpose(query, perm=(0, 2, 1, 3))
    key = tf.transpose(key, perm=(0, 2, 3, 1))
    value = tf.transpose(value, perm=(0, 2, 1, 3))
    attention = tf.matmul(query, key)
    attention /= (d ** 0.5)
    if mask:
        batch_size = tf.shape(X)[0]
        num_step = X.get_shape()[1].value
        N = X.get_shape()[2].value
        mask = tf.ones(shape=(num_step, num_step))
        mask = tf.linalg.LinearOperatorLowerTriangular(mask).to_dense()
        mask = tf.expand_dims(tf.expand_dims(mask, axis=0), axis=0)
        mask = tf.tile(mask, multiples=((K * batch_size), N, 1, 1))
        mask = tf.cast(mask, dtype=tf.bool)
        attention = tf.compat.v2.where(condition=mask, x=attention, y=((- (2 ** 15)) + 1))
    attention = tf.nn.softmax(attention, axis=(- 1))
    X = tf.matmul(attention, value)
    X = tf.transpose(X, perm=(0, 2, 1, 3))
    X = tf.concat(tf.split(X, K, axis=0), axis=(- 1))
    X = FC(X, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    return X

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 38:------------------- similar code ------------------ index = 27, score = 5.0 
def transformAttention(X, STE_P, STE_Q, K, d, bn, bn_decay, is_training):
    '\n    transform attention mechanism\n    X:      [batch_size, P, N, D]\n    STE_P:  [batch_size, P, N, D]\n    STE_Q:  [batch_size, Q, N, D]\n    K:      number of attention heads\n    d:      dimension of each attention outputs\n    return: [batch_size, Q, N, D]\n    '
    D = (K * d)
    query = FC(STE_Q, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    key = FC(STE_P, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    value = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    query = tf.concat(tf.split(query, K, axis=(- 1)), axis=0)
    key = tf.concat(tf.split(key, K, axis=(- 1)), axis=0)
    value = tf.concat(tf.split(value, K, axis=(- 1)), axis=0)
    query = tf.transpose(query, perm=(0, 2, 1, 3))
    key = tf.transpose(key, perm=(0, 2, 3, 1))
    value = tf.transpose(value, perm=(0, 2, 1, 3))
    attention = tf.matmul(query, key)
    attention /= (d ** 0.5)
    attention = tf.nn.softmax(attention, axis=(- 1))
    X = tf.matmul(attention, value)
    X = tf.transpose(X, perm=(0, 2, 1, 3))
    X = tf.concat(tf.split(X, K, axis=0), axis=(- 1))
    X = FC(X, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    return X

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 39:------------------- similar code ------------------ index = 28, score = 5.0 
def convert(target: PreprocessedMelData):
    r = hparams.outputs_per_step
    mel_normalized = ((target.mel - np.array(hparams.average_mel_level_db, dtype=np.float32)) / np.array(hparams.stddev_mel_level_db, dtype=np.float32))
    mel_with_silence = tf.pad(mel_normalized, paddings=[[r, r], [0, 0]], constant_values=hparams.silence_mel_level_db)
    target_length = (target.target_length + (2 * r))
    padded_target_length = (((target_length // r) + 1) * r)

    def padding_function(t):
        tail_padding = (padded_target_length - target_length)
        padding_shape = tf.sparse_tensor_to_dense(tf.SparseTensor(indices=[(0, 1)], values=tf.expand_dims(tail_padding, axis=0), dense_shape=(2, 2)))
        return (lambda : tf.pad(t, paddings=padding_shape, constant_values=hparams.silence_mel_level_db))
    no_padding_condition = tf.equal(tf.to_int64(0), (target_length % r))
    mel = tf.cond(no_padding_condition, (lambda : mel_with_silence), padding_function(mel_with_silence))
    mel.set_shape((None, hparams.num_mels))
    padded_target_length = tf.cond(no_padding_condition, (lambda : target_length), (lambda : padded_target_length))
    done = tf.concat([tf.zeros(((padded_target_length // r) - 1), dtype=tf.float32), tf.ones(1, dtype=tf.float32)], axis=0)
    spec_loss_mask = tf.ones(shape=padded_target_length, dtype=tf.float32)
    binary_loss_mask = tf.ones(shape=(padded_target_length // r), dtype=tf.float32)
    return MelData(target.id, target.key, mel, target.mel_width, padded_target_length, done, spec_loss_mask, binary_loss_mask)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 40:------------------- similar code ------------------ index = 29, score = 5.0 
def call(self, inputs, state):
    (mgc_input, lf0_input) = inputs
    mgc_prenet_output = reduce((lambda acc, pn: pn(acc)), self.mgc_prenets, mgc_input)
    lf0_prenet_output = reduce((lambda acc, pn: pn(acc)), self.lf0_prenets, lf0_input)
    prenet_output = tf.concat([mgc_prenet_output, lf0_prenet_output], axis=(- 1))
    return self._cell(prenet_output, state)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 41:------------------- similar code ------------------ index = 87, score = 5.0 
if (__name__ == '__main__'):
    batch_size = 8
    nclass = 10
    y_true = tf.random.uniform((batch_size,), 0, nclass, dtype=tf.int32)
    y_pred = tf.random.uniform((batch_size, nclass), (- 1), 1, dtype=tf.float32)
    batch_idxs = tf.expand_dims(tf.range(0, batch_size, dtype=tf.int32), 1)
    idxs = tf.concat([batch_idxs, tf.cast(tf.expand_dims(y_true, (- 1)), tf.int32)], 1)
    mask = tf.logical_not(tf.scatter_nd(idxs, tf.ones(tf.shape(idxs)[0], tf.bool), tf.shape(y_pred)))
    sp = tf.expand_dims(tf.gather_nd(y_pred, idxs), 1)
    sn = tf.reshape(tf.boolean_mask(y_pred, mask), (batch_size, (- 1)))
    circleloss = CircleLoss()
    sparsecircleloss = SparseCircleLoss(batch_size=batch_size)
    paircircleloss = PairCircleLoss()
    print('circle loss:\n', circleloss.call(tf.one_hot(y_true, nclass, dtype=tf.float32), y_pred).numpy())
    print('sparse circle loss:\n', sparsecircleloss.call(tf.expand_dims(y_true, (- 1)), y_pred).numpy().ravel())
    print('pair circle loss:\n', paircircleloss.call(sp, sn).numpy().ravel())

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
     ...  = tf.concat

idx = 42:------------------- similar code ------------------ index = 91, score = 5.0 
def sample_and_group_all(xyz, points, use_xyz=True):
    '\n    Inputs:\n        xyz: (batch_size, ndataset, 3) TF tensor\n        points: (batch_size, ndataset, channel) TF tensor, if None will just use xyz as points\n        use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n    Outputs:\n        new_xyz: (batch_size, 1, 3) as (0,0,0)\n        new_points: (batch_size, 1, ndataset, 3+channel) TF tensor\n    Note:\n        Equivalent to sample_and_group with npoint=1, radius=inf, use (0,0,0) as the centroid\n    '
    batch_size = xyz.get_shape()[0].value
    nsample = xyz.get_shape()[1].value
    new_xyz = tf.constant(np.tile(np.array([0, 0, 0]).reshape((1, 1, 3)), (batch_size, 1, 1)), dtype=tf.float32)
    idx = tf.constant(np.tile(np.array(range(nsample)).reshape((1, 1, nsample)), (batch_size, 1, 1)))
    grouped_xyz = tf.reshape(xyz, (batch_size, 1, nsample, 3))
    if (points is not None):
        if use_xyz:
            new_points = tf.concat([xyz, points], axis=2)
        else:
            new_points = points
        new_points = tf.expand_dims(new_points, 1)
    else:
        new_points = grouped_xyz
    return (new_xyz, new_points, idx, grouped_xyz)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
        if  ... :
             ...  = tf.concat

idx = 43:------------------- similar code ------------------ index = 69, score = 5.0 
def sample_and_group(npoint, radius, nsample, xyz, points, knn=False, use_xyz=True):
    '\n    Input:\n        npoint: int32\n        radius: float32\n        nsample: int32\n        xyz: (batch_size, ndataset, 3) TF tensor\n        points: (batch_size, ndataset, channel) TF tensor, if None will just use xyz as points\n        knn: bool, if True use kNN instead of radius search\n        use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n    Output:\n        new_xyz: (batch_size, npoint, 3) TF tensor\n        new_points: (batch_size, npoint, nsample, 3+channel) TF tensor\n        idx: (batch_size, npoint, nsample) TF tensor, indices of local points as in ndataset points\n        grouped_xyz: (batch_size, npoint, nsample, 3) TF tensor, normalized point XYZs\n            (subtracted by seed point XYZ) in local regions\n    '
    new_xyz = gather_point(xyz, farthest_point_sample(npoint, xyz))
    if knn:
        (_, idx) = knn_point(nsample, xyz, new_xyz)
    else:
        (idx, pts_cnt) = query_ball_point(radius, nsample, xyz, new_xyz)
    grouped_xyz = group_point(xyz, idx)
    grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1, 1, nsample, 1])
    if (points is not None):
        grouped_points = group_point(points, idx)
        if use_xyz:
            new_points = tf.concat([grouped_xyz, grouped_points], axis=(- 1))
        else:
            new_points = grouped_points
    else:
        new_points = grouped_xyz
    return (new_xyz, new_points, idx, grouped_xyz)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
        if  ... :
             ...  = tf.concat

idx = 44:------------------- similar code ------------------ index = 16, score = 5.0 
def softmax_by_row(self, z: typing.Any) -> tuple:
    'Conduct softmax on each dimension across the four gates.'
    z_transform = Permute((2, 1))(Reshape((4, self._units))(z))
    size = [(- 1), 1, (- 1)]
    for i in range(0, self._units):
        begin = [0, i, 0]
        z_slice = tf.slice(z_transform, begin, size)
        if (i == 0):
            z_s = tf.nn.softmax(z_slice)
        else:
            z_s = tf.concat([z_s, tf.nn.softmax(z_slice)], 1)
    (zi, zl, zt, zd) = tf.unstack(z_s, axis=2)
    return (zi, zl, zt, zd)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... () ->  ... :
    for  ...  in:
        if:        else:
             ...  = tf.concat

idx = 45:------------------- similar code ------------------ index = 7, score = 5.0 
def _flip_left_right(self, image, mask):
    '\n        Randomly flips image and mask left or right in accord.\n        '
    comb_tensor = tf.concat([image, mask], axis=2)
    comb_tensor = tf.image.random_flip_left_right(comb_tensor, seed=self.seed)
    (image, mask) = tf.split(comb_tensor, [self.channels[0], self.channels[1]], axis=2)
    return (image, mask)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 46:------------------- similar code ------------------ index = 1, score = 5.0 
def conv(attr_hs, attr_as, attr_vs, dim, feature_map_size=2, kernel_size=[2, 4], activation=tf.nn.tanh, layer_num=2):
    attr_as = tf.reshape(attr_as, [(- 1), 1, dim])
    attr_vs = tf.reshape(attr_vs, [(- 1), 1, dim])
    input_avs = tf.concat([attr_as, attr_vs], 1)
    input_shape = input_avs.shape.as_list()
    input_layer = tf.reshape(input_avs, [(- 1), input_shape[1], input_shape[2], 1])
    _conv = input_layer
    _conv = tf.layers.batch_normalization(_conv, 2)
    for i in range(layer_num):
        _conv = tf.layers.conv2d(inputs=_conv, filters=feature_map_size, kernel_size=kernel_size, strides=[1, 1], padding='same', activation=activation)
    _conv = tf.nn.l2_normalize(_conv, 2)
    _shape = _conv.shape.as_list()
    _flat = tf.reshape(_conv, [(- 1), ((_shape[1] * _shape[2]) * _shape[3])])
    dense = tf.layers.dense(inputs=_flat, units=dim, activation=activation)
    dense = tf.nn.l2_normalize(dense)
    score = (- tf.reduce_sum(tf.square((attr_hs - dense)), 1))
    return score

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 47:------------------- similar code ------------------ index = 2, score = 5.0 
def build(self, input_shape):
    '\n        initialize embedding_weights, where\n        id 0 is reserved for UNK, and its embedding fix to all zeros\n        '
    with tf.compat.v1.variable_scope(self.scope_name):
        unknown_id = tf.Variable(tf.zeros_initializer()([1, self.output_dim]), name='-'.join([self.layer_name, 'unknown']), trainable=False)
        normal_ids = tf.compat.v1.get_variable('-'.join([self.layer_name, 'normal']), [(self.input_dim - 1), self.output_dim], initializer=(tf.random_uniform_initializer(minval=(- self.initial_range), maxval=self.initial_range) if self.initial_range else None))
    self.embeddings = tf.concat([unknown_id, normal_ids], axis=0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = tf.concat

idx = 48:------------------- similar code ------------------ index = 3, score = 5.0 
def _multi_perspective_match(mp_dim, reps_rt, att_lt, with_cosine=True, with_mp_cosine=True):
    "\n    The core function of zhiguowang's implementation.\n\n    reference:\n    https://github.com/zhiguowang/BiMPM/blob/master/src/match_utils.py#L207-L223\n    :param mp_dim: about 20\n    :param reps_rt: [batch, len_rt, dim]\n    :param att_lt: [batch, len_rt, dim]\n    :param with_cosine: True\n    :param with_mp_cosine: True\n    :return: [batch, len, 1 + mp_dim]\n    "
    shape_rt = tf.shape(reps_rt)
    batch_size = shape_rt[0]
    len_lt = shape_rt[1]
    match_dim = 0
    match_result_list = []
    if with_cosine:
        cosine_tensor = _cosine_distance(reps_rt, att_lt, False)
        cosine_tensor = tf.reshape(cosine_tensor, [batch_size, len_lt, 1])
        match_result_list.append(cosine_tensor)
        match_dim += 1
    if with_mp_cosine:
        mp_cosine_layer = MpCosineLayer(mp_dim)
        mp_cosine_tensor = mp_cosine_layer([reps_rt, att_lt])
        mp_cosine_tensor = tf.reshape(mp_cosine_tensor, [batch_size, len_lt, mp_dim])
        match_result_list.append(mp_cosine_tensor)
        match_dim += mp_cosine_layer.mp_dim
    match_result = tf.concat(match_result_list, 2)
    return (match_result, match_dim)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 49:------------------- similar code ------------------ index = 4, score = 5.0 
def call(self, inputs, **kwargs):
    '\n        The computation logic of EncodingLayer.\n\n        :param inputs: an input tensor.\n        '
    x = (tf.expand_dims(inputs, 1) * 0)
    x = tf.transpose(x, (0, 1, 3, 2))
    mid = (x + tf.expand_dims(inputs, (- 1)))
    up = tf.transpose(mid, (0, 3, 2, 1))
    inputs_concat = tf.concat([up, mid, (up * mid)], axis=2)
    A = K.dot(self._w_itr_att, inputs_concat)
    SA = tf.nn.softmax(A, axis=2)
    itr_attn = K.batch_dot(SA, inputs)
    inputs_attn_concat = tf.concat([inputs, itr_attn], axis=2)
    concat_dropout = DecayingDropoutLayer(initial_keep_rate=self._initial_keep_rate, decay_interval=self._decay_interval, decay_rate=self._decay_rate)(inputs_attn_concat)
    z = tf.tanh((K.dot(concat_dropout, self._w1) + self._b1))
    r = tf.sigmoid((K.dot(concat_dropout, self._w2) + self._b2))
    f = tf.sigmoid((K.dot(concat_dropout, self._w3) + self._b3))
    encoding = ((r * inputs) + (f * z))
    return encoding

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 50:------------------- similar code ------------------ index = 5, score = 5.0 
def tf_batch_gather(params, indices, axis, name=None):
    '\n    Extension of the batch_gather function in tensorflow\n    (see https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/ops/array_ops.py\n    or https://www.tensorflow.org/api_docs/python/tf/batch_gather)\n    Gather slices from `params` according to `indices` with leading batch dims.\n    This operation assumes that the leading dimensions of `indices` are dense,\n    and the gathers on the axis corresponding to the last dimension of `indices`.\n    More concretely it computes:\n    `result[i1, ..., in, j1, ..., jm, k1, ...., kl] = params[i1, ..., in, indices[i1, ..., in, j1, ..., jm], k1, ..., kl]`\n    Therefore `params` should be a Tensor of shape [A1, ..., AN, C0, B1, ..., BM],\n    `indices` should be a Tensor of shape [A1, ..., AN, C1, ..., CK] and `result` will be\n    a Tensor of size `[A1, ..., AN, C1, ..., CK, B1, ..., BM]`.\n    In the case in which indices is a 1D tensor, this operation is equivalent to\n    `tf.gather`.\n    See also `tf.gather` and `tf.gather_nd`.\n    Args:\n      params: A `Tensor`. The tensor from which to gather values.\n      indices: A `Tensor`. Must be one of the following types: int32, int64. Index\n          tensor. Must be in range `[0, params.shape[axis]`, where `axis` is the\n          last dimension of `indices` itself.\n      axis: A `Tensor`. Must be one of the following types: int32, int64. The axis\n            in `params` to gather `indices` from.\n      name: A name for the operation (optional).\n    Returns:\n      A Tensor. Has the same type as `params`.\n    Raises:\n      ValueError: if `indices` has an unknown shape.\n    '
    with ops.name_scope(name):
        indices = ops.convert_to_tensor(indices, name='indices')
        params = ops.convert_to_tensor(params, name='params')
        indices_shape = tf.shape(indices)
        params_shape = tf.shape(params)
        ndims = indices.shape.ndims
        if (ndims is None):
            raise ValueError('batch_gather does not allow indices with unknown shape.')
        batch_indices = indices
        indices_dtype = indices.dtype.base_dtype
        accum_dim_value = tf.ones((), dtype=indices_dtype)
        casted_params_shape = gen_math_ops.cast(params_shape, indices_dtype)
        for dim in range(axis, 0, (- 1)):
            dim_value = casted_params_shape[(dim - 1)]
            accum_dim_value *= casted_params_shape[dim]
            start = tf.zeros((), dtype=indices_dtype)
            step = tf.ones((), dtype=indices_dtype)
            dim_indices = gen_math_ops._range(start, dim_value, step)
            dim_indices *= accum_dim_value
            dim_shape = tf.stack(((([1] * (dim - 1)) + [dim_value]) + ([1] * (ndims - dim))), axis=0)
            batch_indices += tf.reshape(dim_indices, dim_shape)
        flat_inner_shape_indices = gen_math_ops.prod(indices_shape[:(axis + 1)], [0], False)
        flat_indices = tf.reshape(batch_indices, tf.concat([[flat_inner_shape_indices], indices_shape[(axis + 1):]], axis=0))
        outer_shape = params_shape[(axis + 1):]
        flat_inner_shape_params = gen_math_ops.prod(params_shape[:(axis + 1)], [0], False)
        flat_params = tf.reshape(params, tf.concat([[flat_inner_shape_params], outer_shape], axis=0))
        flat_result = tf.gather(flat_params, flat_indices)
        result = tf.reshape(flat_result, tf.concat([indices_shape, outer_shape], axis=0))
        final_shape = indices.get_shape()[:axis].merge_with(params.get_shape()[:axis])
        final_shape = final_shape.concatenate(indices.get_shape()[axis:])
        final_shape = final_shape.concatenate(params.get_shape()[(axis + 1):])
        result.set_shape(final_shape)
        return result

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  =  ... . ... ( ... , tf.concat)

idx = 51:------------------- similar code ------------------ index = 6, score = 5.0 
def call(self, x):
    start_x = tf.nn.dropout(x, 0.1)
    Qs = [q_w(x) for q_w in self.q_ws]
    Ks = [k_w(x) for k_w in self.k_ws]
    Vs = [v_w(x) for v_w in self.v_ws]
    Es = [tf.matmul(q, k, transpose_b=True) for (q, k) in zip(Qs, Ks)]
    Es = [(self.scale * e) for e in Es]
    scores = [tf.nn.softmax(e) for e in Es]
    context = [tf.matmul(score, v) for (score, v) in zip(scores, Vs)]
    context = tf.concat(context, axis=(- 1))
    x = self.w_out_1(context)
    x = self.layer_norm_1(x)
    x = (start_x + x)
    x = tf.nn.swish(x)
    res_x = x
    res_x_2 = x
    x = self.w_out_2(x)
    x = self.layer_norm_2(x)
    x = (res_x + x)
    x = tf.nn.swish(x)
    res_x = x
    x = self.w_out_3(x)
    x = self.layer_norm_3(x)
    x = ((res_x_2 + res_x) + x)
    x = tf.nn.swish(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 52:------------------- similar code ------------------ index = 8, score = 5.0 
def call(self, dec_input, dec_hidden, enc_output):
    (context_vector, attention_weights) = self.attention(dec_hidden, enc_output)
    x = self.embedding(dec_input)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=(- 1))
    (output, state) = self.gru(x)
    output = tf.reshape(output, ((- 1), output.shape[2]))
    x = self.fc(output)
    return (x, state, attention_weights)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 53:------------------- similar code ------------------ index = 32, score = 5.0 
def call(self, inputs, **kwargs):
    with tf.name_scope('deep_multiply'):
        inputs = tf.concat(inputs, (- 1))
        for (i, hs) in enumerate(self.hiddens):
            with tf.name_scope('deep'):
                deep1 = DeepBlock(hidden=hs, activation=self.activation, prefix='deep_multiply1_{}'.format(i), sparse=self.sparse)(inputs)
                deep2 = DeepBlock(hidden=hs, activation=self.activation, prefix='deep_multiply2_{}'.format(i), sparse=self.sparse)(inputs)
                inputs = ((deep1 + deep2) + tf.multiply(deep1, deep2))
        return inputs

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = tf.concat

idx = 54:------------------- similar code ------------------ index = 9, score = 5.0 
def pointnet_fp_module(xyz1, xyz2, points1, points2, mlp, is_training, bn_decay, scope, bn=True):
    ' PointNet Feature Propogation (FP) Module\n        Input:                                                                                                      \n            xyz1: (batch_size, ndataset1, 3) TF tensor                                                              \n            xyz2: (batch_size, ndataset2, 3) TF tensor, sparser than xyz1                                           \n            points1: (batch_size, ndataset1, nchannel1) TF tensor                                                   \n            points2: (batch_size, ndataset2, nchannel2) TF tensor\n            mlp: list of int32 -- output size for MLP on each point                                                 \n        Return:\n            new_points: (batch_size, ndataset1, mlp[-1]) TF tensor\n    '
    with tf.variable_scope(scope) as sc:
        (dist, idx) = three_nn(xyz1, xyz2)
        dist = tf.maximum(dist, 1e-10)
        norm = tf.reduce_sum((1.0 / dist), axis=2, keep_dims=True)
        norm = tf.tile(norm, [1, 1, 3])
        weight = ((1.0 / dist) / norm)
        interpolated_points = three_interpolate(points2, idx, weight)
        if (points1 is not None):
            new_points1 = tf.concat(axis=2, values=[interpolated_points, points1])
        else:
            new_points1 = interpolated_points
        new_points1 = tf.expand_dims(new_points1, 2)
        for (i, num_out_channel) in enumerate(mlp):
            new_points1 = tf_util.conv2d(new_points1, num_out_channel, [1, 1], padding='VALID', stride=[1, 1], bn=bn, is_training=is_training, scope=('conv_%d' % i), bn_decay=bn_decay)
        new_points1 = tf.squeeze(new_points1, [2])
        return new_points1

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if:
             ...  = tf.concat

idx = 55:------------------- similar code ------------------ index = 10, score = 5.0 
def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):
    "Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ 'Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3',\n      'Mixed_3a', 'Mixed_4a', 'Mixed_5a', 'Mixed_5b', 'Mixed_5c', 'Mixed_5d',\n      'Mixed_5e', 'Mixed_6a', 'Mixed_6b', 'Mixed_6c', 'Mixed_6d', 'Mixed_6e',\n      'Mixed_6f', 'Mixed_6g', 'Mixed_6h', 'Mixed_7a', 'Mixed_7b', 'Mixed_7c',\n      'Mixed_7d']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  "
    end_points = {}

    def add_and_check_final(name, net):
        end_points[name] = net
        return (name == final_endpoint)
    with tf.variable_scope(scope, 'InceptionV4', [inputs]):
        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):
            net = slim.conv2d(inputs, 32, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')
            if add_and_check_final('Conv2d_1a_3x3', net):
                return (net, end_points)
            net = slim.conv2d(net, 32, [3, 3], padding='VALID', scope='Conv2d_2a_3x3')
            if add_and_check_final('Conv2d_2a_3x3', net):
                return (net, end_points)
            net = slim.conv2d(net, 64, [3, 3], scope='Conv2d_2b_3x3')
            if add_and_check_final('Conv2d_2b_3x3', net):
                return (net, end_points)
            with tf.variable_scope('Mixed_3a'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID', scope='MaxPool_0a_3x3')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding='VALID', scope='Conv2d_0a_3x3')
                net = tf.concat(axis=3, values=[branch_0, branch_1])
                if add_and_check_final('Mixed_3a', net):
                    return (net, end_points)
            with tf.variable_scope('Mixed_4a'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')
                    branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding='VALID', scope='Conv2d_1a_3x3')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')
                    branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope='Conv2d_0b_1x7')
                    branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope='Conv2d_0c_7x1')
                    branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding='VALID', scope='Conv2d_1a_3x3')
                net = tf.concat(axis=3, values=[branch_0, branch_1])
                if add_and_check_final('Mixed_4a', net):
                    return (net, end_points)
            with tf.variable_scope('Mixed_5a'):
                with tf.variable_scope('Branch_0'):
                    branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')
                with tf.variable_scope('Branch_1'):
                    branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID', scope='MaxPool_1a_3x3')
                net = tf.concat(axis=3, values=[branch_0, branch_1])
                if add_and_check_final('Mixed_5a', net):
                    return (net, end_points)
            for idx in range(4):
                block_scope = ('Mixed_5' + chr((ord('b') + idx)))
                net = block_inception_a(net, block_scope)
                if add_and_check_final(block_scope, net):
                    return (net, end_points)
            net = block_reduction_a(net, 'Mixed_6a')
            if add_and_check_final('Mixed_6a', net):
                return (net, end_points)
            for idx in range(7):
                block_scope = ('Mixed_6' + chr((ord('b') + idx)))
                net = block_inception_b(net, block_scope)
                if add_and_check_final(block_scope, net):
                    return (net, end_points)
            net = block_reduction_b(net, 'Mixed_7a')
            if add_and_check_final('Mixed_7a', net):
                return (net, end_points)
            for idx in range(3):
                block_scope = ('Mixed_7' + chr((ord('b') + idx)))
                net = block_inception_c(net, block_scope)
                if add_and_check_final(block_scope, net):
                    return (net, end_points)
    raise ValueError(('Unknown final endpoint %s' % final_endpoint))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        with:
            with:
                 ...  = tf.concat

idx = 56:------------------- similar code ------------------ index = 11, score = 5.0 
def get_model(point_cloud, cls_label, is_training, bn_decay=None):
    ' Classification PointNet, input is BxNx3, output Bx40 '
    batch_size = point_cloud.get_shape()[0].value
    num_point = point_cloud.get_shape()[1].value
    end_points = {}
    l0_xyz = tf.slice(point_cloud, [0, 0, 0], [(- 1), (- 1), 3])
    l0_points = tf.slice(point_cloud, [0, 0, 3], [(- 1), (- 1), 3])
    (l1_xyz, l1_points) = pointnet_sa_module_msg(l0_xyz, l0_points, 512, [0.1, 0.2, 0.4], [32, 64, 128], [[32, 32, 64], [64, 64, 128], [64, 96, 128]], is_training, bn_decay, scope='layer1')
    (l2_xyz, l2_points) = pointnet_sa_module_msg(l1_xyz, l1_points, 128, [0.4, 0.8], [64, 128], [[128, 128, 256], [128, 196, 256]], is_training, bn_decay, scope='layer2')
    (l3_xyz, l3_points, l3_indices) = pointnet_sa_module(l2_xyz, l2_points, npoint=None, radius=None, nsample=None, mlp=[256, 512, 1024], mlp2=None, group_all=True, is_training=is_training, bn_decay=bn_decay, scope='layer3')
    l2_points = pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256, 256], is_training, bn_decay, scope='fa_layer1')
    l1_points = pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256, 128], is_training, bn_decay, scope='fa_layer2')
    cls_label_one_hot = tf.one_hot(cls_label, depth=NUM_CATEGORIES, on_value=1.0, off_value=0.0)
    cls_label_one_hot = tf.reshape(cls_label_one_hot, [batch_size, 1, NUM_CATEGORIES])
    cls_label_one_hot = tf.tile(cls_label_one_hot, [1, num_point, 1])
    l0_points = pointnet_fp_module(l0_xyz, l1_xyz, tf.concat([cls_label_one_hot, l0_xyz, l0_points], axis=(- 1)), l1_points, [128, 128], is_training, bn_decay, scope='fp_layer3')
    net = tf_util.conv1d(l0_points, 128, 1, padding='VALID', bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay)
    end_points['feats'] = net
    net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp1')
    net = tf_util.conv1d(net, 50, 1, padding='VALID', activation_fn=None, scope='fc2')
    return (net, end_points)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... ( ... ,  ... , tf.concat,  ... ,,  ... ,  ... ,)

idx = 57:------------------- similar code ------------------ index = 94, score = 5.0 
def write_interpolation(imgs, scale_down=1, circle_crop=False, do_blur=True, texts: str=None, spy_imgs=None):
    'Texts is a list of strings equal to the number of imgs\n  '
    assert (len(texts) == len(imgs))
    writer = imageio.get_writer(f"gallery/{CFG['load_name']}_animation.mp4", format='FFMPEG', fps=20)
    for (i, img) in tqdm(enumerate(imgs)):

        def process_img(img):
            img = tf.cast((img * 255), tf.uint8)
            img = tf.image.resize(img, [int((img.shape[0] / scale_down)), int((img.shape[1] / scale_down))], 'lanczos3', antialias=True)
            if circle_crop:
                img = circle_crop(img)
            if do_blur:
                img = blur(img)[0]
            if (texts is not None):
                img = annotate_image(img, texts[i])
            return img
        img = process_img(img)
        if CFG['use_spy']:
            spy_img = process_img(spy_imgs[i])
            img = tf.concat([img, spy_img], axis=1)
        writer.append_data(np.array(img))
    writer.close()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:

        if:
             ...  = tf.concat

idx = 58:------------------- similar code ------------------ index = 13, score = 5.0 
def calculate_recurrent_unit(self, inputs: typing.Any, states: typing.Any, step: int, h: typing.Any) -> tuple:
    '\n        Calculate recurrent unit.\n\n        :param inputs: A TensorArray which contains interaction\n            between left text and right text.\n        :param states: A TensorArray which stores the hidden state\n            of every step.\n        :param step: Recurrent step.\n        :param h: Hidden state from last operation.\n        '
    i = tf.math.floordiv(step, tf.constant(self._text2_maxlen))
    j = tf.math.mod(step, tf.constant(self._text2_maxlen))
    h_diag = states.read(((i * (self._text2_maxlen + 1)) + j))
    h_top = states.read((((i * (self._text2_maxlen + 1)) + j) + 1))
    h_left = states.read((((i + 1) * (self._text2_maxlen + 1)) + j))
    s_ij = inputs.read(step)
    q = tf.concat([tf.concat([h_top, h_left], 1), tf.concat([h_diag, s_ij], 1)], 1)
    r = self._recurrent_activation(self._time_distributed_dense(self._wr, q, self._br))
    z = self._time_distributed_dense(self._wz, q, self._bz)
    (zi, zl, zt, zd) = self.softmax_by_row(z)
    h_ij_l = self._time_distributed_dense(self._w_ij, s_ij, self._b_ij)
    h_ij_r = K.dot((r * tf.concat([h_left, h_top, h_diag], 1)), self._U)
    h_ij_ = self._activation((h_ij_l + h_ij_r))
    h_ij = ((((zl * h_left) + (zt * h_top)) + (zd * h_diag)) + (zi * h_ij_))
    states = states.write(((((i + 1) * (self._text2_maxlen + 1)) + j) + 1), h_ij)
    h_ij.set_shape(h_top.get_shape())
    return (inputs, states, (step + 1), h_ij)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... () ->  ... :
     ...  = tf.concat

idx = 59:------------------- similar code ------------------ index = 93, score = 5.0 
def call(self, inputs):
    nf = inputs['node_features']
    feature_reps = []
    for (name, layer) in self.nf_w.items():
        nf_rep = layer(nf[name])
        nf_rep = tf.nn.swish(nf_rep)
        feature_reps.append(nf_rep)
    feature_reps = tf.concat(feature_reps, axis=(- 1))
    x = self.w(feature_reps)
    x = self.layer_norm_1(x)
    pre_linear_x = x
    x = tf.nn.swish(x)
    x = self.w_out(x)
    x = self.layer_norm_2(x)
    x = (pre_linear_x + x)
    x = tf.nn.swish(x)
    return x

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 60:------------------- similar code ------------------ index = 31, score = 5.0 
def call(self, queries, keys, values, is_training=True, **kwargs):
    if (self.num_units is None):
        num_units = queries.get_shape().as_list[(- 1)]
    Q = tf.keras.layers.Dense(self.num_units, activation=tf.nn.relu)(queries)
    K = tf.keras.layers.Dense(self.num_units, activation=tf.nn.relu)(keys)
    V = tf.keras.layers.Dense(self.num_units, activation=tf.nn.relu)(values)
    if self.has_residual:
        V_res = tf.keras.layers.Dense(self.num_units, activation=tf.nn.relu)(values)
    Q_ = tf.concat(tf.split(Q, self.num_heads, axis=2), axis=0)
    K_ = tf.concat(tf.split(K, self.num_heads, axis=2), axis=0)
    V_ = tf.concat(tf.split(V, self.num_heads, axis=2), axis=0)
    weights = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))
    weights = (weights / (K_.get_shape().as_list()[(- 1)] ** 0.5))
    weights = tf.nn.softmax(weights)
    weights = CustomDropout(rate=(1 - self.dropout_keep_prob))(weights, is_training)
    outputs = tf.matmul(weights, V_)
    outputs = tf.concat(tf.split(outputs, self.num_heads, axis=0), axis=2)
    if self.has_residual:
        outputs += V_res
    outputs = tf.nn.relu(outputs)
    outputs = self.normalize(outputs)
    return outputs

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 61:------------------- similar code ------------------ index = 86, score = 5.0 
def build_network(self, features, is_training=None):
    '\n\n        :param features:\n        :param is_training:\n        :return:\n        '
    (ev_list, fv_list) = features
    deep_out = self.mlp_block(self.concat((ev_list + fv_list)))
    fwbi = self.fwbi(ev_list)
    fwbi_fc_32 = self.fwbi_fc_32(fwbi)
    fwbi_bn = self.fwbi_bn(fwbi_fc_32)
    fwbi_out = self.fwbi_drop(fwbi_bn)
    logit = tf.concat(values=[deep_out, fwbi_out], axis=1)
    return logit

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 62:------------------- similar code ------------------ index = 33, score = 5.0 
def convert(target: PreprocessedMelData):
    r = hparams.outputs_per_step
    mel_normalized = ((target.mel - np.array(hparams.average_mel_level_db, dtype=np.float32)) / np.array(hparams.stddev_mel_level_db, dtype=np.float32))
    mel_with_silence = tf.pad(mel_normalized, paddings=[[r, r], [0, 0]], constant_values=hparams.silence_mel_level_db)
    target_length = (target.target_length + (2 * r))
    padded_target_length = (((target_length // r) + 1) * r)

    def padding_function(t):
        tail_padding = (padded_target_length - target_length)
        padding_shape = tf.sparse_tensor_to_dense(tf.SparseTensor(indices=[(0, 1)], values=tf.expand_dims(tail_padding, axis=0), dense_shape=(2, 2)))
        return (lambda : tf.pad(t, paddings=padding_shape, constant_values=hparams.silence_mel_level_db))
    no_padding_condition = tf.equal(tf.to_int64(0), (target_length % r))
    mel = tf.cond(no_padding_condition, (lambda : mel_with_silence), padding_function(mel_with_silence))
    mel.set_shape((None, hparams.num_mels))
    padded_target_length = tf.cond(no_padding_condition, (lambda : target_length), (lambda : padded_target_length))
    done = tf.concat([tf.zeros(((padded_target_length // r) - 1), dtype=tf.float32), tf.ones(1, dtype=tf.float32)], axis=0)
    spec_loss_mask = tf.ones(shape=padded_target_length, dtype=tf.float32)
    binary_loss_mask = tf.ones(shape=(padded_target_length // r), dtype=tf.float32)
    return MelData(target.id, target.key, mel, target.mel_width, padded_target_length, done, spec_loss_mask, binary_loss_mask)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 63:------------------- similar code ------------------ index = 68, score = 5.0 
def call(self, inputs, input_lengths=None, **kwargs):
    (input, accent_type) = inputs
    prenet_output = reduce((lambda acc, pn: pn(acc)), self.prenets, input)
    accent_type_prenet_output = reduce((lambda acc, pn: pn(acc)), self.accent_type_prenets, accent_type)
    concatenated = tf.concat([prenet_output, accent_type_prenet_output], axis=(- 1))
    lstm_output = self.cbhg(concatenated, input_lengths=input_lengths)
    self_attention_input = self.self_attention_projection_layer(lstm_output)

    def self_attend(input, alignments, layer):
        (output, alignment) = layer(input, memory_sequence_length=input_lengths)
        return (output, (alignments + alignment))
    (self_attention_output, self_attention_alignments) = reduce((lambda acc, sa: self_attend(acc[0], acc[1], sa)), self.self_attention, (self_attention_input, []))
    return (lstm_output, self_attention_output, self_attention_alignments)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 64:------------------- similar code ------------------ index = 67, score = 5.0 
def transformAttention(X, STE_P, STE_Q, K, d, bn, bn_decay, is_training):
    '\n    transform attention mechanism\n    X:      [batch_size, P, N, D]\n    STE_P:  [batch_size, P, N, D]\n    STE_Q:  [batch_size, Q, N, D]\n    K:      number of attention heads\n    d:      dimension of each attention outputs\n    return: [batch_size, Q, N, D]\n    '
    D = (K * d)
    query = FC(STE_Q, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    key = FC(STE_P, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    value = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    query = tf.concat(tf.split(query, K, axis=(- 1)), axis=0)
    key = tf.concat(tf.split(key, K, axis=(- 1)), axis=0)
    value = tf.concat(tf.split(value, K, axis=(- 1)), axis=0)
    query = tf.transpose(query, perm=(0, 2, 1, 3))
    key = tf.transpose(key, perm=(0, 2, 3, 1))
    value = tf.transpose(value, perm=(0, 2, 1, 3))
    attention = tf.matmul(query, key)
    attention /= (d ** 0.5)
    attention = tf.nn.softmax(attention, axis=(- 1))
    X = tf.matmul(attention, value)
    X = tf.transpose(X, perm=(0, 2, 1, 3))
    X = tf.concat(tf.split(X, K, axis=0), axis=(- 1))
    X = FC(X, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    return X

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 65:------------------- similar code ------------------ index = 65, score = 5.0 
def build_math(self):
    if (self.math_input_network is None):
        self.math_input_network = cfg.build_math_input(scope='math_input_network')
        if ('math' in self.fixed_weights):
            self.math_input_network.fix_variables()
    if (self.math_network is None):
        self.math_network = cfg.build_math_network(scope='math_network')
        if ('math' in self.fixed_weights):
            self.math_network.fix_variables()
    (math_rep, mask) = self.build_math_representation()
    if (self.max_possible_objects is not None):
        (math_rep, _, mask) = apply_mask_and_group_at_front(math_rep, mask)
        n_pad = (self.max_possible_objects - tf.shape(math_rep)[1])
        mask = tf.cast(mask, tf.float32)
        batch_size = tf.shape(math_rep)[0]
        A = math_rep.shape[2]
        math_rep = tf.pad(math_rep, [(0, 0), (0, n_pad), (0, 0)])
        math_rep = tf.reshape(math_rep, (batch_size, self.max_possible_objects, A))
        mask = tf.pad(mask, [(0, 0), (0, n_pad)])
        mask = tf.reshape(mask, (batch_size, self.max_possible_objects, 1))
    mask_shape = tf.concat([tf.shape(math_rep)[:(- 1)], [1]], axis=0)
    mask = tf.reshape(mask, mask_shape)
    math_rep = tf.concat([mask, math_rep], axis=(- 1))
    logits = self.math_network(math_rep, cfg.n_classes, self.is_training)
    self._tensors['prediction'] = tf.nn.softmax(logits)
    recorded_tensors = self.recorded_tensors
    if (self.math_weight is not None):
        self.record_tensors(raw_loss_math=tf.nn.softmax_cross_entropy_with_logits_v2(labels=self._tensors['targets'], logits=logits))
        self.losses['math'] = (self.math_weight * recorded_tensors['raw_loss_math'])
    self.record_tensors(math_accuracy=tf.equal(tf.argmax(logits, axis=1), tf.argmax(self._tensors['targets'], axis=1)), math_1norm=tf.abs((tf.argmax(logits, axis=1) - tf.argmax(self._tensors['targets'], axis=1))), math_correct_prob=tf.reduce_sum((tf.nn.softmax(logits) * self._tensors['targets']), axis=1))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = tf.concat

idx = 66:------------------- similar code ------------------ index = 64, score = 5.0 
def _call(self, objects, background, is_training, appearance_only=False, mask_only=False):
    ' If mask_only==True, then we ignore the provided background, using a black blackground instead,\n            and also ignore the computed appearance, using all-white appearances instead.\n\n        '
    if (not self.initialized):
        self.image_depth = tf_shape(background)[(- 1)]
    single = False
    if isinstance(objects, dict):
        single = True
        objects = [objects]
    _object_maps = []
    _scales = []
    _offsets = []
    _appearance = []
    for (i, obj) in enumerate(objects):
        anchor_box = self.anchor_boxes[i]
        object_shape = self.object_shapes[i]
        object_decoder = self.maybe_build_subnet('object_decoder_for_flight_{}'.format(i), builder_name='build_object_decoder')
        appearance_logit = apply_object_wise(object_decoder, obj.attr, output_size=(object_shape + ((self.image_depth + 1),)), is_training=is_training)
        appearance_logit = (appearance_logit * (([self.color_logit_scale] * self.image_depth) + [self.alpha_logit_scale]))
        appearance_logit = (appearance_logit + (([0.0] * self.image_depth) + [self.alpha_logit_bias]))
        appearance = tf.nn.sigmoid(tf.clip_by_value(appearance_logit, (- 10.0), 10.0))
        _appearance.append(appearance)
        if appearance_only:
            continue
        (batch_size, *obj_leading_shape, _, _, _) = tf_shape(appearance)
        n_objects = np.prod(obj_leading_shape)
        appearance = tf.reshape(appearance, (batch_size, n_objects, *object_shape, (self.image_depth + 1)))
        (obj_colors, obj_alpha) = tf.split(appearance, [self.image_depth, 1], axis=(- 1))
        if mask_only:
            obj_colors = tf.ones_like(obj_colors)
        obj_alpha *= tf.reshape(obj.obj, (batch_size, n_objects, 1, 1, 1))
        z = tf.reshape(obj.z, (batch_size, n_objects, 1, 1, 1))
        obj_importance = tf.maximum(((obj_alpha * z) / self.importance_temp), 0.01)
        object_maps = tf.concat([obj_colors, obj_alpha, obj_importance], axis=(- 1))
        (*_, image_height, image_width, _) = tf_shape(background)
        (yt, xt, ys, xs) = coords_to_image_space(obj.yt, obj.xt, obj.ys, obj.xs, (image_height, image_width), anchor_box, top_left=True)
        scales = tf.concat([ys, xs], axis=(- 1))
        scales = tf.reshape(scales, (batch_size, n_objects, 2))
        offsets = tf.concat([yt, xt], axis=(- 1))
        offsets = tf.reshape(offsets, (batch_size, n_objects, 2))
        _object_maps.append(object_maps)
        _scales.append(scales)
        _offsets.append(offsets)
    if single:
        _appearance = _appearance[0]
    if appearance_only:
        return dict(appearance=_appearance)
    if mask_only:
        background = tf.zeros_like(background)
    output = render_sprites.render_sprites(_object_maps, _scales, _offsets, background)
    return dict(appearance=_appearance, output=output)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
         ...  = tf.concat

idx = 67:------------------- similar code ------------------ index = 63, score = 5.0 
def temporalAttention(X, STE, K, d, bn, bn_decay, is_training, mask=True):
    '\n    temporal attention mechanism\n    X:      [batch_size, num_step, N, D]\n    STE:    [batch_size, num_step, N, D]\n    K:      number of attention heads\n    d:      dimension of each attention outputs\n    return: [batch_size, num_step, N, D]\n    '
    D = (K * d)
    X = tf.concat((X, STE), axis=(- 1))
    query = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    key = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    value = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    query = tf.concat(tf.split(query, K, axis=(- 1)), axis=0)
    key = tf.concat(tf.split(key, K, axis=(- 1)), axis=0)
    value = tf.concat(tf.split(value, K, axis=(- 1)), axis=0)
    query = tf.transpose(query, perm=(0, 2, 1, 3))
    key = tf.transpose(key, perm=(0, 2, 3, 1))
    value = tf.transpose(value, perm=(0, 2, 1, 3))
    attention = tf.matmul(query, key)
    attention /= (d ** 0.5)
    if mask:
        batch_size = tf.shape(X)[0]
        num_step = X.get_shape()[1].value
        N = X.get_shape()[2].value
        mask = tf.ones(shape=(num_step, num_step))
        mask = tf.linalg.LinearOperatorLowerTriangular(mask).to_dense()
        mask = tf.expand_dims(tf.expand_dims(mask, axis=0), axis=0)
        mask = tf.tile(mask, multiples=((K * batch_size), N, 1, 1))
        mask = tf.cast(mask, dtype=tf.bool)
        attention = tf.compat.v2.where(condition=mask, x=attention, y=((- (2 ** 15)) + 1))
    attention = tf.nn.softmax(attention, axis=(- 1))
    X = tf.matmul(attention, value)
    X = tf.transpose(X, perm=(0, 2, 1, 3))
    X = tf.concat(tf.split(X, K, axis=0), axis=(- 1))
    X = FC(X, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    return X

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 68:------------------- similar code ------------------ index = 71, score = 5.0 
def call(self, x: list, **kwargs):
    'Call.'
    (seq_lt, seq_rt) = (x[:5], x[5:])
    (lstm_reps_lt, forward_h_lt, _, backward_h_lt, _) = seq_lt
    (lstm_reps_rt, forward_h_rt, _, backward_h_rt, _) = seq_rt
    match_tensor_list = []
    match_dim = 0
    if self._perspective.get('full'):
        h_lt = tf.concat([forward_h_lt, backward_h_lt], axis=(- 1))
        full_match_tensor = self.full_match([h_lt, lstm_reps_rt])
        match_tensor_list.append(full_match_tensor)
        match_dim += (self._mp_dim + 1)
    if self._perspective.get('max-pooling'):
        max_match_tensor = self.max_pooling_match([lstm_reps_lt, lstm_reps_rt])
        match_tensor_list.append(max_match_tensor)
        match_dim += self._mp_dim
    if self._perspective.get('attentive'):
        attentive_tensor = self.attentive_match([lstm_reps_lt, lstm_reps_rt])
        match_tensor_list.append(attentive_tensor)
        match_dim += (self._mp_dim + 1)
    if self._perspective.get('max-attentive'):
        relevancy_matrix = _calc_relevancy_matrix(lstm_reps_lt, lstm_reps_rt)
        max_attentive_tensor = self.max_attentive_match([lstm_reps_lt, lstm_reps_rt, relevancy_matrix])
        match_tensor_list.append(max_attentive_tensor)
        match_dim += (self._mp_dim + 1)
    mp_tensor = tf.concat(match_tensor_list, axis=(- 1))
    return mp_tensor

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = tf.concat

idx = 69:------------------- similar code ------------------ index = 61, score = 5.0 
def spatialAttention(X, STE, K, d, bn, bn_decay, is_training):
    '\n    spatial attention mechanism\n    X:      [batch_size, num_step, N, D]\n    STE:    [batch_size, num_step, N, D]\n    K:      number of attention heads\n    d:      dimension of each attention outputs\n    return: [batch_size, num_step, N, D]\n    '
    D = (K * d)
    X = tf.concat((X, STE), axis=(- 1))
    query = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    key = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    value = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    query = tf.concat(tf.split(query, K, axis=(- 1)), axis=0)
    key = tf.concat(tf.split(key, K, axis=(- 1)), axis=0)
    value = tf.concat(tf.split(value, K, axis=(- 1)), axis=0)
    attention = tf.matmul(query, key, transpose_b=True)
    attention /= (d ** 0.5)
    attention = tf.nn.softmax(attention, axis=(- 1))
    X = tf.matmul(attention, value)
    X = tf.concat(tf.split(X, K, axis=0), axis=(- 1))
    X = FC(X, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    return X

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 70:------------------- similar code ------------------ index = 60, score = 5.0 
def pointnet_sa_module_msg(xyz, points, npoint, radius_list, nsample_list, mlp_list, is_training, bn_decay, scope, bn=True, use_xyz=True, use_nchw=False):
    ' PointNet Set Abstraction (SA) module with Multi-Scale Grouping (MSG)\n        Input:\n            xyz: (batch_size, ndataset, 3) TF tensor\n            points: (batch_size, ndataset, channel) TF tensor\n            npoint: int32 -- #points sampled in farthest point sampling\n            radius: list of float32 -- search radius in local region\n            nsample: list of int32 -- how many points in each local region\n            mlp: list of list of int32 -- output size for MLP on each point\n            use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n            use_nchw: bool, if True, use NCHW data format for conv2d, which is usually faster than NHWC format\n        Return:\n            new_xyz: (batch_size, npoint, 3) TF tensor\n            new_points: (batch_size, npoint, \\sum_k{mlp[k][-1]}) TF tensor\n    '
    data_format = ('NCHW' if use_nchw else 'NHWC')
    with tf.variable_scope(scope) as sc:
        new_xyz = gather_point(xyz, farthest_point_sample(npoint, xyz))
        new_points_list = []
        for i in range(len(radius_list)):
            radius = radius_list[i]
            nsample = nsample_list[i]
            (idx, pts_cnt) = query_ball_point(radius, nsample, xyz, new_xyz)
            grouped_xyz = group_point(xyz, idx)
            grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1, 1, nsample, 1])
            if (points is not None):
                grouped_points = group_point(points, idx)
                if use_xyz:
                    grouped_points = tf.concat([grouped_points, grouped_xyz], axis=(- 1))
            else:
                grouped_points = grouped_xyz
            if use_nchw:
                grouped_points = tf.transpose(grouped_points, [0, 3, 1, 2])
            for (j, num_out_channel) in enumerate(mlp_list[i]):
                grouped_points = tf_util.conv2d(grouped_points, num_out_channel, [1, 1], padding='VALID', stride=[1, 1], bn=bn, is_training=is_training, scope=('conv%d_%d' % (i, j)), bn_decay=bn_decay)
            if use_nchw:
                grouped_points = tf.transpose(grouped_points, [0, 2, 3, 1])
            new_points = tf.reduce_max(grouped_points, axis=[2])
            new_points_list.append(new_points)
        new_points_concat = tf.concat(new_points_list, axis=(- 1))
        return (new_xyz, new_points_concat)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        for  ...  in:
            if:
                if  ... :
                     ...  = tf.concat

idx = 71:------------------- similar code ------------------ index = 72, score = 5.0 
def write_mosaic(imgs, spy_imgs):
    imgs = tf.concat(imgs, axis=0)
    spy_imgs = tf.concat(spy_imgs, axis=0)
    out = tf.concat([imgs, spy_imgs], axis=1)
    imageio.imwrite(f"gallery/{CFG['load_name']}_mosaic.png", out)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 72:------------------- similar code ------------------ index = 73, score = 5.0 
def build_features(self, features, embedding_suffix=''):
    '\n        categorical feature id starts from -1 (as missing)\n        '
    if self.flags.wide_cols:
        ev_list = [self.EmbeddingDict[key](features[key]) for key in self.CATEGORY_FEATURES if (key in self.flags.wide_cols)]
        fv_list = [self.build_dense_layer(features[key]) for key in self.NUMERICAL_FEATURES if (key in self.flags.wide_cols)]
        wide_list = self.concat((fv_list + ev_list))
    else:
        ev_list = [self.EmbeddingDict[key](features[key]) for key in self.CATEGORY_FEATURES]
        fv_list = [self.build_dense_layer(features[key]) for key in self.NUMERICAL_FEATURES]
        wide_list = self.concat((fv_list + ev_list))
    if self.flags.deep_cols:
        ev_list = [self.EmbeddingDict[key](features[key]) for key in self.CATEGORY_FEATURES if (key in self.flags.deep_cols)]
        fv_list = [self.build_dense_layer(features[key]) for key in self.NUMERICAL_FEATURES if (key in self.flags.deep_cols)]
        deep_list = self.concat((fv_list + ev_list))
    else:
        ev_list = [self.EmbeddingDict[key](features[key]) for key in self.CATEGORY_FEATURES]
        fv_list = [self.build_dense_layer(features[key]) for key in self.NUMERICAL_FEATURES]
        deep_list = self.concat((fv_list + ev_list))
    return (tf.concat(wide_list, (- 1)), tf.concat(deep_list, (- 1)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    return (tf.concat,)

idx = 73:------------------- similar code ------------------ index = 57, score = 5.0 
def _calculate_weighted_contexts(self, tokens_vocab, paths_vocab, attention_param, source_input, path_input, target_input, valid_mask, is_evaluating=False):
    source_word_embed = tf.nn.embedding_lookup(params=tokens_vocab, ids=source_input)
    path_embed = tf.nn.embedding_lookup(params=paths_vocab, ids=path_input)
    target_word_embed = tf.nn.embedding_lookup(params=tokens_vocab, ids=target_input)
    context_embed = tf.concat([source_word_embed, path_embed, target_word_embed], axis=(- 1))
    if (not is_evaluating):
        context_embed = tf.nn.dropout(context_embed, rate=(1 - self.config.DROPOUT_KEEP_RATE))
    flat_embed = tf.reshape(context_embed, [(- 1), self.config.context_vector_size])
    transform_param = tf.compat.v1.get_variable('TRANSFORM', shape=(self.config.context_vector_size, self.config.CODE_VECTOR_SIZE), dtype=tf.float32)
    flat_embed = tf.tanh(tf.matmul(flat_embed, transform_param))
    contexts_weights = tf.matmul(flat_embed, attention_param)
    batched_contexts_weights = tf.reshape(contexts_weights, [(- 1), self.config.MAX_CONTEXTS, 1])
    mask = tf.math.log(valid_mask)
    mask = tf.expand_dims(mask, axis=2)
    batched_contexts_weights += mask
    attention_weights = tf.nn.softmax(batched_contexts_weights, axis=1)
    batched_embed = tf.reshape(flat_embed, shape=[(- 1), self.config.MAX_CONTEXTS, self.config.CODE_VECTOR_SIZE])
    code_vectors = tf.reduce_sum(tf.multiply(batched_embed, attention_weights), axis=1)
    return (code_vectors, attention_weights)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 74:------------------- similar code ------------------ index = 74, score = 5.0 
def call(self, Z, debug=False):
    '\n    Inputs:\n      Z: tensor of shape [batch_size, node_embedding], which is a\n    fixed-dimensional representation of a graph that will be reconstructed to\n    its nodes.\n    '
    batch_size = Z.shape[0]
    expanded_x = self.expand_w(Z)
    expanded_x = tf.nn.swish(expanded_x)
    x = tf.reshape(expanded_x, [batch_size, self.max_nodes, self.graph_hidden_size])
    pos = tf.tile(tf.range(self.max_nodes)[tf.newaxis], [x.shape[0], 1])
    pos = (tf.cast(pos, tf.float32)[(..., tf.newaxis)] / self.max_nodes)
    for (pos_embed, pos_norm) in zip(self.pos_embeds, self.pos_norms):
        pos = pos_embed(pos)
        pos = pos_norm(pos)
    x = tf.concat([pos, x], axis=(- 1))
    x = self.combine_pos(x)
    x = self.combine_pos_norm(x)
    for attn_layer in self.global_attns:
        x = attn_layer(x)
    adj_out = self.adj_w(x)
    adj_out = (self.scale * adj_out)
    adj_out = tf.nn.sigmoid(adj_out)
    nf_out = {}
    for name in self.nf_w.keys():
        w_layer = self.nf_w[name]
        nf_pred = w_layer(x)
        nf_pred = (self.scale * nf_pred)
        nf_pred = tf.nn.softmax(nf_pred)
        nf_out[name] = nf_pred
    return (adj_out, nf_out)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 75:------------------- similar code ------------------ index = 75, score = 5.0 
def attention(tensors):
    'Attention layer.'
    (left, right) = tensors
    tensor_left = tf.expand_dims(left, axis=2)
    tensor_right = tf.expand_dims(right, axis=1)
    tensor_left = K.repeat_elements(tensor_left, len_right, 2)
    tensor_right = K.repeat_elements(tensor_right, len_left, 1)
    tensor_merged = tf.concat([tensor_left, tensor_right], axis=(- 1))
    middle_output = keras.layers.Dense(self._params['fc_num_units'], activation='tanh')(tensor_merged)
    attn_scores = keras.layers.Dense(1)(middle_output)
    attn_scores = tf.squeeze(attn_scores, axis=3)
    exp_attn_scores = tf.math.exp((attn_scores - tf.reduce_max(attn_scores, axis=(- 1), keepdims=True)))
    exp_sum = tf.reduce_sum(exp_attn_scores, axis=(- 1), keepdims=True)
    attention_weights = (exp_attn_scores / exp_sum)
    return K.batch_dot(attention_weights, right)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = tf.concat

idx = 76:------------------- similar code ------------------ index = 76, score = 5.0 
def annotate_image(img, text, side_width=64):
    'Annotate an image with the given text\n  '
    from PIL import Image, ImageDraw, ImageFont
    orig_width = img.shape[1]
    fs = 16
    zeros = tf.zeros((img.shape[0], side_width, img.shape[2]))
    img = tf.concat([img, zeros], axis=1)
    img = np.array(img).astype(np.uint8)
    image = Image.fromarray(img)
    draw = ImageDraw.Draw(image)
    font = ImageFont.truetype('OpenSans.ttf', fs)
    max_height = ((img.shape[0] // 4) - (fs // 2))
    min_height = (((3 * img.shape[0]) // 4) - (fs // 2))
    dist = ((min_height - max_height) // 2)

    def get_height(ratio, path='top'):
        'sinusoidal interpolation\n\n    1 returns center\n    top 0 returns top\n    bot 0 returns bot\n    '
        shift = int((dist * math.sin(((ratio / 2) * math.pi))))
        if (path == 'top'):
            return (max_height + shift)
        else:
            return (min_height - shift)
    top_text = text['top']['text']
    top_color = int((text['top']['amount'] * 255))
    top_color = f'rgb({top_color},{top_color},{top_color})'
    top_height = get_height(text['top']['amount'], path='top')
    draw.text(((orig_width + 8), top_height), top_text, fill=top_color, font=font)
    bot_text = text['bottom']['text']
    bot_color = int((text['bottom']['amount'] * 255))
    bot_color = f'rgb({bot_color},{bot_color},{bot_color})'
    bot_height = get_height(text['bottom']['amount'], path='bot')
    draw.text(((orig_width + 8), bot_height), bot_text, fill=bot_color, font=font)
    return np.array(image)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 77:------------------- similar code ------------------ index = 77, score = 5.0 
def get_model(point_cloud, is_training, bn_decay=None):
    ' Part segmentation PointNet, input is BxNx6 (XYZ NormalX NormalY NormalZ), output Bx50 '
    batch_size = point_cloud.get_shape()[0].value
    num_point = point_cloud.get_shape()[1].value
    end_points = {}
    l0_xyz = tf.slice(point_cloud, [0, 0, 0], [(- 1), (- 1), 3])
    l0_points = tf.slice(point_cloud, [0, 0, 3], [(- 1), (- 1), 3])
    (l1_xyz, l1_points, l1_indices) = pointnet_sa_module(l0_xyz, l0_points, npoint=512, radius=0.2, nsample=64, mlp=[64, 64, 128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer1')
    (l2_xyz, l2_points, l2_indices) = pointnet_sa_module(l1_xyz, l1_points, npoint=128, radius=0.4, nsample=64, mlp=[128, 128, 256], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer2')
    (l3_xyz, l3_points, l3_indices) = pointnet_sa_module(l2_xyz, l2_points, npoint=None, radius=None, nsample=None, mlp=[256, 512, 1024], mlp2=None, group_all=True, is_training=is_training, bn_decay=bn_decay, scope='layer3')
    l2_points = pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256, 256], is_training, bn_decay, scope='fa_layer1')
    l1_points = pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256, 128], is_training, bn_decay, scope='fa_layer2')
    l0_points = pointnet_fp_module(l0_xyz, l1_xyz, tf.concat([l0_xyz, l0_points], axis=(- 1)), l1_points, [128, 128, 128], is_training, bn_decay, scope='fa_layer3')
    net = tf_util.conv1d(l0_points, 128, 1, padding='VALID', bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay)
    end_points['feats'] = net
    net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp1')
    net = tf_util.conv1d(net, 50, 1, padding='VALID', activation_fn=None, scope='fc2')
    return (net, end_points)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... ( ... ,  ... , tf.concat,  ... ,,  ... ,  ... ,)

idx = 78:------------------- similar code ------------------ index = 78, score = 5.0 
def call(self, inputs: list, **kwargs) -> typing.Any:
    '\n        The computation logic of DynamicPoolingLayer.\n\n        :param inputs: two input tensors.\n        '
    self._validate_dpool_size()
    (x, dpool_index) = inputs
    dpool_shape = tf.shape(dpool_index)
    batch_index_one = tf.expand_dims(tf.expand_dims(tf.range(dpool_shape[0]), axis=(- 1)), axis=(- 1))
    batch_index = tf.expand_dims(tf.tile(batch_index_one, [1, self._msize1, self._msize2]), axis=(- 1))
    dpool_index_ex = tf.concat([batch_index, dpool_index], axis=3)
    x_expand = tf.gather_nd(x, dpool_index_ex)
    stride1 = (self._msize1 // self._psize1)
    stride2 = (self._msize2 // self._psize2)
    x_pool = tf.nn.max_pool(x_expand, [1, stride1, stride2, 1], [1, stride1, stride2, 1], 'VALID')
    return x_pool

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... () ->:
     ...  = tf.concat

idx = 79:------------------- similar code ------------------ index = 50, score = 5.0 
def pointnet_sa_module(xyz, points, npoint, radius, nsample, mlp, mlp2, group_all, is_training, bn_decay, scope, bn=True, pooling='max', knn=False, use_xyz=True, use_nchw=False):
    ' PointNet Set Abstraction (SA) Module\n        Input:\n            xyz: (batch_size, ndataset, 3) TF tensor\n            points: (batch_size, ndataset, channel) TF tensor\n            npoint: int32 -- #points sampled in farthest point sampling\n            radius: float32 -- search radius in local region\n            nsample: int32 -- how many points in each local region\n            mlp: list of int32 -- output size for MLP on each point\n            mlp2: list of int32 -- output size for MLP on each region\n            group_all: bool -- group all points into one PC if set true, OVERRIDE\n                npoint, radius and nsample settings\n            use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n            use_nchw: bool, if True, use NCHW data format for conv2d, which is usually faster than NHWC format\n        Return:\n            new_xyz: (batch_size, npoint, 3) TF tensor\n            new_points: (batch_size, npoint, mlp[-1] or mlp2[-1]) TF tensor\n            idx: (batch_size, npoint, nsample) int32 -- indices for local regions\n    '
    data_format = ('NCHW' if use_nchw else 'NHWC')
    with tf.variable_scope(scope) as sc:
        if group_all:
            nsample = xyz.get_shape()[1].value
            (new_xyz, new_points, idx, grouped_xyz) = sample_and_group_all(xyz, points, use_xyz)
        else:
            (new_xyz, new_points, idx, grouped_xyz) = sample_and_group(npoint, radius, nsample, xyz, points, knn, use_xyz)
        if use_nchw:
            new_points = tf.transpose(new_points, [0, 3, 1, 2])
        for (i, num_out_channel) in enumerate(mlp):
            new_points = tf_util.conv2d(new_points, num_out_channel, [1, 1], padding='VALID', stride=[1, 1], bn=bn, is_training=is_training, scope=('conv%d' % i), bn_decay=bn_decay, data_format=data_format)
        if use_nchw:
            new_points = tf.transpose(new_points, [0, 2, 3, 1])
        if (pooling == 'max'):
            new_points = tf.reduce_max(new_points, axis=[2], keep_dims=True, name='maxpool')
        elif (pooling == 'avg'):
            new_points = tf.reduce_mean(new_points, axis=[2], keep_dims=True, name='avgpool')
        elif (pooling == 'weighted_avg'):
            with tf.variable_scope('weighted_avg'):
                dists = tf.norm(grouped_xyz, axis=(- 1), ord=2, keep_dims=True)
                exp_dists = tf.exp(((- dists) * 5))
                weights = (exp_dists / tf.reduce_sum(exp_dists, axis=2, keep_dims=True))
                new_points *= weights
                new_points = tf.reduce_sum(new_points, axis=2, keep_dims=True)
        elif (pooling == 'max_and_avg'):
            max_points = tf.reduce_max(new_points, axis=[2], keep_dims=True, name='maxpool')
            avg_points = tf.reduce_mean(new_points, axis=[2], keep_dims=True, name='avgpool')
            new_points = tf.concat([avg_points, max_points], axis=(- 1))
        if (mlp2 is not None):
            if use_nchw:
                new_points = tf.transpose(new_points, [0, 3, 1, 2])
            for (i, num_out_channel) in enumerate(mlp2):
                new_points = tf_util.conv2d(new_points, num_out_channel, [1, 1], padding='VALID', stride=[1, 1], bn=bn, is_training=is_training, scope=('conv_post_%d' % i), bn_decay=bn_decay, data_format=data_format)
            if use_nchw:
                new_points = tf.transpose(new_points, [0, 2, 3, 1])
        new_points = tf.squeeze(new_points, [2])
        return (new_xyz, new_points, idx)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        if:        elif:
             ...  = tf.concat

idx = 80:------------------- similar code ------------------ index = 95, score = 5.0 
def __call__(self, inputs, state: RNNStateHistoryWrapperState):
    (output, new_rnn_state) = self._cell(inputs, state.rnn_state)
    new_history = tf.concat([state.rnn_state_history, tf.expand_dims(output, axis=1)], axis=1)
    new_history.set_shape([None, None, self.output_size])
    new_state = RNNStateHistoryWrapperState(new_rnn_state, new_history, (state.time + 1))
    return (output, new_state)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 81:------------------- similar code ------------------ index = 81, score = 5.0 
def build_math_representation(self):
    attr_shape = tf.shape(self._tensors['attr'])
    attr = tf.reshape(self._tensors['attr'], ((- 1), self.A))
    math_A = (self.A if (self.math_A is None) else self.math_A)
    math_attr = self.math_input_network(attr, math_A, self.is_training)
    new_shape = tf.concat([attr_shape[:(- 1)], [math_A]], axis=0)
    math_attr = tf.reshape(math_attr, new_shape)
    self._tensors['math_attr'] = math_attr
    return (math_attr, self._tensors['obj'])

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = tf.concat

idx = 82:------------------- similar code ------------------ index = 46, score = 5.0 
def call(self, inputs, input_lengths=None, positional_encoding=None, **kwargs):
    conv_outputs = tf.concat([conv1d(inputs) for conv1d in self.convolution_banks], axis=(- 1))
    maxpool_output = self.maxpool(conv_outputs)
    proj1_output = self.projection1(maxpool_output)
    proj2_output = self.projection2(proj1_output)
    highway_input = (proj2_output + inputs)
    if (highway_input.shape[2] != (self.out_units // 2)):
        highway_input = self.adjustment_layer(highway_input)
    highway_output = reduce((lambda acc, hw: hw(acc)), self.highway_nets, highway_input)
    self_attention_highway_input = self.self_attention_adjustment_layer(highway_input)
    self_attention_highway_output = reduce((lambda acc, hw: hw(acc)), self.self_attention_highway_nets, self_attention_highway_input)
    self_attention_input = (self_attention_highway_output + positional_encoding)
    (self_attention_output, self_attention_alignments) = self.self_attention(self_attention_input, memory_sequence_length=input_lengths)
    self_attention_output = (self_attention_output + self_attention_highway_output)
    (bilstm_outputs, bilstm_states) = tf.nn.bidirectional_dynamic_rnn(ZoneoutLSTMCell((self.out_units // 2), self._is_training, zoneout_factor_cell=self._zoneout_factor_cell, zoneout_factor_output=self._zoneout_factor_output, dtype=self.dtype), ZoneoutLSTMCell((self.out_units // 2), self._is_training, zoneout_factor_cell=self._zoneout_factor_cell, zoneout_factor_output=self._zoneout_factor_output, dtype=self.dtype), highway_output, sequence_length=input_lengths, dtype=highway_output.dtype)
    bilstm_outputs = tf.concat(bilstm_outputs, axis=(- 1))
    return (bilstm_outputs, self_attention_output, self_attention_alignments)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 83:------------------- similar code ------------------ index = 44, score = 5.0 
def get_model(point_cloud, is_training, num_class, bn_decay=None):
    ' Semantic segmentation PointNet, input is BxNx4, output Bxnum_class '
    batch_size = point_cloud.get_shape()[0].value
    num_point = point_cloud.get_shape()[1].value
    end_points = {}
    l0_xyz = tf.slice(point_cloud, [0, 0, 0], [(- 1), (- 1), 3])
    l0_points = tf.slice(point_cloud, [0, 0, 3], [(- 1), (- 1), 1])
    end_points['l0_xyz'] = l0_xyz
    (l1_xyz, l1_points, l1_indices) = pointnet_sa_module(l0_xyz, l0_points, npoint=1024, radius=0.1, nsample=32, mlp=[32, 32, 64], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer1')
    (l2_xyz, l2_points, l2_indices) = pointnet_sa_module(l1_xyz, l1_points, npoint=256, radius=0.2, nsample=32, mlp=[64, 64, 128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer2')
    (l3_xyz, l3_points, l3_indices) = pointnet_sa_module(l2_xyz, l2_points, npoint=64, radius=0.4, nsample=32, mlp=[128, 128, 256], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer3')
    (l4_xyz, l4_points, l4_indices) = pointnet_sa_module(l3_xyz, l3_points, npoint=16, radius=0.8, nsample=32, mlp=[256, 256, 512], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer4')
    l3_points = pointnet_fp_module(l3_xyz, l4_xyz, l3_points, l4_points, [256, 256], is_training, bn_decay, scope='fa_layer1')
    l2_points = pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256, 256], is_training, bn_decay, scope='fa_layer2')
    l1_points = pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256, 128], is_training, bn_decay, scope='fa_layer3')
    l0_points = pointnet_fp_module(l0_xyz, l1_xyz, tf.concat([l0_xyz, l0_points], axis=(- 1)), l1_points, [128, 128, 128], is_training, bn_decay, scope='fa_layer4')
    net = tf_util.conv1d(l0_points, 128, 1, padding='VALID', bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay)
    end_points['feats'] = net
    net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp1')
    net = tf_util.conv1d(net, num_class, 1, padding='VALID', activation_fn=None, scope='fc2')
    return (net, end_points)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... ( ... ,  ... , tf.concat,  ... ,,  ... ,  ... ,)

idx = 84:------------------- similar code ------------------ index = 43, score = 5.0 
def call(self, inputs, input_lengths=None, **kwargs):
    (input, accent_type) = inputs
    prenet_output = reduce((lambda acc, pn: pn(acc)), self.prenets, input)
    accent_type_prenet_output = reduce((lambda acc, pn: pn(acc)), self.accent_type_prenets, accent_type)
    concatenated = tf.concat([prenet_output, accent_type_prenet_output], axis=(- 1))
    cbhg_output = self.cbhg(concatenated, input_lengths=input_lengths)
    return cbhg_output

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 85:------------------- similar code ------------------ index = 42, score = 5.0 
def _meshgrid(height, width):
    with tf.variable_scope('_meshgrid'):
        x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])), tf.transpose(tf.expand_dims(tf.linspace((- 1.0), 1.0, width), 1), [1, 0]))
        y_t = tf.matmul(tf.expand_dims(tf.linspace((- 1.0), 1.0, height), 1), tf.ones(shape=tf.stack([1, width])))
        x_t_flat = tf.reshape(x_t, (1, (- 1)))
        y_t_flat = tf.reshape(y_t, (1, (- 1)))
        ones = tf.ones_like(x_t_flat)
        grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones])
        return grid

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = tf.concat

idx = 86:------------------- similar code ------------------ index = 41, score = 5.0 
def build_network(self, features, is_training=None):
    '\n\n        :param features:\n        :param is_training:\n        :return:\n        '
    fm_out = self.fm_block(features)
    deep_out = self.deep_block(features)
    logit = tf.concat([deep_out, fm_out], (- 1))
    return logit

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 87:------------------- similar code ------------------ index = 83, score = 5.0 
@staticmethod
def _prepare_target(target, hparams):

    def convert(target: PreprocessedMelData):
        r = hparams.outputs_per_step
        mel_normalized = ((target.mel - np.array(hparams.average_mel_level_db, dtype=np.float32)) / np.array(hparams.stddev_mel_level_db, dtype=np.float32))
        mel_with_silence = tf.pad(mel_normalized, paddings=[[r, r], [0, 0]], constant_values=hparams.silence_mel_level_db)
        target_length = (target.target_length + (2 * r))
        padded_target_length = (((target_length // r) + 1) * r)

        def padding_function(t):
            tail_padding = (padded_target_length - target_length)
            padding_shape = tf.sparse_tensor_to_dense(tf.SparseTensor(indices=[(0, 1)], values=tf.expand_dims(tail_padding, axis=0), dense_shape=(2, 2)))
            return (lambda : tf.pad(t, paddings=padding_shape, constant_values=hparams.silence_mel_level_db))
        no_padding_condition = tf.equal(tf.to_int64(0), (target_length % r))
        mel = tf.cond(no_padding_condition, (lambda : mel_with_silence), padding_function(mel_with_silence))
        mel.set_shape((None, hparams.num_mels))
        padded_target_length = tf.cond(no_padding_condition, (lambda : target_length), (lambda : padded_target_length))
        done = tf.concat([tf.zeros(((padded_target_length // r) - 1), dtype=tf.float32), tf.ones(1, dtype=tf.float32)], axis=0)
        spec_loss_mask = tf.ones(shape=padded_target_length, dtype=tf.float32)
        binary_loss_mask = tf.ones(shape=(padded_target_length // r), dtype=tf.float32)
        return MelData(target.id, target.key, mel, target.mel_width, padded_target_length, done, spec_loss_mask, binary_loss_mask)
    return DatasetSource._decode_target(target).map((lambda inputs: convert(inputs)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():

    def  ... ():
         ...  = tf.concat

idx = 88:------------------- similar code ------------------ index = 39, score = 5.0 
def tf_find_connected_components(inp, bg, threshold, colors=None, cosine_threshold=None):
    assert (len(inp.shape) == 4)
    if isinstance(colors, str):
        colors = colors.split()
    mask = (tf.reduce_sum(tf.abs((inp - bg)), axis=3) >= threshold)
    if ((colors is None) or (cosine_threshold is None)):
        output = _find_connected_componenents_body(mask)
        output['color'] = output['obj']
        return output
    objects = []
    for color in colors:
        if isinstance(color, str):
            color = tf.constant(to_rgb(color), tf.float32)
        similarity = tf_cosine_similarity(inp, color)
        color_mask = tf.logical_and((similarity >= cosine_threshold), mask)
        objects.append(_find_connected_componenents_body(color_mask))
    output = dict(normalized_box=tf.concat([o['normalized_box'] for o in objects], axis=1), obj=tf.concat([o['obj'] for o in objects], axis=1), n_objects=tf.reduce_sum(tf.stack([o['n_objects'] for o in objects], axis=1), axis=1), color=tf.concat([(float((i + 1)) * o['obj']) for (i, o) in enumerate(objects)], axis=1))
    output['max_objects'] = tf.reduce_max(output['n_objects'])
    return output

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... ( ... =tf.concat,,,)

idx = 89:------------------- similar code ------------------ index = 38, score = 5.0 
def spatialAttention(X, STE, K, d, bn, bn_decay, is_training):
    '\n    spatial attention mechanism\n    X:      [batch_size, num_step, N, D]\n    STE:    [batch_size, num_step, N, D]\n    K:      number of attention heads\n    d:      dimension of each attention outputs\n    return: [batch_size, num_step, N, D]\n    '
    D = (K * d)
    X = tf.concat((X, STE), axis=(- 1))
    query = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    key = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    value = FC(X, units=D, activations=tf.nn.relu, bn=bn, bn_decay=bn_decay, is_training=is_training)
    query = tf.concat(tf.split(query, K, axis=(- 1)), axis=0)
    key = tf.concat(tf.split(key, K, axis=(- 1)), axis=0)
    value = tf.concat(tf.split(value, K, axis=(- 1)), axis=0)
    attention = tf.matmul(query, key, transpose_b=True)
    attention /= (d ** 0.5)
    attention = tf.nn.softmax(attention, axis=(- 1))
    X = tf.matmul(attention, value)
    X = tf.concat(tf.split(X, K, axis=0), axis=(- 1))
    X = FC(X, units=[D, D], activations=[tf.nn.relu, None], bn=bn, bn_decay=bn_decay, is_training=is_training)
    return X

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 90:------------------- similar code ------------------ index = 84, score = 5.0 
def average_gradients(tower_grads):
    'Calculate the average gradient for each shared variable across all towers.\n  Note that this function provides a synchronization point across all towers.\n  From tensorflow tutorial: cifar10/cifar10_multi_gpu_train.py\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n  Returns:\n     List of pairs of (gradient, variable) where the gradient has been averaged\n     across all towers.\n  '
    average_grads = []
    for grad_and_vars in zip(*tower_grads):
        grads = []
        for (g, v) in grad_and_vars:
            expanded_g = tf.expand_dims(g, 0)
            grads.append(expanded_g)
        grad = tf.concat(axis=0, values=grads)
        grad = tf.reduce_mean(grad, 0)
        v = grad_and_vars[0][1]
        grad_and_var = (grad, v)
        average_grads.append(grad_and_var)
    return average_grads

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
         ...  = tf.concat

idx = 91:------------------- similar code ------------------ index = 85, score = 5.0 
def _crop_random(self, image, mask):
    '\n        Randomly crops image and mask in accord.\n        '
    cond_crop_image = tf.cast(tf.random.uniform([], maxval=2, dtype=tf.int32, seed=self.seed), tf.bool)
    shape = tf.cast(tf.shape(image), tf.float32)
    h = tf.cast((shape[0] * self.crop_percent), tf.int32)
    w = tf.cast((shape[1] * self.crop_percent), tf.int32)
    comb_tensor = tf.concat([image, mask], axis=2)
    comb_tensor = tf.cond(cond_crop_image, (lambda : tf.image.random_crop(comb_tensor, [h, w, (self.channels[0] + self.channels[1])], seed=self.seed)), (lambda : tf.identity(comb_tensor)))
    (image, mask) = tf.split(comb_tensor, [self.channels[0], self.channels[1]], axis=2)
    return (image, mask)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 92:------------------- similar code ------------------ index = 35, score = 5.0 
def call(self, inputs, **kwargs):
    dim = inputs.shape[(- 1)]
    hidden_nn_layers = [inputs]
    final_result = []
    split_tensor0 = tf.split(hidden_nn_layers[0], (dim * [1]), 2)
    for (idx, layer_size) in enumerate(self.layer_size):
        split_tensor = tf.split(hidden_nn_layers[(- 1)], (dim * [1]), 2)
        dot_result_m = tf.matmul(split_tensor0, split_tensor, transpose_b=True)
        dot_result_o = tf.reshape(dot_result_m, shape=[dim, (- 1), (self.field_nums[0] * self.field_nums[idx])])
        dot_result = tf.transpose(dot_result_o, perm=[1, 0, 2])
        curr_out = tf.nn.conv1d(dot_result, filters=self.filters[idx], stride=1, padding='VALID')
        curr_out = tf.nn.bias_add(curr_out, self.bias[idx])
        curr_out = self.activation_layers[idx](curr_out)
        curr_out = tf.transpose(curr_out, perm=[0, 2, 1])
        if self.split_half:
            if (idx != (len(self.layer_size) - 1)):
                (next_hidden, direct_connect) = tf.split(curr_out, (2 * [(layer_size // 2)]), 1)
            else:
                direct_connect = curr_out
                next_hidden = 0
        else:
            direct_connect = curr_out
            next_hidden = curr_out
        final_result.append(direct_connect)
        hidden_nn_layers.append(next_hidden)
    logit = tf.concat(final_result, axis=1)
    logit = tf.reduce_sum(logit, (- 1), keepdims=False)
    return logit

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 93:------------------- similar code ------------------ index = 96, score = 5.0 
def build_network(self, features, is_training=None):
    (wide_part, deep_part) = (features[0], features[1])
    deep_part = self.deep_block(deep_part, is_training=is_training)
    wide_part = self.wide_block(wide_part, is_training=is_training)
    logit = tf.concat([wide_part, deep_part], (- 1))
    return logit

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = tf.concat

idx = 94:------------------- similar code ------------------ index = 59, score = 6.0 
def block_inception_c(inputs, scope=None, reuse=None):
    'Builds Inception-C block for Inception v4 network.'
    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding='SAME'):
        with tf.variable_scope(scope, 'BlockInceptionC', [inputs], reuse=reuse):
            with tf.variable_scope('Branch_0'):
                branch_0 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')
            with tf.variable_scope('Branch_1'):
                branch_1 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
                branch_1 = tf.concat(axis=3, values=[slim.conv2d(branch_1, 256, [1, 3], scope='Conv2d_0b_1x3'), slim.conv2d(branch_1, 256, [3, 1], scope='Conv2d_0c_3x1')])
            with tf.variable_scope('Branch_2'):
                branch_2 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
                branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope='Conv2d_0b_3x1')
                branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope='Conv2d_0c_1x3')
                branch_2 = tf.concat(axis=3, values=[slim.conv2d(branch_2, 256, [1, 3], scope='Conv2d_0d_1x3'), slim.conv2d(branch_2, 256, [3, 1], scope='Conv2d_0e_3x1')])
            with tf.variable_scope('Branch_3'):
                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
                branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope='Conv2d_0b_1x1')
            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    with:
        with:
            with:
                 ...  =  ... .concat
            return tf

idx = 95:------------------- similar code ------------------ index = 49, score = 6.0 
def call(self, raw_inputs, **kwargs):
    outputs = []
    with tf.name_scope('deep'):
        output = tf.concat(raw_inputs, (- 1))
        for (i, hs) in enumerate(self.hidden):
            output = CustomDropout(0.1)(output)
            output = DeepBlock(hidden=hs, activation=self.activation, prefix='deep_{}'.format(i), sparse=self.sparse)(output)
            outputs.append(output)
        '\n            H: column wise matrix of each deep layer\n            '
        H = tf.stack(outputs, axis=2)
        "\n            S = H' * H\n            "
        S = tf.matmul(tf.transpose(H, perm=[0, 2, 1]), H)
        '\n            Column wise softmax as attention\n            '
        attention = tf.nn.softmax(S, axis=1)
        '\n            G = H * A\n            '
        G = tf.matmul(H, attention)
        '\n            Sum over deep layers\n            '
        G = tf.reduce_sum(G, axis=(- 1))
        if self.concat_last_deep:
            return tf.concat([outputs[(- 1)], G], axis=(- 1))
        else:
            return G

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    with:
         ...  =  ... .concat
        if:
            return tf

idx = 96:------------------- similar code ------------------ index = 62, score = 6.0 
def call(self, inputs, input_lengths=None, **kwargs):
    conv_outputs = tf.concat([conv1d(inputs) for conv1d in self.convolution_banks], axis=(- 1))
    maxpool_output = self.maxpool(conv_outputs)
    proj1_output = self.projection1(maxpool_output)
    proj2_output = self.projection2(proj1_output)
    highway_input = (proj2_output + inputs)
    if (highway_input.shape[2] != (self.out_units // 2)):
        highway_input = self.adjustment_layer(highway_input)
    highway_output = reduce((lambda acc, hw: hw(acc)), self.highway_nets, highway_input)
    (outputs, states) = tf.nn.bidirectional_dynamic_rnn(ZoneoutLSTMCell((self.out_units // 2), self._is_training, zoneout_factor_cell=self._zoneout_factor_cell, zoneout_factor_output=self._zoneout_factor_output, lstm_impl=self._lstm_impl, dtype=self.dtype), ZoneoutLSTMCell((self.out_units // 2), self._is_training, zoneout_factor_cell=self._zoneout_factor_cell, zoneout_factor_output=self._zoneout_factor_output, lstm_impl=self._lstm_impl, dtype=self.dtype), highway_output, sequence_length=input_lengths, dtype=highway_output.dtype)
    return tf.concat(outputs, axis=(- 1))

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
     ...  =  ... .concat
    return tf

