------------------------- example 1 ------------------------ 
def get_sub_slice(indices, sub_indices):
    '\n    Safe indexer with nested slices.\n\n    Parameters\n    ----------\n    indices: ndarray or slice\n    sub_indices: ndarray or slice\n\n    Returns\n    -------\n    result: np.array(indices[sub_indices])\n    '
    if (indices is None):
        if isinstance(sub_indices, slice):
            return np.arange(sub_indices.start, sub_indices.stop)
        else:
// your code ...

    elif isinstance(indices, slice):
        return np.arange((indices.start + sub_indices.start), (indices.start + sub_indices.stop))
    else:
        return indices[sub_indices]

------------------------- example 2 ------------------------ 
def _draw_resample_index(n, seed):
    'Compute vector of randomly drawn indices with replacement.\n\n    Draw indices with replacement from the discrete uniform distribution\n    on {0,...,n-1}. We control the randomness by setting the seed to *seed*.\n    If *seed* = -1 we return all indices {0,...,n-1} for debugging.\n\n    Args:\n        n (int): Upper bound for indices and number of indices to draw\n        seed (int): Random number seed.\n\n    Returns:\n        indices (np.array): Resample indices.\n\n    '
    if (seed == (- 1)):
        return np.arange(n)
    np.random.seed(seed)
    indices = np.random.randint(0, n, n)
    return indices

------------------------- example 3 ------------------------ 
if (__name__ == '__main__'):
    import numpy as np
    np.random.seed(314)
    batch_size = 3
    num_hidden = 5
    sequence_length = 4
    model = LinkInFor(num_hidden)
    x = np.random.rand(batch_size, sequence_length, num_hidden).astype(np.float32)
    h = np.random.rand(batch_size, num_hidden).astype(np.float32)
    args = [x, h, np.arange(sequence_length)]
    dprint(model(*args))
    ch2o.generate_testcase(model, args)

------------------------- example 4 ------------------------ 
def prepare_datasets(sig_h5_file, bkd_h5_file, dataset_name='dataset', n_sig=(- 1), n_bkd=(- 1), test_frac=0.1, val_frac=0.0, n_folds=1, shuffle=True, shuffle_seed=1, balance=True):
    'Prepare datasets for network training.\n\n    Combine signal and background images; k-fold into training, validation,\n    test sets. Save to files.\n\n    Args:\n        sig_h5_file, bkd_h5_file: location of h5 files containing signal,\n                                  background images.\n        dataset_name: base filename to use for saving datasets.\n        n_sig, n_bkd: number of signal, background images to load.\n        test_frac: proportion of images to save for testing.\n        val_frac: proportion of images to save for validation. Leave at zero\n                  unless using ROC AUC scoring.\n        n_folds: number of k-folds.\n        auxvars: list of auxvar field names to load.\n        shuffle: if True shuffle images before k-folding.\n        shuffle_seed: seed for shuffling.\n    Returns:\n        file_dict: dict containing list of filenames containing train, test\n                   datasets.\n    TODO: add support for multiple classes.\n    '
// your code ...

    images = np.concatenate((sig_images, bkd_images))
    images = images.reshape((- 1), (images.shape[1] * images.shape[2]))
    auxvars = np.concatenate((sig_auxvars, bkd_auxvars))
    classes = np.concatenate([np.repeat([[1, 0]], n_sig, axis=0), np.repeat([[0, 1]], n_bkd, axis=0)])
    if (test_frac >= 1):
// your code ...

    if (test_frac <= 0):
        rs = np.random.RandomState(shuffle_seed)
        train = np.arange(len(images))
        rs.shuffle(train)
// your code ...

    rs = cross_validation.ShuffleSplit(n_images, n_iter=1, test_size=test_frac, random_state=shuffle_seed)
// your code ...

    if (n_folds > 1):
        kf = cross_validation.KFold(len(train), n_folds, shuffle=True, random_state=shuffle_seed)
        i = 0
        kf_files = []
        for (ktrain, ktest) in kf:
            np.random.shuffle(ktrain)
// your code ...

            i += 1
        file_dict['train'] = kf_files
    else:
        rs = np.random.RandomState(shuffle_seed)
// your code ...

        with h5py.File(out_file, 'w') as h5file:
            n_val = int((val_frac * len(train)))
            h5file.create_dataset('X_val', data=images[train][:n_val])
// your code ...

        file_dict['train'] = out_file
    return file_dict

------------------------- example 5 ------------------------ 
def train(base_lr, batch_sz, gpu_no, model_name, power_s):
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
// your code ...

    acc_count = 0
    while (acc_count < 100):
        if os.path.exists(os.path.join(log_path, ('log_test_%02d.txt' % acc_count))):
            acc_count += 1
        else:
            break
    assert (acc_count < 100)
// your code ...

    if (len(reg_loss_list) != 0):
// your code ...

    if (len(thom_loss_list) != 0):
// your code ...

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
// your code ...

        for i in range(max_epoch):
            t = (i % 390)
            if (t == 0):
                idx = np.arange(0, 50000)
                np.random.shuffle(idx)
                train_data['data'] = train_data['data'][idx]
// your code ...

            (tr_images, tr_labels) = ip.load_train(train_data, batch_sz, t)
// your code ...

        tf.train.Saver().save(sess, os.path.join(save_path, str(i)))
// your code ...


examples  ||  representativeness  ||  number of lines  || number of comments 
example1  ||          2           ||        11         ||         1        
example2  ||          2           ||        7         ||         0        
example3  ||          2           ||        12         ||         0        
example4  ||          4           ||        34         ||         7        
example5  ||          6           ||        29         ||         8        


idx = 0:------------------- similar code ------------------ index = 34, score = 6.0 
def get_sub_slice(indices, sub_indices):
    '\n    Safe indexer with nested slices.\n\n    Parameters\n    ----------\n    indices: ndarray or slice\n    sub_indices: ndarray or slice\n\n    Returns\n    -------\n    result: np.array(indices[sub_indices])\n    '
    if (indices is None):
        if isinstance(sub_indices, slice):
            return np.arange(sub_indices.start, sub_indices.stop)
        else:
            return sub_indices
    elif isinstance(indices, slice):
        return np.arange((indices.start + sub_indices.start), (indices.start + sub_indices.stop))
    else:
        return indices[sub_indices]

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        if:
            return np.arange

idx = 1:------------------- similar code ------------------ index = 41, score = 6.0 
def _draw_resample_index(n, seed):
    'Compute vector of randomly drawn indices with replacement.\n\n    Draw indices with replacement from the discrete uniform distribution\n    on {0,...,n-1}. We control the randomness by setting the seed to *seed*.\n    If *seed* = -1 we return all indices {0,...,n-1} for debugging.\n\n    Args:\n        n (int): Upper bound for indices and number of indices to draw\n        seed (int): Random number seed.\n\n    Returns:\n        indices (np.array): Resample indices.\n\n    '
    if (seed == (- 1)):
        return np.arange(n)
    np.random.seed(seed)
    indices = np.random.randint(0, n, n)
    return indices

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        return np.arange

idx = 2:------------------- similar code ------------------ index = 52, score = 6.0 
def _draw_feature_index(p, ratio, seed):
    'Draw random vector of feature indices.\n\n    Draw np.ceil(p * ratio) many indices from {0,...,p-1} without replacement.\n    We control the randomness by setting the seed to *seed*. If *ratio* = -1 we\n    return all indices {0,...,p-1} for debugging.\n\n    Args:\n        p (int): Number of features.\n        ratio (float): Ratio of features to draw, in [0, 1].\n        seed (int): Random number seed.\n\n    Returns:\n        indices (np.array): Index vector of length p.\n\n    '
    if (ratio == (- 1)):
        return np.arange(p)
    np.random.seed(seed)
    nfeat = int(np.ceil((p * ratio)))
    indices = np.random.choice(p, nfeat, replace=False)
    return indices

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        return np.arange

idx = 3:------------------- similar code ------------------ index = 83, score = 6.0 
if (__name__ == '__main__'):
    import numpy as np
    np.random.seed(314)
    batch_size = 3
    num_hidden = 5
    sequence_length = 4
    model = LinkInFor(num_hidden)
    x = np.random.rand(batch_size, sequence_length, num_hidden).astype(np.float32)
    h = np.random.rand(batch_size, num_hidden).astype(np.float32)
    args = [x, h, np.arange(sequence_length)]
    dprint(model(*args))
    ch2o.generate_testcase(model, args)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
     ...  = [ ... ,  ... , np.arange]

idx = 4:------------------- similar code ------------------ index = 88, score = 6.0 
def prepare_datasets(sig_h5_file, bkd_h5_file, dataset_name='dataset', n_sig=(- 1), n_bkd=(- 1), test_frac=0.1, val_frac=0.0, n_folds=1, shuffle=True, shuffle_seed=1, balance=True):
    'Prepare datasets for network training.\n\n    Combine signal and background images; k-fold into training, validation,\n    test sets. Save to files.\n\n    Args:\n        sig_h5_file, bkd_h5_file: location of h5 files containing signal,\n                                  background images.\n        dataset_name: base filename to use for saving datasets.\n        n_sig, n_bkd: number of signal, background images to load.\n        test_frac: proportion of images to save for testing.\n        val_frac: proportion of images to save for validation. Leave at zero\n                  unless using ROC AUC scoring.\n        n_folds: number of k-folds.\n        auxvars: list of auxvar field names to load.\n        shuffle: if True shuffle images before k-folding.\n        shuffle_seed: seed for shuffling.\n    Returns:\n        file_dict: dict containing list of filenames containing train, test\n                   datasets.\n    TODO: add support for multiple classes.\n    '
    (sig_images, sig_auxvars) = load_images(sig_h5_file, n_sig)
    (bkd_images, bkd_auxvars) = load_images(bkd_h5_file, n_bkd)
    if balance:
        n = min(len(sig_images), len(bkd_images))
        sig_images = sig_images[:n]
        sig_auxvars = sig_auxvars[:n]
        bkd_images = bkd_images[:n]
        bkd_auxvars = bkd_auxvars[:n]
    n_sig = len(sig_images)
    n_bkd = len(bkd_images)
    print('collected {0} signal and {1} background images'.format(n_sig, n_bkd))
    n_images = (n_sig + n_bkd)
    images = np.concatenate((sig_images, bkd_images))
    images = images.reshape((- 1), (images.shape[1] * images.shape[2]))
    auxvars = np.concatenate((sig_auxvars, bkd_auxvars))
    classes = np.concatenate([np.repeat([[1, 0]], n_sig, axis=0), np.repeat([[0, 1]], n_bkd, axis=0)])
    if (test_frac >= 1):
        out_file = (dataset_name + '_test.h5')
        with h5py.File(out_file, 'w') as h5file:
            h5file.create_dataset('X_test', data=images)
            h5file.create_dataset('Y_test', data=classes)
            h5file.create_dataset('auxvars_test', data=auxvars)
        return {'test': out_file}
    if (test_frac <= 0):
        rs = np.random.RandomState(shuffle_seed)
        train = np.arange(len(images))
        rs.shuffle(train)
        out_file = (dataset_name + '_train.h5')
        with h5py.File(out_file, 'w') as h5file:
            n_val = int((val_frac * len(train)))
            h5file.create_dataset('X_val', data=images[train][:n_val])
            h5file.create_dataset('Y_val', data=classes[train][:n_val])
            h5file.create_dataset('auxvars_val', data=auxvars[train][:n_val])
            h5file.create_dataset('X_train', data=images[train][n_val:])
            h5file.create_dataset('Y_train', data=classes[train][n_val:])
            h5file.create_dataset('auxvars_train', data=auxvars[train][n_val:])
        return {'train': out_file}
    rs = cross_validation.ShuffleSplit(n_images, n_iter=1, test_size=test_frac, random_state=shuffle_seed)
    for (trn, tst) in rs:
        (train, test) = (trn, tst)
    out_file = (dataset_name + '_test.h5')
    with h5py.File(out_file, 'w') as h5file:
        h5file.create_dataset('X_test', data=images[test])
        h5file.create_dataset('Y_test', data=classes[test])
        h5file.create_dataset('auxvars_test', data=auxvars[test])
    file_dict = {'test': out_file}
    if (n_folds > 1):
        kf = cross_validation.KFold(len(train), n_folds, shuffle=True, random_state=shuffle_seed)
        i = 0
        kf_files = []
        for (ktrain, ktest) in kf:
            np.random.shuffle(ktrain)
            out_file = (dataset_name + '_train_kf{0}.h5'.format(i))
            with h5py.File(out_file, 'w') as h5file:
                h5file.create_dataset('X_test', data=images[train][ktest])
                h5file.create_dataset('Y_test', data=classes[train][ktest])
                h5file.create_dataset('auxvars_test', data=auxvars[train][ktest])
                n_val = int((val_frac * len(ktrain)))
                h5file.create_dataset('X_val', data=images[train][ktrain][:n_val])
                h5file.create_dataset('Y_val', data=classes[train][ktrain][:n_val])
                h5file.create_dataset('auxvars_val', data=auxvars[train][ktrain][:n_val])
                h5file.create_dataset('X_train', data=images[train][ktrain][n_val:])
                h5file.create_dataset('Y_train', data=classes[train][ktrain][n_val:])
                h5file.create_dataset('auxvars_train', data=auxvars[train][ktrain][n_val:])
            kf_files.append(out_file)
            i += 1
        file_dict['train'] = kf_files
    else:
        rs = np.random.RandomState(shuffle_seed)
        rs.shuffle(train)
        out_file = (dataset_name + '_train.h5')
        with h5py.File(out_file, 'w') as h5file:
            n_val = int((val_frac * len(train)))
            h5file.create_dataset('X_val', data=images[train][:n_val])
            h5file.create_dataset('Y_val', data=classes[train][:n_val])
            h5file.create_dataset('auxvars_val', data=auxvars[train][:n_val])
            h5file.create_dataset('X_train', data=images[train][n_val:])
            h5file.create_dataset('Y_train', data=classes[train][n_val:])
            h5file.create_dataset('auxvars_train', data=auxvars[train][n_val:])
        file_dict['train'] = out_file
    return file_dict

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.arange

idx = 5:------------------- similar code ------------------ index = 24, score = 6.0 
def train(base_lr, batch_sz, gpu_no, model_name, power_s):
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_no
    root_path = os.path.dirname(os.path.realpath(__file__))
    log_path = create_dir(os.path.join(root_path, 'log'))
    save_path = create_dir(os.path.join(root_path, 'weights'))
    acc_count = 0
    while (acc_count < 100):
        if os.path.exists(os.path.join(log_path, ('log_test_%02d.txt' % acc_count))):
            acc_count += 1
        else:
            break
    assert (acc_count < 100)
    log_train_fname = ('log_train_%02d.txt' % acc_count)
    log_test_fname = ('log_test_%02d.txt' % acc_count)
    n_class = 100
    batch_sz = batch_sz
    batch_test = 100
    max_epoch = 42500
    lr = base_lr
    momentum = 0.9
    is_training = tf.placeholder('bool')
    images = tf.placeholder(tf.float32, (None, 32, 32, 3))
    labels = tf.placeholder(tf.int32, None)
    vgg = VGG()
    vgg.build(images, n_class, is_training, model_name, power_s)
    fit_loss = loss2(vgg.score, labels, n_class, 'c_entropy')
    loss_op = fit_loss
    reg_loss_list = tf.losses.get_regularization_losses()
    if (len(reg_loss_list) != 0):
        reg_loss = tf.add_n(reg_loss_list)
        loss_op += reg_loss
    thom_loss_list = tf.get_collection('thomson_loss')
    if (len(thom_loss_list) != 0):
        thom_loss = tf.add_n(thom_loss_list)
        loss_op += thom_loss
    thom_final_list = tf.get_collection('thomson_final')
    if (len(thom_final_list) != 0):
        thom_final = tf.add_n(thom_final_list)
        loss_op += thom_final
    lr_ = tf.placeholder('float')
    update_op = tf.train.MomentumOptimizer(lr_, 0.9).minimize(loss_op)
    predc = vgg.pred
    acc_op = tf.reduce_mean(tf.to_float(tf.equal(labels, tf.to_int32(vgg.pred))))
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        tf.summary.scalar('fit loss', fit_loss)
        if (len(reg_loss_list) != 0):
            tf.summary.scalar('reg loss', reg_loss)
        if (len(thom_loss_list) != 0):
            tf.summary.scalar('thomson loss', thom_loss)
        if (len(thom_final_list) != 0):
            tf.summary.scalar('thomson final loss', thom_final)
        tf.summary.scalar('learning rate', lr)
        tf.summary.scalar('accuracy', acc_op)
        merged = tf.summary.merge_all()
        train_writer = tf.summary.FileWriter((root_path + '/tf_log'), sess.graph)
        print('====================')
        print(('Log will be saved to: ' + log_path))
        with open(os.path.join(log_path, log_train_fname), 'w'):
            pass
        with open(os.path.join(log_path, log_test_fname), 'w'):
            pass
        with open(os.path.join(log_path, log_train_fname), 'a') as train_acc_file:
            train_acc_file.write(('model_name: %s, power_s: %s\n' % (model_name, power_s)))
        with open(os.path.join(log_path, log_test_fname), 'a') as test_acc_file:
            test_acc_file.write(('model_name: %s, power_s: %s\n' % (model_name, power_s)))
        for i in range(max_epoch):
            t = (i % 390)
            if (t == 0):
                idx = np.arange(0, 50000)
                np.random.shuffle(idx)
                train_data['data'] = train_data['data'][idx]
                train_data['fine_labels'] = np.reshape(train_data['fine_labels'], [50000])
                train_data['fine_labels'] = train_data['fine_labels'][idx]
            (tr_images, tr_labels) = ip.load_train(train_data, batch_sz, t)
            if (i == 20000):
                lr *= 0.1
            elif (i == 30000):
                lr *= 0.1
            elif (i == 37500):
                lr *= 0.1
            if (len(thom_loss_list) != 0):
                (summary, fit, reg, thom, thomf, acc, _) = sess.run([merged, fit_loss, reg_loss, thom_loss, thom_final, acc_op, update_op], {lr_: lr, is_training: True, images: tr_images, labels: tr_labels})
                if (((i % 100) == 0) and (i != 0)):
                    print(('====iter_%d: fit=%.4f, reg=%.4f, thom=%.4f, thomf=%.4f, acc=%.4f' % (i, fit, reg, thom, thomf, acc)))
                    with open(os.path.join(log_path, log_train_fname), 'a') as train_acc_file:
                        train_acc_file.write(('====iter_%d: fit=%.4f, reg=%.4f, thom=%.4f, thomf=%.4f, acc=%.4f\n' % (i, fit, reg, thom, thomf, acc)))
                train_writer.add_summary(summary, i)
            else:
                (summary, fit, reg, acc, _) = sess.run([merged, fit_loss, reg_loss, acc_op, update_op], {lr_: lr, is_training: True, images: tr_images, labels: tr_labels})
                if (((i % 100) == 0) and (i != 0)):
                    print(('====iter_%d: fit=%.4f, reg=%.4f, acc=%.4f' % (i, fit, reg, acc)))
                    with open(os.path.join(log_path, log_train_fname), 'a') as train_acc_file:
                        train_acc_file.write(('====iter_%d: fit=%.4f, reg=%.4f, acc=%.4f\n' % (i, fit, reg, acc)))
                train_writer.add_summary(summary, i)
            if (((i % 500) == 0) and (i != 0)):
                n_test = 10000
                acc = 0.0
                for j in range(int((n_test / batch_test))):
                    (te_images, te_labels) = ip.load_test(test_data, batch_test, j)
                    acc = (acc + sess.run(acc_op, {is_training: False, images: te_images, labels: te_labels}))
                acc = ((acc * batch_test) / float(n_test))
                print(('++++iter_%d: test acc=%.4f' % (i, acc)))
                with open(os.path.join(log_path, log_test_fname), 'a') as test_acc_file:
                    test_acc_file.write(('++++iter_%d: test acc=%.4f\n' % (i, acc)))
            if (((i % 10000) == 0) and (i != 0)):
                tf.train.Saver().save(sess, os.path.join(save_path, str(i)))
        tf.train.Saver().save(sess, os.path.join(save_path, str(i)))
        n_test = 10000
        acc = 0.0
        for j in range(int((n_test / batch_test))):
            (te_images, te_labels) = ip.load_test(test_data, batch_test, j)
            acc = (acc + sess.run(acc_op, {is_training: False, images: te_images, labels: te_labels}))
        acc = ((acc * batch_test) / float(n_test))
        print(('++++iter_%d: test acc=%.4f' % (i, acc)))
        with open(os.path.join(log_path, log_test_fname), 'a') as test_acc_file:
            test_acc_file.write(('++++iter_%d: test acc=%.4f\n' % (i, acc)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        for  ...  in:
            if:
                 ...  = np.arange

idx = 6:------------------- similar code ------------------ index = 53, score = 6.0 
def gen_new_list(self):
    np.random.seed(0)
    all_size = self.total_size
    indices = np.arange(len(self.dataset))
    indices = indices[:all_size]
    num_repeat = (((all_size - 1) // indices.shape[0]) + 1)
    indices = np.tile(indices, num_repeat)
    indices = indices[:all_size]
    np.random.shuffle(indices)
    assert (len(indices) == self.total_size)
    return indices

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 7:------------------- similar code ------------------ index = 29, score = 6.0 
def gen_new_list(self):
    np.random.seed(0)
    all_size = (self.total_size * self.world_size)
    indices = np.arange(len(self.dataset))
    indices = indices[:all_size]
    num_repeat = (((all_size - 1) // indices.shape[0]) + 1)
    indices = np.tile(indices, num_repeat)
    indices = indices[:all_size]
    np.random.shuffle(indices)
    beg = (self.total_size * self.rank)
    indices = indices[beg:(beg + self.total_size)]
    assert (len(indices) == self.total_size)
    return indices

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 8:------------------- similar code ------------------ index = 31, score = 6.0 
def main():
    np.random.seed(314)
    batch_size = 3
    num_hidden = 5
    sequence_length = 4
    model = LinkInFor(num_hidden)
    x = np.random.rand(batch_size, sequence_length, num_hidden).astype(np.float32)
    h = np.random.rand(batch_size, num_hidden).astype(np.float32)
    args = [x, h, np.arange(sequence_length)]
    testtools.generate_testcase(model, args)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = [ ... ,  ... , np.arange]

idx = 9:------------------- similar code ------------------ index = 78, score = 6.0 
def icecore_diffuse(d18O, b, time, T, P, depth, depth_horizons, dz, drho):
    '\n        DOCSTRING: Function \'icecore_diffuse\'\n        DESCRIPTION: accounts for diffusion and compaction in the firn.\n\n        Inputs:\n            d18O: ice core isotope ratio, output from sensor Model (permil)\n            b   : average accumulation rate at site (m/year)\n            time: calendar years of record\n            T   : average temperature at site (K)\n            P   : average sea level pressure at site (atm)\n            depth: total depth of core\n            depth_horizons: accumulation by year in ice core--depth horizons (moving downwards in core) in meters\n            dz: step in depth (default = min(depth_horizons)/10.) \n            drho: step in density (default=0.5 kg/m^3)\n\n        Diffusion is computed using a convolution (Gaussian smoothing).\n\n        Functionality: Calculates diffusion length as a function of density\n        in firn, and given vectors of time-depth and density-depth \n        Also expects Pressure in atm, T in K, rho and rho_ice in \n        kg/m^3, but defaults on these, so only the first three arguments\n        must be entered.\n\n        "Time" is really "age" (increasing down core) and is given in years.\n        z is depth in meters\n        rho should be in kg/m^3\n        the vectors rho, time, and z should all correponding with one another\n    '
    import numpy as np
    import scipy
    from scipy import integrate
    import matplotlib.pyplot as plt
    R = 8.314478
    m = 0.01802
    rho_s = 300.0
    rho_d = 822.0
    rho_i = 920.0
    z = (np.arange(0, depth, dz) + dz)
    (rho, zieq, t) = densification(T, b, rho_s, z)
    rho = rho[0:len(z)]
    time_d = np.cumsum((((dz / b) * rho) / rho_i))
    ts = (((time_d * 365.25) * 24) * 3600)
    drho = np.diff(rho)
    dtdrho = (np.diff(ts) / np.diff(rho))
    D = diffusivity(rho, T, P, rho_d, b)
    D = D[0:(- 1)]
    rho = rho[0:(- 1)]
    diffs = (np.diff(z) / np.diff(time_d))
    diffs = diffs[0:(- 1)]
    solidice = np.where((rho >= (rho_d - 5.0)))
    diffusion = np.where((rho < (rho_d - 5.0)))
    sigma_sqrd_dummy = ((((2 * np.power(rho, 2)) * dtdrho) * D) * drho)
    sigma_sqrd = integrate.cumtrapz(sigma_sqrd_dummy)
    rho = rho[0:(- 1)]
    sigma = np.zeros((len(rho) + 1))
    sigma[diffusion] = np.sqrt(((1 / np.power(rho, 2)) * sigma_sqrd))
    sigma[solidice] = sigma[diffusion][(- 1)]
    sigma = sigma[0:(- 1)]
    del18 = np.flipud(d18O)
    depth_horizons = depth_horizons
    years_rev = np.flipud(time)
    z = np.reshape(z, len(z))
    del18 = np.reshape(del18, len(del18))
    iso_interp = np.interp(z, depth_horizons, del18)
    time_interp = np.interp(z, depth_horizons, years_rev)
    diffused_final = np.zeros(len(iso_interp))
    zp = np.arange((- 100), 100, dz)
    if (len(zp) >= (0.5 * len(z))):
        print('Warning: convolution kernal length (zp) is approaching that of half the length of timeseries. Kernal being clipped.')
        bound = ((0.2 * len(z)) * dz)
        zp = np.arange((- bound), bound, dz)
    sigma_dummy = np.tile(0.08, len(sigma))
    for i in range(len(sigma)):
        part1 = (1.0 / (sigma[i] * np.sqrt((2.0 * np.pi))))
        part2 = scipy.exp(((- (zp ** 2)) / (2 * (sigma[i] ** 2))))
        G = (part1 * part2)
        rm = np.mean(iso_interp)
        cdel = (iso_interp - rm)
        diffused = (np.convolve(G, cdel, mode='same') * dz)
        diffused = (diffused + rm)
        diffused_final[i] = diffused[i]
    diffused_timeseries = diffused_final[0:(- 3)]
    final_iso = np.interp(depth_horizons, z[0:(- 3)], diffused_timeseries)
    ice_diffused = final_iso
    return (z, sigma, D, time_d, diffs, ice_diffused, rho, zieq)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = (np.arange +  ... )

idx = 10:------------------- similar code ------------------ index = 77, score = 6.0 
def _shuffle_update_order(self, n):
    self._update_order = np.arange(n)
    np.random.shuffle(self._update_order)
    self._update_order = self._update_order.tolist()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = np.arange

idx = 11:------------------- similar code ------------------ index = 35, score = 6.0 
def __getitem__(self, idx):
    pt_idxs = np.arange(0, self.num_points)
    np.random.shuffle(pt_idxs)
    current_points = torch.from_numpy(self.points[(idx, pt_idxs)].copy()).float()
    current_labels = torch.from_numpy(self.labels[(idx, pt_idxs)].copy()).long()
    return (current_points, current_labels)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 12:------------------- similar code ------------------ index = 20, score = 6.0 
def shuffle_data(data, labels):
    ' Shuffle data and labels.\n        Input:\n          data: B,N,... numpy array\n          label: B,... numpy array\n        Return:\n          shuffled data, label and shuffle indices\n    '
    idx = np.arange(len(labels))
    np.random.shuffle(idx)
    return (data[(idx, ...)], labels[idx], idx)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 13:------------------- similar code ------------------ index = 37, score = 6.0 
def main():
    np.random.seed(314)
    batch_size = 3
    sequence_length = 4
    num_vocabs = 10
    num_hidden = 5
    model_fn = MyLSTM(num_hidden, batch_size, sequence_length)
    (labels, lengths) = sequence_utils.gen_random_sequence(batch_size, sequence_length, num_vocabs)
    xs = []
    for l in lengths:
        xs.append(np.random.rand(l, num_hidden).astype(dtype=np.float32))
    h = np.zeros((batch_size, num_hidden), dtype=np.float32)
    c = np.zeros((batch_size, num_hidden), dtype=np.float32)
    mask = (np.expand_dims(np.arange(sequence_length), 0) < np.expand_dims(lengths, 1)).astype(np.float32)
    args = [xs, h, c, mask]
    testtools.generate_testcase(model_fn, args)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = ( ... . ... (np.arange,  ... ) <)

idx = 14:------------------- similar code ------------------ index = 42, score = 6.0 
if (__name__ == '__main__'):
    import numpy as np
    np.random.seed(314)
    batch_size = 3
    sequence_length = 4
    num_vocabs = 10
    num_hidden = 5
    model_fn = (lambda : MyLSTM(num_hidden, batch_size, sequence_length))
    (labels, lengths) = sequence_utils.gen_random_sequence(batch_size, sequence_length, num_vocabs)
    xs = []
    for l in lengths:
        xs.append(np.random.rand(l, num_hidden).astype(dtype=np.float32))
    h = np.zeros((batch_size, num_hidden), dtype=np.float32)
    c = np.zeros((batch_size, num_hidden), dtype=np.float32)
    mask = (np.expand_dims(np.arange(sequence_length), 0) < np.expand_dims(lengths, 1)).astype(np.float32)
    args = [xs, h, c, mask]
    ch2o.generate_testcase(model_fn, args)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
     ...  = ( ... . ... (np.arange,  ... ) <)

idx = 15:------------------- similar code ------------------ index = 43, score = 6.0 
def gen_new_list(self):
    np.random.seed(0)
    all_size = (self.total_size * self.world_size)
    indices = np.arange(len(self.dataset))
    np.random.shuffle(indices)
    indices = indices[:all_size]
    num_repeat = (((all_size - 1) // indices.shape[0]) + 1)
    indices = np.tile(indices, num_repeat)
    indices = indices[:all_size]
    np.random.shuffle(indices)
    beg = (self.total_size * self.rank)
    indices = indices[beg:(beg + self.total_size)]
    assert (len(indices) == self.total_size)
    return indices

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 16:------------------- similar code ------------------ index = 61, score = 6.0 
def fn(test_name):
    gb = onnx_script.GraphBuilder(test_name)
    if (cell_type == 'LSTM'):
        wr = 8
        perm = [0, 2, 1, 3, 4, 6, 5, 7]
        num_direction = 1
        direction = 'forward'
    elif (cell_type == 'BiLSTM'):
        wr = 16
        perm = (np.tile([0, 2, 1, 3], 4) + (np.repeat(np.arange(4), 4) * 4))
        num_direction = 2
        direction = 'bidirectional'
    elif (cell_type == 'GRU'):
        wr = 6
        perm = [1, 0, 2, 4, 3, 5]
        num_direction = 1
        direction = 'forward'
    elif (cell_type == 'BiGRU'):
        wr = 12
        perm = [1, 0, 2, 4, 3, 5, 7, 6, 8, 10, 9, 11]
        num_direction = 2
        direction = 'bidirectional'
    else:
        raise RuntimeError(('Unknown cell_type: %s' % cell_type))
    embed_size = num_hidden
    np.random.seed(42)
    if ((batch_size == 3) and (sequence_length == 4)):
        labels = np.array([[1, 2, 3, 7], [4, 5, 0, 0], [6, 0, 0, 0]])
        lengths = np.array([4, 2, 1])
        targets = np.array([1, 0, 1])
    else:
        (labels, lengths) = _gen_random_sequence(batch_size, sequence_length, num_vocabs)
        targets = np.random.randint(2, size=batch_size)
    labels = labels.astype(np.int32)
    embed = param_initializer(size=(num_vocabs, embed_size)).astype(np.float32)
    weight = param_initializer(size=(embed_size, (num_hidden * wr))).astype(np.float32)
    bias = param_initializer(size=((num_hidden * wr),)).astype(np.float32)
    linear_w = param_initializer(size=((num_direction * num_hidden), 2)).astype(np.float32)
    linear_b = param_initializer(size=(2,)).astype(np.float32)
    x = F.embed_id(labels, embed)
    state = np.zeros((num_direction, len(labels), num_hidden)).astype(np.float32)
    xs = F.transpose_sequence([v[:l] for (v, l) in zip(x, lengths)])
    ch_weight = np.split(weight, wr, axis=1)
    ch_weight = [ch_weight[i] for i in perm]
    ch_bias = np.split(bias, wr, axis=0)
    ch_bias = [ch_bias[i] for i in perm]
    if (cell_type == 'LSTM'):
        (h, _, rnn_outputs) = F.n_step_lstm(1, 0.0, state, state, [ch_weight], [ch_bias], xs)
    elif (cell_type == 'BiLSTM'):
        (h, _, rnn_outputs) = F.n_step_bilstm(1, 0.0, state, state, [ch_weight[:8], ch_weight[8:]], [ch_bias[:8], ch_bias[8:]], xs)
    elif (cell_type == 'GRU'):
        (h, rnn_outputs) = F.n_step_gru(1, 0.0, state, [ch_weight], [ch_bias], xs)
    elif (cell_type == 'BiGRU'):
        (h, rnn_outputs) = F.n_step_bigru(1, 0.0, state, [ch_weight[:6], ch_weight[6:]], [ch_bias[:6], ch_bias[6:]], xs)
    shape = (len(labels), (num_hidden * num_direction))
    h = F.reshape(h, shape)
    rnn_outputs = F.pad_sequence(rnn_outputs)
    rnn_outputs = F.reshape(rnn_outputs, ((- 1), len(labels), num_direction, num_hidden))
    rnn_outputs = F.transpose(rnn_outputs, axes=[0, 2, 1, 3])
    result = F.linear(h, np.transpose(linear_w), linear_b)
    loss = F.softmax_cross_entropy(result, targets)
    (weight_w, weight_r) = np.split(weight, 2, axis=1)
    labels_v = gb.input('labels', labels)
    lengths_v = gb.input('lengths', lengths)
    targets_v = gb.input('targets', targets)
    embed_v = gb.param('embed', embed)
    weight_w_v = gb.param('weight_w', np.reshape(np.transpose(weight_w), (num_direction, (- 1), embed_size)))
    weight_r_v = gb.param('weight_r', np.reshape(np.transpose(weight_r), (num_direction, (- 1), num_hidden)))
    bias_v = gb.param('bias', np.reshape(bias, (num_direction, (- 1))))
    linear_w_v = gb.param('linear_w', linear_w)
    linear_b_v = gb.param('linear_b', linear_b)
    x = gb.Gather([embed_v, labels_v])
    x = gb.Transpose([x], perm=[1, 0, 2])
    if (cell_type in ['LSTM', 'BiLSTM']):
        (rnn_outputs_v, h) = gb.LSTM([x, weight_w_v, weight_r_v, bias_v, lengths_v], outputs=['rnn_outputs', 'last_state'], hidden_size=num_hidden, direction=direction)
    elif (cell_type in ['GRU', 'BiGRU']):
        (rnn_outputs_v, h) = gb.GRU([x, weight_w_v, weight_r_v, bias_v, lengths_v], outputs=['rnn_outputs', 'last_state'], hidden_size=num_hidden, direction=direction)
    shape_v = gb.const(shape)
    h = gb.Reshape([h, shape_v])
    result_v = gb.Gemm([h, linear_w_v, linear_b_v])
    loss_v = gb.ChainerSoftmaxCrossEntropy([result_v, targets_v])
    if (not output_loss_only):
        gb.output(rnn_outputs_v, rnn_outputs.array)
        gb.output(result_v, result.array)
    gb.output(loss_v, loss.array)
    gb.gen_test()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:    elif:
         ...  = ( + ( ... . ... (np.arange,  ... ) *  ... ))

idx = 17:------------------- similar code ------------------ index = 49, score = 6.0 
def run(self, total_steps):
    ' Runs PPO\n\n        Args:\n            total_steps (int): total number of environment steps to run for\n        '
    N = self.num_workers
    T = self.worker_steps
    E = self.opt_epochs
    A = self.venv.action_space.n
    while (self.taken_steps < total_steps):
        progress = (self.taken_steps / total_steps)
        (obs, rewards, masks, actions, steps) = self.interact()
        ob_shape = obs.size()[2:]
        ep_reward = self.test()
        self.reward_histr.append(ep_reward)
        self.steps_histr.append(self.taken_steps)
        group_size = (len(self.steps_histr) // self.plot_points)
        if (self.plot_reward and ((len(self.steps_histr) % (self.plot_points * 10)) == 0) and (group_size >= 10)):
            (x_means, _, y_means, y_stds) = mean_std_groups(np.array(self.steps_histr), np.array(self.reward_histr), group_size)
            fig = plt.figure()
            fig.set_size_inches(8, 6)
            plt.ticklabel_format(axis='x', style='sci', scilimits=((- 2), 6))
            plt.errorbar(x_means, y_means, yerr=y_stds, ecolor='xkcd:blue', fmt='xkcd:black', capsize=5, elinewidth=1.5, mew=1.5, linewidth=1.5)
            plt.title('Training progress')
            plt.xlabel('Total steps')
            plt.ylabel('Episode reward')
            plt.savefig(self.plot_path, dpi=200)
            plt.clf()
            plt.close()
            plot_timer = 0
        obs_ = obs.view(((((T + 1) * N),) + ob_shape))
        obs_ = Variable(obs_)
        (_, values) = self.policy(obs_)
        values = values.view((T + 1), N, 1)
        (advantages, returns) = gae(rewards, masks, values, self.gamma, self.lambd)
        self.policy_old.load_state_dict(self.policy.state_dict())
        for e in range(E):
            self.policy.zero_grad()
            MB = (steps // self.minibatch_steps)
            b_obs = Variable(obs[:T].view(((steps,) + ob_shape)))
            b_rewards = Variable(rewards.view(steps, 1))
            b_masks = Variable(masks.view(steps, 1))
            b_actions = Variable(actions.view(steps, 1))
            b_advantages = Variable(advantages.view(steps, 1))
            b_returns = Variable(returns.view(steps, 1))
            b_inds = np.arange(steps)
            np.random.shuffle(b_inds)
            for start in range(0, steps, self.minibatch_steps):
                mb_inds = b_inds[start:(start + self.minibatch_steps)]
                mb_inds = cuda_if(torch.from_numpy(mb_inds).long(), self.cuda)
                (mb_obs, mb_rewards, mb_masks, mb_actions, mb_advantages, mb_returns) = [arr[mb_inds] for arr in [b_obs, b_rewards, b_masks, b_actions, b_advantages, b_returns]]
                (mb_pis, mb_vs) = self.policy(mb_obs)
                (mb_pi_olds, mb_v_olds) = self.policy_old(mb_obs)
                (mb_pi_olds, mb_v_olds) = (mb_pi_olds.detach(), mb_v_olds.detach())
                losses = self.objective(self.clip_func(progress), mb_pis, mb_vs, mb_pi_olds, mb_v_olds, mb_actions, mb_advantages, mb_returns)
                (policy_loss, value_loss, entropy_loss) = losses
                loss = ((policy_loss + (value_loss * self.value_coef)) + (entropy_loss * self.entropy_coef))
                set_lr(self.optimizer, self.lr_func(progress))
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm(self.policy.parameters(), self.max_grad_norm)
                self.optimizer.step()
        self.taken_steps += steps
        print(self.taken_steps)
        torch.save({'policy': self.policy.state_dict()}, (('./save/PPO_' + self.env_name) + '.pt'))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    while:
        for  ...  in:
             ...  = np.arange

idx = 18:------------------- similar code ------------------ index = 57, score = 6.0 
def test_smooth_lsf(pop_and_params):
    (pop, params) = pop_and_params
    _reset_default_params(pop, params)
    tmax = 1.0
    wave_lsf = np.arange(4000, 7000.0, 10)
    x = ((wave_lsf - 5500) / 1500.0)
    sigma_lsf = (50 * ((1.0 + (0.4 * x)) + (0.6 * (x ** 2))))
    (w, spec) = pop.get_spectrum(tage=tmax)
    pop.params['smooth_lsf'] = True
    assert (pop.params.dirtiness == 2)
    pop.set_lsf(wave_lsf, sigma_lsf)
    (w, smspec) = pop.get_spectrum(tage=tmax)
    hi = (w > 7100)
    sm = ((w < 7000) & (w > 3000))
    assert np.allclose(((spec[hi] / smspec[hi]) - 1.0), 0.0)
    assert (not np.allclose(((spec[sm] / smspec[sm]) - 1.0), 0.0))
    pop.set_lsf(wave_lsf, (sigma_lsf * 2))
    assert (pop.params.dirtiness == 2)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 19:------------------- similar code ------------------ index = 23, score = 6.0 
def train_batch(self, epochs=1, lr=0.001, seq=1):
    count = (self.settings.num_dataset + 1)
    checkdir = 'checkpoint'
    try:
        os.mkdir(checkdir)
    except FileExistsError:
        print('Folder exists: ', checkdir)
    is_model_compiled = False
    indexes = np.arange(1, count)
    np.random.shuffle(indexes)
    for i in indexes:
        filename = self.settings.dataset
        filename += ('.densemapnet.weights.%d-%d.h5' % (seq, i))
        filepath = os.path.join(checkdir, filename)
        checkpoint = ModelCheckpoint(filepath=filepath, save_weights_only=True, verbose=1, save_best_only=False)
        callbacks = [checkpoint]
        self.load_train_data(i)
        if (self.network is None):
            self.network = DenseMapNet(settings=self.settings)
            self.model = self.network.build_model()
        if (not is_model_compiled):
            print('Using loss=mae on final conv layer')
            self.model.compile(loss=_loss_mae_disparity, optimizer=Adam(lr=lr))
            is_model_compiled = True
        if self.settings.model_weights:
            if self.settings.notrain:
                self.predict_disparity()
                return
        x = [self.train_lx, self.train_rx]
        self.model.fit(x, self.train_dx, epochs=epochs, batch_size=self.settings.batch_size, shuffle=True, callbacks=callbacks)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 20:------------------- similar code ------------------ index = 17, score = 6.0 
def gen_new_list(self):
    np.random.seed(0)
    all_size = (self.total_size * self.world_size)
    indices = np.arange(len(self.dataset))
    np.random.shuffle(indices)
    indices = indices[:all_size]
    num_repeat = (((all_size - 1) // indices.shape[0]) + 1)
    indices = np.tile(indices, num_repeat)
    indices = indices[:all_size]
    np.random.shuffle(indices)
    beg = (self.total_size * self.rank)
    indices = indices[beg:(beg + self.total_size)]
    assert (len(indices) == self.total_size)
    return indices

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 21:------------------- similar code ------------------ index = 7, score = 6.0 
def reset(self):
    self.idxs = np.arange(0, len(self.datapath))
    if self.shuffle:
        np.random.shuffle(self.idxs)
    self.num_batches = (((len(self.datapath) + self.batch_size) - 1) // self.batch_size)
    self.batch_idx = 0

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = np.arange

idx = 22:------------------- similar code ------------------ index = 2, score = 6.0 
def gen_rnn_sentiment_test(cell_type, num_vocabs=10, num_hidden=5, batch_size=3, sequence_length=4, output_loss_only=False, param_initializer=np.random.random):

    def fn(test_name):
        gb = onnx_script.GraphBuilder(test_name)
        if (cell_type == 'LSTM'):
            wr = 8
            perm = [0, 2, 1, 3, 4, 6, 5, 7]
            num_direction = 1
            direction = 'forward'
        elif (cell_type == 'BiLSTM'):
            wr = 16
            perm = (np.tile([0, 2, 1, 3], 4) + (np.repeat(np.arange(4), 4) * 4))
            num_direction = 2
            direction = 'bidirectional'
        elif (cell_type == 'GRU'):
            wr = 6
            perm = [1, 0, 2, 4, 3, 5]
            num_direction = 1
            direction = 'forward'
        elif (cell_type == 'BiGRU'):
            wr = 12
            perm = [1, 0, 2, 4, 3, 5, 7, 6, 8, 10, 9, 11]
            num_direction = 2
            direction = 'bidirectional'
        else:
            raise RuntimeError(('Unknown cell_type: %s' % cell_type))
        embed_size = num_hidden
        np.random.seed(42)
        if ((batch_size == 3) and (sequence_length == 4)):
            labels = np.array([[1, 2, 3, 7], [4, 5, 0, 0], [6, 0, 0, 0]])
            lengths = np.array([4, 2, 1])
            targets = np.array([1, 0, 1])
        else:
            (labels, lengths) = _gen_random_sequence(batch_size, sequence_length, num_vocabs)
            targets = np.random.randint(2, size=batch_size)
        labels = labels.astype(np.int32)
        embed = param_initializer(size=(num_vocabs, embed_size)).astype(np.float32)
        weight = param_initializer(size=(embed_size, (num_hidden * wr))).astype(np.float32)
        bias = param_initializer(size=((num_hidden * wr),)).astype(np.float32)
        linear_w = param_initializer(size=((num_direction * num_hidden), 2)).astype(np.float32)
        linear_b = param_initializer(size=(2,)).astype(np.float32)
        x = F.embed_id(labels, embed)
        state = np.zeros((num_direction, len(labels), num_hidden)).astype(np.float32)
        xs = F.transpose_sequence([v[:l] for (v, l) in zip(x, lengths)])
        ch_weight = np.split(weight, wr, axis=1)
        ch_weight = [ch_weight[i] for i in perm]
        ch_bias = np.split(bias, wr, axis=0)
        ch_bias = [ch_bias[i] for i in perm]
        if (cell_type == 'LSTM'):
            (h, _, rnn_outputs) = F.n_step_lstm(1, 0.0, state, state, [ch_weight], [ch_bias], xs)
        elif (cell_type == 'BiLSTM'):
            (h, _, rnn_outputs) = F.n_step_bilstm(1, 0.0, state, state, [ch_weight[:8], ch_weight[8:]], [ch_bias[:8], ch_bias[8:]], xs)
        elif (cell_type == 'GRU'):
            (h, rnn_outputs) = F.n_step_gru(1, 0.0, state, [ch_weight], [ch_bias], xs)
        elif (cell_type == 'BiGRU'):
            (h, rnn_outputs) = F.n_step_bigru(1, 0.0, state, [ch_weight[:6], ch_weight[6:]], [ch_bias[:6], ch_bias[6:]], xs)
        shape = (len(labels), (num_hidden * num_direction))
        h = F.reshape(h, shape)
        rnn_outputs = F.pad_sequence(rnn_outputs)
        rnn_outputs = F.reshape(rnn_outputs, ((- 1), len(labels), num_direction, num_hidden))
        rnn_outputs = F.transpose(rnn_outputs, axes=[0, 2, 1, 3])
        result = F.linear(h, np.transpose(linear_w), linear_b)
        loss = F.softmax_cross_entropy(result, targets)
        (weight_w, weight_r) = np.split(weight, 2, axis=1)
        labels_v = gb.input('labels', labels)
        lengths_v = gb.input('lengths', lengths)
        targets_v = gb.input('targets', targets)
        embed_v = gb.param('embed', embed)
        weight_w_v = gb.param('weight_w', np.reshape(np.transpose(weight_w), (num_direction, (- 1), embed_size)))
        weight_r_v = gb.param('weight_r', np.reshape(np.transpose(weight_r), (num_direction, (- 1), num_hidden)))
        bias_v = gb.param('bias', np.reshape(bias, (num_direction, (- 1))))
        linear_w_v = gb.param('linear_w', linear_w)
        linear_b_v = gb.param('linear_b', linear_b)
        x = gb.Gather([embed_v, labels_v])
        x = gb.Transpose([x], perm=[1, 0, 2])
        if (cell_type in ['LSTM', 'BiLSTM']):
            (rnn_outputs_v, h) = gb.LSTM([x, weight_w_v, weight_r_v, bias_v, lengths_v], outputs=['rnn_outputs', 'last_state'], hidden_size=num_hidden, direction=direction)
        elif (cell_type in ['GRU', 'BiGRU']):
            (rnn_outputs_v, h) = gb.GRU([x, weight_w_v, weight_r_v, bias_v, lengths_v], outputs=['rnn_outputs', 'last_state'], hidden_size=num_hidden, direction=direction)
        shape_v = gb.const(shape)
        h = gb.Reshape([h, shape_v])
        result_v = gb.Gemm([h, linear_w_v, linear_b_v])
        loss_v = gb.ChainerSoftmaxCrossEntropy([result_v, targets_v])
        if (not output_loss_only):
            gb.output(rnn_outputs_v, rnn_outputs.array)
            gb.output(result_v, result.array)
        gb.output(loss_v, loss.array)
        gb.gen_test()
    return fn

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():

    def  ... ( ... ):
        if:        elif:
             ...  = ( + ( ... . ... (np.arange,  ... ) *  ... ))

idx = 23:------------------- similar code ------------------ index = 5, score = 6.0 
def convert_files_to_npz(input_folder, out_folder, out_prefix):
    files = glob.glob((input_folder + '*LEFT_RGB*.tif'))
    num = len(files)
    print('Number of images = ', num)
    if (num == 0):
        print('No matching files found', file=stderr)
        return
    train_fraction = TRAIN_FRACTION
    num_train = int((train_fraction * num))
    max_per_train = MAX_IMAGES_PER_TRAIN_FILE
    print('Number of training images = ', num_train)
    print('Number of validation images = ', (num - num_train))
    count = 0
    num_files = 0
    disparities = []
    lefts = []
    rights = []
    left_categories = []
    left_agls = []
    indices = np.arange(num)
    np.random.seed(0)
    np.random.shuffle(indices)
    files = [files[i] for i in indices]
    for i in tqdm(range(num)):
        left_name = os.path.basename(files[i])
        start = left_name.find('LEFT_RGB')
        right_name = ((input_folder + left_name[0:start]) + 'RIGHT_RGB.tif')
        left_agl_name = ((input_folder + left_name[0:start]) + 'LEFT_AGL.tif')
        disparity_name = ((input_folder + left_name[0:start]) + 'LEFT_DSP.tif')
        left_cls_name = ((input_folder + left_name[0:start]) + 'LEFT_CLS.tif')
        left_name = (input_folder + left_name)
        left = np.array(tifffile.imread(left_name))
        right = np.array(tifffile.imread(right_name))
        left_cls = np.array(tifffile.imread(left_cls_name))
        disparity = np.array(tifffile.imread(disparity_name))
        left_agl = np.array(tifffile.imread(left_agl_name))
        left_labels = las_to_sequential_labels(left_cls)
        lefts.append(left)
        rights.append(right)
        disparities.append(disparity)
        left_categories.append(left_labels)
        left_agls.append(left_agl)
        count = (count + 1)
        if (((count >= max_per_train) and (i < num_train)) or (i == (num_train - 1))):
            num_files = (num_files + 1)
            print(' ')
            print('Counts for train file ', num_files)
            cats = np.asarray(left_categories)
            max_category = cats.max()
            for j in range(max_category):
                print(j, ': ', len(cats[(cats == j)]))
            print('Writing files...')
            print(' ')
            out_path = Path(out_folder)
            if (not out_path.exists()):
                out_path.mkdir()
            disparity_name = join(out_path, (((out_prefix + '.train.disparity.') + '{:1d}'.format(num_files)) + '.npz'))
            left_name = join(out_path, (((out_prefix + '.train.left.') + '{:1d}'.format(num_files)) + '.npz'))
            right_name = join(out_path, (((out_prefix + '.train.right.') + '{:1d}'.format(num_files)) + '.npz'))
            left_cat_name = join(out_path, (((out_prefix + '.train.left_label.') + '{:1d}'.format(num_files)) + '.npz'))
            left_agl_name = join(out_path, (((out_prefix + '.train.left_agl.') + '{:1d}'.format(num_files)) + '.npz'))
            np.savez_compressed(disparity_name, disparities)
            np.savez_compressed(left_name, lefts)
            np.savez_compressed(right_name, rights)
            np.savez_compressed(left_cat_name, left_categories)
            np.savez_compressed(left_agl_name, left_agls)
            count = 0
            disparities = []
            lefts = []
            rights = []
            left_categories = []
            left_agls = []
    print(' ')
    print('Counts for validation file')
    cats = np.asarray(left_categories)
    max_category = cats.max()
    for j in range(max_category):
        print(j, ': ', len(cats[(cats == j)]))
    print('Writing files...')
    print(' ')
    out_path = Path(out_folder)
    if (not out_path.exists()):
        out_path.mkdir()
    print('Writing validation files')
    print('Number of validation samples = ', len(disparities))
    disparity_name = join(out_path, (out_prefix + '.test.disparity.npz'))
    left_name = join(out_path, (out_prefix + '.test.left.npz'))
    right_name = join(out_path, (out_prefix + '.test.right.npz'))
    left_cat_name = join(out_path, (out_prefix + '.test.left_label.npz'))
    left_agl_name = join(out_path, (out_prefix + '.test.left_agl.npz'))
    np.savez_compressed(disparity_name, disparities)
    np.savez_compressed(left_name, lefts)
    np.savez_compressed(right_name, rights)
    np.savez_compressed(left_cat_name, left_categories)
    np.savez_compressed(left_agl_name, left_agls)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 24:------------------- similar code ------------------ index = 11, score = 6.0 
def reset(self):
    ' reset order of h5 files '
    self.file_idxs = np.arange(0, len(self.h5_files))
    if self.shuffle:
        np.random.shuffle(self.file_idxs)
    self.current_data = None
    self.current_label = None
    self.current_file_idx = 0
    self.batch_idx = 0

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = np.arange

idx = 25:------------------- similar code ------------------ index = 12, score = 6.0 
def hot_one_vector(y, max):
    import numpy as np
    labels_hot_vector = np.zeros((y.shape[0], max), dtype=np.int32)
    labels_hot_vector[(np.arange(y.shape[0]), y)] = 1
    return labels_hot_vector

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ... [(np.arange,  ... )]

idx = 26:------------------- similar code ------------------ index = 98, score = 6.0 
def train_model(self):
    checkdir = 'checkpoint'
    try:
        os.mkdir(checkdir)
    except FileExistsError:
        print('Folder exists: ', checkdir)
    lr = (self.args.lr + self.args.decay)
    for i in range(1, (self.args.n_epochs + 1)):
        lr = (lr - self.args.decay)
        indexes = np.arange(1, (self.args.n_trains + 1))
        np.random.shuffle(indexes)
        is_compiled = False
        for j in indexes:
            self.load_train_data(j)
            filename = ('us3d.icnet.weights.%d-%d.h5' % (i, j))
            filepath = os.path.join(checkdir, filename)
            checkpoint = ModelCheckpoint(filepath=filepath, save_weights_only=True, verbose=1, save_best_only=False)
            predict_callback = LambdaCallback(on_epoch_end=(lambda epoch, logs: self.compute_accuracy()))
            callbacks = [checkpoint, predict_callback]
            height = self.train_images[0].shape[0]
            width = self.train_images[0].shape[1]
            bands = self.train_images[0].shape[2]
            myloss = tversky_loss
            if (self.model is None):
                self.model = build_icnet(height, width, bands, self.n_classes, weights_path=self.args.checkpoint, train=True)
            if (not is_compiled):
                self.model.compile(optimizer=Adam(lr=lr), loss=myloss, loss_weights=[1.0, 0.4, 0.16])
                is_compiled = True
            self.model.fit(self.train_images, [self.Y1, self.Y2, self.Y3], epochs=1, batch_size=self.args.batch_size, shuffle=True, callbacks=callbacks)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
         ...  = np.arange

idx = 27:------------------- similar code ------------------ index = 100, score = 6.0 
def shuffle_data(data, labels):
    ' Shuffle data and labels.\n        Input:\n          data: B,N,... numpy array\n          label: B,... numpy array\n        Return:\n          shuffled data, label and shuffle indices\n    '
    idx = np.arange(len(labels))
    np.random.shuffle(idx)
    return (data[(idx, ...)], labels[idx], idx)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 28:------------------- similar code ------------------ index = 18, score = 5.0 
@lru_cache(maxsize=1)
def _center_function(population_size):
    centers = np.arange(0, population_size)
    centers = (centers / (population_size - 1))
    centers -= 0.5
    return centers

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 29:------------------- similar code ------------------ index = 44, score = 5.0 
def plot_hyperparam_env(ax, env):
    ylabel = list(relevant_param('AIRL').keys())
    r1 = np.arange(len(ylabel))
    barWidth = 0.125
    for (i, alg) in enumerate(algorithms):
        relevant_dict = relevant_param(alg)
        data = read_hyperparam(alg, env)
        xdata = []
        for key in ylabel:
            if (key in relevant_dict.keys()):
                hyperparam_range = relevant_dict[key]
                hyperparam_value = data[key]
                xdata.append((hyperparam_range.index(hyperparam_value) + 1))
            else:
                xdata.append(0)
        r = [(x + (i * barWidth)) for x in r1]
        ax.bar(r, xdata, color=colors[i], width=barWidth, edgecolor='white', label=alg)
    ax.set_xticks([(r + barWidth) for r in range(len(ylabel))])
    ax.set_ylabel(env)
    ax.set_yticklabels([])
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=30)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 30:------------------- similar code ------------------ index = 40, score = 5.0 
def _some_variables():
    parent = (np.array([0, 1, 2, 3, 4, 5, 6, 1, 8, 9, 10, 11, 12, 1, 14, 15, 16, 17, 18, 19, 16, 21, 22, 23, 24, 25, 26, 24, 28, 16, 30, 31, 32, 33, 34, 35, 33, 37]) - 1)
    offset = np.array([0, 0, 0, 0, 0, 0, 1.65674, (- 1.80282), 0.62477, 2.5972, (- 7.13576), 0, 2.49236, (- 6.8477), 0, 0.19704, (- 0.54136), 2.14581, 0, 0, 1.11249, 0, 0, 0, (- 1.6107), (- 1.80282), 0.62476, (- 2.59502), (- 7.12977), 0, (- 2.4678), (- 6.78024), 0, (- 0.23024), (- 0.63258), 2.13368, 0, 0, 1.11569, 0, 0, 0, 0.01961, 2.0545, (- 0.14112), 0.01021, 2.06436, (- 0.05921), 0, 0, 0, 0.00713, 1.56711, 0.14968, 0.03429, 1.56041, (- 0.10006), 0.01305, 1.6256, (- 0.05265), 0, 0, 0, 3.54205, 0.90436, (- 0.17364), 4.86513, 0, 0, 3.35554, 0, 0, 0, 0, 0, 0.66117, 0, 0, 0.53306, 0, 0, 0, 0, 0, 0.5412, 0, 0.5412, 0, 0, 0, (- 3.49802), 0.75994, (- 0.32616), (- 5.02649), 0, 0, (- 3.36431), 0, 0, 0, 0, 0, (- 0.73041), 0, 0, (- 0.58887), 0, 0, 0, 0, 0, (- 0.59786), 0, 0.59786]).reshape((- 1), 3)
    rotInd = [[6, 5, 4], [9, 8, 7], [12, 11, 10], [15, 14, 13], [18, 17, 16], [21, 20, 19], [], [24, 23, 22], [27, 26, 25], [30, 29, 28], [33, 32, 31], [36, 35, 34], [], [39, 38, 37], [42, 41, 40], [45, 44, 43], [48, 47, 46], [51, 50, 49], [54, 53, 52], [], [57, 56, 55], [60, 59, 58], [63, 62, 61], [66, 65, 64], [69, 68, 67], [72, 71, 70], [], [75, 74, 73], [], [78, 77, 76], [81, 80, 79], [84, 83, 82], [87, 86, 85], [90, 89, 88], [93, 92, 91], [], [96, 95, 94], []]
    posInd = []
    for ii in np.arange(38):
        if (ii == 0):
            posInd.append([1, 2, 3])
        else:
            posInd.append([])
    expmapInd = np.split((np.arange(4, 118) - 1), 38)
    return (parent, offset, posInd, expmapInd)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in np.arange:
idx = 31:------------------- similar code ------------------ index = 6, score = 5.0 
def _some_variables():
    parent = (np.array([0, 1, 2, 3, 4, 5, 6, 1, 8, 9, 10, 11, 12, 1, 14, 15, 16, 17, 18, 19, 16, 21, 22, 23, 24, 25, 26, 24, 28, 16, 30, 31, 32, 33, 34, 35, 33, 37]) - 1)
    offset = np.array([0, 0, 0, 0, 0, 0, 1.65674, (- 1.80282), 0.62477, 2.5972, (- 7.13576), 0, 2.49236, (- 6.8477), 0, 0.19704, (- 0.54136), 2.14581, 0, 0, 1.11249, 0, 0, 0, (- 1.6107), (- 1.80282), 0.62476, (- 2.59502), (- 7.12977), 0, (- 2.4678), (- 6.78024), 0, (- 0.23024), (- 0.63258), 2.13368, 0, 0, 1.11569, 0, 0, 0, 0.01961, 2.0545, (- 0.14112), 0.01021, 2.06436, (- 0.05921), 0, 0, 0, 0.00713, 1.56711, 0.14968, 0.03429, 1.56041, (- 0.10006), 0.01305, 1.6256, (- 0.05265), 0, 0, 0, 3.54205, 0.90436, (- 0.17364), 4.86513, 0, 0, 3.35554, 0, 0, 0, 0, 0, 0.66117, 0, 0, 0.53306, 0, 0, 0, 0, 0, 0.5412, 0, 0.5412, 0, 0, 0, (- 3.49802), 0.75994, (- 0.32616), (- 5.02649), 0, 0, (- 3.36431), 0, 0, 0, 0, 0, (- 0.73041), 0, 0, (- 0.58887), 0, 0, 0, 0, 0, (- 0.59786), 0, 0.59786]).reshape((- 1), 3)
    rotInd = [[6, 5, 4], [9, 8, 7], [12, 11, 10], [15, 14, 13], [18, 17, 16], [21, 20, 19], [], [24, 23, 22], [27, 26, 25], [30, 29, 28], [33, 32, 31], [36, 35, 34], [], [39, 38, 37], [42, 41, 40], [45, 44, 43], [48, 47, 46], [51, 50, 49], [54, 53, 52], [], [57, 56, 55], [60, 59, 58], [63, 62, 61], [66, 65, 64], [69, 68, 67], [72, 71, 70], [], [75, 74, 73], [], [78, 77, 76], [81, 80, 79], [84, 83, 82], [87, 86, 85], [90, 89, 88], [93, 92, 91], [], [96, 95, 94], []]
    posInd = []
    for ii in np.arange(38):
        if (ii == 0):
            posInd.append([1, 2, 3])
        else:
            posInd.append([])
    expmapInd = np.split((np.arange(4, 118) - 1), 38)
    return (parent, offset, posInd, expmapInd)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in np.arange:
idx = 32:------------------- similar code ------------------ index = 46, score = 5.0 
def transform(self, X):
    '\n        Compute the codes associated to input matrix X, decomposing it onto\n        the dictionary\n\n        Parameters\n        ----------\n        X: ndarray, shape = (n_samples, n_features)\n\n        Returns\n        -------\n        code: ndarray, shape = (n_samples, n_components)\n        '
    check_is_fitted(self, 'components_')
    dtype = self.components_.dtype
    X = check_array(X, order='C', dtype=dtype.type)
    if (X.flags['WRITEABLE'] is False):
        X = X.copy()
    (n_samples, n_features) = X.shape
    if ((not hasattr(self, 'G_agg')) or (self.G_agg != 'full')):
        G = self.components_.dot(self.components_.T)
    else:
        G = self.G_
    Dx = X.dot(self.components_.T)
    code = np.ones((n_samples, self.n_components), dtype=dtype)
    sample_indices = np.arange(n_samples)
    size_job = ceil((n_samples / self.n_threads))
    batches = list(gen_batches(n_samples, size_job))
    par_func = (lambda batch: _enet_regression_single_gram(G, Dx[batch], X[batch], code, get_sub_slice(sample_indices, batch), self.code_l1_ratio, self.code_alpha, self.code_pos, self.tol, self.max_iter))
    if (self.n_threads > 1):
        res = self._pool.map(par_func, batches)
        _ = list(res)
    else:
        _enet_regression_single_gram(G, Dx, X, code, sample_indices, self.code_l1_ratio, self.code_alpha, self.code_pos, self.tol, self.max_iter)
    return code

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 33:------------------- similar code ------------------ index = 45, score = 5.0 
def __getitem__(self, index):
    (speaker_id, name) = self.all_files[index]
    speaker_onehot = (np.arange(len(self.index)) == speaker_id).astype(np.long)
    audio = np.load(f'{self.path}/{speaker_id}/{name}.npy')
    return (speaker_onehot, audio)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = (np.arange ==  ... )

idx = 34:------------------- similar code ------------------ index = 38, score = 5.0 
def __iter__(self):
    if cfg.TRAIN.ASPECT_GROUPING:
        (n, rem) = divmod(self.num_data, cfg.TRAIN.IMS_PER_BATCH)
        round_num_data = (n * cfg.TRAIN.IMS_PER_BATCH)
        indices = np.arange(round_num_data)
        npr.shuffle(indices.reshape((- 1), cfg.TRAIN.IMS_PER_BATCH))
        if (rem != 0):
            indices = np.append(indices, np.arange(round_num_data, (round_num_data + rem)))
        ratio_index = self.ratio_index[indices]
        ratio_list_minibatch = self.ratio_list_minibatch[indices]
    else:
        rand_perm = npr.permutation(self.num_data)
        ratio_list = self.ratio_list[rand_perm]
        ratio_index = self.ratio_index[rand_perm]
        ratio_list_minibatch = cal_minibatch_ratio(ratio_list)
    return iter(zip(ratio_index.tolist(), ratio_list_minibatch.tolist()))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:
         ...  = np.arange

idx = 35:------------------- similar code ------------------ index = 4, score = 5.0 
def compute_gt_annotations(anchors, annotations, negative_overlap=0.4, positive_overlap=0.5):
    ' Obtain indices of gt annotations with the greatest overlap.\n\tArgs\n\t\tanchors: np.array of annotations of shape (N, 4) for (x1, y1, x2, y2).\n\t\tannotations: np.array of shape (N, 5) for (x1, y1, x2, y2, label).\n\t\tnegative_overlap: IoU overlap for negative anchors (all anchors with overlap < negative_overlap are negative).\n\t\tpositive_overlap: IoU overlap or positive anchors (all anchors with overlap > positive_overlap are positive).\n\tReturns\n\t\tpositive_indices: indices of positive anchors\n\t\tignore_indices: indices of ignored anchors\n\t\targmax_overlaps_inds: ordered overlaps indices\n\t'
    overlaps = compute_overlap(anchors.astype(np.float64), annotations.astype(np.float64))
    argmax_overlaps_inds = np.argmax(overlaps, axis=1)
    max_overlaps = overlaps[(np.arange(overlaps.shape[0]), argmax_overlaps_inds)]
    positive_indices = (max_overlaps >= positive_overlap)
    ignore_indices = ((max_overlaps > negative_overlap) & (~ positive_indices))
    return (positive_indices, ignore_indices, argmax_overlaps_inds)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... [(np.arange,  ... )]

idx = 36:------------------- similar code ------------------ index = 47, score = 5.0 
def plot_returns(env, batch_size, shared_returns, plot_running):
    'Plots episodic returns\n\n    Args:\n        env: An instance of DoubleInvertedPendulumEnv\n        batch_size: An int representing timesteps_per_batch provided to the PPO learn function\n        shared_returns: A manager dictionary object containing `episodic returns` and `episodic lengths`\n        plot_running: A multiprocessing Value object containing 0/1.\n            1: Continue plotting, 0: Terminate plotting loop\n    '
    print('Started plotting routine')
    import matplotlib.pyplot as plt
    plt.ion()
    time.sleep(5.0)
    fig = plt.figure(figsize=(20, 6))
    ax = fig.add_subplot(111)
    (hl11,) = ax.plot([], [])
    fig.suptitle('Simulated Double Pendulum', fontsize=14)
    ax.set_title('Learning Curve')
    ax.set_xlabel('Time Step')
    ax.set_ylabel('Average Returns')
    count = 0
    old_size = len(shared_returns['episodic_returns'])
    returns = []
    while plot_running.value:
        if ((count % 20) == 0):
            if (len(shared_returns['episodic_returns']) > old_size):
                returns.append(np.mean(shared_returns['episodic_returns'][(- (len(shared_returns['episodic_returns']) - old_size)):]))
                old_size = len(shared_returns['episodic_returns'])
                hl11.set_ydata(returns)
                hl11.set_xdata((batch_size * np.arange(len(returns))))
                ax.set_ylim([np.min(returns), np.max(returns)])
                ax.set_xlim([0, int((len(returns) * batch_size))])
                fig.canvas.draw()
                fig.canvas.flush_events()
        time.sleep(0.01)
        fig.canvas.draw()
        fig.canvas.flush_events()
        count += 1

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    while:
        if:
            if:
                 ... . ... (( ...  * np.arange))

idx = 37:------------------- similar code ------------------ index = 48, score = 5.0 
def diagonal_inds(array):
    '\n    Returns the indices required to go along first 2 dims of tensor in diag fashion\n    :param tensor: thing\n    :return: \n    '
    assert (len(array.shape) >= 2)
    assert (array.shape[0] == array.shape[1])
    size = array.shape[0]
    arange_inds = np.arange(0, size, dtype=np.int64)
    return ((size + 1) * arange_inds)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 38:------------------- similar code ------------------ index = 3, score = 5.0 
def process_batch_data(self, input_batch, flag_same=False):
    (im_faces, im_lndm, im_msk, im_ind) = input_batch
    im_faces = [item.float().to(self.device_comp) for item in im_faces]
    im_lndm = [item.float().to(self.device_comp) for item in im_lndm]
    im_msk = [item.float().to(self.device_comp) for item in im_msk]
    labels_one_hot = np.zeros((int(im_faces[0].shape[0]), self.num_classes))
    if (len(im_ind) > 1):
        labels_one_hot[(np.arange(int(im_faces[1].shape[0])), im_ind[1])] = 1
    labels_one_hot = torch.tensor(labels_one_hot).float().to(self.device_comp)
    if flag_same:
        if (len(im_ind) > 1):
            label_same = ((im_ind[0] == im_ind[1]) / 1)
        else:
            label_same = ((im_ind[0] == im_ind[0]) / 1)
        return (im_faces, im_lndm, im_msk, labels_one_hot, label_same.to(self.device_comp))
    return (im_faces, im_lndm, im_msk, labels_one_hot)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ... [(np.arange,)]

idx = 39:------------------- similar code ------------------ index = 50, score = 5.0 
@pytest.mark.parametrize('t, min_leaf', zip(tt, min_leafs))
def test__compute_valid_splitting_indices_with_empty_output(t, min_leaf):
    out = _compute_valid_splitting_indices(t, min_leaf)
    assert_array_equal(out, np.arange(0))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ... ( ... , np.arange)

idx = 40:------------------- similar code ------------------ index = 51, score = 5.0 
def create_circle_region_for(observation: cluster.Observation):
    mask_fits = fits.open(observation.cluster.combined_mask)
    region_map = observation.cluster.scale_map_region_index
    scale_map = observation.cluster.scale_map
    mask = mask_fits[0].data
    bounds = scale_map.shape
    xvals = np.arange(bounds[1])
    yvals = np.arange(bounds[0])
    print('Making circular fitting regions for observation {}'.format(observation.id))
    image_fits = fits.open(observation.acisI_comb_img)
    image_header = image_fits[0].header
    cdelt1p = image_header['CDELT1P']
    cdelt2p = image_header['CDELT2P']
    crval1p = image_header['CRVAL1P']
    crval2p = image_header['CRVAL2P']
    crpix1p = image_header['CRPIX1P']
    crpix2p = image_header['CRPIX2P']
    radii = ((mask * scale_map) * cdelt1p)
    newx = ((((xvals + 1) - crpix1p) * cdelt1p) + crval1p)
    newy = ((((yvals + 1) - crpix2p) * cdelt2p) + crval2p)
    (xx, yy) = np.meshgrid(newx, newy)
    non_zero_indices = np.nonzero(radii)
    nz_rad = radii[non_zero_indices]
    nz_x = xx[non_zero_indices]
    nz_y = yy[non_zero_indices]
    obs_regions = region_map[non_zero_indices]
    region_array = np.array([(i, j, k, l) for (i, j, k, l) in zip(nz_x, nz_y, nz_rad, obs_regions)])
    regions = [['circle({x},{y},{rad})'.format(x=x[0], y=x[1], rad=x[2]), int(x[3])] for x in region_array]
    observation.scale_map_region_list = regions

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 41:------------------- similar code ------------------ index = 39, score = 5.0 
if (__name__ == '__main__'):
    parser = build_parser()
    args = parser.parse_args()
    model_name = args.model_name
    model_type = ('double' if (model_name == 'double_albert') else 'siamese')
    checkpoint_dir = args.checkpoint_dir
    log_dir = args.log_dir
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    main_logger = init_logger(log_dir, f'finetune_main_{model_name}.log')
    test = pd.read_csv(f'{args.data_dir}test.csv')
    train = pd.read_csv(f'{args.data_dir}train.csv')
    for col in TARGETS:
        train[col] = train[col].rank(method='average')
    train[TARGETS] = MinMaxScaler().fit_transform(train[TARGETS])
    y = train[TARGETS].values
    (ids_train, seg_ids_train) = tokenize(train, pretrained_model_str=pretrained_models[model_name])
    (cat_features_train, _) = get_ohe_categorical_features(train, test, 'category')
    device = 'cuda'
    num_workers = 10
    n_folds = 10
    lr = 1e-05
    n_epochs = 10
    bs = 2
    grad_accum = 4
    weight_decay = 0.01
    loss_fn = nn.BCEWithLogitsLoss()
    init_seed()
    folds = GroupKFold(n_splits=n_folds).split(X=train['question_body'], groups=train['question_body'])
    oofs = np.zeros((len(train), N_TARGETS))
    main_logger.info(f'Start finetuning model {model_name}...')
    for (fold_id, (train_index, valid_index)) in enumerate(folds):
        main_logger.info(f'Fold {(fold_id + 1)} started at {time.ctime()}')
        fold_logger = init_logger(log_dir, f'finetune_fold_{(fold_id + 1)}_{model_name}.log')
        loader = DataLoader(TextDataset(cat_features_train, ids_train['question'], ids_train['answer'], seg_ids_train['question'], seg_ids_train['answer'], np.arange(len(train)), y), batch_size=bs, shuffle=False, num_workers=num_workers)
        model = models[model_name]()
        checkpoint_file = f'{checkpoint_dir}{model_name}_fold_{(fold_id + 1)}_best.pth'
        fold_logger.info(f'Precompute transformer outputs for model {model_name}...')
        (q_outputs, a_outputs) = get_model_outputs(model, loader, checkpoint_file, device, model_type)
        train_loader = DataLoader(TransformerOutputDataset(cat_features_train, q_outputs, a_outputs, train_index, y), batch_size=bs, shuffle=True, num_workers=num_workers)
        valid_loader = DataLoader(TransformerOutputDataset(cat_features_train, q_outputs, a_outputs, valid_index, y), batch_size=bs, shuffle=False, num_workers=num_workers, drop_last=False)
        optimizer = AdamW(get_optimizer_param_groups(model.head, lr, weight_decay))
        learner = Learner(model.head, optimizer, train_loader, valid_loader, loss_fn, device, n_epochs, f'{model_name}_head_fold_{(fold_id + 1)}', checkpoint_dir, scheduler=None, metric_spec={'spearmanr': spearmanr_torch}, monitor_metric=True, minimize_score=False, logger=fold_logger, grad_accum=grad_accum, batch_step_scheduler=False, eval_at_start=True)
        learner.train()
        oofs[valid_index] = infer(learner.model, valid_loader, learner.best_checkpoint_file, device)
        head_checkpoint_file = f'{checkpoint_dir}{model_name}_head_fold_{(fold_id + 1)}_best.pth'
        checkpoint = torch.load(head_checkpoint_file)
        model.head.load_state_dict(checkpoint['model_state_dict'])
        model.half()
        tuned_checkpoint_file = f'{checkpoint_dir}{model_name}_tuned_fold_{(fold_id + 1)}_best.pth'
        torch.save({'model_state_dict': model.state_dict()}, tuned_checkpoint_file)
    main_logger.info(f'Finished tuning {model_name}')
    ix = np.where((train.groupby('question_body')['host'].transform('count') == 1))[0]
    main_logger.info('CVs:')
    main_logger.info(get_cvs(oofs, y, ix))
    os.makedirs('oofs/', exist_ok=True)
    pd.DataFrame(oofs, columns=TARGETS).to_csv(f'oofs/{model_name}_tuned_oofs.csv')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
    for in:
         ...  =  ... ( ... ( ... ,,,,, np.arange,  ... ),,,)

idx = 42:------------------- similar code ------------------ index = 9, score = 5.0 
def _get_policy(self):
    total_novelty = []
    for (policy, _) in self.meta_population:
        (reward, bc) = self.agent.rollout(policy)
        novelty = self._calculate_novelty(bc, self._archive)
        total_novelty.append(novelty)
    total_novelty = np.array(total_novelty)
    meta_population_probability = (total_novelty / np.sum(total_novelty))
    self.idx = np.random.choice(np.arange(len(self.meta_population), dtype=np.int), p=meta_population_probability)
    (policy, optimizer) = self.meta_population[self.idx]
    return (policy, optimizer)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 =  ... . ... (np.arange,)

idx = 43:------------------- similar code ------------------ index = 8, score = 5.0 
def create_dating_schedule(person_df, n_meeting=10):
    '\n    Function to create speed dating schedule at CCN 2018 conference\n\n    Parameters\n    ==========\n    person_df: pandas dataframe contains - PersonID, FullName, Abstract\n    n_meeting: int, number of meeting we would like to have\n\n    Output\n    ======\n    schedule: list, list of person id and person ids to meet in the \n        following format: [PersonID, [PersonID to meet]]\n    '
    persons_1 = list(map(preprocess, list(person_df['Abstract'])))
    persons_2 = list(map(preprocess, list(person_df['Abstract'])))
    A = compute_affinity(persons_1, persons_2, n_components=10, min_df=1, max_df=0.8, weighting='tfidf', projection='pca')
    A[(np.arange(len(A)), np.arange(len(A)))] = (- 1000)
    (v, K, d) = create_lp_matrix(A, min_reviewers_per_paper=n_meeting, max_reviewers_per_paper=n_meeting, min_papers_per_reviewer=n_meeting, max_papers_per_reviewer=n_meeting)
    x_sol = linprog(v, K, d)['x']
    b = create_assignment(x_sol, A)
    output = []
    for i in range(len(b)):
        r = [list(person_df['PersonID'])[b_] for b_ in np.nonzero(b[i])[0]]
        output.append([list(person_df.PersonID)[i], r])
    schedule = nest_answer(output, format_answer(color_graph(build_line_graph(output))))
    return schedule

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ... [(np.arange,)]

idx = 44:------------------- similar code ------------------ index = 27, score = 5.0 
def drop(self, rows: Union[(int, List[int], ndarray, None)]=None, columns: Union[(str, int, List[IntStr], ndarray, None)]=None):
    if (rows is None):
        return self._drop_just_cols(columns)
    if (columns is None):
        return self._drop_just_rows(rows)
    if isinstance(columns, (int, str, np.integer)):
        columns = [columns]
    elif isinstance(columns, ndarray):
        columns = utils.try_to_squeeze_array(columns)
    elif (not isinstance(columns, list)):
        raise TypeError('Rows must either be an int, list/array of ints or None')
    if isinstance(rows, int):
        rows = [rows]
    elif isinstance(rows, ndarray):
        rows = utils.try_to_squeeze_array(rows)
    elif (not isinstance(rows, list)):
        raise TypeError('Rows must either be an int, list/array of ints or None')
    new_rows: List[int] = []
    for row in rows:
        if (not isinstance(row, int)):
            raise TypeError('All the row values in your list must be integers')
        if ((row < (- len(self))) or (row >= len(self))):
            raise IndexError(f'Integer location {row} for the rows is out of range')
        if (row < 0):
            new_rows.append((len(self) + row))
        else:
            new_rows.append(row)
    column_strings: List[str] = []
    for col in columns:
        if isinstance(col, str):
            column_strings.append(col)
        elif isinstance(col, (int, np.integer)):
            column_strings.append(self._columns[col])
    self._validate_column_name_list(column_strings)
    column_set: Set[str] = set(column_strings)
    new_rows = np.isin(np.arange(len(self)), new_rows, invert=True)
    new_columns = [col for col in self._columns if (col not in column_set)]
    new_column_info: ColInfoT = {}
    data_dict: Dict[(str, List[int])] = defaultdict(list)
    for (i, col) in enumerate(new_columns):
        (dtype, loc) = self._get_col_dtype_loc(col)
        cur_loc = len(data_dict[dtype])
        new_column_info[col] = utils.Column(dtype, cur_loc, i)
        data_dict[dtype].append(loc)
    new_data = {}
    for (dtype, locs) in data_dict.items():
        new_data[dtype] = self._data[dtype][np.ix_(new_rows, locs)]
    return self._construct_from_new(new_data, new_column_info, np.asarray(new_columns, dtype='O'))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... (np.arange,  ... ,)

idx = 45:------------------- similar code ------------------ index = 15, score = 5.0 
def decode(self, segms, bboxes, labels, sizes):
    "Decodes back to masks.\n\n        Args:\n            segms (iterable of arrays): An iterable of arrays of\n                shape :math:`(R_n, n\\_class, M, M)`.\n            bboxes (iterable of arrays): An iterable of arrays of\n                shape :math:`(R_n, 4)`.\n            labels (iterable of arrays): An iterable of arrays of\n                shape :math:`(R_n,)`.\n            sizes (list of tuples of two ints): A list of\n                :math:`(H_n, W_n)`, where :math:`H_n` and :math:`W_n`\n                are height and width of the :math:`n`-th image.\n\n        Returns:\n            list of arrays:\n            This list contains instance segmentation for each image\n            in the batch.\n            More precisely, this is a list of boolean arrays of shape\n            :math:`(R'_n, H_n, W_n)`, where :math:`R'_n` is the number of\n            bounding boxes in the :math:`n`-th image.\n        "
    xp = chainer.backends.cuda.get_array_module(*segms)
    if (xp != np):
        raise ValueError('MaskHead.decode only supports numpy inputs for now.')
    masks = []
    for (bbox, segm, label, size) in zip(bboxes, segms, labels, sizes):
        if (len(segm) > 0):
            masks.append(segm_to_mask(segm[(np.arange(len(label)), (label + 1))], bbox, size))
        else:
            masks.append(np.zeros(((0,) + size), dtype=np.bool))
    return masks

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
        if:
             ... . ... ( ... ( ... [(np.arange,)],  ... ,  ... ))

idx = 46:------------------- similar code ------------------ index = 14, score = 5.0 
@pytest.mark.parametrize('systematic, sparse', product([False, True], [False, True]))
def test_decoding_did_not_converge(systematic, sparse):
    n = 15
    d_v = 4
    d_c = 5
    seed = 0
    (H, G) = make_ldpc(n, d_v, d_c, seed=seed, systematic=systematic, sparse=sparse)
    assert (not binaryproduct(H, G).any())
    (n, k) = G.shape
    snr = 1
    v = (np.arange(k) % 2)
    y = encode(G, v, snr, seed=seed)
    with pytest.warns(UserWarning, match='stopped before convergence'):
        decode(H, y, snr, maxiter=1)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = (np.arange %  ... )

idx = 47:------------------- similar code ------------------ index = 25, score = 5.0 
def shuffle_samples(self):
    image_indices = np.random.permutation(np.arange(self.num_samples))
    self.images = self.images[image_indices]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  =  ... . ... (np.arange)

idx = 48:------------------- similar code ------------------ index = 26, score = 5.0 
def append_multiple_new_old_arrays(self):
    df1 = self.df.append({'a': 10, 'b': np.arange(10, 15), 'h': np.arange(5), 'i': np.linspace(2.4, 20.9, 5)}, axis='columns')
    data2 = {'a': array([10, 10, 10, 10, 10]), 'b': array([10, 11, 12, 13, 14]), 'c': array([None, 'e', 'e', 'a', 'z'], dtype=object), 'd': array([False, False, True, False, True]), 'e': array([0, 20, 30, 4, 4]), 'f': array(['a', None, 'ad', None, 'ad'], dtype=object), 'g': array([2.4, 7.025, 11.65, 16.275, 20.9]), 'h': array([0, 1, 2, 3, 4]), 'i': array([2.4, 7.025, 11.65, 16.275, 20.9])}
    df2 = dx.DataFrame(data2)
    assert_frame_equal(df1, df2)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  =  ... . ... ({ ... :  ... ,  ... : np.arange,  ... :,  ... :},)

idx = 49:------------------- similar code ------------------ index = 28, score = 5.0 
def pixel_edges(jet_size=1.0, pixel_size=(0.1, 0.1), border_size=0.25):
    'Return pixel edges required to contain all subjets.\n\n    border_size is interpreted as a fraction of the jet_size\n    '
    im_edge = ((1.0 + border_size) * jet_size)
    return (np.arange((- im_edge), (im_edge + pixel_size[0]), pixel_size[0]), np.arange((- im_edge), (im_edge + pixel_size[1]), pixel_size[1]))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    return (np.arange,)

idx = 50:------------------- similar code ------------------ index = 36, score = 5.0 
def _compute_and_log_stats(roidb):
    classes = roidb[0]['dataset'].classes
    char_len = np.max([len(c) for c in classes])
    hist_bins = np.arange((len(classes) + 1))
    gt_hist = np.zeros(len(classes), dtype=np.int)
    for entry in roidb:
        gt_inds = np.where(((entry['gt_classes'] > 0) & (entry['is_crowd'] == 0)))[0]
        gt_classes = entry['gt_classes'][gt_inds]
        gt_hist += np.histogram(gt_classes, bins=hist_bins)[0]
    logger.debug('Ground-truth class histogram:')
    for (i, v) in enumerate(gt_hist):
        logger.debug('{:d}{:s}: {:d}'.format(i, classes[i].rjust(char_len), v))
    logger.debug(('-' * char_len))
    logger.debug('{:s}: {:d}'.format('total'.rjust(char_len), np.sum(gt_hist)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 51:------------------- similar code ------------------ index = 13, score = 5.0 
def animate_activities(trajectory_or_activities, title='', shape=None, save=False, interval=50, colormap='Greys', vmin=None, vmax=None, show_grid=False, show_margin=True, scale=0.6, dpi=80, blit=True, with_timestep=False):
    if (len(trajectory_or_activities) is 0):
        raise Exception('there are no activities')
    if isinstance(trajectory_or_activities[0], State):
        activities = get_activities_over_time_as_list(trajectory_or_activities)
    else:
        activities = trajectory_or_activities
    if (shape is not None):
        activities = _reshape_for_animation(activities, shape)
    cmap = plt.get_cmap(colormap)
    (fig, ax) = plt.subplots()
    title_text = plt.title(title)
    if (not show_margin):
        fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
    grid_linewidth = 0.0
    if show_grid:
        plt.xticks(np.arange((- 0.5), len(activities[0][0]), 1), '')
        plt.yticks(np.arange((- 0.5), len(activities[0]), 1), '')
        plt.tick_params(axis='both', which='both', length=0)
        grid_linewidth = 0.5
    vertical = np.arange((- 0.5), len(activities[0][0]), 1)
    horizontal = np.arange((- 0.5), len(activities[0]), 1)
    lines = ([[(x, y) for y in ((- 0.5), horizontal[(- 1)])] for x in vertical] + [[(x, y) for x in ((- 0.5), vertical[(- 1)])] for y in horizontal])
    grid = mcoll.LineCollection(lines, linestyles='-', linewidths=grid_linewidth, color='grey')
    ax.add_collection(grid)
    im = plt.imshow(activities[0], animated=True, cmap=cmap, vmin=vmin, vmax=vmax)
    if (not show_margin):
        (baseheight, basewidth) = im.get_size()
        fig.set_size_inches((basewidth * scale), (baseheight * scale), forward=True)
    i = {'index': 0}

    def updatefig(*args):
        i['index'] += 1
        if (i['index'] == len(activities)):
            i['index'] = 0
        im.set_array(activities[i['index']])
        if with_timestep:
            title_text.set_text(('timestep: %s' % (i['index'] + 1)))
        return (im, grid, title_text)
    ani = animation.FuncAnimation(fig, updatefig, interval=interval, blit=blit, save_count=len(activities))
    if save:
        ani.save('evolved.gif', dpi=dpi, writer='imagemagick')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if  ... :
         ... . ... (np.arange,  ... )

idx = 52:------------------- similar code ------------------ index = 30, score = 5.0 
def calculate_effective_times(cluster: cluster.ClusterObj):
    start_time = time.time()
    scale_map = cluster.scale_map
    number_of_regions = cluster.number_of_regions
    nx = scale_map.shape[0]
    ny = scale_map.shape[1]
    effective_data_times = np.zeros(scale_map.shape)
    effective_background_times = np.zeros(scale_map.shape)
    for observation in cluster.observations:
        print('Starting observation {obs}'.format(obs=observation.id))
        high_energy_data = observation.acisI_high_energy_combined_image
        background = observation.backI_high_energy_combined_image
        sum_acis_high_energy = np.sum(high_energy_data)
        sum_back_high_energy = np.sum(background)
        bg_to_data_ratio = (sum_back_high_energy / sum_acis_high_energy)
        source_subtracted_data = observation.acisI_nosrc_combined_mask
        exposure_time = observation.acisI_high_energy_combined_image_header['EXPOSURE']
        (YY, XX) = np.meshgrid(np.arange(ny), np.arange(nx))
        counter = 0
        print('Starting effective exposure time calculations...')
        for x in range(nx):
            for y in range(ny):
                if (scale_map[(x, y)] >= 1):
                    radius = np.sqrt((((x - XX) ** 2) + ((y - YY) ** 2)))
                    region = np.where((radius <= scale_map[(x, y)]))
                    source_subtracted_area = np.sum(source_subtracted_data[region])
                    total_area = source_subtracted_data[region].size
                    fractional_area = (source_subtracted_area / total_area)
                    fractional_exposure_time = (fractional_area * exposure_time)
                    effective_data_times[(x, y)] = fractional_exposure_time
                    effective_background_times[(x, y)] = (fractional_exposure_time * bg_to_data_ratio)
                    counter += 1
                    if (((counter % 1000) == 0) or (counter == number_of_regions) or (counter == 1)):
                        time_elapsed = time.strftime('%H hours %M minutes %S seconds.', time.gmtime((time.time() - start_time)))
                        _update_effective_exposure_time(obsid=observation.id, current_region=counter, number_regions=number_of_regions, time_elapsed=time_elapsed)
        observation.effective_data_time = effective_data_times
        observation.effective_background_time = effective_background_times

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
 =  ... . ... (np.arange,)

idx = 53:------------------- similar code ------------------ index = 32, score = 5.0 
def load_class_id(self, data_dir, total_num):
    if os.path.isfile((data_dir + '/class_info.pickle')):
        with open((data_dir + '/class_info.pickle'), 'rb') as f:
            class_id = pickle.load(f)
    else:
        class_id = np.arange(total_num)
    return class_id

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:    else:
         ...  = np.arange

idx = 54:------------------- similar code ------------------ index = 33, score = 5.0 
def shift(shape, stride, anchors):
    shift_h = (np.arange(0, shape[0]) * stride)
    shift_w = (np.arange(0, shape[1]) * stride)
    (shift_h, shift_w) = np.meshgrid(shift_h, shift_w)
    shifts = np.vstack((shift_h.ravel(), shift_w.ravel())).transpose()
    A = anchors.shape[0]
    K = shifts.shape[0]
    all_anchors = (anchors.reshape((1, A, 2)) + shifts.reshape((1, K, 2)).transpose((1, 0, 2)))
    all_anchors = all_anchors.reshape(((K * A), 2))
    return all_anchors

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = (np.arange *  ... )

idx = 55:------------------- similar code ------------------ index = 10, score = 5.0 
def generate_metadata(self):
    for id in self.ids:
        math_patches = []
        (height, width, channels) = self.images[id[1]].shape
        current_page_boxes = self.math_ground_truth[id[1]]
        n_horizontal = np.ceil((width / self.window))
        n_vertical = np.ceil((height / self.window))
        h = np.arange(0, ((n_horizontal - 1) + self.stride), self.stride)
        v = np.arange(0, ((n_vertical - 1) + self.stride), self.stride)
        crop_size = self.window
        if (((self.split == 'train') or (self.split == 'validate')) and self.is_math[id[1]]):
            for i in h:
                for j in v:
                    x_l = int(np.round((crop_size * i)))
                    x_h = (x_l + self.window)
                    y_l = int(np.round((crop_size * j)))
                    y_h = (y_l + self.window)
                    image_box = [x_l, y_l, x_h, y_h]
                    current_page_boxes = copy.deepcopy(self.math_ground_truth[id[1]])
                    for box in current_page_boxes:
                        if box_utils.intersects(image_box, box):
                            box[0] = max(x_l, box[0])
                            box[1] = max(y_l, box[1])
                            box[2] = min(x_h, box[2])
                            box[3] = min(y_h, box[3])
                            box[0] = (box[0] - x_l)
                            box[2] = (box[2] - x_l)
                            box[1] = (box[1] - y_l)
                            box[3] = (box[3] - y_l)
                            if ((feature_extractor.width(box) > 0) and (feature_extractor.height(box) > 0)):
                                self.metadata.append([id[1], x_l, y_l])
                                break
        elif (self.split == 'test'):
            for i in h:
                for j in v:
                    x_l = int(np.round((crop_size * i)))
                    y_l = int(np.round((crop_size * j)))
                    self.metadata.append([id[1], x_l, y_l])

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
         ...  = np.arange

idx = 56:------------------- similar code ------------------ index = 19, score = 5.0 
def _some_variables():
    parent = (np.array([0, 1, 2, 3, 4, 5, 1, 7, 8, 9, 10, 1, 12, 13, 14, 15, 13, 17, 18, 19, 20, 21, 20, 23, 13, 25, 26, 27, 28, 29, 28, 31]) - 1)
    offset = np.array([0.0, 0.0, 0.0, (- 132.948591), 0.0, 0.0, 0.0, (- 442.894612), 0.0, 0.0, (- 454.206447), 0.0, 0.0, 0.0, 162.767078, 0.0, 0.0, 74.999437, 132.948826, 0.0, 0.0, 0.0, (- 442.894413), 0.0, 0.0, (- 454.20659), 0.0, 0.0, 0.0, 162.767426, 0.0, 0.0, 74.999948, 0.0, 0.1, 0.0, 0.0, 233.383263, 0.0, 0.0, 257.077681, 0.0, 0.0, 121.134938, 0.0, 0.0, 115.002227, 0.0, 0.0, 257.077681, 0.0, 0.0, 151.034226, 0.0, 0.0, 278.882773, 0.0, 0.0, 251.733451, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.999627, 0.0, 100.000188, 0.0, 0.0, 0.0, 0.0, 0.0, 257.077681, 0.0, 0.0, 151.031437, 0.0, 0.0, 278.892924, 0.0, 0.0, 251.72868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.999888, 0.0, 137.499922, 0.0, 0.0, 0.0, 0.0]).reshape((- 1), 3)
    rotInd = [[5, 6, 4], [8, 9, 7], [11, 12, 10], [14, 15, 13], [17, 18, 16], [], [20, 21, 19], [23, 24, 22], [26, 27, 25], [29, 30, 28], [], [32, 33, 31], [35, 36, 34], [38, 39, 37], [41, 42, 40], [], [44, 45, 43], [47, 48, 46], [50, 51, 49], [53, 54, 52], [56, 57, 55], [], [59, 60, 58], [], [62, 63, 61], [65, 66, 64], [68, 69, 67], [71, 72, 70], [74, 75, 73], [], [77, 78, 76], []]
    expmapInd = np.split((np.arange(4, 100) - 1), 32)
    return (parent, offset, rotInd, expmapInd)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... ((np.arange -  ... ),  ... )

idx = 57:------------------- similar code ------------------ index = 56, score = 5.0 
def plot_dxl_reacher(env, batch_size, shared_returns, plot_running):
    ' Visualizes the DXL reacher task and plots episodic returns\n\n    Args:\n        env: An instance of DxlReacher1DEnv\n        batch_size: An int representing timesteps_per_batch provided to the PPO learn function\n        shared_returns: A manager dictionary object containing `episodic returns` and `episodic lengths`\n        plot_running: A multiprocessing Value object containing 0/1.\n            1: Continue plotting, 0: Terminate plotting loop\n    '
    print('Started plotting routine')
    import matplotlib.pyplot as plt
    plt.ion()
    time.sleep(5.0)
    fig = plt.figure(figsize=(20, 6))
    ax1 = fig.add_subplot(121)
    (hl1,) = ax1.plot([], [], markersize=10, marker='o', color='r')
    (hl2,) = ax1.plot([], [], markersize=10, marker='o', color='b')
    ax1.set_xlabel('X')
    ax1.set_ylabel('Y')
    ax2 = fig.add_subplot(122)
    (hl11,) = ax2.plot([], [])
    fig.suptitle('DXL Reacher', fontsize=14)
    ax2.set_title('Learning Curve')
    ax2.set_xlabel('Time Step')
    ax2.set_ylabel('Average Returns')
    count = 0
    old_size = len(shared_returns['episodic_returns'])
    while plot_running.value:
        hl1.set_ydata([1])
        hl1.set_xdata([env._target_pos_.value])
        hl2.set_ydata([1])
        hl2.set_xdata([env._present_pos_[(- 1)]])
        ax1.set_ylim([0, 2])
        ax1.set_xlim([env.angle_low, env.angle_high])
        ax1.set_title(('Current Reward: ' + str(env._reward_.value)))
        ax1.set_xlim(ax1.get_xlim()[::(- 1)])
        ax1.set_ylim(ax1.get_ylim()[::(- 1)])
        copied_returns = copy.deepcopy(shared_returns)
        if ((not copied_returns['write_lock']) and (len(copied_returns['episodic_returns']) > old_size)):
            returns = np.array(copied_returns['episodic_returns'])
            old_size = len(copied_returns['episodic_returns'])
            window_size_steps = 5000
            x_tick = 1000
            if copied_returns['episodic_lengths']:
                ep_lens = np.array(copied_returns['episodic_lengths'])
            else:
                ep_lens = (batch_size * np.arange(len(returns)))
            cum_episode_lengths = np.cumsum(ep_lens)
            if (cum_episode_lengths[(- 1)] >= x_tick):
                steps_show = np.arange(x_tick, (cum_episode_lengths[(- 1)] + 1), x_tick)
                rets = []
                for i in range(len(steps_show)):
                    rets_in_window = returns[((cum_episode_lengths > max(0, ((x_tick * (i + 1)) - window_size_steps))) * (cum_episode_lengths < (x_tick * (i + 1))))]
                    if rets_in_window.any():
                        rets.append(np.mean(rets_in_window))
                hl11.set_xdata((np.arange(1, (len(rets) + 1)) * x_tick))
                ax2.set_xlim([x_tick, (len(rets) * x_tick)])
                hl11.set_ydata(rets)
                ax2.set_ylim([np.min(rets), (np.max(rets) + 50)])
        time.sleep(0.01)
        fig.canvas.draw()
        fig.canvas.flush_events()
        count += 1

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    while:
        if:
            if:            else:
                 ...  = ( ...  * np.arange)

idx = 58:------------------- similar code ------------------ index = 0, score = 5.0 
def plot_dxl_tracker(env, batch_size, shared_returns, plot_running):
    ' Visualizes the DXL tracker task and plots episodic returns\n\n    Args:\n        env: An instance of DxlTracker1DEnv\n        batch_size: An int representing timesteps_per_batch provided to the PPO learn function\n        shared_returns: A manager dictionary object containing `episodic returns` and `episodic lengths`\n        plot_running: A multiprocessing Value object containing 0/1.\n            1: Continue plotting, 0: Terminate plotting loop\n    '
    print('Started plotting routine')
    import matplotlib.pyplot as plt
    plt.ion()
    time.sleep(5.0)
    fig = plt.figure(figsize=(20, 6))
    ax1 = fig.add_subplot(121)
    (hl1,) = ax1.plot([], [], markersize=10, marker='o', color='r')
    (hl2,) = ax1.plot([], [], markersize=10, marker='o', color='b')
    ax1.set_xlabel('X')
    ax1.set_ylabel('Y')
    ax2 = fig.add_subplot(122)
    (hl11,) = ax2.plot([], [])
    fig.suptitle('DXL Tracker', fontsize=14)
    ax2.set_title('Learning Curve')
    ax2.set_xlabel('Time Step')
    ax2.set_ylabel('Average Returns')
    count = 0
    old_size = len(shared_returns['episodic_returns'])
    while plot_running.value:
        hl1.set_ydata([1])
        hl1.set_xdata([env._target_pos_.value])
        hl2.set_ydata([1])
        hl2.set_xdata([env._present_pos_[(- 1)]])
        ax1.set_ylim([0, 2])
        ax1.set_xlim([env.angle_low, env.angle_high])
        ax1.set_title(('Current Reward: ' + str(env._reward_.value)))
        ax1.set_xlim(ax1.get_xlim()[::(- 1)])
        ax1.set_ylim(ax1.get_ylim()[::(- 1)])
        copied_returns = copy.deepcopy(shared_returns)
        if ((not copied_returns['write_lock']) and (len(copied_returns['episodic_returns']) > old_size)):
            returns = np.array(copied_returns['episodic_returns'])
            old_size = len(copied_returns['episodic_returns'])
            window_size_steps = 5000
            x_tick = 1000
            if copied_returns['episodic_lengths']:
                ep_lens = np.array(copied_returns['episodic_lengths'])
            else:
                ep_lens = (batch_size * np.arange(len(returns)))
            cum_episode_lengths = np.cumsum(ep_lens)
            if (cum_episode_lengths[(- 1)] >= x_tick):
                steps_show = np.arange(x_tick, (cum_episode_lengths[(- 1)] + 1), x_tick)
                rets = []
                for i in range(len(steps_show)):
                    rets_in_window = returns[((cum_episode_lengths > max(0, ((x_tick * (i + 1)) - window_size_steps))) * (cum_episode_lengths < (x_tick * (i + 1))))]
                    if rets_in_window.any():
                        rets.append(np.mean(rets_in_window))
                hl11.set_xdata((np.arange(1, (len(rets) + 1)) * x_tick))
                ax2.set_xlim([x_tick, (len(rets) * x_tick)])
                hl11.set_ydata(rets)
                ax2.set_ylim([np.min(rets), (np.max(rets) + 50)])
        time.sleep(0.01)
        fig.canvas.draw()
        fig.canvas.flush_events()
        count += 1

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    while:
        if:
            if:            else:
                 ...  = ( ...  * np.arange)

idx = 59:------------------- similar code ------------------ index = 54, score = 5.0 
def plot_performance_quad(returns, fig_path=None, fig_name='heat_map_quad', font_size=20):

    def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):
        new_cmap = colors.LinearSegmentedColormap.from_list('trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval), cmap(np.linspace(minval, maxval, n)))
        return new_cmap
    fig = plt.figure(figsize=(16, 9))
    fig.suptitle(returns.name, fontsize=16)
    gs = gridspec.GridSpec(2, 2, wspace=0.2, hspace=0.3)
    ax_heatmap = plt.subplot(gs[(0, 0)])
    ax_monthly = plt.subplot(gs[(0, 1)])
    ax_box_plot = plt.subplot(gs[(1, 0)])
    ax_yearly = plt.subplot(gs[(1, 1)])
    monthly_ret_table = pf.timeseries.aggregate_returns(returns, 'monthly')
    monthly_ret_table = monthly_ret_table.unstack().round(3)
    ax = plt.gca()
    cmap = cm.viridis
    new_cmap = truncate_colormap(cmap, 0.2, 0.8)
    sns.heatmap((monthly_ret_table.fillna(0) * 100.0), annot=True, annot_kws={'size': font_size}, alpha=1.0, center=0.0, cbar=False, mask=monthly_ret_table.isna(), cmap=new_cmap, ax=ax_heatmap)
    ax_heatmap.set_xticklabels(np.arange(0.5, 12.5, step=1))
    ax_heatmap.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)
    ylabels = ax_heatmap.get_yticklabels()
    ax_heatmap.set_yticklabels(ylabels, rotation=45)
    ax_heatmap.set_xlabel('')
    ax_heatmap.set_ylabel('')
    pf.plotting.plot_monthly_returns_dist(returns, ax=ax_monthly)
    ax_monthly.xaxis.set_major_formatter(FormatStrFormatter('%.1f%%'))
    ax_monthly.set_xlabel('')
    leg1 = ax_monthly.legend(['mean'], framealpha=0.0, prop={'size': font_size})
    for text in leg1.get_texts():
        text.set_label('mean')
    df_weekly = pf.timeseries.aggregate_returns(returns, convert_to='weekly')
    df_monthly = pf.timeseries.aggregate_returns(returns, convert_to='monthly')
    pf.plotting.plot_return_quantiles(returns, df_weekly, df_monthly, ax=ax_box_plot)
    pf.plotting.plot_annual_returns(returns, ax=ax_yearly)
    _ = ax_yearly.legend(['mean'], framealpha=0.0, prop={'size': font_size})
    ax_yearly.xaxis.set_major_formatter(FormatStrFormatter('%.1f%%'))
    plt.xticks(rotation=45)
    ax_yearly.set_xlabel('')
    ax_yearly.set_ylabel('')
    for ax in [ax_box_plot, ax_heatmap, ax_monthly, ax_yearly]:
        for item in (([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels()) + ax.get_yticklabels()):
            item.set_fontsize(font_size)
    for items in (ax_yearly.get_yticklabels() + ax_heatmap.get_yticklabels()):
        items.set_fontsize((font_size - 5))
    if (fig_path is not None):
        if Path.is_dir(fig_path):
            plt.savefig((fig_path / fig_name), dpi=600, bbox_inches='tight', transparent=True)
        return fig

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():

     ... . ... (np.arange)

idx = 60:------------------- similar code ------------------ index = 109, score = 5.0 
def optimize_rounding_params(oofs, y, verbose=True, ix=None):
    ix = (ix if (ix is not None) else np.arange(oofs.shape[0]))
    opt_ds = []
    opt_indices = []
    for idx in range(N_TARGETS):
        scores = [np.nan_to_num(spearmanr(scale(oofs[(ix, idx)], d), y[(ix, idx)])[0]) for d in ds]
        opt_d = ds[np.argmax(scores)]
        if ((np.max(scores) - spearmanr(oofs[(ix, idx)], y[(ix, idx)])[0]) > 0.002):
            if verbose:
                print(idx, opt_d, np.max(scores))
            opt_ds.append(opt_d)
            opt_indices.append(idx)
    return (opt_ds, opt_indices)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = ( ...  if else np.arange)

idx = 61:------------------- similar code ------------------ index = 87, score = 5.0 
def plot_confusion_matrix(cm, classes=[], title='Confusion matrix', cmap=plt.cm.Blues, fmt='d'):
    '\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    '
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    thresh = (cm.max() / 2.0)
    for (i, j) in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[(i, j)], fmt), horizontalalignment='center', color=('white' if (cm[(i, j)] > thresh) else 'black'))
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 62:------------------- similar code ------------------ index = 111, score = 5.0 
def plot_ur5_reacher(env, batch_size, shared_returns, plot_running):
    'Helper process for visualize the tasks and episodic returns.\n\n    Args:\n        env: An instance of ReacherEnv\n        batch_size: An int representing timesteps_per_batch provided to the PPO learn function\n        shared_returns: A manager dictionary object containing `episodic returns` and `episodic lengths`\n        plot_running: A multiprocessing Value object containing 0/1.\n            1: Continue plotting, 0: Terminate plotting loop\n    '
    print('Started plotting routine')
    import matplotlib.pyplot as plt
    plt.ion()
    time.sleep(5.0)
    fig = plt.figure(figsize=(20, 6))
    ax1 = fig.add_subplot(131)
    (hl1,) = ax1.plot([], [], markersize=10, marker='o', color='r')
    (hl2,) = ax1.plot([], [], markersize=10, marker='o', color='b')
    ax1.set_xlabel('X', fontsize=14)
    h = ax1.set_ylabel('Y', fontsize=14)
    h.set_rotation(0)
    ax3 = fig.add_subplot(132)
    (hl3,) = ax3.plot([], [], markersize=10, marker='o', color='r')
    (hl4,) = ax3.plot([], [], markersize=10, marker='o', color='b')
    ax3.set_xlabel('Z', fontsize=14)
    h = ax3.set_ylabel('Y', fontsize=14)
    h.set_rotation(0)
    ax2 = fig.add_subplot(133)
    (hl11,) = ax2.plot([], [])
    count = 0
    old_size = len(shared_returns['episodic_returns'])
    while plot_running.value:
        plt.suptitle('Reward: {:.2f}'.format(env._reward_.value), x=0.375, fontsize=14)
        hl1.set_ydata([env._x_target_[1]])
        hl1.set_xdata([env._x_target_[2]])
        hl2.set_ydata([env._x_[1]])
        hl2.set_xdata([env._x_[2]])
        ax1.set_ylim([env._end_effector_low[1], env._end_effector_high[1]])
        ax1.set_xlim([env._end_effector_low[2], env._end_effector_high[2]])
        ax1.set_title('X-Y plane', fontsize=14)
        ax1.set_xlim(ax1.get_xlim()[::(- 1)])
        ax1.set_ylim(ax1.get_ylim()[::(- 1)])
        hl3.set_ydata([env._x_target_[1]])
        hl3.set_xdata([env._x_target_[0]])
        hl4.set_ydata([env._x_[1]])
        hl4.set_xdata([env._x_[0]])
        ax3.set_ylim([env._end_effector_low[1], env._end_effector_high[1]])
        ax3.set_xlim([env._end_effector_low[0], env._end_effector_high[0]])
        ax3.set_title('Y-Z plane', fontsize=14)
        ax3.set_xlim(ax3.get_xlim()[::(- 1)])
        ax3.set_ylim(ax3.get_ylim()[::(- 1)])
        copied_returns = copy.deepcopy(shared_returns)
        if ((not copied_returns['write_lock']) and (len(copied_returns['episodic_returns']) > old_size)):
            returns = np.array(copied_returns['episodic_returns'])
            old_size = len(copied_returns['episodic_returns'])
            window_size_steps = 5000
            x_tick = 1000
            if copied_returns['episodic_lengths']:
                ep_lens = np.array(copied_returns['episodic_lengths'])
            else:
                ep_lens = (batch_size * np.arange(len(returns)))
            cum_episode_lengths = np.cumsum(ep_lens)
            if (cum_episode_lengths[(- 1)] >= x_tick):
                steps_show = np.arange(x_tick, (cum_episode_lengths[(- 1)] + 1), x_tick)
                rets = []
                for i in range(len(steps_show)):
                    rets_in_window = returns[((cum_episode_lengths > max(0, ((x_tick * (i + 1)) - window_size_steps))) * (cum_episode_lengths < (x_tick * (i + 1))))]
                    if rets_in_window.any():
                        rets.append(np.mean(rets_in_window))
                hl11.set_xdata((np.arange(1, (len(rets) + 1)) * x_tick))
                ax2.set_xlim([x_tick, (len(rets) * x_tick)])
                hl11.set_ydata(rets)
                ax2.set_ylim([np.min(rets), (np.max(rets) + 50)])
        time.sleep(0.01)
        fig.canvas.draw()
        fig.canvas.flush_events()
        count += 1

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    while:
        if:
            if:            else:
                 ...  = ( ...  * np.arange)

idx = 63:------------------- similar code ------------------ index = 90, score = 5.0 
def npairsampling(self, batch, labels):
    "\n        This methods finds N-Pairs in a batch given by the classes provided in labels in the\n        creation fashion proposed in 'Improved Deep Metric Learning with Multi-class N-pair Loss Objective'.\n\n        Args:\n            batch:  np.ndarray or torch.Tensor, batch-wise embedded training samples.\n            labels: np.ndarray or torch.Tensor, ground truth labels corresponding to batch.\n        Returns:\n            list of sampled data tuples containing reference indices to the position IN THE BATCH.\n        "
    if isinstance(labels, torch.Tensor):
        labels = labels.detach().cpu().numpy()
    (label_set, count) = np.unique(labels, return_counts=True)
    label_set = label_set[(count >= 2)]
    pos_pairs = np.array([np.random.choice(np.where((labels == x))[0], 2, replace=False) for x in label_set])
    neg_tuples = []
    for idx in range(len(pos_pairs)):
        neg_tuples.append(pos_pairs[(np.delete(np.arange(len(pos_pairs)), idx), 1)])
    neg_tuples = np.array(neg_tuples)
    sampled_npairs = [[a, p, *list(neg)] for ((a, p), neg) in zip(pos_pairs, neg_tuples)]
    return sampled_npairs

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ... . ... ( ... [( ... . ... (np.arange,  ... ),  ... )])

idx = 64:------------------- similar code ------------------ index = 91, score = 5.0 
def co_nms(self, p_det_rois, scores):
    pre_topN = (cfg.TRAIN.PRUNE_PAIRS_PRE_NMS_TOP_N if self.training else cfg.TEST.PRUNE_PAIRS_PRE_NMS_TOP_N)
    post_topN = (cfg.TRAIN.PRUNE_PAIRS_POST_NMS_TOP_N if self.training else cfg.TEST.PRUNE_PAIRS_POST_NMS_TOP_N)
    nms_thr = (cfg.TRAIN.PRUNE_PAIRS_NMS_THRESH if self.training else cfg.TEST.PRUNE_PAIRS_NMS_THRESH)
    if (p_det_rois.shape[0] > pre_topN):
        keep_inds = np.argsort((- scores.ravel()))[:pre_topN]
        p_det_rois = p_det_rois[keep_inds]
    else:
        keep_inds = np.arange(p_det_rois.shape[0], dtype=np.int64)
    p_dets = np.concatenate((p_det_rois, scores[keep_inds]), (- 1))
    keep_inds_nms = box_utils.co_nms(p_dets, nms_thr)
    keep_inds = keep_inds[keep_inds_nms]
    if (keep_inds.shape[0] > post_topN):
        sort_inds = np.argsort((- scores[keep_inds].ravel()))[:post_topN]
        keep_inds = keep_inds[sort_inds]
    return keep_inds

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:    else:
         ...  = np.arange

idx = 65:------------------- similar code ------------------ index = 92, score = 5.0 
def recover_closest_one_dataset(feature_matrix_all, image_paths, save_path, n_image_samples=10, n_closest=3):
    '\n    Provide sample recoveries.\n\n    Args:\n        feature_matrix_all: np.ndarray [n_samples x embed_dim], full data embedding of test samples.\n        image_paths:        list [n_samples], list of datapaths corresponding to <feature_matrix_all>\n        save_path:          str, where to store sample image.\n        n_image_samples:    Number of sample recoveries.\n        n_closest:          Number of closest recoveries to show.\n    Returns:\n        Nothing!\n    '
    image_paths = np.array([x[0] for x in image_paths])
    sample_idxs = np.random.choice(np.arange(len(feature_matrix_all)), n_image_samples)
    faiss_search_index = faiss.IndexFlatL2(feature_matrix_all.shape[(- 1)])
    faiss_search_index.add(feature_matrix_all)
    (_, closest_feature_idxs) = faiss_search_index.search(feature_matrix_all, (n_closest + 1))
    sample_paths = image_paths[closest_feature_idxs][sample_idxs]
    (f, axes) = plt.subplots(n_image_samples, (n_closest + 1))
    for (i, (ax, plot_path)) in enumerate(zip(axes.reshape((- 1)), sample_paths.reshape((- 1)))):
        ax.imshow(np.array(Image.open(plot_path)))
        ax.set_xticks([])
        ax.set_yticks([])
        if (i % (n_closest + 1)):
            ax.axvline(x=0, color='g', linewidth=13)
        else:
            ax.axvline(x=0, color='r', linewidth=13)
    f.set_size_inches(10, 20)
    f.tight_layout()
    f.savefig(save_path)
    plt.close()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... (np.arange,  ... )

idx = 66:------------------- similar code ------------------ index = 93, score = 5.0 
def shift(shape, stride, anchors):
    shift_h = (np.arange(0, shape[0]) * stride)
    shift_w = (np.arange(0, shape[1]) * stride)
    (shift_h, shift_w) = np.meshgrid(shift_h, shift_w)
    shifts = np.vstack((shift_h.ravel(), shift_w.ravel())).transpose()
    A = anchors.shape[0]
    K = shifts.shape[0]
    all_anchors = (anchors.reshape((1, A, 2)) + shifts.reshape((1, K, 2)).transpose((1, 0, 2)))
    all_anchors = all_anchors.reshape(((K * A), 2))
    return all_anchors

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = (np.arange *  ... )

idx = 67:------------------- similar code ------------------ index = 94, score = 5.0 
def spearmanr_np(preds, targets, ix=None, ignore_hard_targets=False, optimized_rounding=False):
    ix = (ix if (ix is not None) else np.arange(preds.shape[0]))
    n_targets = (N_TARGETS - (ignore_hard_targets * len(hard_targets)))
    if optimized_rounding:
        preds = optimized_round(preds, targets, verbose=False, ix=ix)
    score = 0
    for (i, t) in enumerate(TARGETS):
        if (ignore_hard_targets and (t in hard_targets)):
            continue
        score_i = spearmanr(preds[(ix, i)], targets[(ix, i)]).correlation
        score += np.nan_to_num((score_i / n_targets))
    return score

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = ( ...  if else np.arange)

idx = 68:------------------- similar code ------------------ index = 95, score = 5.0 
@pytest.mark.parametrize('systematic, sparse', product([False, True], [False, True]))
def test_decoding(systematic, sparse):
    n = 15
    d_v = 4
    d_c = 5
    seed = 0
    (H, G) = make_ldpc(n, d_v, d_c, seed=seed, systematic=systematic, sparse=sparse)
    assert (not binaryproduct(H, G).any())
    (n, k) = G.shape
    snr = 10
    v = (np.arange(k) % 2)
    y = encode(G, v, snr, seed)
    d = decode(H, y, snr)
    x = get_message(G, d)
    assert (abs((v - x)).sum() == 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = (np.arange %  ... )

idx = 69:------------------- similar code ------------------ index = 96, score = 5.0 
def _compute_ranks(rewards):
    rewards = np.array(rewards)
    ranks = np.empty(rewards.size, dtype=int)
    ranks[rewards.argsort()] = np.arange(rewards.size)
    return ranks

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = np.arange

idx = 70:------------------- similar code ------------------ index = 97, score = 5.0 
def randomsampling(self, batch, labels):
    '\n        This methods finds all available triplets in a batch given by the classes provided in labels, and randomly\n        selects <len(batch)> triplets.\n\n        Args:\n            batch:  np.ndarray or torch.Tensor, batch-wise embedded training samples.\n            labels: np.ndarray or torch.Tensor, ground truth labels corresponding to batch.\n        Returns:\n            list of sampled data tuples containing reference indices to the position IN THE BATCH.\n        '
    if isinstance(labels, torch.Tensor):
        labels = labels.detach().numpy()
    unique_classes = np.unique(labels)
    indices = np.arange(len(batch))
    class_dict = {i: indices[(labels == i)] for i in unique_classes}
    sampled_triplets = [list(it.product([x], [x], [y for y in unique_classes if (x != y)])) for x in unique_classes]
    sampled_triplets = [x for y in sampled_triplets for x in y]
    sampled_triplets = [[x for x in list(it.product(*[class_dict[j] for j in i])) if (x[0] != x[1])] for i in sampled_triplets]
    sampled_triplets = [x for y in sampled_triplets for x in y]
    sampled_triplets = random.sample(sampled_triplets, batch.shape[0])
    return sampled_triplets

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 71:------------------- similar code ------------------ index = 99, score = 5.0 
def recover_closest_inshop(query_feature_matrix_all, gallery_feature_matrix_all, query_image_paths, gallery_image_paths, save_path, n_image_samples=10, n_closest=3):
    '\n    Provide sample recoveries.\n\n    Args:\n        query_feature_matrix_all:   np.ndarray [n_query_samples x embed_dim], full data embedding of query samples.\n        gallery_feature_matrix_all: np.ndarray [n_gallery_samples x embed_dim], full data embedding of gallery samples.\n        query_image_paths:          list [n_samples], list of datapaths corresponding to <query_feature_matrix_all>\n        gallery_image_paths:        list [n_samples], list of datapaths corresponding to <gallery_feature_matrix_all>\n        save_path:          str, where to store sample image.\n        n_image_samples:    Number of sample recoveries.\n        n_closest:          Number of closest recoveries to show.\n    Returns:\n        Nothing!\n    '
    (query_image_paths, gallery_image_paths) = (np.array(query_image_paths), np.array(gallery_image_paths))
    sample_idxs = np.random.choice(np.arange(len(query_feature_matrix_all)), n_image_samples)
    faiss_search_index = faiss.IndexFlatL2(gallery_feature_matrix_all.shape[(- 1)])
    faiss_search_index.add(gallery_feature_matrix_all)
    (_, closest_feature_idxs) = faiss_search_index.search(query_feature_matrix_all, n_closest)
    image_paths = gallery_image_paths[closest_feature_idxs]
    image_paths = np.concatenate([query_image_paths.reshape((- 1), 1), image_paths], axis=(- 1))
    sample_paths = image_paths[closest_feature_idxs][sample_idxs]
    (f, axes) = plt.subplots(n_image_samples, (n_closest + 1))
    for (i, (ax, plot_path)) in enumerate(zip(axes.reshape((- 1)), sample_paths.reshape((- 1)))):
        ax.imshow(np.array(Image.open(plot_path)))
        ax.set_xticks([])
        ax.set_yticks([])
        if (i % (n_closest + 1)):
            ax.axvline(x=0, color='g', linewidth=13)
        else:
            ax.axvline(x=0, color='r', linewidth=13)
    f.set_size_inches(10, 20)
    f.tight_layout()
    f.savefig(save_path)
    plt.close()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... (np.arange,  ... )

idx = 72:------------------- similar code ------------------ index = 101, score = 5.0 
def recover_closest_inshop(query_feature_matrix_all, gallery_feature_matrix_all, query_image_paths, gallery_image_paths, save_path, n_image_samples=10, n_closest=3):
    '\n    Provide sample recoveries.\n\n    Args:\n        query_feature_matrix_all:   np.ndarray [n_query_samples x embed_dim], full data embedding of query samples.\n        gallery_feature_matrix_all: np.ndarray [n_gallery_samples x embed_dim], full data embedding of gallery samples.\n        query_image_paths:          list [n_samples], list of datapaths corresponding to <query_feature_matrix_all>\n        gallery_image_paths:        list [n_samples], list of datapaths corresponding to <gallery_feature_matrix_all>\n        save_path:          str, where to store sample image.\n        n_image_samples:    Number of sample recoveries.\n        n_closest:          Number of closest recoveries to show.\n    Returns:\n        Nothing!\n    '
    (query_image_paths, gallery_image_paths) = (np.array(query_image_paths), np.array(gallery_image_paths))
    sample_idxs = np.random.choice(np.arange(len(query_feature_matrix_all)), n_image_samples)
    faiss_search_index = faiss.IndexFlatL2(gallery_feature_matrix_all.shape[(- 1)])
    faiss_search_index.add(gallery_feature_matrix_all)
    (_, closest_feature_idxs) = faiss_search_index.search(query_feature_matrix_all, n_closest)
    image_paths = gallery_image_paths[closest_feature_idxs]
    image_paths = np.concatenate([query_image_paths.reshape((- 1), 1), image_paths], axis=(- 1))
    sample_paths = image_paths[closest_feature_idxs][sample_idxs]
    (f, axes) = plt.subplots(n_image_samples, (n_closest + 1))
    for (i, (ax, plot_path)) in enumerate(zip(axes.reshape((- 1)), sample_paths.reshape((- 1)))):
        ax.imshow(np.array(Image.open(plot_path)))
        ax.set_xticks([])
        ax.set_yticks([])
        if (i % (n_closest + 1)):
            ax.axvline(x=0, color='g', linewidth=13)
        else:
            ax.axvline(x=0, color='r', linewidth=13)
    f.set_size_inches(10, 20)
    f.tight_layout()
    f.savefig(save_path)
    plt.close()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... (np.arange,  ... )

idx = 73:------------------- similar code ------------------ index = 102, score = 5.0 
def disk(radius, alias_blur=0.1, dtype=np.float32):
    if (radius <= 8):
        L = np.arange((- 8), (8 + 1))
        ksize = (3, 3)
    else:
        L = np.arange((- radius), (radius + 1))
        ksize = (5, 5)
    (X, Y) = np.meshgrid(L, L)
    aliased_disk = np.array((((X ** 2) + (Y ** 2)) <= (radius ** 2)), dtype=dtype)
    aliased_disk /= np.sum(aliased_disk)
    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.arange

idx = 74:------------------- similar code ------------------ index = 103, score = 5.0 
def __init__(self, data_path, targetFile=None, targetClass=None):
    if (targetFile is None):
        random.seed(5566)
        from fnmatch import fnmatch
        file_list = []
        for (path, subdirs, files) in os.walk(data_path):
            for name in files:
                if fnmatch(name, '*.jpg'):
                    file_list.append(os.path.join(path, name))
        random.shuffle(file_list)
        FILENAME_RE = re.compile('(\\d+).(\\d+).jpg')
        temp_data = []
        temp_labels = []
        for f in file_list:
            img = scipy.misc.imread(f)
            if ((img.shape[0] < 299) or (img.shape[1] < 299)):
                continue
            img = ((np.array(scipy.misc.imresize(img, (299, 299)), dtype=np.float32) / 255) - 0.5)
            if (img.shape != (299, 299, 3)):
                continue
            img = np.expand_dims(img, axis=0)
            temp_data.append(img)
            filename_search = FILENAME_RE.search(f)
            temp_labels.append(int(filename_search.group(1)))
        data_num = len(temp_data)
        print('Imagenet load # testing images:{}'.format(data_num))
        temp_data = np.concatenate(temp_data)
        temp_labels = np.array(temp_labels)
        self.test_data = temp_data
        self.test_labels = np.zeros((data_num, 1001))
        self.test_labels[(np.arange(data_num), temp_labels)] = 1
    else:
        print('Target file:{}'.format(targetFile))
        (temp_data, temp_label) = readimg(targetFile, force=True)
        self.test_data = np.array(temp_data)
        temp_label = np.array(temp_label)
        self.test_labels = np.zeros((1, 1001))
        self.test_labels[(0, temp_label)] = 1
        print('Read target file {}'.format(targetFile))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ... . ... [(np.arange,  ... )]

idx = 75:------------------- similar code ------------------ index = 104, score = 5.0 
def intersectionAndUnion(output, target, K, ignore_index=255):
    assert (output.ndim in [1, 2, 3])
    assert (output.shape == target.shape)
    output = output.reshape(output.size).copy()
    target = target.reshape(target.size)
    output[np.where((target == ignore_index))[0]] = 255
    intersection = output[np.where((output == target))[0]]
    (area_intersection, _) = np.histogram(intersection, bins=np.arange((K + 1)))
    (area_output, _) = np.histogram(output, bins=np.arange((K + 1)))
    (area_target, _) = np.histogram(target, bins=np.arange((K + 1)))
    area_union = ((area_output + area_target) - area_intersection)
    return (area_intersection, area_union, area_target)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 =  ... . ... ( ... ,  ... =np.arange)

idx = 76:------------------- similar code ------------------ index = 105, score = 5.0 
def fusion_stitch_grid(filename, annotations_dir, output_dir, image_dir='/home/psm2208/data/GTDB/images/', gt_dir='/home/psm2208/data/GTDB/', char_gt='', thresh=20):
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    if (not os.path.exists(output_dir)):
        os.mkdir(output_dir)
    pages_list = read_page_info(filename, annotations_dir, image_dir, gt_dir, char_gt)
    pool = Pool(processes=32)
    total = str(len(pages_list))
    math_cache = pool.map(read_math_regions, pages_list)
    pool.close()
    pool.join()
    fusion_list = []
    for (i, page) in enumerate(pages_list):
        pdf_name = page[1]
        page_num = page[2]
        for a in np.arange(0.3, 1.1, 0.1):
            for b in np.arange(0.0, 1.1, 0.1):
                for c in np.arange(0.0, 1.1, 0.1):
                    fusion_list.append((pdf_name, page_num, output_dir, math_cache[i], a, b, c))
    pool = Pool(processes=32)
    total = str(len(fusion_list))
    start = time.time()
    init = start
    for (i, _) in enumerate(pool.imap_unordered(fusion, fusion_list), 1):
        print(((('\nprogress: ' + str(i)) + '/') + total))
        if ((i % 100) == 0):
            current = time.time()
            print('\nTime taken for last 100, total time:', (current - start), (current - init))
            start = time.time()
    pool.close()
    pool.join()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
        for  ...  in np.arange:
idx = 77:------------------- similar code ------------------ index = 106, score = 5.0 
def return_adj_matrix(dataframe):
    classes = sorted(dataframe.processed_classes.unique())
    classes_dict = {v: k for (k, v) in enumerate(classes)}
    class_combinations = list(combinations(np.arange(0, len(classes)), r=2))
    sent_bert = SentenceTransformer('bert-base-nli-mean-tokens').eval()
    sentence_embeddings = sent_bert.encode(classes)
    adj_matrix = np.zeros((len(sentence_embeddings), len(sentence_embeddings)))
    normalised_sentence_embeddings = [(i / norm(i)) for i in sentence_embeddings]
    for class_tuple in class_combinations:
        (u, v) = (class_tuple[0], class_tuple[1])
        adj_matrix[class_tuple] = adj_matrix[(v, u)] = (1 - sum((normalised_sentence_embeddings[u] * normalised_sentence_embeddings[v])))
    return adj_matrix

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  =  ... ( ... (np.arange,))

idx = 78:------------------- similar code ------------------ index = 107, score = 5.0 
def load_clips_tsn(fname, clip_len=16, n_clips=1, is_validation=False):
    if (not os.path.exists(fname)):
        print(('Missing: ' + fname))
        return []
    capture = cv2.VideoCapture(fname)
    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
    if ((frame_count == 0) or (frame_width == 0) or (frame_height == 0)):
        print('loading error, switching video ...')
        print(fname)
        return []
    total_frames = frame_count
    sampling_period = max((total_frames // n_clips), 1)
    n_snipets = min(n_clips, (total_frames // sampling_period))
    if (not is_validation):
        starts = np.random.randint(0, max(1, (sampling_period - clip_len)), n_snipets)
    else:
        starts = np.zeros(n_snipets)
    offsets = np.arange(0, total_frames, sampling_period)
    selection = np.concatenate([np.arange((of + s), ((of + s) + clip_len)) for (of, s) in zip(offsets, starts)])
    frames = []
    count = ret_count = 0
    while (count < (selection[(- 1)] + clip_len)):
        (retained, frame) = capture.read()
        if (count not in selection):
            count += 1
            continue
        if (not retained):
            if (len(frames) > 0):
                frame = np.copy(frames[(- 1)])
            else:
                frame = (255 * np.random.rand(frame_height, frame_width, 3)).astype('uint8')
            frames.append(frame)
            ret_count += 1
            count += 1
            continue
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames.append(frame)
        count += 1
    capture.release()
    frames = np.stack(frames)
    total = (n_clips * clip_len)
    while (frames.shape[0] < total):
        frames = np.concatenate([frames, frames[:(total - frames.shape[0])]])
    frames = frames.reshape([n_clips, clip_len, frame_height, frame_width, 3])
    return frames

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 79:------------------- similar code ------------------ index = 84, score = 5.0 
@njit
def _compute_valid_splitting_indices(t, min_leaf):
    'Compute valid split indices for treatment array *t* given *min_leaf*.\n\n    Given an array *t* of treatment status and an integer *min_leaf* --denoting\n    the minimum number of allowed observations of each type in a leaf node--\n    computes a sequence of indices on which we can split *t* and get that each\n    resulting side contains a minimum of *min_leaf* treated and untreated\n    observations. Returns an empty sequence if no split is possible.\n\n    Args:\n        t (np.array): 1d array containing the treatment status as treated =\n            True and untreated = False.\n        min_leaf (int): Minimum number of observations of each type (treated,\n            untreated) allowed in a leaf; has to be greater than 1.\n\n    Returns:\n        out (np.array): a sequence of indices representing valid splitting\n            points.\n\n    '
    out = np.arange(0)
    n = len(t)
    if (n < (2 * min_leaf)):
        return out
    left_index_treated = np.argmax((np.cumsum(t) == min_leaf))
    if (left_index_treated == 0):
        return out
    left_index_untreated = np.argmax((np.cumsum((~ t)) == min_leaf))
    if (left_index_untreated == 0):
        return out
    tmparray = np.array([left_index_treated, left_index_untreated])
    left = np.max(tmparray)
    right_index_treated = np.argmax((np.cumsum(t[::(- 1)]) == min_leaf))
    if (right_index_treated == 0):
        return out
    right_index_untreated = np.argmax((np.cumsum((~ t[::(- 1)])) == min_leaf))
    if (right_index_untreated == 0):
        return out
    tmparray = np.array([right_index_treated, right_index_untreated])
    right = (n - np.max(tmparray))
    if (left > (right - 1)):
        return out
    else:
        out = np.arange(left, (right - 1))
        return out

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 80:------------------- similar code ------------------ index = 82, score = 5.0 
def shuffle_samples(self):
    image_indices = np.random.permutation(np.arange(self.num_samples))
    self.images = self.images[image_indices]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  =  ... . ... (np.arange)

idx = 81:------------------- similar code ------------------ index = 81, score = 5.0 
def plot_ur5_reacher(env, batch_size, shared_returns, plot_running):
    'Helper process for visualize the tasks and episodic returns.\n\n    Args:\n        env: An instance of ReacherEnv\n        batch_size: An int representing timesteps_per_batch provided to the PPO learn function\n        shared_returns: A manager dictionary object containing `episodic returns` and `episodic lengths`\n        plot_running: A multiprocessing Value object containing 0/1.\n            1: Continue plotting, 0: Terminate plotting loop\n    '
    print('Started plotting routine')
    import matplotlib.pyplot as plt
    plt.ion()
    time.sleep(5.0)
    fig = plt.figure(figsize=(20, 6))
    ax1 = fig.add_subplot(131)
    (hl1,) = ax1.plot([], [], markersize=10, marker='o', color='r')
    (hl2,) = ax1.plot([], [], markersize=10, marker='o', color='b')
    ax1.set_xlabel('X', fontsize=14)
    h = ax1.set_ylabel('Y', fontsize=14)
    h.set_rotation(0)
    ax3 = fig.add_subplot(132)
    (hl3,) = ax3.plot([], [], markersize=10, marker='o', color='r')
    (hl4,) = ax3.plot([], [], markersize=10, marker='o', color='b')
    ax3.set_xlabel('Z', fontsize=14)
    h = ax3.set_ylabel('Y', fontsize=14)
    h.set_rotation(0)
    ax2 = fig.add_subplot(133)
    (hl11,) = ax2.plot([], [])
    count = 0
    old_size = len(shared_returns['episodic_returns'])
    while plot_running.value:
        plt.suptitle('Reward: {:.2f}'.format(env._reward_.value), x=0.375, fontsize=14)
        hl1.set_ydata([env._x_target_[1]])
        hl1.set_xdata([env._x_target_[2]])
        hl2.set_ydata([env._x_[1]])
        hl2.set_xdata([env._x_[2]])
        ax1.set_ylim([env._end_effector_low[1], env._end_effector_high[1]])
        ax1.set_xlim([env._end_effector_low[2], env._end_effector_high[2]])
        ax1.set_title('X-Y plane', fontsize=14)
        ax1.set_xlim(ax1.get_xlim()[::(- 1)])
        ax1.set_ylim(ax1.get_ylim()[::(- 1)])
        hl3.set_ydata([env._x_target_[1]])
        hl3.set_xdata([env._x_target_[0]])
        hl4.set_ydata([env._x_[1]])
        hl4.set_xdata([env._x_[0]])
        ax3.set_ylim([env._end_effector_low[1], env._end_effector_high[1]])
        ax3.set_xlim([env._end_effector_low[0], env._end_effector_high[0]])
        ax3.set_title('Y-Z plane', fontsize=14)
        ax3.set_xlim(ax3.get_xlim()[::(- 1)])
        ax3.set_ylim(ax3.get_ylim()[::(- 1)])
        copied_returns = copy.deepcopy(shared_returns)
        if ((not copied_returns['write_lock']) and (len(copied_returns['episodic_returns']) > old_size)):
            returns = np.array(copied_returns['episodic_returns'])
            old_size = len(copied_returns['episodic_returns'])
            window_size_steps = 5000
            x_tick = 1000
            if copied_returns['episodic_lengths']:
                ep_lens = np.array(copied_returns['episodic_lengths'])
            else:
                ep_lens = (batch_size * np.arange(len(returns)))
            cum_episode_lengths = np.cumsum(ep_lens)
            if (cum_episode_lengths[(- 1)] >= x_tick):
                steps_show = np.arange(x_tick, (cum_episode_lengths[(- 1)] + 1), x_tick)
                rets = []
                for i in range(len(steps_show)):
                    rets_in_window = returns[((cum_episode_lengths > max(0, ((x_tick * (i + 1)) - window_size_steps))) * (cum_episode_lengths < (x_tick * (i + 1))))]
                    if rets_in_window.any():
                        rets.append(np.mean(rets_in_window))
                hl11.set_xdata((np.arange(1, (len(rets) + 1)) * x_tick))
                ax2.set_xlim([x_tick, (len(rets) * x_tick)])
                hl11.set_ydata(rets)
                ax2.set_ylim([np.min(rets), (np.max(rets) + 50)])
        time.sleep(0.01)
        fig.canvas.draw()
        fig.canvas.flush_events()
        count += 1

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    while:
        if:
            if:            else:
                 ...  = ( ...  * np.arange)

idx = 82:------------------- similar code ------------------ index = 67, score = 5.0 
def get_states(self):
    if (len(LinkAction.LINKS) == 0):
        return
    rand_link = self.pick(list(LinkAction.LINKS))
    rand_link_src = rand_link[0]
    nodes_dict = self.vizgraph.get_nodes_dict()
    src_viz = nodes_dict[('viz_' + str(rand_link_src))]
    computed_filter = src_viz.get_computed_filter()
    df = self.df
    sql_statement = 'SELECT * FROM df '
    if (len(computed_filter) > 0):
        sql_statement += ('WHERE ' + computed_filter)
    df_result = pandasql.sqldf(sql_statement, locals())
    if df_result.empty:
        return None
    filter_per_dim = []
    for bin_dim in range(len(src_viz.binning)):
        filters = []
        dim = src_viz.binning[bin_dim]['dimension']
        field = list(filter((lambda x: (x['field'] == dim)), self.sample_json['tables']['fact']['fields']))[0]
        if (field['type'] == 'quantitative'):
            bin_width = float(src_viz.binning[bin_dim]['width'])
            min_val = df_result[dim].min()
            max_val = df_result[dim].max()
            min_index = math.floor((min_val / bin_width))
            max_index = math.floor((max_val / bin_width))
            num_bins = 0
            if (np.random.rand() < 0.4):
                num_bins = 1
            else:
                num_bins = (random.randint(1, (max_index - min_index)) if (max_index > min_index) else 1)
            selected_bins = np.random.choice(np.arange(min_index, (max_index + 1)), size=num_bins, replace=False)
            for selected_bin in selected_bins:
                range_min = (selected_bin * bin_width)
                range_max = ((selected_bin + 1) * bin_width)
                filt = ('(%s >= %s and %s < %s)' % (dim, '{:.1f}'.format(range_min), dim, '{:.1f}'.format(range_max)))
                filters.append(filt)
        else:
            all_bins = df_result[dim].unique().tolist()
            num_bins = random.randint(1, len(all_bins))
            selected_bins = np.random.choice(all_bins, size=num_bins, replace=False)
            for selected_bin in list(selected_bins):
                filt = ("(%s = '%s')" % (dim, selected_bin))
                filters.append(filt)
        filter_per_dim.append(' or '.join(filters))
    filter_per_dim = [('(%s)' % f) for f in filter_per_dim]
    return Operation(OrderedDict({'name': ('viz_%s' % rand_link_src), 'selection': ' and '.join(filter_per_dim)}))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
        if:
             ...  =  ... . ... (np.arange,,)

idx = 83:------------------- similar code ------------------ index = 112, score = 5.0 
if (__name__ == '__main__'):
    '\n    Example script to create dating schedule for CCN 2018 conference\n    '
    person_df = pd.ExcelFile('CCN18_MindMatchData.xlsx').parse('Grid Results')
    person_df['FullName'] = ((person_df['NameFirst'] + ' ') + person_df['NameLast'])
    person_df['PersonID'] = np.arange(len(person_df))
    person_id_map = {r['PersonID']: r['FullName'] for (_, r) in person_df.iterrows()}
    person_affil_map = {r['PersonID']: r['Affiliation'] for (_, r) in person_df.iterrows()}
    schedule = create_dating_schedule(person_df)
    n_timeslot = (len(schedule[0][(- 1)]) + 1)
    person_schedule_all = schedule_to_timeslot(schedule, n_timeslot=n_timeslot)
    n_meeting = 6
    output_text = []
    for person_schedule_df in person_schedule_all:
        output_text.extend(['You are: ', str(person_id_map[person_schedule_df.person.unique()[0]])])
        output_text.extend(['--------------------'])
        output_text.extend(['Dating schedule'])
        output_text.extend(['--------------------'])
        r = 0
        for i in range(1, (n_meeting + 1)):
            person_to_meet = [l for l in list(person_schedule_df[i]) if (not pd.isnull(l))]
            if (len(person_to_meet) > 0):
                table_number = person_schedule_df['table_number'].iloc[r]
                output_text.extend([('timeslot: %d, table number: %d, date: %s' % (i, table_number, person_id_map[person_to_meet[0]]))])
                r += 1
            else:
                output_text.extend([('timeslot: %d, Waiting area!' % i)])
        output_text.extend([''])
    with open('output_date_schedule.txt', 'w') as f:
        for l in output_text:
            f.write('{}\n'.format(l))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
 = np.arange

idx = 84:------------------- similar code ------------------ index = 58, score = 5.0 
def get_states(self):
    src_viz_num = random.randint(0, VizAction.VIZ_COUNTER)
    src_viz = list(self.vizgraph.get_nodes())[src_viz_num]
    computed_filter = src_viz.get_computed_filter()
    df = self.df
    sql_statement = 'SELECT * FROM df '
    if (len(computed_filter) > 0):
        sql_statement += ('WHERE ' + computed_filter)
    df_result = pandasql.sqldf(sql_statement, locals())
    if df_result.empty:
        return None
    filter_per_dim = []
    for bin_dim in range(len(src_viz.binning)):
        filters = []
        dim = src_viz.binning[bin_dim]['dimension']
        field = list(filter((lambda x: (x['field'] == dim)), self.sample_json['tables']['fact']['fields']))[0]
        if (field['type'] == 'quantitative'):
            bin_width = float(src_viz.binning[bin_dim]['width'])
            min_val = df_result[dim].min()
            max_val = df_result[dim].max()
            min_index = math.floor((min_val / bin_width))
            max_index = math.floor((max_val / bin_width))
            num_bins = 0
            if (np.random.rand() < 0.4):
                num_bins = 1
            else:
                num_bins = (random.randint(1, (max_index - min_index)) if (max_index > min_index) else 1)
            selected_bins = np.random.choice(np.arange(min_index, (max_index + 1)), size=num_bins, replace=False)
            for selected_bin in selected_bins:
                range_min = (selected_bin * bin_width)
                range_max = ((selected_bin + 1) * bin_width)
                filt = ('(%s >= %s and %s < %s)' % (dim, '{:.1f}'.format(range_min), dim, '{:.1f}'.format(range_max)))
                filters.append(filt)
        else:
            all_bins = df_result[dim].unique().tolist()
            num_bins = random.randint(1, len(all_bins))
            selected_bins = np.random.choice(all_bins, size=num_bins, replace=False)
            for selected_bin in list(selected_bins):
                filt = ("(%s = '%s')" % (dim, selected_bin))
                filters.append(filt)
        filter_per_dim.append(' or '.join(filters))
    filter_per_dim = [('(%s)' % f) for f in filter_per_dim]
    return Operation(OrderedDict({'name': ('viz_%s' % src_viz_num), 'filter': ' and '.join(filter_per_dim)}))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
        if:
             ...  =  ... . ... (np.arange,,)

idx = 85:------------------- similar code ------------------ index = 59, score = 5.0 
def plot_hyperparam(ax, alg, param):
    relevant_dict = relevant_param(alg)
    hyperparam_range = relevant_dict[param]
    r1 = np.arange(len(envs))
    xdata = []
    for (i, env) in enumerate(envs):
        data = read_hyperparam(alg, env)
        hyperparam_value = data[param]
        xdata.append((hyperparam_range.index(hyperparam_value) + 1))
    barlist = ax.bar(r1, xdata, edgecolor='white')
    for (i, c) in enumerate(['r', 'g', 'b', 'm']):
        barlist[i].set_color(c)
    plt.setp(ax.yaxis.get_majorticklabels(), rotation=30)
    ax.yaxis.set_tick_params(pad=0)
    ax.set_xticklabels([])
    ax.set_yticks(range((len(hyperparam_range) + 1)))
    if ('learning_rate' in param):
        hyperparam_range = ['3e-5', '3e-4']
    ax.set_yticklabels(['', *hyperparam_range])
    ax.margins(x=0.0, y=0.0, tight=True)
    return barlist

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 86:------------------- similar code ------------------ index = 60, score = 5.0 
def mask_head_loss_post(segms, mask_roi_indices, gt_segms, gt_mask_labels, batchsize):
    'Loss function for Mask Head (post).\n\n     Args:\n         segms (array): An array whose shape is :math:`(R, n\\_class, M, M)`,\n             where :math:`R` is the total number of RoIs in the given batch.\n         mask_roi_indices (array): A list of arrays returned by\n             :func:`mask_head_loss_pre`.\n         gt_segms (list of arrays): A list of arrays returned by\n             :func:`mask_head_loss_pre`.\n         gt_mask_labels (list of arrays): A list of arrays returned by\n             :func:`mask_head_loss_pre`.\n         batchsize (int): The size of batch.\n\n     Returns:\n        chainer.Variable:\n        Mask loss.\n    '
    xp = cuda.get_array_module(segms.array)
    mask_roi_indices = xp.hstack(mask_roi_indices).astype(np.int32)
    gt_segms = xp.vstack(gt_segms)
    gt_mask_labels = xp.hstack(gt_mask_labels).astype(np.int32)
    mask_loss = F.sigmoid_cross_entropy(segms[(np.arange(len(gt_mask_labels)), gt_mask_labels)], gt_segms.astype(np.int32))
    return mask_loss

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  =  ... . ... ( ... [(np.arange,  ... )],)

idx = 87:------------------- similar code ------------------ index = 63, score = 5.0 
def visualize_means(means, num_classes, data_name, save_path, name):
    '\n    Visualization of means, e.g. of latent code z.\n\n    Parameters:\n        means (torch.Tensor): 2-D Tensor with one mean z vector per class.\n        num_classes (int): Defines number of classes.\n        data_name (str): Dataset name. Used for naming.\n        save_path (str): Saving path.\n        name (str): Name for type of mean, e.g. "z".\n    '
    classes = np.arange(0, num_classes)
    plt.figure(figsize=(20, 20))
    ax = sns.heatmap(means.cpu().numpy(), cmap='BrBG')
    ax.set_title(data_name, fontsize=title_font_size)
    ax.set_xlabel((name + ' mean activations'), fontsize=axes_font_size)
    ax.set_yticklabels(classes, rotation=0)
    plt.savefig(os.path.join(save_path, (name + '_mean_activations.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 88:------------------- similar code ------------------ index = 64, score = 5.0 
def test_append_one_array(self):
    df1 = self.df.append({'h': np.arange(5)}, axis='columns')
    data2 = {'a': array([9, 10, 9, 9, 10]), 'b': array([0.0, nan, nan, 0.0, 1.0]), 'c': array([None, 'e', 'e', 'a', 'z'], dtype=object), 'd': array([False, False, True, False, True]), 'e': array([0, 20, 30, 4, 4]), 'f': array(['a', None, 'ad', None, 'ad'], dtype=object), 'g': array([nan, nan, nan, nan, nan]), 'h': array([0, 1, 2, 3, 4])}
    df2 = dx.DataFrame(data2)
    assert_frame_equal(df1, df2)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  =  ... . ... ({ ... : np.arange},)

idx = 89:------------------- similar code ------------------ index = 65, score = 5.0 
def test_shuffle():
    ind = np.arange(10)
    rs = RandomState(seed=0)
    rs.shuffle(ind)
    assert_array_equal(ind, [2, 8, 4, 9, 1, 6, 7, 3, 0, 5])

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 90:------------------- similar code ------------------ index = 66, score = 5.0 
def plot_hyperparam_alg(ax, alg):
    relevant_dict = relevant_param(alg)
    ylabel = list(relevant_dict.keys())
    xdata = []
    r1 = np.arange(len(ylabel))
    barWidth = 0.2
    color = ['r', 'g', 'b', 'm']
    for (i, env) in enumerate(envs):
        data = read_hyperparam(alg, env)
        xdata = []
        for key in ylabel:
            hyperparam_range = relevant_dict[key]
            hyperparam_value = data[key]
            xdata.append((hyperparam_range.index(hyperparam_value) + 1))
        r = [(x + (i * barWidth)) for x in r1]
        ax.bar(r, xdata, color=color[i], width=barWidth, edgecolor='white', label=env)
    ax.set_xticks([(r + barWidth) for r in range(len(ylabel))])
    ax.set_xticklabels(ylabel)
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=30)
    ax.set_ylabel(alg)
    ax.set_yticklabels([])

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 91:------------------- similar code ------------------ index = 108, score = 5.0 
def load_clips_npy(fname, clip_len=16, n_clips=1, is_validation=False):
    if (not os.path.exists(fname)):
        return []
    try:
        clip = np.load(fname, mmap_mode='r')
    except ValueError:
        print('MMAP ERROR!!')
        return []
    (frame_count, H, W, ch) = clip.shape
    total_frames = min(frame_count, 300)
    sampling_period = max((total_frames // n_clips), 1)
    n_snipets = min(n_clips, (total_frames // sampling_period))
    if (not is_validation):
        starts = np.random.randint(0, max(1, (sampling_period - clip_len)), n_snipets)
    else:
        starts = np.zeros(n_snipets)
    offsets = np.arange(0, total_frames, sampling_period)
    selection = np.concatenate([np.arange((of + s), ((of + s) + clip_len)) for (of, s) in zip(offsets, starts)])
    selection = selection[(selection < frame_count)]
    clip = clip[selection.astype(int)]
    total = (n_clips * clip_len)
    while (clip.shape[0] < total):
        clip = np.concatenate([clip, clip[:(total - clip.shape[0])]])
    clip = clip.reshape([n_clips, clip_len, H, W, 3])
    return clip

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.arange

idx = 92:------------------- similar code ------------------ index = 80, score = 5.0 
def create_circle_regions(cluster):
    start_time = time.time()
    scale_map_fits = fits.open(cluster.scale_map_file)
    mask_fits = fits.open(cluster.combined_mask)
    region_map = cluster.scale_map_region_index
    scale_map = scale_map_fits[0].data
    mask = mask_fits[0].data
    bounds = scale_map.shape
    xvals = np.arange(bounds[1])
    yvals = np.arange(bounds[0])
    for observation in cluster.observations:
        print('Making circular fitting regions for observation {}'.format(observation.id))
        image_fits = fits.open(observation.acisI_comb_img)
        image_header = image_fits[0].header
        cdelt1p = image_header['CDELT1P']
        cdelt2p = image_header['CDELT2P']
        crval1p = image_header['CRVAL1P']
        crval2p = image_header['CRVAL2P']
        crpix1p = image_header['CRPIX1P']
        crpix2p = image_header['CRPIX2P']
        radii = ((mask * scale_map) * cdelt1p)
        newx = ((((xvals + 1) - crpix1p) * cdelt1p) + crval1p)
        newy = ((((yvals + 1) - crpix2p) * cdelt2p) + crval2p)
        (xx, yy) = np.meshgrid(newx, newy)
        non_zero_indices = np.nonzero(radii)
        nz_rad = radii[non_zero_indices]
        nz_x = xx[non_zero_indices]
        nz_y = yy[non_zero_indices]
        obs_regions = region_map[non_zero_indices]
        region_array = np.array([(i, j, k, l) for (i, j, k, l) in zip(nz_x, nz_y, nz_rad, obs_regions)])
        regions = [['circle({x},{y},{rad})'.format(x=x[0], y=x[1], rad=x[2]), int(x[3])] for x in region_array]
        observation.scale_map_region_list = regions
    end_time = time.time()
    print('Time elapsed making regions for fit: {:0.2f} (s)'.format((end_time - start_time)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.arange

idx = 93:------------------- similar code ------------------ index = 69, score = 5.0 
def plot_grid(activities, shape=None, slice=(- 1), title='', colormap='Greys', vmin=None, vmax=None, node_annotations=None, show_grid=False):
    if (shape is not None):
        activities = np.array(activities).reshape((len(activities), shape[0], shape[1]))[slice]
    cmap = plt.get_cmap(colormap)
    plt.title(title)
    plt.imshow(activities, interpolation='none', cmap=cmap, vmin=vmin, vmax=vmax)
    if (node_annotations is not None):
        for i in range(len(node_annotations)):
            for j in range(len(node_annotations[i])):
                plt.text(j, i, node_annotations[i][j], ha='center', va='center', color='grey', fontdict={'weight': 'bold', 'size': 6})
    if show_grid:
        plt.grid(which='major', axis='both', linestyle='-', color='grey', linewidth=0.5)
        plt.xticks(np.arange((- 0.5), len(activities[0]), 1), '')
        plt.yticks(np.arange((- 0.5), len(activities), 1), '')
        plt.tick_params(axis='both', which='both', length=0)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if  ... :
         ... . ... (np.arange,  ... )

idx = 94:------------------- similar code ------------------ index = 21, score = 5.0 
def aranges(*shape):
    r = np.prod(shape)
    return np.arange(r).reshape(shape).astype(np.float32)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    return np.arange

idx = 95:------------------- similar code ------------------ index = 16, score = 5.0 
def aranges(xp, *shape):
    r = np.prod(shape)
    return np.arange(r).reshape(shape).astype(np.float32)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    return np.arange

idx = 96:------------------- similar code ------------------ index = 55, score = 5.0 
if (__name__ == '__main__'):
    import numpy as np
    if ((len(sys.argv) > 1) and sys.argv[1].startswith('-')):
        parser = argparse.ArgumentParser()
        parser.add_argument('--data', type=str, default=None, help='pickle data which contains hs, rois and roi_indices')
        parser.add_argument('--index', type=int, default=0, help='which h in hs is used')
        parser.add_argument('--out', type=str, default='.', help='')
        parser.add_argument('--gpu', action='store_true')
        args = parser.parse_args()
        print(args.data)
        with open(args.data, 'rb') as f:
            data = pickle.load(f)
        hs = [data_var.array for data_var in data['hs']]
        rois = data['rois']
        roi_indices = data['roi_indices']
        if args.gpu:
            hs = [chainer.cuda.to_gpu(h) for h in hs]
            rois = chainer.cuda.to_gpu(rois)
            roi_indices = chainer.cuda.to_gpu(roi_indices)
        assert (len(hs) == 5)
        assert (len(rois) == 5)
        assert (len(roi_indices) == 5)
        for (i, FPN_ROIAlign2D) in enumerate((FPN_ROIAlign2D_1st_scale, FPN_ROIAlign2D_2nd_scale, FPN_ROIAlign2D_3rd_scale, FPN_ROIAlign2D_4th_scale, FPN_ROIAlign2D_5th_scale)):
            ch2o.generate_testcase(FPN_ROIAlign2D(F.roi_average_align_2d), [hs[i], rois[i], roi_indices[i]], output_dir=os.path.join(args.out, 'fpn_roi_align_2d_pyramid{}_scale'.format(i)), use_gpu=args.gpu)
    else:
        x = np.arange((((2 * 3) * 5) * 5)).reshape((2, 3, 5, 5)).astype(np.float32)
        rois = np.array([[0, 1, 3, 4], [1, 0.3, 4, 2.6]]).astype(np.float32)
        roi_indices = np.array([0, 1]).astype(np.int32)
        ch2o.generate_testcase(ROIPool2D(F.roi_max_pooling_2d, 7, 1.2), [x, rois, roi_indices], subname='max_pool')
        ch2o.generate_testcase(ROIPool2D(F.roi_average_pooling_2d, 7, 1.2), [x, rois, roi_indices], subname='avg_pool')
        ch2o.generate_testcase(ROIAlign2D(F.roi_max_align_2d, 7, 1.2, 2), [x, rois, roi_indices], subname='max_align')
        ch2o.generate_testcase(ROIAlign2D(F.roi_average_align_2d, 7, 1.2, 3), [x, rois, roi_indices], subname='avg_align')

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
if:
    import  ...  as np
    if:    else:
         ...  =  ... .arange

idx = 97:------------------- similar code ------------------ index = 86, score = 5.0 
def _get_backprop_tests(dtype):
    chainer.config.dtype = dtype
    F = chainer.functions
    tests = []

    def test(name, model, *inputs, **kwargs):
        tests.append(BackpropTest(name, model, inputs, dtype, **kwargs))

    def aranges(*shape):
        r = np.prod(shape)
        return np.arange(r).reshape(shape).astype(np.float32)

    class Nop(chainer.Chain):

        def forward(self, x):
            return x
    test('nop', Nop(), aranges(2, 3))

    class AddSelf(chainer.Chain):

        def forward(self, x):
            return (x + x)
    test('add_self', AddSelf(), aranges(2, 3))

    class Linear(chainer.Chain):

        def __init__(self):
            super(Linear, self).__init__()
            with self.init_scope():
                self.l1 = L.Linear(None, 10)

        def forward(self, x):
            return F.relu(self.l1(x))
    test('linear', Linear(), aranges(2, 3))

    class LinearNoBias(chainer.Chain):

        def __init__(self):
            super(LinearNoBias, self).__init__()
            with self.init_scope():
                self.l1 = L.Linear(None, 10, nobias=True)

        def forward(self, x):
            return F.relu(self.l1(x))
    test('linear_nobias', LinearNoBias(), aranges(2, 3))

    class SoftmaxCrossEntropy(chainer.Chain):

        def __init__(self):
            super(SoftmaxCrossEntropy, self).__init__()
            with self.init_scope():
                self.l1 = L.Linear(None, 10)

        def forward(self, x, t):
            return F.softmax_cross_entropy(self.l1(x), t)
    test('softmax_cross_entropy', SoftmaxCrossEntropy(), aranges(2, 3), np.array([1, 0], dtype=np.int32))

    class LRN(chainer.Chain):

        def __init__(self):
            super(LRN, self).__init__()
            with self.init_scope():
                self.l1 = L.Linear(None, 10)

        def forward(self, x):
            return F.local_response_normalization(self.l1(x))
    test('lrn', LRN(), aranges(2, 3))

    class Stack(chainer.Chain):

        def __init__(self, axis):
            super(Stack, self).__init__()
            self.axis = axis
            with self.init_scope():
                self.l1 = L.Linear(None, 4)
                self.l2 = L.Linear(None, 4)

        def forward(self, x, y):
            xs = [(self.l1(x) * 2), (self.l2(y) * 3)]
            return F.stack(xs, axis=self.axis)
    test('stack', Stack(0), aranges(2, 3), (aranges(2, 3) + 1))
    test('stack_axis1', Stack(1), aranges(2, 3), (aranges(2, 3) + 1))

    class Concat(chainer.Chain):

        def __init__(self, axis):
            super(Concat, self).__init__()
            self.axis = axis
            with self.init_scope():
                self.l1 = L.Linear(None, 4)
                self.l2 = L.Linear(None, 4)

        def forward(self, x, y):
            xs = [(self.l1(x) * 2), (self.l2(y) * 3)]
            return F.concat(xs, axis=self.axis)
    test('concat', Concat(0), aranges(2, 3), (aranges(2, 3) + 1))
    test('concat_axis1', Concat(1), aranges(2, 3), (aranges(2, 3) + 1))

    class Separate(chainer.Chain):

        def __init__(self, axis):
            super(Separate, self).__init__()
            self.axis = axis
            with self.init_scope():
                self.l1 = L.Linear(None, 3)

        def forward(self, x):
            x = self.l1(x)
            xs = F.separate(x, axis=self.axis)
            return (((((xs[0] * xs[1]) * xs[1]) * xs[2]) * xs[2]) * xs[2])
    test('separate', Separate(0), aranges(3, 2))
    test('separate_axis1', Separate(1), aranges(3, 2))

    class Lookup(chainer.Chain):

        def __init__(self):
            super(Lookup, self).__init__()
            with self.init_scope():
                self.l1 = L.Linear(None, 4)
                self.l2 = L.Linear(None, 4)

        def forward(self, x, y, z):
            xs = [(self.l1(x) * 2), (self.l1(y) * 3), (self.l2(z) * 4)]
            return ((((((xs[0] * xs[2]) * xs[0]) * xs[1]) * xs[2]) * xs[2]) * xs[(- 1)])
    test('lookup', Lookup(), aranges(2, 3), (aranges(2, 3) + 1), (aranges(2, 3) + 2))

    class GetSlice(chainer.Chain):

        def __init__(self):
            super(GetSlice, self).__init__()
            with self.init_scope():
                self.l1 = L.Linear(None, 4)
                self.l2 = L.Linear(None, 4)

        def forward(self, x, y, z):
            xs = [(self.l1(x) * 2), (self.l1(y) * 3), (self.l2(z) * 4)]
            a = xs[0:2]
            b = xs[1:3]
            return (((a[0] * a[1]) * b[0]) * b[1])
    test('get_slice', GetSlice(), aranges(2, 3), (aranges(2, 3) + 1), (aranges(2, 3) + 2))

    class DynamicSlice(chainer.Chain):

        def __init__(self):
            super(DynamicSlice, self).__init__()
            with self.init_scope():
                self.l1 = L.Linear(None, 5)

        def forward(self, x):
            x = self.l1(x)
            a = x[1:3]
            b = x[2:4]
            return (a * b)
    test('dynamic_slice', DynamicSlice(), aranges(4, 2))

    class If(chainer.Chain):

        def __init__(self, cond):
            super(If, self).__init__()
            self.cond = cond
            with self.init_scope():
                self.l1 = L.Linear(None, 5)

        def forward(self, x):
            x = self.l1(x)
            if self.cond:
                x = (x * 3)
            else:
                x = (x * (- 2))
            return x
    test('if_true', If(True), aranges(4, 2))
    test('if_false', If(False), aranges(4, 2))

    class IfPartiallyDifferentiable(chainer.Chain):

        def __init__(self, cond):
            super(IfPartiallyDifferentiable, self).__init__()
            self.cond = cond
            with self.init_scope():
                self.l1 = L.Linear(None, 5)

        def forward(self, x):
            x = self.l1(x)
            y = 2
            if self.cond:
                x = (x * 3)
                y = 3
            else:
                x = (x * (- 2))
                y = 4
            return x[:y]
    test('if_pd_true', IfPartiallyDifferentiable(True), aranges(4, 2))
    test('if_pd_false', IfPartiallyDifferentiable(False), aranges(4, 2))

    class For(chainer.Chain):

        def __init__(self):
            super(For, self).__init__()
            with self.init_scope():
                self.l1 = L.Linear(4, 4)

        def forward(self, x):
            for _ in range(3):
                x = self.l1(x)
            return x
    test('for', For(), aranges(3, 4))

    class Embed(chainer.Chain):

        def __init__(self):
            super(Embed, self).__init__()
            with self.init_scope():
                self.emb = L.EmbedID(7, 4)

        def forward(self, x):
            return self.emb(x)
    test('embed', Embed(), np.array([3, 4, 5, 5, 5, 2]), skip_shape_inference=True)

    class Pad(chainer.Chain):

        def __init__(self):
            super(Pad, self).__init__()
            with self.init_scope():
                self.linear = L.Linear(None, 4)

        def forward(self, x):
            xs = F.separate(x)
            ys = []
            for x in xs:
                ys.append(self.linear(x))
            return F.pad_sequence(ys)
    test('pad', Pad(), aranges(5, 4, 3))
    return tests

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ( ... ):
    def  ... ():
        return np.arange


idx = 98:------------------- similar code ------------------ index = 110, score = 5.0 
def aranges(*shape):
    r = np.prod(shape)
    return np.arange(r).reshape(shape).astype(np.float32)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    return np.arange

idx = 99:------------------- similar code ------------------ index = 62, score = 5.0 
def aranges(*shape):
    r = np.prod(shape)
    return np.arange(r).reshape(shape).astype(np.float32)

------------------- similar code (pruned) ------------------ score = 0.35714285714285715 
def  ... ():
    return np.arange

