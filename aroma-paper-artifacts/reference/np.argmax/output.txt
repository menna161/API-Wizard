------------------------- example 1 ------------------------ 
def forward(self, v1):
    return np.argmax(v1, axis=1)

------------------------- example 2 ------------------------ 
def generate_attack_data_set(data, num_sample, img_offset, model, attack_type='targeted', random_target_class=None, shift_index=False):
    '\n    Generate the data for conducting attack. Only select the data being classified correctly.\n    '
// your code ...

    pred_labels = np.argmax(model.model.predict(data.test_data), axis=1)
    true_labels = np.argmax(data.test_labels, axis=1)
    correct_data_indices = np.where([(1 if (x == y) else 0) for (x, y) in zip(pred_labels, true_labels)])
    print('Total testing data:{}, correct classified data:{}'.format(len(data.test_labels), len(correct_data_indices[0])))
    data.test_data = data.test_data[correct_data_indices]
    data.test_labels = data.test_labels[correct_data_indices]
    true_labels = true_labels[correct_data_indices]
    np.random.seed(img_offset)
    class_num = data.test_labels.shape[1]
    for sample_index in range(num_sample):
        if (attack_type == 'targeted'):
            if (random_target_class is not None):
                np.random.seed(0)
                seq_imagenet = [1, 4, 6, 8, 9, 13, 15, 16, 19, 20, 22, 24, 25, 27, 28, 30, 34, 35, 36, 37, 38, 44, 49, 51, 56, 59, 60, 61, 62, 63, 67, 68, 70, 71, 74, 75, 76, 77, 78, 79, 82, 84, 85, 87, 88, 91, 94, 96, 97, 99]
                seq = [seq_imagenet[(img_offset + sample_index)]]
                while (seq == true_labels[(img_offset + sample_index)]):
                    seq = np.random.choice(random_target_class, 1)
            else:
                seq = list(range(class_num))
                seq.remove(true_labels[(img_offset + sample_index)])
            for s in seq:
                if (shift_index and (s == 0)):
                    s += 1
                orig_img.append(data.test_data[(img_offset + sample_index)])
                target_labels.append(np.eye(class_num)[s])
                orig_labels.append(data.test_labels[(img_offset + sample_index)])
                orig_img_id.append((img_offset + sample_index))
        elif (attack_type == 'untargeted'):
            orig_img.append(data.test_data[(img_offset + sample_index)])
// your code ...

            orig_img_id.append((img_offset + sample_index))
    orig_img = np.array(orig_img)
    target_labels = np.array(target_labels)
// your code ...


------------------------- example 3 ------------------------ 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------------- example 4 ------------------------ 
def log(self):
    'This function is executed after every optimization. So it can be used \n        to interact with the system during training.'
    idx = np.argmax(self.population_returns)
    reward = self.population_returns[(idx, 0)]
    print(f'Reward: {reward}')
    if (reward == 500):
        self.best = self.population_parameters[idx]
        self.terminate()

------------------------- example 5 ------------------------ 
def get_tour_graph(self, points, permutation_matrix):
    '\n        A point is a tuple (x, y), where x is the x-coordinate and y is the y-coordinate of the point.\n        The permutation_matrix is n x n matrix, where n is the number of points in the TSP. Each row represents a point,\n        and each column represents the position of that point in the tour.\n        :param points: the list of points (tuples) which represent the points in the TSP\n        :param permutation_matrix: an n x n matrix, where n is the number of points, which represents the final tour\n        :return: a NetworkX Graph representing the tour, a dictionary defining the NetworkX positions for the Graph, and\n                 the total tour length\n        '
// your code ...

    assert (len(points) == len(permutation_matrix[0])), 'the number of columns in permutation_matrix does not match the number of points'
    G = nx.Graph()
// your code ...

    for (point_index, row) in enumerate(permutation_matrix):
        tour_index = np.argmax(row)
        if (tour_index in tour_index_to_point_index):
            raise Exception(('a point has already claimed position #%s in the tour' % str((tour_index + 1))))
        tour_index_to_point_index[tour_index] = point_index
    distances = []
    for (tour_index, point_index) in tour_index_to_point_index.items():
        from_point_index = point_index
        to_point_index = tour_index_to_point_index[((tour_index + 1) % len(points))]
        G.add_edge(from_point_index, to_point_index)
// your code ...

        distances.append(math.sqrt((((to_point[0] - from_point[0]) ** 2) + ((to_point[1] - from_point[1]) ** 2))))
    pos = {i: point for (i, point) in enumerate(points)}
// your code ...


examples  ||  representativeness  ||  number of lines  || number of comments 
example1  ||          2           ||        2         ||         0        
example2  ||          2           ||        37         ||         3        
example3  ||          5           ||        3         ||         0        
example4  ||          3           ||        8         ||         0        
example5  ||          5           ||        20         ||         4        


idx = 0:------------------- similar code ------------------ index = 7, score = 6.0 
def get_data_sample(self, i=0):
    '\n        :param i: the attack target label id\n        '
    if (i > (self.total_classes - 2)):
        assert False
    self.X_origin = self.all_orig_img[i:(i + 1)]
    self.orig_img_id = self.all_orig_img_id[i:(i + 1)][0]
    self.input_label = self.all_orig_labels_int[i:(i + 1)][0]
    X_orig_img_file = os.path.join(self.results_folder, f'X_{self.dataset_name}_origin_{self.input_label}_id{self.orig_img_id}')
    if ('imagenet' in self.dataset_name):
        np.save(X_orig_img_file, np.array([0]))
    else:
        np.save(X_orig_img_file, self.X_origin)
    target_label_vector = self.all_target_labels[i:(i + 1)]
    self.target_label = np.argmax(target_label_vector, 1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        np
 =  ... .argmax

idx = 1:------------------- similar code ------------------ index = 24, score = 6.0 
def forward(self, v1):
    return np.argmax(v1, axis=1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return np.argmax

idx = 2:------------------- similar code ------------------ index = 6, score = 6.0 
def convert_model(model: 'chainer.Chain', args=[]):
    values.reset_field_and_attributes()
    utils.reset_guid()
    values.function_converters.clear()
    values.builtin_function_converters.clear()
    values.instance_converters.clear()

    def instance_converter(m, i):
        if links_builtin.is_builtin_chainer_link(i):
            return links_builtin.ChainerLinkInstance(m, i)
        if isinstance(i, chainer.ChainList):
            module = values.Object(values.ModuleValue(sys.modules[i.__module__]))
            return links_builtin.ChainerChainListInstance(module, i)
        if isinstance(i, chainer.Link):
            module = values.Object(values.ModuleValue(sys.modules[i.__module__]))
            return links_builtin.ChainerChainInstance(module, i)
        return None
    values.instance_converters.append(instance_converter)
    custom_functions_module = values.Object(values.ModuleValue(custom_functions))
    functions_onnx_module = values.Object(values.ModuleValue(functions_onnx))

    def ret_same(funcArgs):
        return functions.generate_value_with_same_type(funcArgs.keywords['x'].get_value())
    values.function_converters[functions_onnx.onnx_abs] = values.FuncValue(functions_builtin.ChainerFunction(functions_onnx.onnx_abs, ret_value_func=ret_same), None, module=functions_onnx_module)
    c_variable = values.FuncValue(functions_ndarray.NDArrayFunction(), None)
    values.function_converters[chainer.Variable] = c_variable

    def add_chainer_function(func, ret_value_func=None):
        if (ret_value_func is None):
            f = values.FuncValue(functions_builtin.ChainerFunction(func), None)
        else:
            f = values.FuncValue(functions_builtin.ChainerFunction(func, ret_value_func=ret_value_func), None)
        values.function_converters[func] = f

    def ret_tuple(funcArgs=None):
        ret = values.TupleValue()
        ret.vtype = values.TensorValue
        return ret
    for f in F.__dict__.items():
        if inspect.isfunction(f[1]):
            values.function_converters[f[1]] = values.FuncValue(functions.UnimplementedFunction(f[1]), None)
    add_chainer_function(F.elu)
    add_chainer_function(F.leaky_relu)
    add_chainer_function(F.log_softmax)
    add_chainer_function(F.relu)
    add_chainer_function(F.selu)
    add_chainer_function(F.sigmoid)
    add_chainer_function(F.softmax)
    add_chainer_function(F.tanh)
    add_chainer_function(F.softmax_cross_entropy)
    add_chainer_function(F.pad_sequence)
    add_chainer_function(F.average_pooling_2d)
    add_chainer_function(F.unpooling_2d)
    add_chainer_function(F.reshape)
    add_chainer_function(F.transpose)
    add_chainer_function(F.split_axis, ret_value_func=ret_tuple)
    add_chainer_function(F.hstack)
    add_chainer_function(F.vstack)
    add_chainer_function(F.stack)
    add_chainer_function(F.separate, ret_value_func=ret_tuple)
    add_chainer_function(F.squeeze)
    add_chainer_function(F.swapaxes)
    add_chainer_function(F.dropout)
    add_chainer_function(F.concat)
    add_chainer_function(F.matmul)
    add_chainer_function(F.max_pooling_2d)
    add_chainer_function(F.resize_images)
    add_chainer_function(F.broadcast_to)
    add_chainer_function(F.expand_dims)
    add_chainer_function(F.local_response_normalization)
    add_chainer_function(F.mean)
    add_chainer_function(F.average)
    add_chainer_function(F.sum)
    add_chainer_function(F.maximum)
    add_chainer_function(F.minimum)
    add_chainer_function(F.max)
    add_chainer_function(F.min)
    values.function_converters[F.absolute] = values.FuncValue(functions.UserDefinedFunction(custom_functions.chainer_absolute), None, module=custom_functions_module)
    add_chainer_function(F.sin)
    add_chainer_function(F.sinh)
    add_chainer_function(F.sign)
    add_chainer_function(F.cos)
    add_chainer_function(F.cosh)
    add_chainer_function(F.tan)
    add_chainer_function(F.tanh)
    add_chainer_function(F.arcsin)
    add_chainer_function(F.arccos)
    add_chainer_function(F.arctan)
    add_chainer_function(F.exp)
    add_chainer_function(F.log)
    add_chainer_function(F.sqrt)
    add_chainer_function(F.clip)
    values.function_converters[F.argmax] = values.FuncValue(functions_builtin.ChainerArgminmaxFunction(F.argmax), None)
    values.function_converters[F.argmin] = values.FuncValue(functions_builtin.ChainerArgminmaxFunction(F.argmin), None)
    values.function_converters[F.clipped_relu] = values.FuncValue(functions.UserDefinedFunction(custom_functions.chainer_clipped_relu), None, module=custom_functions_module)
    if (int(chainer.__version__[0]) >= 6):
        add_chainer_function(F.roi_max_pooling_2d)
        add_chainer_function(F.roi_average_pooling_2d)
        add_chainer_function(F.roi_max_align_2d)
    add_chainer_function(F.roi_average_align_2d)
    f_array = values.FuncValue(functions_ndarray.NDArrayFunction(), None)
    f_zeros = values.FuncValue(functions_ndarray.NDArrayZerosFunction(), None)
    f_full = values.FuncValue(functions_ndarray.NDArrayFullFunction(), None)
    f_ceil = values.FuncValue(functions_ndarray.NDArrayCeilFunction(), None)
    f_cumsum = values.FuncValue(functions_ndarray.NDArrayCumsumFunction(), None)
    f_maximum = values.FuncValue(functions_ndarray.NDArrayChainerFunction(functions_ndarray.dummy_maximum), None)
    f_minimum = values.FuncValue(functions_ndarray.NDArrayChainerFunction(functions_ndarray.dummy_minimum), None)
    f_argmax = values.FuncValue(functions_ndarray.NDarrayArgminmaxFunction(functions_ndarray.dummy_argmax), None)
    f_argmin = values.FuncValue(functions_ndarray.NDarrayArgminmaxFunction(functions_ndarray.dummy_argmin), None)
    f_round = values.FuncValue(functions_ndarray.NDarrayRoundFunction(functions_ndarray.dummy_round), None)
    f_sqrt = values.FuncValue(functions_ndarray.NDarraySqrtFunction(functions_ndarray.dummy_sqrt), None)
    f_stack = values.FuncValue(functions_ndarray.NDarrayStackFunction(functions_ndarray.dummy_stack), None)
    f_reshape = values.FuncValue(functions_ndarray.NDarrayReshapeFunction(functions_ndarray.dummy_reshape), None)
    f_transpose = values.FuncValue(functions_ndarray.NDarrayTransposeFunction(functions_ndarray.dummy_transpose), None)
    f_int32 = values.FuncValue(functions_ndarray.NDArrayInt32(), None)
    f_float32 = values.FuncValue(functions_ndarray.NDArrayFloat32(), None)
    values.function_converters[np.array] = f_array
    values.function_converters[np.zeros] = f_zeros
    values.function_converters[np.full] = f_full
    values.function_converters[np.ceil] = f_ceil
    values.function_converters[np.cumsum] = f_cumsum
    values.function_converters[np.int32] = f_int32
    values.function_converters[np.float32] = f_float32
    values.function_converters[np.maximum] = f_maximum
    values.function_converters[np.minimum] = f_minimum
    values.function_converters[np.argmax] = f_argmax
    values.function_converters[np.argmin] = f_argmin
    values.function_converters[np.round] = f_round
    values.function_converters[np.sqrt] = f_sqrt
    values.function_converters[np.stack] = f_stack
    values.function_converters[np.reshape] = f_reshape
    values.function_converters[np.transpose] = f_transpose
    values.function_converters[np.clip] = values.FuncValue(functions.UserDefinedFunction(custom_functions.numpy_clip), None, module=custom_functions_module)
    values.function_converters[np.absolute] = values.FuncValue(functions.UserDefinedFunction(custom_functions.numpy_absolute), None, module=custom_functions_module)
    values.function_converters[custom_functions.check_attribute_value] = values.FuncValue(functions.CheckAttributeValueFunction(), None, module=custom_functions_module)
    values.function_converters[custom_functions.check_attribute_scalar] = values.FuncValue(functions.CheckAttributeScalarFunction(), None, module=custom_functions_module)
    values.builtin_function_converters['abs'] = values.FuncValue(functions.UserDefinedFunction(custom_functions.builtin_absolute), None, module=custom_functions_module)
    m_range = values.FuncValue(functions_builtin.RangeFunction(), None)
    values.builtin_function_converters['range'] = m_range
    m_len = values.FuncValue(functions_builtin.LenFunction(), None)
    values.builtin_function_converters['len'] = m_len
    values.function_converters[six.moves.range] = m_range
    m_list = values.FuncValue(functions_builtin.ListFunction(), None)
    values.builtin_function_converters['list'] = m_list
    m_print = values.FuncValue(functions_builtin.PrintFunction(), None)
    values.builtin_function_converters['print'] = m_print
    m_getattr = values.FuncValue(functions_builtin.GetAttrFunction(), None)
    values.builtin_function_converters['getattr'] = m_getattr
    m_hasattr = values.FuncValue(functions_builtin.HasAttrFunction(), None)
    values.builtin_function_converters['hasattr'] = m_hasattr
    m_to_gpu = values.FuncValue(functions_builtin.CopyFunction(cuda.to_gpu), None)
    values.function_converters[cuda.to_gpu] = m_to_gpu
    m_to_cpu = values.FuncValue(functions_builtin.CopyFunction(cuda.to_cpu), None)
    values.function_converters[cuda.to_cpu] = m_to_cpu

    def add_veval_flag_function(name: 'str', func):
        f = values.FuncValue(functions_builtin.VEvalContextFunction(func), None)
        values.builtin_function_converters[name] = f
    add_veval_flag_function('eval_as_written_target', flags.eval_as_written_target)
    add_veval_flag_function('ignore_branch', flags.ignore_branch)
    add_veval_flag_function('for_unroll', flags.for_unroll)
    default_module = values.Object(values.ModuleValue(sys.modules[model.__module__]))
    model_inst = values.parse_instance(default_module, '', model)
    forward_func = model_inst.try_get_and_store_obj('forward', None)
    finput = functions.FunctionArgInput()
    value_args = []
    ind = 0
    node_input = nodes.NodeInput('input')
    for arg in args:
        varg = values.parse_instance(default_module, '', arg, None)
        varg.name = ('in_' + str(ind))
        varg.get_value().name = ('in_' + str(ind))
        varg.get_value().internal_value = None
        finput.inputs.append(varg)
        value_args.append(varg.get_value())
        ind += 1
    node_input.set_outputs(value_args)
    graph = Graph()
    graph.root_graph = graph
    graph.add_node(node_input)
    forward_func_value = forward_func.get_value()
    ret = forward_func_value.func.vcall(default_module, graph, forward_func_value.obj, finput).get_obj()
    assert ((ret is None) or isinstance(ret, values.Object))

    def try_get_value(value) -> 'values.Value':
        if isinstance(value, values.Value):
            return value
        if isinstance(value, values.Object):
            return value.get_value()
        if isinstance(value, values.Attribute):
            return value.get_obj().get_value()
    if ((ret is None) or isinstance(ret, values.NoneValue)):
        if config.show_warnings:
            print('Failed to compile. output is None.')
        return (value_args, None, graph)
    ret_ = []
    if isinstance(ret.get_value(), values.TupleValue):
        if (ret.get_value().internal_value is not None):
            for v in ret.get_value().internal_value:
                assert (v is not None)
                ret_.append(try_get_value(v))
        else:
            ret_ = [ret.get_value()]
    elif isinstance(ret, list):
        ret_ = [r.get_value() for r in ret]
    else:
        ret_ = [ret.get_value()]
    for v in value_args:
        graph.add_input_value(v)
    for v in ret_:
        graph.add_output_value(v)
    return (value_args, ret_, graph)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
     ... . ... [np.argmax]

idx = 3:------------------- similar code ------------------ index = 16, score = 6.0 
def predict_one(self, sample):
    if (not self.trained):
        sys.stderr.write('Model should be trained or loaded before doing predict\n')
        sys.exit((- 1))
    return np.argmax(self.model.predict(np.array([sample])))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return np.argmax

idx = 4:------------------- similar code ------------------ index = 32, score = 6.0 
def get_most_frequent_category(cat_array):
    cats = np.zeros(NUM_CATEGORIES)
    for i in range(NUM_CATEGORIES):
        cats[i] = len(cat_array[(cat_array == i)])
    return np.argmax(cats)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    return np.argmax

idx = 5:------------------- similar code ------------------ index = 2, score = 6.0 
def forward(self, v1):
    return np.argmax(v1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return np.argmax

idx = 6:------------------- similar code ------------------ index = 30, score = 6.0 
def generate_attack_data_set(data, num_sample, img_offset, model, attack_type='targeted', random_target_class=None, shift_index=False):
    '\n    Generate the data for conducting attack. Only select the data being classified correctly.\n    '
    orig_img = []
    orig_labels = []
    target_labels = []
    orig_img_id = []
    pred_labels = np.argmax(model.model.predict(data.test_data), axis=1)
    true_labels = np.argmax(data.test_labels, axis=1)
    correct_data_indices = np.where([(1 if (x == y) else 0) for (x, y) in zip(pred_labels, true_labels)])
    print('Total testing data:{}, correct classified data:{}'.format(len(data.test_labels), len(correct_data_indices[0])))
    data.test_data = data.test_data[correct_data_indices]
    data.test_labels = data.test_labels[correct_data_indices]
    true_labels = true_labels[correct_data_indices]
    np.random.seed(img_offset)
    class_num = data.test_labels.shape[1]
    for sample_index in range(num_sample):
        if (attack_type == 'targeted'):
            if (random_target_class is not None):
                np.random.seed(0)
                seq_imagenet = [1, 4, 6, 8, 9, 13, 15, 16, 19, 20, 22, 24, 25, 27, 28, 30, 34, 35, 36, 37, 38, 44, 49, 51, 56, 59, 60, 61, 62, 63, 67, 68, 70, 71, 74, 75, 76, 77, 78, 79, 82, 84, 85, 87, 88, 91, 94, 96, 97, 99]
                seq = [seq_imagenet[(img_offset + sample_index)]]
                while (seq == true_labels[(img_offset + sample_index)]):
                    seq = np.random.choice(random_target_class, 1)
            else:
                seq = list(range(class_num))
                seq.remove(true_labels[(img_offset + sample_index)])
            for s in seq:
                if (shift_index and (s == 0)):
                    s += 1
                orig_img.append(data.test_data[(img_offset + sample_index)])
                target_labels.append(np.eye(class_num)[s])
                orig_labels.append(data.test_labels[(img_offset + sample_index)])
                orig_img_id.append((img_offset + sample_index))
        elif (attack_type == 'untargeted'):
            orig_img.append(data.test_data[(img_offset + sample_index)])
            target_labels.append(data.test_labels[(img_offset + sample_index)])
            orig_labels.append(data.test_labels[(img_offset + sample_index)])
            orig_img_id.append((img_offset + sample_index))
    orig_img = np.array(orig_img)
    target_labels = np.array(target_labels)
    orig_labels = np.array(orig_labels)
    orig_img_id = np.array(orig_img_id)
    return (orig_img, target_labels, orig_labels, orig_img_id)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 7:------------------- similar code ------------------ index = 19, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 8:------------------- similar code ------------------ index = 25, score = 6.0 
def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum((outputs == labels))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 9:------------------- similar code ------------------ index = 4, score = 5.0 
def visualize(self, name='unnamed', view=True):
    dot = Digraph(name=name)
    node_names = []
    for n_i in range(self.num_nodes):
        ord_ = (np.argmax(self.node_features['order'][n_i]) - 1)
        node_name = f"{self.attrs['value_tokens_list'][n_i]} ord={ord_}, i={n_i}"
        dot.node(node_name)
        node_names.append(node_name)
    for n_i in range(self.num_nodes):
        src_name = node_names[n_i]
        for (a_i, adj_bool) in enumerate(self.adj[n_i]):
            if adj_bool:
                targ_name = node_names[a_i]
                dot.edge(src_name, targ_name)
    os.makedirs('gviz', exist_ok=True)
    dot.render(os.path.join('gviz', f'{name}'), format='pdf', view=view)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ...  = (np.argmax -  ... )

idx = 10:------------------- similar code ------------------ index = 5, score = 5.0 
def log(self):
    'This function is executed after every optimization. So it can be used \n        to interact with the system during training.'
    idx = np.argmax(self.population_returns)
    reward = self.population_returns[(idx, 0)]
    print(f'Reward: {reward}')
    if (reward == 500):
        self.best = self.population_parameters[idx]
        self.terminate()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = np.argmax

idx = 11:------------------- similar code ------------------ index = 3, score = 5.0 
def get_tour_graph(self, points, permutation_matrix):
    '\n        A point is a tuple (x, y), where x is the x-coordinate and y is the y-coordinate of the point.\n        The permutation_matrix is n x n matrix, where n is the number of points in the TSP. Each row represents a point,\n        and each column represents the position of that point in the tour.\n        :param points: the list of points (tuples) which represent the points in the TSP\n        :param permutation_matrix: an n x n matrix, where n is the number of points, which represents the final tour\n        :return: a NetworkX Graph representing the tour, a dictionary defining the NetworkX positions for the Graph, and\n                 the total tour length\n        '
    assert (len(points) == len(permutation_matrix)), 'the number of rows in permutation_matrix does not match the number of points'
    assert (len(points) == len(permutation_matrix[0])), 'the number of columns in permutation_matrix does not match the number of points'
    G = nx.Graph()
    for point_index in range(len(points)):
        G.add_node(point_index)
    tour_index_to_point_index = {}
    for (point_index, row) in enumerate(permutation_matrix):
        tour_index = np.argmax(row)
        if (tour_index in tour_index_to_point_index):
            raise Exception(('a point has already claimed position #%s in the tour' % str((tour_index + 1))))
        tour_index_to_point_index[tour_index] = point_index
    distances = []
    for (tour_index, point_index) in tour_index_to_point_index.items():
        from_point_index = point_index
        to_point_index = tour_index_to_point_index[((tour_index + 1) % len(points))]
        G.add_edge(from_point_index, to_point_index)
        from_point = points[from_point_index]
        to_point = points[to_point_index]
        distances.append(math.sqrt((((to_point[0] - from_point[0]) ** 2) + ((to_point[1] - from_point[1]) ** 2))))
    pos = {i: point for (i, point) in enumerate(points)}
    return (G, pos, sum(distances))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
         ...  = np.argmax

idx = 12:------------------- similar code ------------------ index = 14, score = 5.0 
def locate_probable_sentence(text, out, inter, cmp):
    sentences = split_sentences(text)
    sentences = list(filter((lambda x: (x != '')), sentences))
    dict_of_words = list(map(generate_word_dict, sentences))
    point_arr = ([None] * len(sentences))
    idx = 0
    for d in dict_of_words:
        points = 0
        points += reduce((lambda y, x: ((y + 1) if (x in d) else y)), out.split(' '), 0)
        points += reduce((lambda y, x: ((y + 1) if (x in d) else y)), inter.split(' '), 0)
        points += reduce((lambda y, x: ((y + 1) if (x in d) else y)), inter.split(' '), 0)
        point_arr[idx] = points
        idx += 1
    loc_best = np.argmax(point_arr)
    likely_sentence = sentences[loc_best]
    return (likely_sentence, point_arr)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 13:------------------- similar code ------------------ index = 8, score = 5.0 
if (__name__ == '__main__'):
    n_neurons = 784
    n_timesteps = 4
    initial_conditions = read_image_activities('./mnist-class3.csv')
    initial_weights = read_weights('./mlp_layer1-2_weights.csv')
    timestep_to_weights = {1: read_weights('./mlp_layer2-3_weights.csv'), 2: read_weights('./mlp_layer3-4_weights.csv'), 3: {}}
    timestep_to_activation = {1: relu, 2: relu, 3: identity}
    network = ntm.Network()
    [network.add_edge(i, j) for i in range(n_neurons) for j in range(n_neurons)]
    set_weights(network, initial_weights)

    def activity_rule(ctx):
        V = 0
        for neighbour_label in ctx.neighbour_labels:
            V += (ctx.connection_states[neighbour_label][0]['weight'] * ctx.activities[neighbour_label])
        activity = timestep_to_activation[ctx.timestep](V)
        return activity

    def topology_rule(ctx):
        curr_network = ctx.network
        new_weights = timestep_to_weights[ctx.timestep]
        set_weights(curr_network, new_weights)
        return curr_network
    trajectory = ntm.evolve(network, initial_conditions=initial_conditions, activity_rule=activity_rule, topology_rule=topology_rule, update_order=ntm.UpdateOrder.ACTIVITIES_FIRST, timesteps=n_timesteps)
    vals = [trajectory[(- 1)].activities[i] for i in range(10)]
    plt.title(('predicted class: %s' % np.argmax(np.log(softmax(vals)))))
    plt.imshow(np.array([initial_conditions[i] for i in range(n_neurons)]).reshape((28, 28)), cmap='gray_r')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
     ... . ... (( ...  % np.argmax))

idx = 14:------------------- similar code ------------------ index = 9, score = 5.0 
def _plot_reconstruction(self, updater, fetched):
    inp = fetched['inp']
    output = fetched['output']
    prediction = fetched.get('prediction', None)
    targets = fetched.get('targets', None)
    sqrt_N = int(np.ceil(np.sqrt(self.N)))
    fig_height = 20
    fig_width = (4.5 * fig_height)
    (fig, axes) = plt.subplots(sqrt_N, (3 * sqrt_N), figsize=(fig_width, fig_height))
    for (n, (pred, gt)) in enumerate(zip(output, inp)):
        i = int((n / sqrt_N))
        j = int((n % sqrt_N))
        ax = axes[(i, (3 * j))]
        ax.set_axis_off()
        self.imshow(ax, gt)
        if (targets is not None):
            _target = targets[n]
            _prediction = prediction[n]
            title = 'target={}, prediction={}'.format(np.argmax(_target), np.argmax(_prediction))
            ax.set_title(title)
        ax = axes[(i, ((3 * j) + 1))]
        ax.set_axis_off()
        self.imshow(ax, pred)
        ax = axes[(i, ((3 * j) + 2))]
        ax.set_axis_off()
        diff = (np.abs((gt - pred)).sum(2) / 3)
        self.imshow(ax, diff)
    plt.subplots_adjust(left=0, right=1, top=0.9, bottom=0, wspace=0.1, hspace=0.2)
    self.savefig('sampled_reconstruction', fig, updater)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
        if:
             ...  =  ... . ... (np.argmax,)

idx = 15:------------------- similar code ------------------ index = 10, score = 5.0 
def val(epoch):
    global best_val_loss
    print('Val Epoch: {}'.format(epoch))
    net.eval()
    val_loss = 0
    val_loss_seg = 0
    val_loss_exist = 0
    progressbar = tqdm(range(len(val_loader)))
    with torch.no_grad():
        for (batch_idx, sample) in enumerate(val_loader):
            img = sample['img'].to(device)
            segLabel = sample['segLabel'].to(device)
            exist = sample['exist'].to(device)
            (seg_pred, exist_pred, loss_seg, loss_exist, loss) = net(img, segLabel, exist)
            if isinstance(net, torch.nn.DataParallel):
                loss_seg = loss_seg.sum()
                loss_exist = loss_exist.sum()
                loss = loss.sum()
            gap_num = 5
            if (((batch_idx % gap_num) == 0) and (batch_idx < (50 * gap_num))):
                origin_imgs = []
                seg_pred = seg_pred.detach().cpu().numpy()
                exist_pred = exist_pred.detach().cpu().numpy()
                for b in range(len(img)):
                    img_name = sample['img_name'][b]
                    img = cv2.imread(img_name)
                    img = transform_val_img({'img': img})['img']
                    lane_img = np.zeros_like(img)
                    color = np.array([[255, 125, 0], [0, 255, 0], [0, 0, 255], [0, 255, 255]], dtype='uint8')
                    coord_mask = np.argmax(seg_pred[b], axis=0)
                    for i in range(0, 4):
                        if (exist_pred[(b, i)] > 0.5):
                            lane_img[(coord_mask == (i + 1))] = color[i]
                    img = cv2.addWeighted(src1=lane_img, alpha=0.8, src2=img, beta=1.0, gamma=0.0)
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    lane_img = cv2.cvtColor(lane_img, cv2.COLOR_BGR2RGB)
                    cv2.putText(lane_img, '{}'.format([(1 if (exist_pred[(b, i)] > 0.5) else 0) for i in range(4)]), (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (255, 255, 255), 2)
                    origin_imgs.append(img)
                    origin_imgs.append(lane_img)
                tensorboard.image_summary('img_{}'.format(batch_idx), origin_imgs, epoch)
            val_loss += loss.item()
            val_loss_seg += loss_seg.item()
            val_loss_exist += loss_exist.item()
            progressbar.set_description('batch loss: {:.3f}'.format(loss.item()))
            progressbar.update(1)
    progressbar.close()
    iter_idx = ((epoch + 1) * len(train_loader))
    tensorboard.scalar_summary('val_loss', val_loss, iter_idx)
    tensorboard.scalar_summary('val_loss_seg', val_loss_seg, iter_idx)
    tensorboard.scalar_summary('val_loss_exist', val_loss_exist, iter_idx)
    tensorboard.writer.flush()
    print('------------------------\n')
    if (val_loss < best_val_loss):
        best_val_loss = val_loss
        save_name = os.path.join(exp_dir, (exp_name + '.pth'))
        copy_name = os.path.join(exp_dir, (exp_name + '_best.pth'))
        shutil.copyfile(save_name, copy_name)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    with:
        for in:
            if:
                for  ...  in:
                     ...  = np.argmax

idx = 16:------------------- similar code ------------------ index = 11, score = 5.0 
if (__name__ == '__main__'):
    (epoch_num, batch_size, train_type, train_percent, dev_percent, learn_rate, model_type, device) = parse_args()
    print('\n=====Arguments====')
    print('epoch num {}'.format(epoch_num))
    print('batch size {}'.format(batch_size))
    print('train type {}'.format(train_type))
    print('train percent {}'.format(train_percent))
    print('dev percent {}'.format(dev_percent))
    print('learn rate {}'.format(learn_rate))
    print('model type {}'.format(model_type))
    print('device {}'.format(device))
    print('=====Arguments====\n')
    if ((train_percent + dev_percent) >= 1.0):
        print('ERROR! Train data percentage plus dev data percentage is {}! Make sure the sum is below 1.0!'.format((train_percent + dev_percent)))
        exit(1)
    BERT_VEC_LENGTH = 1024
    (deep_model, optimiser) = build_model(model_type, (BERT_VEC_LENGTH * 2), learn_rate)
    if ('gpu' in device):
        deep_model.to('cuda')
    sorted_scores = read_sorted_scores()
    (train, dev, test, all) = parse_split_data(sorted_scores, train_percent, dev_percent)
    train_pairs = build_pairs(train)
    dev_pairs = build_pairs(dev)
    test_pairs = build_pairs(test)
    print(len(train_pairs), len(dev_pairs), len(test_pairs))
    with open('data/doc_summ_bert_vectors.pkl', 'rb') as ff:
        all_vec_dic = pickle.load(ff)
    pcc_list = []
    weights_list = []
    for ii in range(epoch_num):
        print('\n=====EPOCH {}====='.format(ii))
        loss = pair_train_rewarder(all_vec_dic, train_pairs, deep_model, optimiser, batch_size, device)
        print('--> loss', loss)
        results = test_rewarder(all_vec_dic, dev, deep_model, device)
        for metric in results:
            print('{}\t{}'.format(metric, np.mean(results[metric])))
        pcc_list.append(np.mean(results['pcc']))
        weights_list.append(copy.deepcopy(deep_model.state_dict()))
    idx = np.argmax(pcc_list)
    best_result = pcc_list[idx]
    print('\n======Best results come from epoch no. {}====='.format(idx))
    deep_model.load_state_dict(weights_list[idx])
    test_results = test_rewarder(all_vec_dic, test, deep_model, device)
    print('Its performance on the test set is:')
    for metric in test_results:
        print('{}\t{}'.format(metric, np.mean(test_results[metric])))
    model_weight_name = 'pcc{0:.4f}_'.format(np.mean(test_results['pcc']))
    model_weight_name += 'epoch{}_batch{}_{}_trainPercent{}_lrate{}_{}.model'.format(epoch_num, batch_size, train_type, train_percent, learn_rate, model_type)
    torch.save(weights_list[idx], os.path.join(MODEL_WEIGHT_DIR, model_weight_name))
    print('\nbest model weight saved to: {}'.format(os.path.join(MODEL_WEIGHT_DIR, model_weight_name)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
if:
     ...  = np.argmax

idx = 17:------------------- similar code ------------------ index = 1, score = 5.0 
def compute_gt_annotations(anchors, annotations, negative_overlap=0.4, positive_overlap=0.5):
    ' Obtain indices of gt annotations with the greatest overlap.\n\tArgs\n\t\tanchors: np.array of annotations of shape (N, 4) for (x1, y1, x2, y2).\n\t\tannotations: np.array of shape (N, 5) for (x1, y1, x2, y2, label).\n\t\tnegative_overlap: IoU overlap for negative anchors (all anchors with overlap < negative_overlap are negative).\n\t\tpositive_overlap: IoU overlap or positive anchors (all anchors with overlap > positive_overlap are positive).\n\tReturns\n\t\tpositive_indices: indices of positive anchors\n\t\tignore_indices: indices of ignored anchors\n\t\targmax_overlaps_inds: ordered overlaps indices\n\t'
    overlaps = compute_overlap(anchors.astype(np.float64), annotations.astype(np.float64))
    argmax_overlaps_inds = np.argmax(overlaps, axis=1)
    max_overlaps = overlaps[(np.arange(overlaps.shape[0]), argmax_overlaps_inds)]
    positive_indices = (max_overlaps >= positive_overlap)
    ignore_indices = ((max_overlaps > negative_overlap) & (~ positive_indices))
    return (positive_indices, ignore_indices, argmax_overlaps_inds)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 18:------------------- similar code ------------------ index = 12, score = 5.0 
def evaluate(generator, model, iou_threshold=0.5, score_threshold=0.05, max_detections=100, save_path=None):
    ' Evaluate a given dataset using a given model.\n\t# Arguments\n\t\tgenerator       : The generator that represents the dataset to evaluate.\n\t\tmodel           : The model to evaluate.\n\t\tiou_threshold   : The threshold used to consider when a detection is positive or negative.\n\t\tscore_threshold : The score confidence threshold to use for detections.\n\t\tmax_detections  : The maximum number of detections to use per image.\n\t\tsave_path       : The path to save images with visualized detections to.\n\t# Returns\n\t\tA dict mapping class names to mAP scores.\n\t'
    (all_detections, all_inferences) = _get_detections(generator, model, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path)
    all_annotations = _get_annotations(generator)
    average_precisions = {}
    for label in range(generator.num_classes()):
        if (not generator.has_label(label)):
            continue
        false_positives = np.zeros((0,))
        true_positives = np.zeros((0,))
        scores = np.zeros((0,))
        num_annotations = 0.0
        for i in range(generator.size()):
            detections = all_detections[i][label]
            annotations = all_annotations[i][label]
            num_annotations += annotations.shape[0]
            detected_annotations = []
            for d in detections:
                scores = np.append(scores, d[4])
                if (annotations.shape[0] == 0):
                    false_positives = np.append(false_positives, 1)
                    true_positives = np.append(true_positives, 0)
                    continue
                overlaps = compute_overlap(np.expand_dims(d, axis=0), annotations)
                assigned_annotation = np.argmax(overlaps, axis=1)
                max_overlap = overlaps[(0, assigned_annotation)]
                if ((max_overlap >= iou_threshold) and (assigned_annotation not in detected_annotations)):
                    false_positives = np.append(false_positives, 0)
                    true_positives = np.append(true_positives, 1)
                    detected_annotations.append(assigned_annotation)
                else:
                    false_positives = np.append(false_positives, 1)
                    true_positives = np.append(true_positives, 0)
        if (num_annotations == 0):
            average_precisions[label] = (0, 0)
            continue
        indices = np.argsort((- scores))
        false_positives = false_positives[indices]
        true_positives = true_positives[indices]
        false_positives = np.cumsum(false_positives)
        true_positives = np.cumsum(true_positives)
        recall = (true_positives / num_annotations)
        precision = (true_positives / np.maximum((true_positives + false_positives), np.finfo(np.float64).eps))
        average_precision = _compute_ap(recall, precision)
        average_precisions[label] = (average_precision, num_annotations)
        inference_time = (np.sum(all_inferences) / generator.size())
    print_results(generator, average_precisions, inference_time)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
        for  ...  in:
            for  ...  in  ... :
                 ...  = np.argmax

idx = 19:------------------- similar code ------------------ index = 13, score = 5.0 
def _plot_reconstruction(self, updater, fetched):
    inp = fetched['inp']
    output = fetched['output']
    prediction = fetched.get('prediction', None)
    targets = fetched.get('targets', None)
    (_, image_height, image_width, _) = inp.shape
    obj = fetched['obj'].reshape(self.N, (- 1))
    anchor_box = updater.network.anchor_box
    (yt, xt, ys, xs) = np.split(fetched['normalized_box'], 4, axis=(- 1))
    (yt, xt, ys, xs) = coords_to_pixel_space(yt, xt, ys, xs, (image_height, image_width), anchor_box, top_left=True)
    box = np.concatenate([yt, xt, ys, xs], axis=(- 1))
    box = box.reshape(self.N, (- 1), 4)
    n_annotations = fetched.get('n_annotations', ([0] * self.N))
    annotations = fetched.get('annotations', None)
    actions = fetched.get('actions', None)
    sqrt_N = int(np.ceil(np.sqrt(self.N)))
    on_colour = np.array(to_rgb('xkcd:azure'))
    off_colour = np.array(to_rgb('xkcd:red'))
    cutoff = 0.5
    (fig, axes) = plt.subplots((2 * sqrt_N), (2 * sqrt_N), figsize=(20, 20))
    axes = np.array(axes).reshape((2 * sqrt_N), (2 * sqrt_N))
    for (n, (pred, gt)) in enumerate(zip(output, inp)):
        i = int((n / sqrt_N))
        j = int((n % sqrt_N))
        ax1 = axes[((2 * i), (2 * j))]
        self.imshow(ax1, gt)
        title = ''
        if (prediction is not None):
            title += 'target={}, prediction={}'.format(np.argmax(targets[n]), np.argmax(prediction[n]))
        if (actions is not None):
            title += ', actions={}'.format(actions[(n, 0)])
        ax1.set_title(title)
        ax2 = axes[((2 * i), ((2 * j) + 1))]
        self.imshow(ax2, pred)
        ax3 = axes[(((2 * i) + 1), (2 * j))]
        self.imshow(ax3, pred)
        ax4 = axes[(((2 * i) + 1), ((2 * j) + 1))]
        self.imshow(ax4, pred)
        for (o, (top, left, height, width)) in zip(obj[n], box[n]):
            colour = ((o * on_colour) + ((1 - o) * off_colour))
            rect = patches.Rectangle((left, top), width, height, linewidth=2, edgecolor=colour, facecolor='none')
            ax4.add_patch(rect)
            if (o > cutoff):
                rect = patches.Rectangle((left, top), width, height, linewidth=2, edgecolor=colour, facecolor='none')
                ax3.add_patch(rect)
        for k in range(n_annotations[n]):
            (valid, _, _, top, bottom, left, right) = annotations[n][k]
            if (not valid):
                continue
            height = (bottom - top)
            width = (right - left)
            rect = patches.Rectangle((left, top), width, height, linewidth=1, edgecolor='xkcd:yellow', facecolor='none')
            ax1.add_patch(rect)
            rect = patches.Rectangle((left, top), width, height, linewidth=1, edgecolor='xkcd:yellow', facecolor='none')
            ax3.add_patch(rect)
            rect = patches.Rectangle((left, top), width, height, linewidth=1, edgecolor='xkcd:yellow', facecolor='none')
            ax4.add_patch(rect)
        for ax in axes.flatten():
            ax.set_axis_off()
    if (prediction is None):
        plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0.1, hspace=0.1)
    else:
        plt.subplots_adjust(left=0, right=1, top=0.9, bottom=0, wspace=0.1, hspace=0.2)
    self.savefig('sampled_reconstruction', fig, updater)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
        if:
             ...  +=  ... . ... (np.argmax,)

idx = 20:------------------- similar code ------------------ index = 36, score = 5.0 
def optimize_rounding_params(oofs, y, verbose=True, ix=None):
    ix = (ix if (ix is not None) else np.arange(oofs.shape[0]))
    opt_ds = []
    opt_indices = []
    for idx in range(N_TARGETS):
        scores = [np.nan_to_num(spearmanr(scale(oofs[(ix, idx)], d), y[(ix, idx)])[0]) for d in ds]
        opt_d = ds[np.argmax(scores)]
        if ((np.max(scores) - spearmanr(oofs[(ix, idx)], y[(ix, idx)])[0]) > 0.002):
            if verbose:
                print(idx, opt_d, np.max(scores))
            opt_ds.append(opt_d)
            opt_indices.append(idx)
    return (opt_ds, opt_indices)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ...  =  ... [np.argmax]

idx = 21:------------------- similar code ------------------ index = 18, score = 5.0 
def visualize(img, seg_pred, exist_pred):
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    lane_img = np.zeros_like(img)
    color = np.array([[255, 125, 0], [0, 255, 0], [0, 0, 255], [0, 255, 255]], dtype='uint8')
    coord_mask = np.argmax(seg_pred, axis=0)
    for i in range(0, 4):
        if (exist_pred[(0, i)] > 0.5):
            lane_img[(coord_mask == (i + 1))] = color[i]
    img = cv2.addWeighted(src1=lane_img, alpha=0.8, src2=img, beta=1.0, gamma=0.0)
    return img

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 22:------------------- similar code ------------------ index = 15, score = 5.0 
def to_int_preds(y):
    return [int(np.argmax(y_i)) for y_i in y.cpu()]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    return [ ... (np.argmax)]

idx = 23:------------------- similar code ------------------ index = 17, score = 5.0 
def decode(evidence_identifier: nn.Module, evidence_classifier: nn.Module, unconditioned_evidence_identifier: nn.Module, save_dir: str, data_name: str, data: List[Annotation], model_pars: dict, sep_token_id: int, identifier_transform: Callable[([Annotation], List[Tuple[(torch.IntTensor, Tuple[(torch.IntTensor, torch.IntTensor, torch.IntTensor)], int)]])], classifier_transform: Callable[([Annotation], List[Tuple[(torch.IntTensor, Tuple[(torch.IntTensor, torch.IntTensor, torch.IntTensor)], int)]])], detokenizer=None, conditioned: bool=True):
    bad_ids = set(['2206488'])
    data = list(filter((lambda x: (x.doc.docid not in bad_ids)), data))
    save_dir = os.path.join(save_dir, 'decode', data_name)
    os.makedirs(save_dir, exist_ok=True)
    instances_save_file = os.path.join(save_dir, f'{data_name}_instances_output.pkl')
    identifier_save_file = os.path.join(save_dir, f'{data_name}_identifier_output.pkl')
    classifier_save_file = os.path.join(save_dir, f'{data_name}_classifier_output.pkl')
    oracle_classifier_save_file = os.path.join(save_dir, f'{data_name}_oracle_classifier_output.pkl')
    unconditioned_identifier_file = os.path.join(save_dir, f'{data_name}_unconditioned_identifier_output.pkl')
    logging.info(f'Decoding {len(data)} documents from {data_name}')
    batch_size = model_pars['evidence_identifier']['batch_size']
    evidence_classes = model_pars['evidence_classifier']['classes']
    criterion = nn.CrossEntropyLoss(reduction='none')
    with torch.no_grad():
        if os.path.exists(instances_save_file):
            logging.info(f'Loading instances from {instances_save_file}')
            (instances, oracle_instances) = torch.load(instances_save_file)
        else:
            logging.info(f'Generating and saving instances to {instances_save_file}')
            oracle_instances = oracle_decoding_instances(data)
            instances = decoding_instances(data, identifier_transform, classifier_transform)
            torch.save((instances, oracle_instances), instances_save_file)
        logging.info(f'Have {len(instances)} instances in our data')
        evidence_identifier.eval()
        evidence_classifier.eval()
        if os.path.exists(identifier_save_file):
            logging.info(f'Loading evidence identification predictions on {data_name} from {identifier_save_file}')
            (id_loss, id_soft_pred, id_hard_pred, id_truth) = torch.load(identifier_save_file)
        else:
            logging.info(f'Making evidence identification predictions on {data_name} and saving to {identifier_save_file}')
            decode_target = ('identifier' if conditioned else 'unconditioned_identifier')
            (id_loss, id_soft_pred, id_hard_pred, id_truth) = make_preds_epoch(evidence_identifier, [instance.to_model_input(decode_target) for instance in instances], sep_token_id, batch_size, device=next(evidence_identifier.parameters()).device, criterion=criterion)
            torch.save((id_loss, id_soft_pred, id_hard_pred, id_truth), identifier_save_file)
        if os.path.exists(classifier_save_file):
            logging.info(f'Loading evidence classification predictions on {data_name} from {classifier_save_file}')
            (cls_loss, cls_soft_pred, cls_hard_pred, cls_truth) = torch.load(classifier_save_file)
        else:
            logging.info(f'Making evidence classification predictions on {data_name} and saving to {classifier_save_file}')
            decode_target = ('classifier' if conditioned else 'unconditioned_classifier')
            (cls_loss, cls_soft_pred, cls_hard_pred, cls_truth) = make_preds_epoch(evidence_classifier, [instance.to_model_input(decode_target) for instance in instances], sep_token_id, batch_size, device=next(evidence_classifier.parameters()).device, criterion=criterion)
            torch.save((cls_loss, cls_soft_pred, cls_hard_pred, cls_truth), classifier_save_file)
        if (os.path.exists(unconditioned_identifier_file) and (unconditioned_evidence_identifier is not None)):
            logging.info('Loading unconditioned evidence identification data')
            (docid_to_evidence_snippets, unid_soft_pred, unid_hard_pred) = torch.load(unconditioned_identifier_file)
        elif (unconditioned_evidence_identifier is not None):
            logging.info('Computing unconditioned evidence identification data')
            docid_to_evidence_snippets = locate_known_evidence_snippets(data)
            (_, unid_soft_pred, unid_hard_pred, _) = make_preds_epoch(unconditioned_evidence_identifier, [instance.to_model_input('unconditioned_identifier') for instance in instances], sep_token_id, batch_size, device=next(unconditioned_evidence_identifier.parameters()).device, criterion=None)
            torch.save([docid_to_evidence_snippets, unid_soft_pred, unid_hard_pred], unconditioned_identifier_file)
        else:
            docid_to_evidence_snippets = locate_known_evidence_snippets(data)
            (unid_soft_pred, unid_hard_pred) = (None, None)
        logging.info('Aggregating information for scoring')
        annotations_with_evidence = set()
        annotations_without_evidence = set()
        top1_pipeline_id_truth = dict()
        top1_pipeline_unid_truth = dict()
        top1_pipeline_cls_prediction = dict()
        oracle_pipeline_predictions = defaultdict(list)
        all_id_predictions = dict()
        all_id_truths = dict()
        all_cls_predictions = dict()
        all_unconditional_id_predictions = dict()
        correct_evidence_predictions = dict()
        incorrect_evidence_predictions = dict()
        no_evidence_predictions = dict()
        counterfactual_incorrect_evidence_predictions = dict()
        unconditioned_cls = dict()
        total_length = 0
        for ann in data:
            doc_length = len(ann.tokenized_sentences)
            key = (ann.prompt_id, ann.doc.docid)
            id_predictions = id_soft_pred[total_length:(total_length + doc_length)]
            hard_id_predictions = id_hard_pred[total_length:(total_length + doc_length)]
            id_truths = id_truth[total_length:(total_length + doc_length)]
            has_ev = (sum(id_truths) > 0)
            if has_ev:
                annotations_with_evidence.add(key)
            else:
                annotations_without_evidence.add(key)
            cls_predictions = cls_hard_pred[total_length:(total_length + doc_length)]
            best_id_sent_idx = np.argmax([id_pred[1] for id_pred in id_predictions])
            id_tru = id_truth[(total_length + best_id_sent_idx)]
            cls_tru = cls_truth[(total_length + best_id_sent_idx)]
            cls_pre = cls_hard_pred[(total_length + best_id_sent_idx)]
            top1_pipeline_id_truth[key] = id_tru
            top1_pipeline_cls_prediction[key] = (cls_tru, cls_pre)
            if (id_tru == 1):
                correct_evidence_predictions[key] = (cls_tru, cls_pre)
            elif has_ev:
                incorrect_evidence_predictions[key] = (cls_tru, cls_pre)
            else:
                no_evidence_predictions[key] = (cls_tru, cls_pre)
            assert (len(id_truths) == len(hard_id_predictions))
            for (id_sent_truth, cls_pred) in zip(id_truths, cls_predictions):
                if (id_sent_truth == 1):
                    oracle_pipeline_predictions[key].append((ann.significance_class, cls_pred))
            all_id_predictions[key] = [id_pred[1] for id_pred in id_predictions]
            all_id_truths[key] = id_truths
            all_cls_predictions[key] = cls_predictions
            if unid_soft_pred:
                unid_soft_predictions = unid_soft_pred[total_length:(total_length + doc_length)]
                all_unconditional_id_predictions[key] = [id_pred[1] for id_pred in unid_soft_predictions]
                best_unid_sent_idx = np.argmax(all_unconditional_id_predictions[key])
                top1_pipeline_unid_truth[key] = id_truth[(total_length + best_unid_sent_idx)]
                unconditioned_cls[key] = (cls_truth[(total_length + best_unid_sent_idx)], np.argmax(cls_predictions[best_unid_sent_idx]))
            total_length += len(ann.tokenized_sentences)
        assert (total_length == len(cls_hard_pred))
        assert (total_length == len(id_hard_pred))
        logging.info(f'Of {len(data)} annotations, {len(annotations_with_evidence)} have evidence spans, {len(annotations_without_evidence)} do not')

        def id_scores(id_tru, id_soft_preds, top1_values, top1_cls_preds=None, preds_dict=None, preds_truth=None):
            id_auc = roc_auc_score(id_tru, id_soft_preds)
            id_top1_acc = accuracy_score(([1] * len(top1_values)), list(top1_values))
            id_all_acc = accuracy_score(id_tru, [round(x) for x in id_soft_preds])
            if (preds_dict is not None):
                assert (preds_truth is not None)
                mrr = 0
                query_count = 0
                for k in preds_truth.keys():
                    pt = zip(preds_dict[k], preds_truth[k])
                    pt = sorted(pt, key=(lambda x: x[0]), reverse=True)
                    for (pos, (_, t)) in enumerate(pt):
                        if (t == 1):
                            mrr += (1 / (1 + pos))
                            query_count += 1
                            break
                mrr = (mrr / query_count)
            else:
                mrr = None
            logging.info(f'identification auc {id_auc}, top1 acc: {id_top1_acc}, everything acc: {id_all_acc}, mrr: {mrr}')
            if (top1_cls_preds is not None):
                assert (preds_dict is not None)
                mistakes = defaultdict((lambda : defaultdict((lambda : 0))))
                for (eyeD, (cls_tru, _)) in top1_cls_preds.items():
                    pt = zip(preds_dict[eyeD], preds_truth[eyeD])
                    pt = sorted(pt, key=(lambda x: x[0]), reverse=True)
                    if (pt[0][1] == 1):
                        mistakes[cls_tru]['tp'] += 1
                    else:
                        mistakes[cls_tru]['fp'] += 1
                mistakes_str = []
                for (tru, preds) in mistakes.items():
                    (tp, fp) = (preds['tp'], preds['fp'])
                    frac = (tp / (tp + fp))
                    mistakes_str.append(f'cls {tru} evid accuracy {frac}')
                mistakes_str = '  '.join(mistakes_str)
                logging.info(f'Evidence ID accuracy breakdown by classification types {mistakes_str}')
        ev_only_id_soft_pred = list(itertools.chain.from_iterable((all_id_predictions[x] for x in annotations_with_evidence)))
        ev_only_id_truth = list(itertools.chain.from_iterable((all_id_truths[x] for x in annotations_with_evidence)))
        id_scores(ev_only_id_truth, ev_only_id_soft_pred, [top1_pipeline_id_truth[x] for x in annotations_with_evidence], top1_cls_preds=top1_pipeline_cls_prediction, preds_dict=all_id_predictions, preds_truth=all_id_truths)
        (pipeline_truth, pipeline_pred) = zip(*top1_pipeline_cls_prediction.values())
        e2e_score(pipeline_truth, pipeline_pred, 'Pipeline', evidence_classes)
        (oracle_truth, oracle_pred) = zip(*[random.choice(x) for x in oracle_pipeline_predictions.values()])
        e2e_score(oracle_truth, oracle_pred, 'Oracle (one)', evidence_classes)
        (oracle_all_truth, oracle_all_pred) = zip(*itertools.chain.from_iterable(oracle_pipeline_predictions.values()))
        e2e_score(oracle_all_truth, oracle_all_pred, 'Oracle (all)', evidence_classes)
        (correct_ev_truths, correct_ev_preds) = zip(*correct_evidence_predictions.values())
        e2e_score(correct_ev_truths, correct_ev_preds, 'Correct evidence only', evidence_classes)
        (incorrect_ev_truths, incorrect_ev_preds) = zip(*incorrect_evidence_predictions.values())
        e2e_score(incorrect_ev_truths, incorrect_ev_preds, 'Incorrect evidence only', evidence_classes)
        (counterfactual_ev_truths, counterfactual_ev_preds) = zip(*[random.choice(oracle_pipeline_predictions[key]) for key in set(incorrect_evidence_predictions.keys())])
        e2e_score(counterfactual_ev_truths, counterfactual_ev_preds, 'Counterfactual evidence only', evidence_classes)
        augmented_counterfactual_ev_truths = (counterfactual_ev_truths + correct_ev_truths)
        augmented_counterfactual_ev_preds = (counterfactual_ev_preds + correct_ev_preds)
        e2e_score(augmented_counterfactual_ev_truths, augmented_counterfactual_ev_preds, 'Augmented counterfactual evidence only', evidence_classes)
        if (unconditioned_evidence_identifier is not None):
            logging.info('Unconditional identifier scores (note the classifier is conditional)')
            ev_only_unid_soft_pred = list(itertools.chain.from_iterable((all_unconditional_id_predictions[x] for x in annotations_with_evidence)))
            id_scores(ev_only_id_truth, ev_only_unid_soft_pred, [top1_pipeline_unid_truth[x] for x in annotations_with_evidence], top1_cls_preds=top1_pipeline_cls_prediction, preds_dict=all_unconditional_id_predictions, preds_truth=all_id_truths)
            (unid_cls_truth, unid_cls_pred) = zip(*unconditioned_cls.values())
            e2e_score(unid_cls_truth, unid_cls_pred, 'Unconditioned identifier (how important is the ICO to finding the evidence)', evidence_classes)

        def detok(sent):
            return ' '.join((detokenizer[(x.item() if isinstance(x, torch.Tensor) else x)] for x in sent))

        def view(id_predictions, unid_soft_predictions, id_truths, cls_predictions, cls_truths, ann, best, p=False):
            out = []
            out.append(f'ico: {detok(ann.i)} vs. {detok(ann.c)} for {detok(ann.o)}')
            if (unid_soft_predictions is None):
                unid_soft_predictions = [torch.tensor([0.0, 0.0]) for _ in id_predictions]
            for (idx, (idp, uidp, idt, clsp, clst, sent)) in enumerate(zip(id_predictions, unid_soft_predictions, id_truths, cls_predictions, cls_truths, ann.tokenized_sentences)):
                best_marker = ('*' if (idx == best) else '')
                out.append(detok(sent))
                out.append(f'  id pred{best_marker}: {idp[1].item():.3f}, unid id pred: {uidp[1].item():.3f}, id_truth: {idt}')
                out.append(f'  cls pred: {clsp}, cls truth: {clst}')
            if p:
                print('\n'.join(out))
            return '\n'.join(out)
        total_length = 0
        debug_dir = os.path.join(save_dir, 'debug')
        incorrect_debug_dir = os.path.join(save_dir, 'debug_incorrect')
        os.makedirs(debug_dir, exist_ok=True)
        os.makedirs(incorrect_debug_dir, exist_ok=True)
        for ann in data:
            key = (ann.prompt_id, ann.doc.docid)
            id_predictions = id_soft_pred[total_length:(total_length + len(ann.tokenized_sentences))]
            hard_id_predictions = id_hard_pred[total_length:(total_length + len(ann.tokenized_sentences))]
            if (unid_soft_pred is not None):
                unid_soft_predictions = unid_soft_pred[total_length:(total_length + len(ann.tokenized_sentences))]
            else:
                unid_soft_predictions = None
            id_truths = id_truth[total_length:(total_length + len(ann.tokenized_sentences))]
            cls_predictions = cls_hard_pred[total_length:(total_length + len(ann.tokenized_sentences))]
            best_id_sent_idx = np.argmax([id_pred[1] for id_pred in id_predictions])
            id_tru = id_truth[(total_length + best_id_sent_idx)]
            cls_tru = cls_truth[(total_length + best_id_sent_idx)]
            cls_trus = cls_truth[total_length:(total_length + len(ann.tokenized_sentences))]
            cls_pre = cls_hard_pred[(total_length + best_id_sent_idx)]
            pretty = view(id_predictions, unid_soft_predictions, id_truths, cls_predictions, cls_trus, ann, best_id_sent_idx, p=False)
            with open(os.path.join(debug_dir, (((str(ann.doc.docid) + '_') + str(ann.prompt_id)) + '.txt')), 'w') as of:
                of.write(pretty)
            if (id_tru == 0):
                with open(os.path.join(incorrect_debug_dir, (((str(ann.doc.docid) + '_') + str(ann.prompt_id)) + '.txt')), 'w') as of:
                    of.write(pretty)
            total_length += len(ann.tokenized_sentences)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
        for  ...  in  ... :
             ...  = np.argmax

idx = 24:------------------- similar code ------------------ index = 35, score = 5.0 
def _update_model(self, X_all, Y_all_raw, itr=0):
    '\n        :param X_all: observed input data\n        :param Y_all_raw: observed output raw data\n        :param itr: BO iteration counter\n        '
    if self.normalize_Y:
        Y_all = ((Y_all_raw - Y_all_raw.mean()) / Y_all_raw.std())
    else:
        Y_all = Y_all_raw
    if self.sparse.startswith('SUB'):
        (X_all, Y_all) = subset_select(X_all, Y_all, select_metric=self.sparse)
    if (self.model is None):
        self.input_dim = X_all.shape[1]
        self.input_dim_opt_ex = list(range(self.input_dim))
    else:
        self.model.set_XY(X_all, Y_all)
    if ((itr % int((self.update_interval * 8))) == 0):
        input_dim_permutate_list = [np.random.permutation(range(self.input_dim)) for i in range(self.n_decomp)]
        input_dim_permutate_list.append(self.input_dim_opt_ex)
        if ('ADD' in self.sparse):
            print('learn the decomposition with subset observed data')
            (X_ob, Y_ob) = subset_select_for_learning(X_all, Y_all, select_metric=self.sparse)
        else:
            (X_ob, Y_ob) = (X_all, Y_all)
        ll_list = []
        submodel_list = []
        for (i, input_dim_i) in enumerate(input_dim_permutate_list):
            (sub_model_i, ll_i) = self._create_model_sub(X_ob, Y_ob, input_dim_i)
            print(f'll for decom {i} ={ll_i}')
            ll_list.append(ll_i)
            submodel_list.append(sub_model_i)
        mlh_idx = np.argmax(ll_list)
        self.model = submodel_list[mlh_idx]
        self.model.set_XY(X_all, Y_all)
        input_dim_opt = input_dim_permutate_list[mlh_idx]
        self.active_dims_list = split(input_dim_opt, self.n_sub)
        self.model_kern_list = [self.model.kern.__dict__[f'k{k_indx}'] for k_indx in range(self.n_sub)]
        self.input_dim_opt_ex = input_dim_opt.copy()
        print(f'opt_decom={mlh_idx}')
    if ((itr % self.update_interval) == 0):
        self.model.optimize_restarts(num_restarts=self.optimize_restarts, optimizer=self.optimizer, max_iters=self.max_iters, verbose=self.verbose)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.argmax

idx = 25:------------------- similar code ------------------ index = 20, score = 5.0 
def forward(self, x, t):
    h = self.bn1(self.conv1(x))
    h = F.max_pooling_2d(F.relu(h), 3, stride=2)
    h = self.res2(h)
    h = self.res3(h)
    h = self.res4(h)
    h = self.res5(h)
    h = F.average_pooling_2d(h, 7, stride=1)
    h = self.fc(h)
    loss = self.softmax_cross_entropy(h, t)
    if self.compute_accuracy:
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, np.argmax(t, axis=1))}, self)
    else:
        chainer.report({'loss': loss}, self)
    return loss

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ... . ... ({ ... :  ... ,  ... :  ... . ... ( ... , np.argmax)},  ... )

idx = 26:------------------- similar code ------------------ index = 21, score = 5.0 
def accuracy(output, labels):
    corr_output = np.argmax(output, axis=1)
    acc = (np.sum((corr_output == labels)) / float(labels.size))
    return acc

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 27:------------------- similar code ------------------ index = 22, score = 5.0 
def ErrorRateAt95Recall(labels, scores):
    distances = (1.0 / (scores + 1e-08))
    recall_point = 0.95
    labels = labels[np.argsort(distances)]
    threshold_index = np.argmax((np.cumsum(labels) >= (recall_point * np.sum(labels))))
    FP = np.sum((labels[:threshold_index] == 0))
    TN = np.sum((labels[threshold_index:] == 0))
    return (float(FP) / float((FP + TN)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 28:------------------- similar code ------------------ index = 23, score = 5.0 
@njit
def _compute_valid_splitting_indices(t, min_leaf):
    'Compute valid split indices for treatment array *t* given *min_leaf*.\n\n    Given an array *t* of treatment status and an integer *min_leaf* --denoting\n    the minimum number of allowed observations of each type in a leaf node--\n    computes a sequence of indices on which we can split *t* and get that each\n    resulting side contains a minimum of *min_leaf* treated and untreated\n    observations. Returns an empty sequence if no split is possible.\n\n    Args:\n        t (np.array): 1d array containing the treatment status as treated =\n            True and untreated = False.\n        min_leaf (int): Minimum number of observations of each type (treated,\n            untreated) allowed in a leaf; has to be greater than 1.\n\n    Returns:\n        out (np.array): a sequence of indices representing valid splitting\n            points.\n\n    '
    out = np.arange(0)
    n = len(t)
    if (n < (2 * min_leaf)):
        return out
    left_index_treated = np.argmax((np.cumsum(t) == min_leaf))
    if (left_index_treated == 0):
        return out
    left_index_untreated = np.argmax((np.cumsum((~ t)) == min_leaf))
    if (left_index_untreated == 0):
        return out
    tmparray = np.array([left_index_treated, left_index_untreated])
    left = np.max(tmparray)
    right_index_treated = np.argmax((np.cumsum(t[::(- 1)]) == min_leaf))
    if (right_index_treated == 0):
        return out
    right_index_untreated = np.argmax((np.cumsum((~ t[::(- 1)])) == min_leaf))
    if (right_index_untreated == 0):
        return out
    tmparray = np.array([right_index_treated, right_index_untreated])
    right = (n - np.max(tmparray))
    if (left > (right - 1)):
        return out
    else:
        out = np.arange(left, (right - 1))
        return out

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 29:------------------- similar code ------------------ index = 26, score = 5.0 
def model_prediction(model, inputs):
    prob = model.model.predict(inputs)
    predicted_class = np.argmax(prob)
    prob_str = np.array2string(prob).replace('\n', '')
    return (prob, predicted_class, prob_str)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = np.argmax

idx = 30:------------------- similar code ------------------ index = 27, score = 5.0 
def add(self, predicted, target):
    '\n        Computes the confusion matrix of K x K size where K is no of classes\n\n        Paramaters:\n            predicted (tensor): Can be an N x K tensor of predicted scores obtained from\n                the model for N examples and K classes or an N-tensor of\n                integer values between 0 and K-1.\n            target (tensor): Can be a N-tensor of integer values assumed to be integer\n                values between 0 and K-1 or N x K tensor, where targets are\n                assumed to be provided as one-hot vectors\n        '
    predicted = predicted.cpu().numpy()
    target = target.cpu().numpy()
    assert (predicted.shape[0] == target.shape[0]), 'number of targets and predicted outputs do not match'
    if (np.ndim(predicted) != 1):
        assert (predicted.shape[1] == self.k), 'number of predictions does not match size of confusion matrix'
        predicted = np.argmax(predicted, 1)
    else:
        assert ((predicted.max() < self.k) and (predicted.min() >= 0)), 'predicted values are not between 1 and k'
    onehot_target = (np.ndim(target) != 1)
    if onehot_target:
        assert (target.shape[1] == self.k), 'Onehot target does not match size of confusion matrix'
        assert ((target >= 0).all() and (target <= 1).all()), 'in one-hot encoding, target values should be 0 or 1'
        assert (target.sum(1) == 1).all(), 'multi-label setting is not supported'
        target = np.argmax(target, 1)
    else:
        assert ((predicted.max() < self.k) and (predicted.min() >= 0)), 'predicted values are not between 0 and k-1'
    x = (predicted + (self.k * target))
    bincount_2d = np.bincount(x.astype(np.int32), minlength=(self.k ** 2))
    assert (bincount_2d.size == (self.k ** 2))
    conf = bincount_2d.reshape((self.k, self.k))
    self.conf += conf

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.argmax

idx = 31:------------------- similar code ------------------ index = 28, score = 5.0 
def get_freer_gpu():
    '\n    Find which gpu is free\n    '
    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')
    memory_available = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]
    return int(np.argmax(memory_available))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    return  ... (np.argmax)

idx = 32:------------------- similar code ------------------ index = 29, score = 5.0 
def _update_model(self, X_all, Y_all_raw, itr=0):
    '\n        :param X_all: observed input data\n        :param Y_all_raw: observed output raw data\n        :param itr: BO iteration counter\n        '
    if self.normalize_Y:
        Y_all = ((Y_all_raw - Y_all_raw.mean()) / Y_all_raw.std())
        self.Y_mean = Y_all_raw.mean()
        self.Y_std = Y_all_raw.std()
    else:
        Y_all = Y_all_raw
    if self.sparse.startswith('SUB'):
        (X_all, Y_all) = subset_select(X_all, Y_all, select_metric=self.sparse)
    if ((itr % int((8 * self.update_interval))) == 0):
        if ('ADD' in self.sparse):
            print('learn the optimal dr with subset observed data')
            (X_ob, Y_ob) = subset_select_for_learning(X_all, Y_all, select_metric=self.sparse)
        else:
            (X_ob, Y_ob) = (X_all, Y_all)
        ll_list = []
        model_list = []
        for dr in self.dr_list:
            X_all_d_r = downsample_projection(self.dim_reduction, X_ob, int((dr ** 2)), self.high_dim, nchannel=self.nchannel, align_corners=True)
            model = self._create_model(X_all_d_r, Y_ob)
            ll_dr = model.log_likelihood()
            print(f'dr={dr}, ll={ll_dr}')
            ll_list.append(ll_dr)
            model_list.append(model)
        mle_idx = np.argmax(ll_list)
        self.opt_dr = int(self.dr_list[mle_idx])
        self.model = model_list[mle_idx]
        print(f'opt_dr={self.opt_dr}')
    else:
        X_all_d_r = downsample_projection(self.dim_reduction, X_all, int((self.opt_dr ** 2)), self.high_dim, nchannel=self.nchannel, align_corners=True)
        self.model.set_XY(X_all_d_r, Y_all)
    if ((itr % self.update_interval) == 0):
        self.model.optimize_restarts(num_restarts=self.optimize_restarts, optimizer=self.optimizer, max_iters=self.max_iters, verbose=self.verbose)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.argmax

idx = 33:------------------- similar code ------------------ index = 31, score = 5.0 
def add(self, predicted, target):
    'Computes the confusion matrix\n\n        The shape of the confusion matrix is K x K, where K is the number\n        of classes.\n\n        Keyword arguments:\n        - predicted (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n        predicted scores obtained from the model for N examples and K classes,\n        or an N-tensor/array of integer values between 0 and K-1.\n        - target (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n        ground-truth classes for N examples and K classes, or an N-tensor/array\n        of integer values between 0 and K-1.\n\n        '
    (_, predicted) = predicted.max(1)
    predicted = predicted.view((- 1))
    target = target.view((- 1))
    if torch.is_tensor(predicted):
        predicted = predicted.cpu().numpy()
    if torch.is_tensor(target):
        target = target.cpu().numpy()
    assert (predicted.shape[0] == target.shape[0]), 'number of targets and predicted outputs do not match'
    if (np.ndim(predicted) != 1):
        assert (predicted.shape[1] == self.num_classes), 'number of predictions does not match size of confusion matrix'
        predicted = np.argmax(predicted, 1)
    else:
        assert ((predicted.max() < self.num_classes) and (predicted.min() >= 0)), 'predicted values are not between 0 and k-1'
    if (np.ndim(target) != 1):
        assert (target.shape[1] == self.num_classes), 'Onehot target does not match size of confusion matrix'
        assert ((target >= 0).all() and (target <= 1).all()), 'in one-hot encoding, target values should be 0 or 1'
        assert (target.sum(1) == 1).all(), 'multi-label setting is not supported'
        target = np.argmax(target, 1)
    else:
        assert ((target.max() < self.num_classes) and (target.min() >= 0)), 'target values are not between 0 and k-1'
    x = (predicted + (self.num_classes * target))
    bincount_2d = np.bincount(x.astype(np.int32), minlength=(self.num_classes ** 2))
    assert (bincount_2d.size == (self.num_classes ** 2))
    conf = bincount_2d.reshape((self.num_classes, self.num_classes))
    self.conf += conf

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = np.argmax

idx = 34:------------------- similar code ------------------ index = 33, score = 5.0 
def evaluation(model, dataset, device, save_mask=True, plot_roc=True, print_metric=True):
    '\n    Function to perform an evaluation of a trained model. We compute different metrics show in the dictionary\n    to_plot_metrics and plot the ROC over different thresholds.\n    :param model: a trained model\n    :param dataset: dataset of images\n    :param device: GPU or CPU. Used to transfer the dataset to the right device.\n    :param save_mask: Boolean to call or not saveMask to plot the mask predicted by the model\n    :param plot_roc: Boolean to plot and save the ROC computer over the different thresholds\n    :param print_metric: Boolean to plot or not the different metrics computed over the thresholds\n    :return: the dictionary containing the metrics\n    '
    model.eval()
    loss = 0
    last_masks = ([None] * len(dataset))
    last_truths = ([None] * len(dataset))
    thesholds = [0, 1e-07, 1e-06, 5e-06, 1e-05, 2.5e-05, 5e-05, 0.0001, 0.00025, 0.0005, 0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.2, 0.4, 0.6, 0.8, 1]
    n_thesholds = len(thesholds)
    to_plot_metrics = dict([('F1', np.zeros(n_thesholds)), ('Recall', np.zeros(n_thesholds)), ('Precision', np.zeros(n_thesholds)), ('TP', np.zeros(n_thesholds)), ('TN', np.zeros(n_thesholds)), ('FP', np.zeros(n_thesholds)), ('FN', np.zeros(n_thesholds)), ('AUC', 0), ('TPR', np.zeros(n_thesholds)), ('FPR', np.zeros(n_thesholds))])
    with tqdm(desc=f'Validation', unit='img') as progress_bar:
        for (i, (image, ground_truth)) in enumerate(dataset):
            image = image[(0, ...)]
            ground_truth = ground_truth[(0, ...)]
            last_truths[i] = ground_truth
            image = image.to(device)
            ground_truth = ground_truth.to(device)
            with torch.no_grad():
                mask_predicted = model(image)
            last_masks[i] = mask_predicted
            progress_bar.set_postfix(**{'loss': loss})
            bce_weight = torch.Tensor([1, 8]).to(device)
            loss += compute_loss(mask_predicted, ground_truth, bce_weight=bce_weight)
            get_metrics(mask_predicted[(0, 0)], ground_truth[0], to_plot_metrics, thesholds)
            progress_bar.update()
    if save_mask:
        save_masks(last_masks, last_truths, str(device), max_img=50, shuffle=False, color='red', filename='mask_predicted_test.png', threshold=thesholds[np.argmax(to_plot_metrics['F1'])])
    if print_metric:
        print_metrics(to_plot_metrics, len(dataset), 'test set')
    nb_images = len(dataset)
    for (k, v) in to_plot_metrics.items():
        to_plot_metrics[k] = (v / nb_images)
    if plot_roc:
        plt.title('Receiver Operating Characteristic')
        plt.plot(to_plot_metrics['FPR'], to_plot_metrics['TPR'], 'b', label=('AUC = %0.2f' % to_plot_metrics['AUC']))
        plt.legend(loc='lower right')
        plt.plot([0, 1], [0, 1], 'r--')
        plt.xlim([0, 1])
        plt.ylim([0, 1])
        plt.ylabel('True Positive Rate')
        plt.xlabel('False Positive Rate')
        plt.savefig('ROC.png')
        plt.show()
        plt.close('ROC.png')
    loss /= len(dataset)
    to_plot_metrics['loss'] = loss
    to_plot_metrics['best_threshold'] = thesholds[np.argmax(to_plot_metrics['F1'])]
    return to_plot_metrics

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if  ... :
         ... ( ... ,  ... ,,,,,,  ... = ... [np.argmax])

idx = 35:------------------- similar code ------------------ index = 34, score = 5.0 
def tta_real_test(nets, all=False, labels=land_classes, norm=False, test_set=None, stride=600, batch_size=5, window_size=(512, 512)):
    test_files = loadtestimg(test_set)
    idlist = loadids(test_set)
    all_preds = []
    num_class = len(labels)
    ids = []
    total_ids = 0
    for k in test_set[IDS].keys():
        total_ids += len(test_set[IDS][k])
    for (img, id) in tqdm(zip(test_files, idlist), total=total_ids, leave=False):
        img = np.asarray(img, dtype='float32')
        img = st.ToTensor()(img)
        img = (img / 255.0)
        if norm:
            img = st.Normalize(*mean_std)(img)
        img = img.cpu().numpy().transpose((1, 2, 0))
        stime = time.time()
        with torch.no_grad():
            pred = fusion_prediction(nets, image=img, scales=[1.0], batch_size=batch_size, num_class=num_class, wsize=window_size)
        print('inference cost time: ', (time.time() - stime))
        pred = np.argmax(pred, axis=(- 1))
        for key in ['boundaries', 'masks']:
            pred = (pred * np.array((cv2.imread(os.path.join('/media/liu/diskb/data/Agriculture-Vision/test', key, (id + '.png')), (- 1)) / 255), dtype=int))
        filename = './{}.png'.format(id)
        cv2.imwrite(os.path.join(output_path, filename), pred)
        all_preds.append(pred)
        ids.append(id)
    if all:
        return (all_preds, ids)
    else:
        return all_preds

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
         ...  = np.argmax

idx = 36:------------------- similar code ------------------ index = 0, score = 5.0 
def __init__(self, dataset_name, num_img=1, img_offset=0, epsilon=0.05, rescale=True, dim_reduction=None, low_dim=None, high_dim=784, obj_metric=2, results_folder=None, directory=None):
    '\n        CNN Classifier on MNIST, CIFAR10 and ImageNet\n\n        :param dataset_name: image dataset name\n        :param num_img: number of images to be attacked (default=1)\n        :param img_offset: the image id e.g. img_offset=4 means 4th image in the correctly classified test set\n        :param epsilon: maximum perturbation\n        :param rescale: rescale the adversarial image to the range of the original image\n        :param dim_reduction: dimension reduction method used in upsampling\n        :param low_dim: reduced dimension (drxdr)\n        :param high_dim: image dimension (e.g. 32x32 for CIFAR10) or high-dimensional search space for imagenet (96x96)\n        :param obj_metric: Metric used to compute objective function (default = 2)\n        :param results_folder: results saving folder directory\n        :param directory: BayesOpt Attack code directory\n        '
    self.epsilon = epsilon
    self.dataset_name = dataset_name
    self.dim_reduction = dim_reduction
    self.num_img = num_img
    self.low_dim = low_dim
    self.high_dim = high_dim
    self.objective_metric = obj_metric
    self.results_folder = results_folder
    self.rescale = rescale
    folder_path = os.path.join(directory, 'objective_func/tf_models/')
    if ('mnist' in dataset_name):
        self.d1 = 28
        self.nchannel = 1
        self.dataset_name = 'mnist'
        self.total_classes = 10
        (data, model) = (MNIST(folder_path), MNISTModel(f'{folder_path}models/mnist', use_softmax=True))
    elif ('cifar10' in dataset_name):
        self.d1 = 32
        self.nchannel = 3
        self.total_classes = 10
        self.dataset_name = 'cifar10'
        (data, model) = (CIFAR(folder_path), CIFARModel(f'{folder_path}models/cifar', use_softmax=True))
    elif ('imagenet' in dataset_name):
        from objective_func.tf_models.setup_inception import InceptionModel, ImageNetDataNP
        self.d1 = 299
        self.nchannel = 3
        self.total_classes = 1001
        self.dataset_name = 'imagenet'
        (data, model) = (ImageNetDataNP(folder_path), InceptionModel(folder_path, use_softmax=True))
    random_target = False
    shift_index = False
    attack_type = 'targeted'
    print(f'Loading data and classification model: {self.dataset_name}')
    if ('imagenet' in dataset_name):
        all_class = np.array(range(int((self.total_classes - 1))))
        (all_orig_img, all_target_labels, all_orig_labels, all_orig_img_id) = util.generate_attack_data_set(data, num_img, img_offset, model, attack_type=attack_type, random_target_class=all_class, shift_index=True)
    elif random_target:
        class_num = data.test_labels.shape[1]
        (all_orig_img, all_target_labels, all_orig_labels, all_orig_img_id) = util.generate_attack_data_set(data, num_img, img_offset, model, attack_type=attack_type, random_target_class=list(range(class_num)), shift_index=shift_index)
    else:
        (all_orig_img, all_target_labels, all_orig_labels, all_orig_img_id) = util.generate_attack_data_set(data, num_img, img_offset, model, attack_type=attack_type, shift_index=shift_index)
    self.blackbox_model = model.model
    self.all_orig_img = all_orig_img
    self.all_target_labels = all_target_labels
    self.all_orig_img_id = all_orig_img_id
    self.all_orig_labels_int = np.argmax(all_orig_labels, 1)
    if ((dataset_name == 'mnist') or (dataset_name == 'cifar10')):
        if (self.all_orig_labels_int[0] != self.all_orig_labels_int[(- 1)]):
            assert False

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = np.argmax

