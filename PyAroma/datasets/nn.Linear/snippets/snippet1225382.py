from .weight_norm import WeightNorm
from .reparameterization import Reparameterization


def apply_weight_norm(module, name='', dim=0, hook_child=True):
    '\n    Applies weight normalization to a parameter in the given module.\n    If no parameter is provided, applies weight normalization to all\n    parameters in model (except 1-d vectors and scalars).\n\n    .. math::\n         \\mathbf{w} = g \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\n\n    Weight normalization is a reparameterization that decouples the magnitude\n    of a weight tensor from its direction. This replaces the parameter specified\n    by `name` (e.g. "weight") with two parameters: one specifying the magnitude\n    (e.g. "weight_g") and one specifying the direction (e.g. "weight_v").\n    Weight normalization is implemented via a hook that recomputes the weight\n    tensor from the magnitude and direction before every :meth:`~Module.forward`\n    call.\n\n    By default, with `dim=0`, the norm is computed independently per output\n    channel/plane. To compute a norm over the entire weight tensor, use\n    `dim=None`.\n\n    See https://arxiv.org/abs/1602.07868\n\n    Args:\n        module (nn.Module): containing module\n        name (str, optional): name of weight parameter\n        dim (int, optional): dimension over which to compute the norm\n        hook_child (boolean, optional): adds reparameterization hook to direct parent of the \n            parameters. If False, it\'s added to `module` instead. Default: True\n\n    Returns:\n        The original module with the weight norm hook\n\n    Example::\n\n        >>> m = apply_weight_norm(nn.Linear(20, 40), name=\'weight\')\n        Linear (20 -> 40)\n        >>> m.weight_g.size()\n        torch.Size([40, 1])\n        >>> m.weight_v.size()\n        torch.Size([40, 20])\n\n    '
    return apply_reparameterization(module, reparameterization=WeightNorm, hook_child=hook_child, name=name, dim=dim)
