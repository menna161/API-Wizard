------------------------- example 1 ------------------------ 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)

------------------------- example 2 ------------------------ 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)
        elif isinstance(m, nn.BatchNorm1d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------------- example 3 ------------------------ 
def initialize(self):
    for module in self.modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            nn.init.xavier_uniform_(module.weight)
            nn.init.zeros_(module.bias)
    nn.init.xavier_uniform_(self.block2[(- 1)].weight, gain=1e-05)

------------------------- example 4 ------------------------ 
def _init_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear)):
            mynn.init.XavierFill(m.weight)
            if (m.bias is not None):
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)

------------------------- example 5 ------------------------ 
def weights_init_(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight, gain=1)
        torch.nn.init.constant_(m.bias, 0)

examples  ||  representativeness  ||  number of lines  || number of comments   ||  relevancy  
example1  ||          14           ||        5         ||         0        ||        0.2         
example2  ||          7           ||        8         ||         0        ||        0.125         
example3  ||          2           ||        6         ||         0        ||        0.16666666666666666         
example4  ||          6           ||        9         ||         0        ||        0.2222222222222222         
example5  ||          6           ||        4         ||         0        ||        0.25         

avg       ||          0.8177570093457943           ||        6.4         ||         0.0        ||         19.27777777777778        

idx = 0:------------------- similar code ------------------ index = 202, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 1:------------------- similar code ------------------ index = 479, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)
        elif isinstance(m, nn.BatchNorm1d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 2:------------------- similar code ------------------ index = 322, score = 7.0 
def initialize(self):
    for module in self.modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            nn.init.xavier_uniform_(module.weight)
            nn.init.zeros_(module.bias)
    nn.init.xavier_uniform_(self.block2[(- 1)].weight, gain=1e-05)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , (, nn.Linear)):
idx = 3:------------------- similar code ------------------ index = 35, score = 7.0 
def _init_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear)):
            mynn.init.XavierFill(m.weight)
            if (m.bias is not None):
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if ( or  ... ( ... , nn.Linear)):
idx = 4:------------------- similar code ------------------ index = 504, score = 7.0 
def weights_init_(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight, gain=1)
        torch.nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if  ... ( ... , nn.Linear):
idx = 5:------------------- similar code ------------------ index = 526, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 6:------------------- similar code ------------------ index = 296, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 7:------------------- similar code ------------------ index = 289, score = 7.0 
def initialize(self):
    for module in self.modules():
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.zeros_(module.bias)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 8:------------------- similar code ------------------ index = 553, score = 7.0 
def _initialize_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            if ((m.in_channels != m.out_channels) or (m.out_channels != m.groups) or (m.bias is not None)):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if (m.bias is not None):
                    nn.init.constant_(m.bias, 0)
            else:
                print('Not initializing')
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 9:------------------- similar code ------------------ index = 574, score = 7.0 
def _initialize_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if (m.bias is not None):
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 10:------------------- similar code ------------------ index = 245, score = 7.0 
def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, filter_size=1, pool_only=True):
    super(DenseNet, self).__init__()
    if pool_only:
        self.features = nn.Sequential(OrderedDict([('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)), ('norm0', nn.BatchNorm2d(num_init_features)), ('relu0', nn.ReLU(inplace=True)), ('max0', nn.MaxPool2d(kernel_size=3, stride=1, padding=1)), ('pool0', Downsample(filt_size=filter_size, stride=2, channels=num_init_features))]))
    else:
        self.features = nn.Sequential(OrderedDict([('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=1, padding=3, bias=False)), ('norm0', nn.BatchNorm2d(num_init_features)), ('relu0', nn.ReLU(inplace=True)), ('ds0', Downsample(filt_size=filter_size, stride=2, channels=num_init_features)), ('max0', nn.MaxPool2d(kernel_size=3, stride=1, padding=1)), ('pool0', Downsample(filt_size=filter_size, stride=2, channels=num_init_features))]))
    num_features = num_init_features
    for (i, num_layers) in enumerate(block_config):
        block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)
        self.features.add_module(('denseblock%d' % (i + 1)), block)
        num_features = (num_features + (num_layers * growth_rate))
        if (i != (len(block_config) - 1)):
            trans = _Transition(num_input_features=num_features, num_output_features=(num_features // 2), filter_size=filter_size)
            self.features.add_module(('transition%d' % (i + 1)), trans)
            num_features = (num_features // 2)
    self.features.add_module('norm5', nn.BatchNorm2d(num_features))
    self.classifier = nn.Linear(num_features, num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            if ((m.in_channels != m.out_channels) or (m.out_channels != m.groups) or (m.bias is not None)):
                nn.init.kaiming_normal_(m.weight)
            else:
                print('Not initializing')
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
 = nn
    for  ...  in:
        if:        elif  ... ( ... ,  ... .Linear):
idx = 11:------------------- similar code ------------------ index = 232, score = 7.0 
def __init__(self, nc, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):
    '\n        The init function of the  MobileNet V2 Module\n\n        :param nc: Network controller\n        :param num_classes: the number of output classes\n        :param width_mult: The width multiple\n        :param inverted_residual_setting: A list of the block configurations\n        :param round_nearest: Rounding to nearest value\n        '
    super(MobileNetV2, self).__init__()
    block = InvertedResidual
    input_channel = 32
    last_channel = 1280
    if (inverted_residual_setting is None):
        inverted_residual_setting = [[1, 16, 1, 1], [6, 24, 2, 2], [6, 32, 3, 2], [6, 64, 4, 2], [6, 96, 3, 1], [6, 160, 3, 2], [6, 320, 1, 1]]
    if ((len(inverted_residual_setting) == 0) or (len(inverted_residual_setting[0]) != 4)):
        raise ValueError('inverted_residual_setting should be non-empty or a 4-element list, got {}'.format(inverted_residual_setting))
    input_channel = _make_divisible((input_channel * width_mult), round_nearest)
    self.last_channel = _make_divisible((last_channel * max(1.0, width_mult)), round_nearest)
    features = [ConvBNNonLinear(nc, 3, input_channel, stride=2)]
    for (t, c, n, s) in inverted_residual_setting:
        output_channel = _make_divisible((c * width_mult), round_nearest)
        for i in range(n):
            stride = (s if (i == 0) else 1)
            features.append(block(nc, input_channel, output_channel, stride, expand_ratio=t))
            input_channel = output_channel
    features.append(ConvBNNonLinear(nc, input_channel, self.last_channel, kernel_size=1))
    self.features = nn.Sequential(*features)
    self.classifier = nn.Sequential(nn.Dropout(0.2), layers.FullyConnected(nc, self.last_channel, num_classes))
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out')
            if (m.bias is not None):
                nn.init.zeros_(m.bias)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.ones_(m.weight)
            nn.init.zeros_(m.bias)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
            nn.init.zeros_(m.bias)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 12:------------------- similar code ------------------ index = 220, score = 7.0 
def __init__(self, params):
    super(CoILICRA, self).__init__()
    self.params = params
    number_first_layer_channels = 0
    for (_, sizes) in g_conf.SENSORS.items():
        number_first_layer_channels += (sizes[0] * g_conf.NUMBER_FRAMES_FUSION)
    sensor_input_shape = next(iter(g_conf.SENSORS.values()))
    sensor_input_shape = [number_first_layer_channels, sensor_input_shape[1], sensor_input_shape[2]]
    if ('conv' in params['perception']):
        perception_convs = Conv(params={'channels': ([number_first_layer_channels] + params['perception']['conv']['channels']), 'kernels': params['perception']['conv']['kernels'], 'strides': params['perception']['conv']['strides'], 'dropouts': params['perception']['conv']['dropouts'], 'end_layer': True})
        perception_fc = FC(params={'neurons': ([perception_convs.get_conv_output(sensor_input_shape)] + params['perception']['fc']['neurons']), 'dropouts': params['perception']['fc']['dropouts'], 'end_layer': False})
        self.perception = nn.Sequential(*[perception_convs, perception_fc])
        number_output_neurons = params['perception']['fc']['neurons'][(- 1)]
    elif ('res' in params['perception']):
        resnet_module = importlib.import_module('network.models.building_blocks.resnet')
        resnet_module = getattr(resnet_module, params['perception']['res']['name'])
        self.perception = resnet_module(pretrained=g_conf.PRE_TRAINED, num_classes=params['perception']['res']['num_classes'])
        number_output_neurons = params['perception']['res']['num_classes']
    else:
        raise ValueError('invalid convolution layer type')
    self.measurements = FC(params={'neurons': ([len(g_conf.INPUTS)] + params['measurements']['fc']['neurons']), 'dropouts': params['measurements']['fc']['dropouts'], 'end_layer': False})
    self.join = Join(params={'after_process': FC(params={'neurons': ([(params['measurements']['fc']['neurons'][(- 1)] + number_output_neurons)] + params['join']['fc']['neurons']), 'dropouts': params['join']['fc']['dropouts'], 'end_layer': False}), 'mode': 'cat'})
    self.speed_branch = FC(params={'neurons': (([params['join']['fc']['neurons'][(- 1)]] + params['speed_branch']['fc']['neurons']) + [1]), 'dropouts': (params['speed_branch']['fc']['dropouts'] + [0.0]), 'end_layer': True})
    branch_fc_vector = []
    for i in range(params['branches']['number_of_branches']):
        branch_fc_vector.append(FC(params={'neurons': (([params['join']['fc']['neurons'][(- 1)]] + params['branches']['fc']['neurons']) + [len(g_conf.TARGETS)]), 'dropouts': (params['branches']['fc']['dropouts'] + [0.0]), 'end_layer': True}))
    self.branches = Branching(branch_fc_vector)
    if ('conv' in params['perception']):
        for m in self.modules():
            if (isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear)):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0.1)
    else:
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0.1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:
        for  ...  in:
            if ( or  ... ( ... , nn.Linear)):
idx = 13:------------------- similar code ------------------ index = 605, score = 7.0 
def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):
    super(DenseNet, self).__init__()
    self.features = nn.Sequential(OrderedDict([('conv0', nn.Conv2d(4, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)), ('norm0', nn.BatchNorm2d(num_init_features)), ('relu0', nn.ReLU(inplace=True)), ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))
    num_features = num_init_features
    for (i, num_layers) in enumerate(block_config):
        block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)
        self.features.add_module(('denseblock%d' % (i + 1)), block)
        num_features = (num_features + (num_layers * growth_rate))
        if (i != (len(block_config) - 1)):
            trans = _Transition(num_input_features=num_features, num_output_features=(num_features // 2))
            self.features.add_module(('transition%d' % (i + 1)), trans)
            num_features = (num_features // 2)
    self.features.add_module('norm5', nn.BatchNorm2d(num_features))
    self.classifier = nn.Linear(num_features, num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal(m.weight.data)
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
 = nn
    for  ...  in:
        if:        elif  ... ( ... ,  ... .Linear):
idx = 14:------------------- similar code ------------------ index = 613, score = 7.0 
def init_normal(m):
    if (type(m) == nn.Linear):
        nn.init.uniform_(m.weight, 0, 0.01)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if ( == nn.Linear):
idx = 15:------------------- similar code ------------------ index = 208, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 16:------------------- similar code ------------------ index = 619, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear)):
            kaiming_init(m)
    for m in self.modules():
        if isinstance(m, nn.ConvTranspose2d):
            normal_init(m, std=0.001)
    nn.init.constant_(self.deconv2.bias, (- np.log((0.99 / 0.01))))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if ( or  ... ( ... , nn.Linear)):
idx = 17:------------------- similar code ------------------ index = 751, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 18:------------------- similar code ------------------ index = 749, score = 7.0 
def _init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 19:------------------- similar code ------------------ index = 193, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)
        elif isinstance(m, nn.BatchNorm1d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 20:------------------- similar code ------------------ index = 183, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 21:------------------- similar code ------------------ index = 647, score = 7.0 
def _init_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear)):
            mynn.init.XavierFill(m.weight)
            if (m.bias is not None):
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if ( or  ... ( ... , nn.Linear)):
idx = 22:------------------- similar code ------------------ index = 173, score = 7.0 
def _init_weights(self, module):
    ' Initialize the weights '
    if isinstance(module, nn.Linear):
        torch.nn.init.xavier_uniform_(module.weight.data)
    elif isinstance(module, nn.LayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if (isinstance(module, nn.Linear) and (module.bias is not None)):
        module.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if  ... ( ... , nn.Linear):
idx = 23:------------------- similar code ------------------ index = 96, score = 7.0 
def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, memory_efficient=False):
    super(DenseNet, self).__init__()
    self.features = nn.Sequential(OrderedDict([('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)), ('norm0', nn.BatchNorm2d(num_init_features)), ('relu0', nn.ReLU(inplace=True)), ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))
    num_features = num_init_features
    for (i, num_layers) in enumerate(block_config):
        block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate, memory_efficient=memory_efficient)
        self.features.add_module(('denseblock%d' % (i + 1)), block)
        num_features = (num_features + (num_layers * growth_rate))
        if (i != (len(block_config) - 1)):
            trans = _Transition(num_input_features=num_features, num_output_features=(num_features // 2))
            self.features.add_module(('transition%d' % (i + 1)), trans)
            num_features = (num_features // 2)
    self.features.add_module('norm5', nn.BatchNorm2d(num_features))
    self.classifier = nn.Linear(num_features, num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
 = nn
    for  ...  in:
        if:        elif  ... ( ... ,  ... .Linear):
idx = 24:------------------- similar code ------------------ index = 660, score = 7.0 
def initialize_weights(model):
    '\n    Function to initialize network weights.\n    NOTE: NOT USED IN MAIN SCRIPT.\n\n    Args:\n        model: PyTorch Network\n    Returns:\n        Nothing!\n    '
    for (idx, module) in enumerate(model.modules()):
        if isinstance(module, nn.Conv2d):
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(module, nn.BatchNorm2d):
            nn.init.constant_(module.weight, 1)
            nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.Linear):
            module.weight.data.normal_(0, 0.01)
            module.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for in:
        if:        elif  ... ( ... , nn.Linear):
idx = 25:------------------- similar code ------------------ index = 684, score = 7.0 
def __init__(self, word_embeddings_out_dim: int):
    super(Duet, self).__init__()
    NUM_HIDDEN_NODES = word_embeddings_out_dim
    POOLING_KERNEL_WIDTH_QUERY = 18
    POOLING_KERNEL_WIDTH_DOC = 100
    DROPOUT_RATE = 0
    NUM_POOLING_WINDOWS_DOC = 99
    MAX_DOC_TERMS = 2000
    MAX_QUERY_TERMS = 30
    self.cosine_module = CosineMatrixAttention()
    self.duet_local = nn.Sequential(nn.Conv1d(MAX_DOC_TERMS, NUM_HIDDEN_NODES, kernel_size=1), nn.ReLU(), Flatten(), nn.Dropout(p=DROPOUT_RATE), nn.Linear((NUM_HIDDEN_NODES * MAX_QUERY_TERMS), NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE), nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE))
    self.duet_dist_q = nn.Sequential(nn.Conv1d(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES, kernel_size=3), nn.ReLU(), nn.MaxPool1d(POOLING_KERNEL_WIDTH_QUERY), Flatten(), nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES), nn.ReLU())
    self.duet_dist_d = nn.Sequential(nn.Conv1d(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES, kernel_size=3), nn.ReLU(), nn.MaxPool1d(POOLING_KERNEL_WIDTH_DOC, stride=1), nn.Conv1d(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES, kernel_size=1), nn.ReLU())
    self.duet_dist = nn.Sequential(Flatten(), nn.Dropout(p=DROPOUT_RATE), nn.Linear((NUM_HIDDEN_NODES * NUM_POOLING_WINDOWS_DOC), NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE), nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE))
    self.duet_comb = nn.Sequential(nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE), nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES), nn.ReLU(), nn.Dropout(p=DROPOUT_RATE), nn.Linear(NUM_HIDDEN_NODES, 1), nn.ReLU())

    def init_normal(m):
        if (type(m) == nn.Linear):
            nn.init.uniform_(m.weight, 0, 0.01)
    self.duet_comb.apply(init_normal)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
 =  ... . ... (,,,, nn,,,,,)
    def  ... ( ... ):
        if ( ==  ... .Linear):
idx = 26:------------------- similar code ------------------ index = 695, score = 7.0 
def weight_xavier_init(*models):
    for model in models:
        for module in model.modules():
            if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear)):
                nn.init.orthogonal_(module.weight)
                if (module.bias is not None):
                    module.bias.data.zero_()
            elif isinstance(module, nn.BatchNorm2d):
                module.weight.data.fill_(1)
                module.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    for  ...  in  ... :
        for  ...  in:
            if ( or  ... ( ... , nn.Linear)):
idx = 27:------------------- similar code ------------------ index = 701, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 28:------------------- similar code ------------------ index = 127, score = 7.0 
def weights_init(m):
    if isinstance(m, nn.Conv2d):
        nn.init.orthogonal(m.weight.data, gain=0.7)
        nn.init.constant(m.bias.data, 0.01)
    if isinstance(m, nn.Linear):
        nn.init.orthogonal(m.weight.data, gain=0.01)
        nn.init.constant(m.bias.data, 0.0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if  ... ( ... , nn.Linear):
idx = 29:------------------- similar code ------------------ index = 482, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 30:------------------- similar code ------------------ index = 781, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 31:------------------- similar code ------------------ index = 355, score = 7.0 
def _initialize_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            if ((m.in_channels != m.out_channels) or (m.out_channels != m.groups) or (m.bias is not None)):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if (m.bias is not None):
                    nn.init.constant_(m.bias, 0)
            else:
                print('Not initializing')
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 32:------------------- similar code ------------------ index = 416, score = 7.0 
def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), use_spectral_norm=True, num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):
    super(DenseNet, self).__init__()
    self.features = nn.Sequential(OrderedDict([('conv0', spectral_norm(nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False), use_spectral_norm)), ('norm0', nn.BatchNorm2d(num_init_features)), ('relu0', nn.ReLU()), ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))
    num_features = num_init_features
    for (i, num_layers) in enumerate(block_config):
        block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate, use_spectral_norm=use_spectral_norm)
        self.features.add_module(('denseblock%d' % (i + 1)), block)
        num_features = (num_features + (num_layers * growth_rate))
        if (i != (len(block_config) - 1)):
            trans = _Transition(num_input_features=num_features, num_output_features=(num_features // 2), use_spectral_norm=use_spectral_norm)
            self.features.add_module(('transition%d' % (i + 1)), trans)
            num_features = (num_features // 2)
    self.features.add_module('norm5', nn.BatchNorm2d(num_features))
    self.conv_last = spectral_norm(nn.Conv2d(num_features, 256, kernel_size=3), use_spectral_norm)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight.data)
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 33:------------------- similar code ------------------ index = 338, score = 7.0 
def __init__(self, input_dim=(128, 256), pred_input_dim=(256, 256), pred_inter_dim=(256, 256)):
    super().__init__()
    self.conv3_1r = conv(input_dim[0], 128, kernel_size=3, stride=1)
    self.conv3_1t = conv(input_dim[0], 256, kernel_size=3, stride=1)
    self.conv3_2t = conv(256, pred_input_dim[0], kernel_size=3, stride=1)
    self.prroi_pool3r = PrRoIPool2D(3, 3, (1 / 8))
    self.prroi_pool3t = PrRoIPool2D(5, 5, (1 / 8))
    self.fc3_1r = conv(128, 256, kernel_size=3, stride=1, padding=0)
    self.conv4_1r = conv(input_dim[1], 256, kernel_size=3, stride=1)
    self.conv4_1t = conv(input_dim[1], 256, kernel_size=3, stride=1)
    self.conv4_2t = conv(256, pred_input_dim[1], kernel_size=3, stride=1)
    self.prroi_pool4r = PrRoIPool2D(1, 1, (1 / 16))
    self.prroi_pool4t = PrRoIPool2D(3, 3, (1 / 16))
    self.fc34_3r = conv((256 + 256), pred_input_dim[0], kernel_size=1, stride=1, padding=0)
    self.fc34_4r = conv((256 + 256), pred_input_dim[1], kernel_size=1, stride=1, padding=0)
    self.fc3_rt = LinearBlock(pred_input_dim[0], pred_inter_dim[0], 5)
    self.fc4_rt = LinearBlock(pred_input_dim[1], pred_inter_dim[1], 3)
    self.iou_predictor = nn.Linear((pred_inter_dim[0] + pred_inter_dim[1]), 1, bias=True)
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear)):
            nn.init.kaiming_normal_(m.weight.data, mode='fan_in')
            if (m.bias is not None):
                m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
 = nn
    for  ...  in:
        if ( or  ... ( ... ,  ... .Linear)):
idx = 34:------------------- similar code ------------------ index = 445, score = 7.0 
def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, memory_efficient=False):
    super(DenseNet, self).__init__()
    self.features = nn.Sequential(OrderedDict([('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)), ('norm0', nn.BatchNorm2d(num_init_features)), ('relu0', nn.ReLU(inplace=True)), ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))
    num_features = num_init_features
    for (i, num_layers) in enumerate(block_config):
        block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate, memory_efficient=memory_efficient)
        self.features.add_module(('denseblock%d' % (i + 1)), block)
        num_features = (num_features + (num_layers * growth_rate))
        if (i != (len(block_config) - 1)):
            trans = _Transition(num_input_features=num_features, num_output_features=(num_features // 2))
            self.features.add_module(('transition%d' % (i + 1)), trans)
            num_features = (num_features // 2)
    self.features.add_module('norm5', nn.BatchNorm2d(num_features))
    self.classifier = nn.Linear(num_features, num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
 = nn
    for  ...  in:
        if:        elif  ... ( ... ,  ... .Linear):
idx = 35:------------------- similar code ------------------ index = 372, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0.1)
        elif isinstance(m, nn.BatchNorm1d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 36:------------------- similar code ------------------ index = 383, score = 7.0 
def __init__(self, num_classes=1000, width_mult=1.0, filter_size=1):
    super(MobileNetV2, self).__init__()
    block = InvertedResidual
    input_channel = 32
    last_channel = 1280
    inverted_residual_setting = [[1, 16, 1, 1], [6, 24, 2, 2], [6, 32, 3, 2], [6, 64, 4, 2], [6, 96, 3, 1], [6, 160, 3, 2], [6, 320, 1, 1]]
    input_channel = int((input_channel * width_mult))
    self.last_channel = int((last_channel * max(1.0, width_mult)))
    features = [ConvBNReLU(3, input_channel, stride=2)]
    for (t, c, n, s) in inverted_residual_setting:
        output_channel = int((c * width_mult))
        for i in range(n):
            stride = (s if (i == 0) else 1)
            features.append(block(input_channel, output_channel, stride, expand_ratio=t, filter_size=filter_size))
            input_channel = output_channel
    features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))
    self.features = nn.Sequential(*features)
    self.classifier = nn.Sequential(nn.Linear(self.last_channel, num_classes))
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out')
            if (m.bias is not None):
                nn.init.zeros_(m.bias)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.ones_(m.weight)
            nn.init.zeros_(m.bias)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
            nn.init.zeros_(m.bias)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
 =  ... . ... (nn)
    for  ...  in:
        if:        elif  ... ( ... ,  ... .Linear):
idx = 37:------------------- similar code ------------------ index = 424, score = 7.0 
def init_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear)):
            kaiming_init(m)
    for m in self.modules():
        if isinstance(m, nn.ConvTranspose2d):
            normal_init(m, std=0.001)
    nn.init.constant_(self.deconv2.bias, (- np.log((0.99 / 0.01))))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if ( or  ... ( ... , nn.Linear)):
idx = 38:------------------- similar code ------------------ index = 466, score = 7.0 
def _initialize_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear)):
            import scipy.stats as stats
            X = stats.truncnorm((- 2), 2, scale=0.01)
            values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)
            values = values.view(m.weight.size())
            with torch.no_grad():
                m.weight.copy_(values)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if ( or  ... ( ... , nn.Linear)):
idx = 39:------------------- similar code ------------------ index = 366, score = 7.0 
def init_weights(self):
    super(ConvFCBBoxHead, self).init_weights()
    for module_list in [self.shared_fcs, self.cls_fcs, self.reg_fcs]:
        for m in module_list.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        for  ...  in:
            if  ... ( ... , nn.Linear):
idx = 40:------------------- similar code ------------------ index = 397, score = 7.0 
def _initialize_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if (m.bias is not None):
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 41:------------------- similar code ------------------ index = 817, score = 7.0 
def init_weights(self):
    super(ConvFCBBoxHead, self).init_weights()
    for module_list in [self.shared_fcs, self.cls_fcs, self.reg_fcs]:
        for m in module_list.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        for  ...  in:
            if  ... ( ... , nn.Linear):
idx = 42:------------------- similar code ------------------ index = 407, score = 7.0 
def layer_init(self):
    for layer in self.cnn:
        if isinstance(layer, (nn.Conv2d, nn.Linear)):
            nn.init.orthogonal_(layer.weight, gain=1)
            nn.init.constant_(layer.bias, val=0)
    for (name, param) in self.rnn.named_parameters():
        if ('weight' in name):
            nn.init.orthogonal_(param)
        elif ('bias' in name):
            nn.init.constant_(param, 0)
    nn.init.orthogonal_(self.critic_linear.weight, gain=1)
    nn.init.constant_(self.critic_linear.bias, val=0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , (, nn.Linear)):
idx = 43:------------------- similar code ------------------ index = 184, score = 6.0 
def fc_deterministic_policy(env, hidden1=400, hidden2=300):
    return nn.Sequential(nn.Linear(env.state_space.shape[0], hidden1), nn.LeakyReLU(), nn.Linear(hidden1, hidden2), nn.LeakyReLU(), nn.Linear(hidden2, env.action_space.shape[0]))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn. ... ( ... .Linear,,,,)

idx = 44:------------------- similar code ------------------ index = 198, score = 6.0 
def _initialize_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = ((m.kernel_size[0] * m.kernel_size[1]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
            if (m.bias is not None):
                m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.weight.data.normal_(0, 0.01)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 45:------------------- similar code ------------------ index = 378, score = 6.0 
def weights_init(m):
    if isinstance(m, nn.Linear):
        init.kaiming_uniform_(m.weight, a=math.sqrt(5))
        if (m.bias is not None):
            (fan_in, _) = init._calculate_fan_in_and_fan_out(m.weight)
            bound = (1 / math.sqrt(fan_in))
            init.uniform_(m.bias, (- bound), bound)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if  ... ( ... , nn.Linear):
idx = 46:------------------- similar code ------------------ index = 696, score = 6.0 
def _initialize_weights(self, pretrained):
    for m in self.modules():
        if pretrained:
            break
        elif isinstance(m, nn.Conv2d):
            m.weight.data.normal_(0, 0.001)
            if (m.bias is not None):
                m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.weight.data.normal_(0, 0.01)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    for  ...  in:
        if  ... :
            break
        elif  ... ( ... , nn.Linear):
idx = 47:------------------- similar code ------------------ index = 618, score = 6.0 
def init_weights(self):
    normal_init(self.fc_cls, std=0.01)
    normal_init(self.fc_reg, std=0.001)
    for m in self.fc_branch.modules():
        if isinstance(m, nn.Linear):
            xavier_init(m, distribution='uniform')

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 48:------------------- similar code ------------------ index = 125, score = 6.0 
def get_cnn_miniimagenet(hidden_size, n_classes):

    def conv_layer(ic, oc):
        return nn.Sequential(nn.Conv2d(ic, oc, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(oc, momentum=1.0, affine=True, track_running_stats=False))
    net = nn.Sequential(conv_layer(3, hidden_size), conv_layer(hidden_size, hidden_size), conv_layer(hidden_size, hidden_size), conv_layer(hidden_size, hidden_size), nn.Flatten(), nn.Linear(((hidden_size * 5) * 5), n_classes))
    initialize(net)
    return net

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():

    def  ... ():
        return nn
     ...  =  ... . ... (,,,,,  ... .Linear)

idx = 49:------------------- similar code ------------------ index = 607, score = 6.0 
def init_model(self, model):
    '\n        Loops over all convolutional, fully-connexted and batch-normalization\n        layers and calls the layer_init function for each module (layer) in\n        the model to initialize weights and biases for the whole model.\n\n        Parameters:\n            model (torch.nn.Module): Model architecture\n        '
    for m in model.modules():
        if isinstance(m, (nn.Linear, nn.Conv2d)):
            self.layer_init(m)
        elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):
            if m.affine:
                m.weight.data.fill_(1)
                m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    for  ...  in:
        if  ... ( ... , (nn.Linear,)):
idx = 50:------------------- similar code ------------------ index = 710, score = 6.0 
def _initialize_weights(self):
    for m in self.modules():
        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear)):
            m.weight.data = nn.init.kaiming_normal_(m.weight.data)
            if (m.bias is not None):
                m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if ( or  ... ( ... , nn.Linear)):
idx = 51:------------------- similar code ------------------ index = 697, score = 6.0 
def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):
    super(WideResNet, self).__init__()
    nChannels = [16, (16 * widen_factor), (32 * widen_factor), (64 * widen_factor)]
    assert (((depth - 4) % 6) == 0), 'depth should be 6n+4'
    n = ((depth - 4) // 6)
    block = BasicBlock
    self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1, padding=1, bias=False)
    self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)
    self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)
    self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)
    self.bn1 = nn.BatchNorm2d(nChannels[3])
    self.relu = nn.ReLU(inplace=True)
    self.fc = nn.Linear(nChannels[3], num_classes)
    self.nChannels = nChannels[3]
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = ((m.kernel_size[0] * m.kernel_size[1]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
 = nn
    for  ...  in:
        if:        elif  ... ( ... ,  ... .Linear):
idx = 52:------------------- similar code ------------------ index = 675, score = 6.0 
def _init_weights(self, module):
    ' Initialize the weights '
    if isinstance(module, (nn.Linear, nn.Embedding)):
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
    elif isinstance(module, AlbertLayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if (isinstance(module, nn.Linear) and (module.bias is not None)):
        module.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if  ... ( ... , (nn.Linear,)):
idx = 53:------------------- similar code ------------------ index = 138, score = 6.0 
def fc_bcq_deterministic_policy(env, hidden1=400, hidden2=300):
    return nn.Sequential(nn.Linear((env.state_space.shape[0] + env.action_space.shape[0]), hidden1), nn.LeakyReLU(), nn.Linear(hidden1, hidden2), nn.LeakyReLU(), nn.Linear(hidden2, env.action_space.shape[0]))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn. ... ( ... .Linear,,,,)

idx = 54:------------------- similar code ------------------ index = 686, score = 6.0 
def weights_normal_init(model, dev=0.01):
    if isinstance(model, list):
        for m in model:
            weights_normal_init(m, dev)
    else:
        for m in model.modules():
            if isinstance(m, nn.Conv2d):
                m.weight.data.normal_(0.0, dev)
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0.0, dev)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:    else:
        for  ...  in:
            if:            elif  ... ( ... , nn.Linear):
idx = 55:------------------- similar code ------------------ index = 392, score = 6.0 
def __init__(self, depth, w_base, label):
    super(ShakeResNet, self).__init__()
    n_units = ((depth - 2) / 6)
    in_chs = [16, w_base, (w_base * 2), (w_base * 4)]
    self.in_chs = in_chs
    self.c_in = nn.Conv2d(3, in_chs[0], 3, padding=1)
    self.layer1 = self._make_layer(n_units, in_chs[0], in_chs[1])
    self.layer2 = self._make_layer(n_units, in_chs[1], in_chs[2], 2)
    self.layer3 = self._make_layer(n_units, in_chs[2], in_chs[3], 2)
    self.fc_out = nn.Linear(in_chs[3], label)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = ((m.kernel_size[0] * m.kernel_size[1]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
 = nn
    for  ...  in:
        if:        elif  ... ( ... ,  ... .Linear):
idx = 56:------------------- similar code ------------------ index = 149, score = 6.0 
def init_weights(self, pretrained=None):
    if isinstance(pretrained, str):
        logger = get_root_logger()
        load_checkpoint(self, pretrained, strict=False, logger=logger)
    elif (pretrained is None):
        for m in self.features.modules():
            if isinstance(m, nn.Conv2d):
                kaiming_init(m)
            elif isinstance(m, nn.BatchNorm2d):
                constant_init(m, 1)
            elif isinstance(m, nn.Linear):
                normal_init(m, std=0.01)
    else:
        raise TypeError('pretrained must be a str or None')
    for m in self.extra.modules():
        if isinstance(m, nn.Conv2d):
            xavier_init(m, distribution='uniform')
    constant_init(self.l2_norm, self.l2_norm.scale)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    if:    elif:
        for  ...  in:
            if:            elif  ... ( ... , nn.Linear):
idx = 57:------------------- similar code ------------------ index = 643, score = 6.0 
def _init_weights(self):

    def _init(m):
        if isinstance(m, nn.Conv2d):
            mynn.init.MSRAFill(m.weight)
        elif isinstance(m, nn.Linear):
            mynn.init.XavierFill(m.weight)
            init.constant_(m.bias, 0)
    self.apply(_init)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):

    def  ... ( ... ):
        if:        elif  ... ( ... , nn.Linear):
idx = 58:------------------- similar code ------------------ index = 666, score = 6.0 
def lin_layer(n_in, n_out, dropout):
    return nn.Sequential(nn.Linear(n_in, n_out), GELU(), nn.Dropout(dropout))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn. ... ( ... .Linear,,)

idx = 59:------------------- similar code ------------------ index = 665, score = 6.0 
def fc_reward(env, hidden1=400, hidden2=300):
    return nn.Sequential(nn.Linear((env.state_space.shape[0] + env.action_space.shape[0]), hidden1), nn.LeakyReLU(), nn.Linear(hidden1, hidden2), nn.LeakyReLU(), nn.Linear(hidden2, 1))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn. ... ( ... .Linear,,,,)

idx = 60:------------------- similar code ------------------ index = 166, score = 6.0 
def _initialize_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = ((m.kernel_size[0] * m.kernel_size[1]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
            if (m.bias is not None):
                m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.weight.data.normal_(0, 0.01)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 61:------------------- similar code ------------------ index = 451, score = 6.0 
def reset_params(self):
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            init.kaiming_normal(m.weight, mode='fan_out')
            if (m.bias is not None):
                init.constant(m.bias, 0)
        elif isinstance(m, nn.BatchNorm2d):
            init.constant(m.weight, 1)
            init.constant(m.bias, 0)
        elif isinstance(m, nn.Linear):
            init.normal(m.weight, std=0.001)
            if (m.bias is not None):
                init.constant(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 62:------------------- similar code ------------------ index = 426, score = 6.0 
def __init__(self, maxdisp):
    super(PSMNet, self).__init__()
    self.maxdisp = maxdisp
    self.feature_extraction = feature_extraction()
    self.feature_disp_pre = SparseConvNet()
    self.dres0 = nn.Sequential(convbn_3d(96, 32, 3, 1, 1), nn.ReLU(inplace=True), convbn_3d(32, 32, 3, 1, 1), nn.ReLU(inplace=True))
    self.dres1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1), nn.ReLU(inplace=True), convbn_3d(32, 32, 3, 1, 1))
    self.dres2 = hourglass(32)
    self.dres3 = hourglass(32)
    self.dres4 = hourglass(32)
    self.classif1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1), nn.ReLU(inplace=True), nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1, bias=False))
    self.classif2 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1), nn.ReLU(inplace=True), nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1, bias=False))
    self.classif3 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1), nn.ReLU(inplace=True), nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1, bias=False))
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = ((m.kernel_size[0] * m.kernel_size[1]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
        elif isinstance(m, nn.Conv3d):
            n = (((m.kernel_size[0] * m.kernel_size[1]) * m.kernel_size[2]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm3d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 63:------------------- similar code ------------------ index = 596, score = 6.0 
def _init(m):
    if isinstance(m, nn.Conv2d):
        mynn.init.MSRAFill(m.weight)
    elif isinstance(m, nn.Linear):
        mynn.init.XavierFill(m.weight)
        init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if:    elif  ... ( ... , nn.Linear):
idx = 64:------------------- similar code ------------------ index = 353, score = 6.0 
def group_weight(weight_group, module, norm_layer, lr):
    group_decay = []
    group_no_decay = []
    for m in module.modules():
        if isinstance(m, nn.Linear):
            group_decay.append(m.weight)
            if (m.bias is not None):
                group_no_decay.append(m.bias)
        elif isinstance(m, (nn.Conv2d, nn.Conv3d)):
            group_decay.append(m.weight)
            if (m.bias is not None):
                group_no_decay.append(m.bias)
        elif (isinstance(m, norm_layer) or isinstance(m, nn.GroupNorm)):
            if (m.weight is not None):
                group_no_decay.append(m.weight)
            if (m.bias is not None):
                group_no_decay.append(m.bias)
    assert (len(list(module.parameters())) == (len(group_decay) + len(group_no_decay)))
    weight_group.append(dict(params=group_decay, lr=lr))
    weight_group.append(dict(params=group_no_decay, weight_decay=0.0, lr=lr))
    return weight_group

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    for  ...  in:
        if  ... ( ... , nn.Linear):
idx = 65:------------------- similar code ------------------ index = 517, score = 6.0 
def fc_v(env, hidden1=400, hidden2=300):
    return nn.Sequential(nn.Linear(env.state_space.shape[0], hidden1), nn.LeakyReLU(), nn.Linear(hidden1, hidden2), nn.LeakyReLU(), nn.Linear(hidden2, 1))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    return nn. ... ( ... .Linear,,,,)

idx = 66:------------------- similar code ------------------ index = 312, score = 6.0 
def _initialize_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = ((m.kernel_size[0] * m.kernel_size[1]) * m.out_channels)
            m.weight.data.normal_(0, math.sqrt((2.0 / n)))
            if (m.bias is not None):
                m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.weight.data.normal_(0, 0.01)
            m.bias.data.zero_()

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    for  ...  in:
        if:        elif  ... ( ... , nn.Linear):
idx = 67:------------------- similar code ------------------ index = 508, score = 6.0 
def weight_init(m):
    "\n    Initializes a model's parameters.\n    Credits to: https://gist.github.com/jeasinema\n\n    Usage:\n        model = Model()\n        model.apply(weight_init)\n    "
    if isinstance(m, nn.Conv1d):
        init.normal_(m.weight.data)
        if (m.bias is not None):
            init.normal_(m.bias.data)
    elif isinstance(m, nn.Conv2d):
        init.xavier_normal_(m.weight.data)
        if (m.bias is not None):
            init.normal_(m.bias.data)
    elif isinstance(m, nn.Conv3d):
        init.xavier_normal_(m.weight.data)
        if (m.bias is not None):
            init.normal_(m.bias.data)
    elif isinstance(m, nn.ConvTranspose1d):
        init.normal_(m.weight.data)
        if (m.bias is not None):
            init.normal_(m.bias.data)
    elif isinstance(m, nn.ConvTranspose2d):
        init.xavier_normal_(m.weight.data)
        if (m.bias is not None):
            init.normal_(m.bias.data)
    elif isinstance(m, nn.ConvTranspose3d):
        init.xavier_normal_(m.weight.data)
        if (m.bias is not None):
            init.normal_(m.bias.data)
    elif isinstance(m, nn.BatchNorm1d):
        init.normal_(m.weight.data, mean=0, std=1)
        init.constant_(m.bias.data, 0)
    elif isinstance(m, nn.BatchNorm2d):
        init.normal_(m.weight.data, mean=0, std=1)
        init.constant_(m.bias.data, 0)
    elif isinstance(m, nn.BatchNorm3d):
        init.normal_(m.weight.data, mean=0, std=1)
        init.constant_(m.bias.data, 0)
    elif isinstance(m, nn.Linear):
        init.xavier_normal_(m.weight.data)
        try:
            init.normal_(m.bias.data)
        except AttributeError:
            pass
    elif isinstance(m, nn.LSTM):
        for param in m.parameters():
            if (len(param.shape) >= 2):
                init.orthogonal_(param.data)
            else:
                init.normal_(param.data)
    elif isinstance(m, nn.LSTMCell):
        for param in m.parameters():
            if (len(param.shape) >= 2):
                init.orthogonal_(param.data)
            else:
                init.normal_(param.data)
    elif isinstance(m, nn.GRU):
        for param in m.parameters():
            if (len(param.shape) >= 2):
                init.orthogonal_(param.data)
            else:
                init.normal_(param.data)
    elif isinstance(m, nn.GRUCell):
        for param in m.parameters():
            if (len(param.shape) >= 2):
                init.orthogonal_(param.data)
            else:
                init.normal_(param.data)

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if:    elif  ... ( ... , nn.Linear):
idx = 68:------------------- similar code ------------------ index = 506, score = 6.0 
def weights_init(m):
    'custom weights initialization.'
    cname = m.__class__
    if ((cname == nn.Linear) or (cname == nn.Conv2d) or (cname == nn.ConvTranspose2d)):
        m.weight.data.normal_(0.0, 0.02)
    elif (cname == nn.BatchNorm2d):
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)
    else:
        print(('%s is not initialized.' % cname))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if (( ...  == nn.Linear) or or):
idx = 69:------------------- similar code ------------------ index = 348, score = 6.0 
def weights_init(m):
    'custom weights initialization.'
    cname = m.__class__
    if ((cname == nn.Linear) or (cname == nn.Conv2d) or (cname == nn.ConvTranspose2d)):
        m.weight.data.normal_(0.0, 0.02)
    elif (cname == nn.BatchNorm2d):
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)
    else:
        print(('%s is not initialized.' % cname))

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ( ... ):
    if (( ...  == nn.Linear) or or):
idx = 70:------------------- similar code ------------------ index = 505, score = 7.0 
def Linear(i_dim, o_dim, bias=True):
    m = nn.Linear(i_dim, o_dim, bias)
    nn.init.normal_(m.weight, std=0.02)
    if bias:
        nn.init.constant_(m.bias, 0.0)
    return m

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = nn.Linear

idx = 71:------------------- similar code ------------------ index = 514, score = 7.0 
def Linear(i_dim, o_dim, bias=True):
    m = nn.Linear(i_dim, o_dim, bias)
    nn.init.normal_(m.weight, std=0.02)
    if bias:
        nn.init.constant_(m.bias, 0.0)
    return m

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = nn.Linear

idx = 72:------------------- similar code ------------------ index = 189, score = 6.0 
def __init__(self, n_head, d_model, d_k, d_v, addition_input=0, dropout=0.1, attn_dropout=0.1):
    super().__init__()
    self.n_head = n_head
    self.d_k = d_k
    self.d_v = d_v
    self.w_qs = nn.Linear(d_model, (n_head * d_k))
    self.w_ks = nn.Linear((d_model + addition_input), (n_head * d_k))
    self.w_vs = nn.Linear((d_model + addition_input), (n_head * d_v))
    nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt((2.0 / (d_model + d_k))))
    nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt((2.0 / ((d_model + addition_input) + d_k))))
    nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt((2.0 / ((d_model + addition_input) + d_v))))
    self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5), attn_dropout=attn_dropout)
    self.layer_norm = nn.LayerNorm(d_model)
    self.fc = nn.Linear((n_head * d_v), d_model)
    nn.init.xavier_normal_(self.fc.weight)
    self.dropout = nn.Dropout(dropout)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 73:------------------- similar code ------------------ index = 192, score = 6.0 
def __init__(self, rep_dim=128, bias_terms=True):
    super().__init__()
    self.rep_dim = rep_dim
    self.fc1 = nn.Linear(2, 256, bias=bias_terms)
    nn.init.xavier_normal_(self.fc1.weight, gain=nn.init.calculate_gain('leaky_relu'))
    self.bn1 = nn.BatchNorm1d(256, affine=bias_terms)
    self.fc2 = nn.Linear(256, self.rep_dim, bias=bias_terms)
    nn.init.xavier_normal_(self.fc2.weight, gain=nn.init.calculate_gain('sigmoid'))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 74:------------------- similar code ------------------ index = 203, score = 6.0 
def __init__(self, nc, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None):
    super(ResNet, self).__init__()
    self.nc = nc
    self.inplanes = 64
    self.dilation = 1
    if (replace_stride_with_dilation is None):
        replace_stride_with_dilation = [False, False, False]
    if (len(replace_stride_with_dilation) != 3):
        raise ValueError('replace_stride_with_dilation should be None or a 3-element tuple, got {}'.format(replace_stride_with_dilation))
    self.groups = groups
    self.base_width = width_per_group
    self.conv1_bn = ConvBN(nc, 3, self.inplanes, kernel_size=7, stride=2, padding=3)
    self.relu = NonLinear(nc, self.inplanes)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)
            elif isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 75:------------------- similar code ------------------ index = 437, score = 6.0 
def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None):
    super(ResNet, self).__init__()
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    self._norm_layer = norm_layer
    self.inplanes = 64
    self.dilation = 1
    if (replace_stride_with_dilation is None):
        replace_stride_with_dilation = [False, False, False]
    if (len(replace_stride_with_dilation) != 3):
        raise ValueError('replace_stride_with_dilation should be None or a 3-element tuple, got {}'.format(replace_stride_with_dilation))
    self.groups = groups
    self.base_width = width_per_group
    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = norm_layer(self.inplanes)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)
            elif isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 76:------------------- similar code ------------------ index = 620, score = 6.0 
def __init__(self, channel_index, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None):
    super(ResNet, self).__init__()
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    self._norm_layer = norm_layer
    self.inplanes = 64
    self.dilation = 1
    self.channel_index = channel_index
    self.input_channel = self.inplanes
    if (replace_stride_with_dilation is None):
        replace_stride_with_dilation = [False, False, False]
    if (len(replace_stride_with_dilation) != 3):
        raise ValueError('replace_stride_with_dilation should be None or a 3-element tuple, got {}'.format(replace_stride_with_dilation))
    self.groups = groups
    self.base_width = width_per_group
    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = norm_layer(self.inplanes)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear(self.input_channel, num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 77:------------------- similar code ------------------ index = 704, score = 6.0 
def __init__(self, dim_in, dim_conv, dim_hidden, dim_out, n_layers, kernel_size, stride):
    '\n        :param dim_in: input channels\n        :param dim_conv: conv output channels\n        :param dim_hidden: MLP and LSTM output dim\n        :param dim_out: latent variable dimension\n        '
    nn.Module.__init__(self)
    self.mlc = MultiLayerConv(dim_in, dim_conv, n_layers, kernel_size, stride=stride)
    self.mlp = MLP(dim_conv, dim_hidden, n_layers=1)
    self.lstm = nn.LSTMCell((dim_hidden + (4 * dim_out)), dim_hidden)
    self.mean_update = nn.Linear(dim_hidden, dim_out)
    self.logvar_update = nn.Linear(dim_hidden, dim_out)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 78:------------------- similar code ------------------ index = 207, score = 6.0 
def __init__(self, block, layers, num_classes=1000):
    self.inplanes = 64
    super(ResNet, self).__init__()
    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = nn.BatchNorm2d(64)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
    self.avgpool = nn.AvgPool2d(2, stride=0)
    if (block.__name__ == 'Bottleneck'):
        self.fc = nn.Linear(6144, num_classes)
    else:
        self.fc = nn.Linear(1536, num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
 = nn.Linear

idx = 79:------------------- similar code ------------------ index = 446, score = 6.0 
def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):
    super(ResNet, self).__init__()
    self.inplanes = 64
    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = nn.BatchNorm2d(64)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
    self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=2)
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)
            elif isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 80:------------------- similar code ------------------ index = 609, score = 6.0 
def _create_fcnn(input_size, hidden_size, output_size, activation_function, dropout=0, final_gain=1.0):
    assert (activation_function in ACTIVATION_FUNCTIONS.keys())
    (network_dims, layers) = ((input_size, hidden_size, hidden_size), [])
    for l in range((len(network_dims) - 1)):
        layer = nn.Linear(network_dims[l], network_dims[(l + 1)])
        nn.init.orthogonal_(layer.weight, gain=nn.init.calculate_gain(activation_function))
        nn.init.constant_(layer.bias, 0)
        layers.append(layer)
        if (dropout > 0):
            layers.append(nn.Dropout(p=dropout))
        layers.append(ACTIVATION_FUNCTIONS[activation_function]())
    final_layer = nn.Linear(network_dims[(- 1)], output_size)
    nn.init.orthogonal_(final_layer.weight, gain=final_gain)
    nn.init.constant_(final_layer.bias, 0)
    layers.append(final_layer)
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ...  = nn.Linear

idx = 81:------------------- similar code ------------------ index = 214, score = 6.0 
def __init__(self, n_head, d_k, d_in):
    super().__init__()
    self.n_head = n_head
    self.d_k = d_k
    self.d_in = d_in
    self.fc1_q = nn.Linear(d_in, (n_head * d_k))
    nn.init.normal_(self.fc1_q.weight, mean=0, std=np.sqrt((2.0 / d_k)))
    self.fc1_k = nn.Linear(d_in, (n_head * d_k))
    nn.init.normal_(self.fc1_k.weight, mean=0, std=np.sqrt((2.0 / d_k)))
    self.fc2 = nn.Sequential(nn.BatchNorm1d((n_head * d_k)), nn.Linear((n_head * d_k), (n_head * d_k)))
    self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 82:------------------- similar code ------------------ index = 640, score = 6.0 
def __init__(self, dim_in, dim_out, n_layers):
    nn.Module.__init__(self)
    self.dim_in = dim_in
    self.dim_out = dim_out
    self.layers = nn.ModuleList([])
    for i in range(n_layers):
        self.layers.append(nn.Linear(dim_in, dim_out))
        dim_in = dim_out

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
         ... . ... (nn.Linear)

idx = 83:------------------- similar code ------------------ index = 385, score = 6.0 
def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None):
    super(ResNet, self).__init__()
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    self._norm_layer = norm_layer
    self.inplanes = 64
    self.dilation = 1
    if (replace_stride_with_dilation is None):
        replace_stride_with_dilation = [False, False, False]
    if (len(replace_stride_with_dilation) != 3):
        raise ValueError('replace_stride_with_dilation should be None or a 3-element tuple, got {}'.format(replace_stride_with_dilation))
    self.groups = groups
    self.base_width = width_per_group
    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = norm_layer(self.inplanes)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)
            elif isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 84:------------------- similar code ------------------ index = 683, score = 6.0 
def make_cls(hidden_dim):
    layers = []
    for i in range(2):
        layers += [nn.Dropout(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]
    return nn.Sequential(*layers)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    for  ...  in:
         ...  += [, nn.Linear,]

idx = 85:------------------- similar code ------------------ index = 144, score = 6.0 
def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, padding='ZeroPad'):
    super(ResNet, self).__init__()
    self.padding = getattr(Utils.CubePad, padding)
    self.inplanes = 64
    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=0, bias=False)
    self.bn1 = nn.BatchNorm2d(64)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)
    self.layer1 = self._make_layer(block, 64, layers[0], padding=self.padding)
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, padding=self.padding)
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, padding=self.padding)
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, padding=self.padding)
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)
            elif isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 86:------------------- similar code ------------------ index = 679, score = 6.0 
def __init__(self, num_inputs, num_outputs, action_space, squash_mean=False, squash_distribution=False, initial_stddev=(1 / 3.0), min_stddev=0.0, stddev_transform=torch.nn.functional.softplus):
    super().__init__()
    self.fc_mean = nn.Linear(num_inputs, num_outputs)
    self.action_space_mean = torch.nn.Parameter(torch.tensor(((action_space.low + action_space.high) / 2.0), dtype=torch.float), requires_grad=False)
    self.action_space_magnitude = torch.nn.Parameter(torch.tensor(((action_space.high - action_space.low) / 2.0), dtype=torch.float), requires_grad=False)
    self.squash_mean = squash_mean
    self.squash_distribution = squash_distribution
    initial_stddev_before_transform = torch.tensor(initial_stddev, dtype=torch.float)
    if (stddev_transform == torch.exp):
        initial_stddev_before_transform = torch.log(initial_stddev_before_transform)
    elif (stddev_transform == torch.nn.functional.softplus):
        initial_stddev_before_transform = torch.log((torch.exp(initial_stddev_before_transform) - 1.0))
    else:
        assert False, 'unknown stddev transform function'
    self.stddev_before_transform = AddBias((torch.ones(num_outputs) * initial_stddev_before_transform))
    self.stddev_transform = stddev_transform
    min_stddev = (torch.ones(num_outputs) * torch.tensor(min_stddev, dtype=torch.float))
    self.min_stddev = torch.nn.Parameter(min_stddev, requires_grad=False)
    nn.init.orthogonal_(self.fc_mean.weight, gain=0.5)
    nn.init.constant_(self.fc_mean.bias, 0.0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 87:------------------- similar code ------------------ index = 667, score = 6.0 
def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, norm_layer=None, filter_size=1, pool_only=True):
    super(ResNet, self).__init__()
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    planes = [int(((width_per_group * groups) * (2 ** i))) for i in range(4)]
    self.inplanes = planes[0]
    if pool_only:
        self.conv1 = nn.Conv2d(3, planes[0], kernel_size=7, stride=2, padding=6, bias=False)
    else:
        self.conv1 = nn.Conv2d(3, planes[0], kernel_size=7, stride=1, padding=6, bias=False)
    self.bn1 = norm_layer(planes[0])
    self.relu = nn.ReLU(inplace=True)
    if pool_only:
        self.maxpool = nn.Sequential(*[nn.MaxPool2d(kernel_size=2, stride=1), Downsample(filt_size=filter_size, stride=2, channels=planes[0])])
    else:
        self.maxpool = nn.Sequential(*[Downsample(filt_size=filter_size, stride=2, channels=planes[0]), nn.MaxPool2d(kernel_size=2, stride=1), Downsample(filt_size=filter_size, stride=2, channels=planes[0])])
    self.layer1 = self._make_layer(block, planes[0], layers[0], groups=groups, norm_layer=norm_layer)
    self.layer2 = self._make_layer(block, planes[1], layers[1], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)
    self.layer3 = self._make_layer(block, planes[2], layers[2], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)
    self.layer4 = self._make_layer(block, planes[3], layers[3], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((planes[3] * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            if ((m.in_channels != m.out_channels) or (m.out_channels != m.groups) or (m.bias is not None)):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            else:
                print('Not initializing')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)
            elif isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 88:------------------- similar code ------------------ index = 420, score = 6.0 
def __init__(self, block, layers, in_channels=3, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None):
    super(ResNet, self).__init__()
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    self._norm_layer = norm_layer
    self.inplanes = 64
    self.dilation = 1
    if (replace_stride_with_dilation is None):
        replace_stride_with_dilation = [False, False, False]
    if (len(replace_stride_with_dilation) != 3):
        raise ValueError('replace_stride_with_dilation should be None or a 3-element tuple, got {}'.format(replace_stride_with_dilation))
    self.groups = groups
    self.base_width = width_per_group
    self.conv1 = nn.Conv2d(in_channels, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = norm_layer(self.inplanes)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)
            elif isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 89:------------------- similar code ------------------ index = 386, score = 6.0 
def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):
    super(ResNet, self).__init__()
    self.inplanes = 64
    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = nn.BatchNorm2d(64)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
    self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=2)
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)
            elif isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 90:------------------- similar code ------------------ index = 478, score = 6.0 
def __init__(self, dim_inputs, dim_context=0):
    super().__init__()
    self.init = False
    self.dim_inputs = dim_inputs
    self.log_scale = nn.Parameter(torch.zeros(dim_inputs))
    self.shift = nn.Parameter(torch.zeros(dim_inputs))
    if (dim_context > 0):
        self.linear = nn.Linear(dim_context, (2 * dim_inputs))
        nn.init.uniform_(self.linear.weight, a=(- 0.001), b=0.001)
        nn.init.constant_(self.linear.bias, 0)
    else:
        self.linear = None

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
 = nn.Linear

idx = 91:------------------- similar code ------------------ index = 224, score = 6.0 
def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, norm_layer=None, dropout_prob0=0.0):
    super(PyConvResNet, self).__init__()
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    self.inplanes = 64
    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = norm_layer(64)
    self.relu = nn.ReLU(inplace=True)
    self.layer1 = self._make_layer(block, 64, layers[0], stride=2, norm_layer=norm_layer, pyconv_kernels=[3, 5, 7, 9], pyconv_groups=[1, 4, 8, 16])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer, pyconv_kernels=[3, 5, 7], pyconv_groups=[1, 4, 8])
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer, pyconv_kernels=[3, 5], pyconv_groups=[1, 4])
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer, pyconv_kernels=[3], pyconv_groups=[1])
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    if (dropout_prob0 > 0.0):
        self.dp = nn.Dropout(dropout_prob0, inplace=True)
        print('Using Dropout with the prob to set to 0 of: ', dropout_prob0)
    else:
        self.dp = None
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, PyConvBlock):
                nn.init.constant_(m.bn3.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 92:------------------- similar code ------------------ index = 314, score = 6.0 
def __init__(self, n_head, d_k, d_in):
    super().__init__()
    self.n_head = n_head
    self.d_k = d_k
    self.d_in = d_in
    self.Q = nn.Parameter(torch.zeros((n_head, d_k))).requires_grad_(True)
    nn.init.normal_(self.Q, mean=0, std=np.sqrt((2.0 / d_k)))
    self.fc1_k = nn.Linear(d_in, (n_head * d_k))
    nn.init.normal_(self.fc1_k.weight, mean=0, std=np.sqrt((2.0 / d_k)))
    self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 93:------------------- similar code ------------------ index = 515, score = 6.0 
def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None):
    super(ResNet, self).__init__()
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    self._norm_layer = norm_layer
    self.inplanes = 64
    self.dilation = 1
    if (replace_stride_with_dilation is None):
        replace_stride_with_dilation = [False, False, False]
    if (len(replace_stride_with_dilation) != 3):
        raise ValueError('replace_stride_with_dilation should be None or a 3-element tuple, got {}'.format(replace_stride_with_dilation))
    self.groups = groups
    self.base_width = width_per_group
    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = norm_layer(self.inplanes)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)
            elif isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 94:------------------- similar code ------------------ index = 311, score = 6.0 
def __init__(self, bert_model: Union[(str, AutoModel)], dropout: float=0.0, trainable: bool=True, sample_train_type='lambdaloss', sample_n=1, sample_context='ck', top_k_chunks=3, chunk_size=50, overlap=7, padding_idx: int=0) -> None:
    super().__init__()
    if isinstance(bert_model, str):
        self.bert_model = AutoModel.from_pretrained(bert_model)
    else:
        self.bert_model = bert_model
    for p in self.bert_model.parameters():
        p.requires_grad = trainable
    self._classification_layer = torch.nn.Linear(self.bert_model.config.hidden_size, 1)
    self.top_k_chunks = top_k_chunks
    self.top_k_scoring = nn.Parameter(torch.full([1, self.top_k_chunks], 1, dtype=torch.float32, requires_grad=True))
    self.padding_idx = padding_idx
    self.chunk_size = chunk_size
    self.overlap = overlap
    self.extended_chunk_size = (self.chunk_size + (2 * self.overlap))
    self.sample_train_type = sample_train_type
    self.sample_n = sample_n
    self.sample_context = sample_context
    if (self.sample_context == 'ck'):
        i = 3
        self.sample_cnn3 = nn.Sequential(nn.ConstantPad1d((0, (i - 1)), 0), nn.Conv1d(kernel_size=i, in_channels=self.bert_model.config.dim, out_channels=self.bert_model.config.dim), nn.ReLU())
    elif (self.sample_context == 'ck-small'):
        i = 3
        self.sample_projector = nn.Linear(self.bert_model.config.dim, 384)
        self.sample_cnn3 = nn.Sequential(nn.ConstantPad1d((0, (i - 1)), 0), nn.Conv1d(kernel_size=i, in_channels=384, out_channels=128), nn.ReLU())
    elif (self.sample_context == 'tk'):
        self.tk_projector = nn.Linear(self.bert_model.config.dim, 384)
        encoder_layer = nn.TransformerEncoderLayer(384, 8, dim_feedforward=384, dropout=0)
        self.tk_contextualizer = nn.TransformerEncoder(encoder_layer, 1, norm=None)
        self.tK_mixer = nn.Parameter(torch.full([1], 0.5, dtype=torch.float32, requires_grad=True))
    self.sampling_binweights = nn.Linear(11, 1, bias=True)
    torch.nn.init.uniform_(self.sampling_binweights.weight, (- 0.01), 0.01)
    self.kernel_alpha_scaler = nn.Parameter(torch.full([1, 1, 11], 1, dtype=torch.float32, requires_grad=True))
    self.register_buffer('mu', nn.Parameter(torch.tensor([1.0, 0.9, 0.7, 0.5, 0.3, 0.1, (- 0.1), (- 0.3), (- 0.5), (- 0.7), (- 0.9)]), requires_grad=False).view(1, 1, 1, (- 1)))
    self.register_buffer('sigma', nn.Parameter(torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]), requires_grad=False).view(1, 1, 1, (- 1)))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... () -> None:
    if:    elif:
 = nn.Linear

idx = 95:------------------- similar code ------------------ index = 512, score = 6.0 
def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None):
    super(ResNet, self).__init__()
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    self._norm_layer = norm_layer
    self.inplanes = 64
    self.dilation = 1
    if (replace_stride_with_dilation is None):
        replace_stride_with_dilation = [False, False, False]
    if (len(replace_stride_with_dilation) != 3):
        raise ValueError('replace_stride_with_dilation should be None or a 3-element tuple, got {}'.format(replace_stride_with_dilation))
    self.groups = groups
    self.base_width = width_per_group
    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = norm_layer(self.inplanes)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)
            elif isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 96:------------------- similar code ------------------ index = 511, score = 6.0 
def __init__(self, n_kernels: int):
    super(KNRM, self).__init__()
    self.mu = Variable(torch.cuda.FloatTensor(self.kernel_mus(n_kernels)), requires_grad=False).view(1, 1, 1, n_kernels)
    self.sigma = Variable(torch.cuda.FloatTensor(self.kernel_sigmas(n_kernels)), requires_grad=False).view(1, 1, 1, n_kernels)
    self.cosine_module = CosineMatrixAttention()
    self.dense = nn.Linear(n_kernels, 1, bias=False)
    torch.nn.init.uniform_(self.dense.weight, (- 0.014), 0.014)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 97:------------------- similar code ------------------ index = 313, score = 6.0 
def __init__(self, num_person=2, device=None):
    self.device = device
    super(Audio_Visual_Fusion, self).__init__()
    self.num_person = num_person
    self.input_dim = ((8 * 257) + (256 * self.num_person))
    self.audio_output = Audio_Model()
    self.video_output = Video_Model()
    self.lstm = nn.LSTM(self.input_dim, 400, num_layers=1, bias=True, batch_first=True, bidirectional=True)
    self.fc1 = nn.Linear(400, 600)
    torch.nn.init.xavier_uniform_(self.fc1.weight)
    self.fc2 = nn.Linear(600, 600)
    torch.nn.init.xavier_uniform_(self.fc2.weight)
    self.fc3 = nn.Linear(600, 600)
    torch.nn.init.xavier_uniform_(self.fc3.weight)
    self.complex_mask_layer = nn.Linear(600, ((2 * 257) * self.num_person))
    torch.nn.init.xavier_uniform_(self.complex_mask_layer.weight)
    self.drop1 = nn.Dropout(0.2)
    self.drop2 = nn.Dropout(0.2)
    self.drop3 = nn.Dropout(0.2)
    self.batch_norm1 = nn.BatchNorm1d(298)
    self.batch_norm2 = nn.BatchNorm1d(298)
    self.batch_norm3 = nn.BatchNorm1d(298)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 98:------------------- similar code ------------------ index = 360, score = 6.0 
def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, norm_layer=None):
    super(OctResNet, self).__init__()
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    self.inplanes = 64
    self.groups = groups
    self.base_width = width_per_group
    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = norm_layer(self.inplanes)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, alpha_in=0)
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer, alpha_out=0, output=True)
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

idx = 99:------------------- similar code ------------------ index = 470, score = 6.0 
def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None):
    super(ResNet, self).__init__()
    if (norm_layer is None):
        norm_layer = nn.BatchNorm2d
    self._norm_layer = norm_layer
    self.inplanes = 64
    self.dilation = 1
    if (replace_stride_with_dilation is None):
        replace_stride_with_dilation = [False, False, False]
    if (len(replace_stride_with_dilation) != 3):
        raise ValueError('replace_stride_with_dilation should be None or a 3-element tuple, got {}'.format(replace_stride_with_dilation))
    self.groups = groups
    self.base_width = width_per_group
    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=6, bias=False)
    self.bn1 = norm_layer(self.inplanes)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])
    self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear((512 * block.expansion), num_classes)
    self.zeropad = nn.ZeroPad2d(2)
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    if zero_init_residual:
        for m in self.modules():
            if isinstance(m, Bottleneck):
                nn.init.constant_(m.bn3.weight, 0)
            elif isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
 = nn.Linear

