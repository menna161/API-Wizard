------------------------- example 1 ------------------------ 
def draw_predict(fig_id=None, y_test=None, y_pred=None, filename=None, pathsave=None):
    plt.figure(fig_id)
    plt.plot(y_test)
    plt.plot(y_pred)
    plt.ylabel('CPU')
    plt.xlabel('Timestamp')
    plt.legend(['Actual', 'Predict'], loc='upper right')
    plt.savefig(((pathsave + filename) + '.png'))
    plt.close()
    return None

------------------------- example 2 ------------------------ 
def visualize_openset_classification(data, other_data_dicts, dict_key, data_name, thresholds, save_path, tailsize):
    "\n    Visualization of percentage of datasets considered as statistical outliers evaluated for different\n    Weibull CDF rejection priors.\n\n    Parameters:\n        data (list): Dataset outlier percentages per rejection prior value for the trained dataset's validation set.\n        other_data_dicts (dictionary of dictionaries):\n            Dataset outlier percentages per rejection prior value for an unseen dataset.\n        dict_key (str): Dictionary key of the values to visualize\n        data_name (str): Original trained dataset's name.\n        thresholds (list): List of integers with rejection prior values.\n        save_path (str): Saving path.\n        tailsize (int): Weibull model's tailsize.\n    "
    lw = 10
    plt.figure(figsize=(20, 20))
    plt.plot(thresholds, data, label=data_name, color=colors[0], linestyle='solid', linewidth=lw)
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        plt.plot(thresholds, other_data_dict[dict_key], label=other_data_name, color=colors[c], linestyle=linestyles[(c % len(linestyles))], linewidth=lw)
        c += 1
    plt.xlabel('Weibull CDF outlier rejection prior $\\Omega_t$', fontsize=axes_font_size)
    plt.ylabel('Percentage of dataset outliers', fontsize=axes_font_size)
    plt.xlim(left=(- 0.05), right=1.05)
    plt.ylim(bottom=(- 0.05), top=1.05)
    plt.legend(loc=0, fontsize=(legend_font_size - 15))
    plt.savefig(os.path.join(save_path, ((((((data_name + '_') + ','.join(list(other_data_dicts.keys()))) + '_outlier_classification') + '_tailsize_') + str(tailsize)) + '.pdf')), bbox_inches='tight')

------------------------- example 3 ------------------------ 
def plot_xo_2stage_decoder_kind(extension):
    attn_yolo_path = os.path.join(data_dir, 'xo/pretrained/yolo/run_search_yolo-xo-continue_env=xo_decoder-kind=attn_alg=yolo-xo-continue_duration=long_seed=0_2018_06_07_12_12_19')
    mlp_yolo_path = os.path.join(data_dir, 'xo/pretrained/yolo/run_search_yolo-xo-continue_env=xo_decoder-kind=mlp_alg=yolo-xo-continue_duration=long_seed=0_2018_06_07_12_17_35')
    seq_yolo_path = os.path.join(data_dir, 'xo/pretrained/yolo/run_search_yolo-xo-continue_env=xo_decoder-kind=seq_alg=yolo-xo-continue_duration=long_seed=0_2018_06_07_12_13_03')
    attn_simple_path = os.path.join(data_dir, 'xo/full/simple/run_search_conv-xo_env=xo_decoder-kind=attn_alg=simple-xo_duration=long_seed=0_2018_06_08_17_49_36')
    mlp_simple_path = os.path.join(data_dir, 'xo/full/simple/run_search_conv-xo_env=xo_decoder-kind=mlp_alg=simple-xo_duration=long_seed=0_2018_06_08_17_50_40')
    seq_simple_path = os.path.join(data_dir, 'xo/full/simple/run_search_conv-xo_env=xo_decoder-kind=seq_alg=simple-xo_duration=long_seed=0_2018_06_08_17_50_20')
    plt.figure(figsize=(5, 3.5))
    ax = plt.gca()
    measure = 'math_accuracy'
    (x, y, *yerr) = get_arithmetic_data([attn_yolo_path], 'n_train', measure, 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='attn-yolo')
    attn_colour = line.lines[0].get_c()
    (x, y, *yerr) = get_arithmetic_data([mlp_yolo_path], 'n_train', measure, 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='mlp-yolo')
    mlp_colour = line.lines[0].get_c()
    (x, y, *yerr) = get_arithmetic_data([seq_yolo_path], 'n_train', measure, 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='seq-yolo')
    seq_colour = line.lines[0].get_c()
    measure = 'math_accuracy'
    (x, y, *yerr) = get_arithmetic_data([attn_simple_path], 'n_train', measure, 0, 'ci95')
    ax.errorbar(x, y, yerr=yerr, label='attn-simple', c=attn_colour, ls='--')
    (x, y, *yerr) = get_arithmetic_data([mlp_simple_path], 'n_train', measure, 0, 'ci95')
    ax.errorbar(x, y, yerr=yerr, label='mlp-simple', c=mlp_colour, ls='--')
    (x, y, *yerr) = get_arithmetic_data([seq_simple_path], 'n_train', measure, 0, 'ci95')
    ax.errorbar(x, y, yerr=yerr, label='seq-simple', c=seq_colour, ls='--')
    ax.set_ylabel('Accuracy', fontsize=12)
    ax.set_xlabel('\\# Training Samples', fontsize=12)
    ax.tick_params(axis='both', labelsize=14)
    ax.set_ylim((0.0, 1.05))
    ax.set_xticks(x)
    plt.legend()
    plt.show()

------------------------- example 4 ------------------------ 
def plot_action_mask(plot_env, meta_actor_critic, hidden_size, device):
    meta_recurrent_hidden_states = torch.zeros(1, hidden_size, device=device)
    masks = torch.zeros(1, 1, device=device)
    base_heat_map = np.zeros((plot_env.height, plot_env.width))
    arm_heat_map = np.zeros((plot_env.height, plot_env.width))
    plot_env.reset()
    plot_env.agent_orientation = 3
    plot_env.target_pos = np.array([3, 5])
    for col in range(1, (plot_env.width - 1)):
        if (col == (plot_env.width // your code ... 2)):
            continue
        if (col > (plot_env.width // your code ... 2)):
            plot_env.door_state = plot_env.door_max_state
        for row in range(1, (plot_env.height - 1)):
            plot_env.agent_pos = np.array([row, col])
            observations = [plot_env.get_state()]
            batch = batch_obs(observations)
            for sensor in batch:
                batch[sensor] = batch[sensor].to(device)
            with torch.no_grad():
                (_, _, _, _, _, _, action_mask_probs) = meta_actor_critic.act(batch, meta_recurrent_hidden_states, masks)
            base_heat_map[(row, col)] = (action_mask_probs[0][0].item() + action_mask_probs[0][2].item())
            arm_heat_map[(row, col)] = (action_mask_probs[0][1].item() + action_mask_probs[0][2].item())
            print(row, col, action_mask_probs)
    print(arm_heat_map)
    plt.figure(0)
    plt.imshow(base_heat_map, cmap='hot', interpolation='nearest')
    plt.figure(1)
    plt.imshow(arm_heat_map, cmap='hot', interpolation='nearest')
    plt.show()
    assert False

------------------------- example 5 ------------------------ 
def show(image):
    plt.figure(figsize=(15, 15))
    plt.imshow(image, interpolation='nearest')
    plt.show()

examples  ||  representativeness  ||  number of lines  || number of comments   ||  relevancy  
example1  ||          2           ||        10         ||         0        ||        0.3         
example2  ||          2           ||        15         ||         0        ||        0.2         
example3  ||          2           ||        33         ||         0        ||        0.09090909090909091         
example4  ||          2           ||        29         ||         2        ||        0.06896551724137931         
example5  ||          2           ||        4         ||         0        ||        0.25         

avg       ||          1.36986301369863           ||        18.2         ||         0.4        ||         18.197492163009404        

idx = 0:------------------- similar code ------------------ index = 0, score = 7.0 
def plot_order(generation_idx, obs, out_path=None):
    'Plot generation coordinate list. A star on the curve\n    denotes the pixel generated last. obs is a three-tuple of input image dimensions,\n    (input-channels-unused, num_rows, num_cols)'
    plt.figure(figsize=(3, 3))
    plt.hlines((np.arange((- 1), obs[1]) + 0.5), xmin=(- 0.5), xmax=(obs[2] - 0.5), alpha=0.5)
    plt.vlines((np.arange((- 1), obs[2]) + 0.5), ymin=(- 0.5), ymax=(obs[1] - 0.5), alpha=0.5)
    (rows, cols) = zip(*generation_idx)
    plt.plot(cols, rows, color='r')
    plt.scatter([cols[(- 1)]], [rows[(- 1)]], marker='*', s=100, c='k')
    plt.xticks(np.arange(obs[1]))
    plt.axis('equal')
    plt.gca().invert_yaxis()
    if out_path:
        plt.savefig(out_path)
    else:
        plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 1:------------------- similar code ------------------ index = 16, score = 7.0 
def draw_predict(fig_id=None, y_test=None, y_pred=None, filename=None, pathsave=None):
    plt.figure(fig_id)
    plt.plot(y_test)
    plt.plot(y_pred)
    plt.ylabel('CPU')
    plt.xlabel('Timestamp')
    plt.legend(['Actual', 'Predict'], loc='upper right')
    plt.savefig(((pathsave + filename) + '.png'))
    plt.close()
    return None

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure
    return None

idx = 2:------------------- similar code ------------------ index = 69, score = 7.0 
def visualize_openset_classification(data, other_data_dicts, dict_key, data_name, thresholds, save_path, tailsize):
    "\n    Visualization of percentage of datasets considered as statistical outliers evaluated for different\n    Weibull CDF rejection priors.\n\n    Parameters:\n        data (list): Dataset outlier percentages per rejection prior value for the trained dataset's validation set.\n        other_data_dicts (dictionary of dictionaries):\n            Dataset outlier percentages per rejection prior value for an unseen dataset.\n        dict_key (str): Dictionary key of the values to visualize\n        data_name (str): Original trained dataset's name.\n        thresholds (list): List of integers with rejection prior values.\n        save_path (str): Saving path.\n        tailsize (int): Weibull model's tailsize.\n    "
    lw = 10
    plt.figure(figsize=(20, 20))
    plt.plot(thresholds, data, label=data_name, color=colors[0], linestyle='solid', linewidth=lw)
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        plt.plot(thresholds, other_data_dict[dict_key], label=other_data_name, color=colors[c], linestyle=linestyles[(c % len(linestyles))], linewidth=lw)
        c += 1
    plt.xlabel('Weibull CDF outlier rejection prior $\\Omega_t$', fontsize=axes_font_size)
    plt.ylabel('Percentage of dataset outliers', fontsize=axes_font_size)
    plt.xlim(left=(- 0.05), right=1.05)
    plt.ylim(bottom=(- 0.05), top=1.05)
    plt.legend(loc=0, fontsize=(legend_font_size - 15))
    plt.savefig(os.path.join(save_path, ((((((data_name + '_') + ','.join(list(other_data_dicts.keys()))) + '_outlier_classification') + '_tailsize_') + str(tailsize)) + '.pdf')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 3:------------------- similar code ------------------ index = 32, score = 7.0 
def plot_image_grid(images_np, nrow=8, factor=1, interpolation='lanczos'):
    'Draws images in a grid\n    \n    Args:\n        images_np: list of images, each image is np.array of size 3xHxW of 1xHxW\n        nrow: how many images will be in one row\n        factor: size if the plt.figure \n        interpolation: interpolation used in plt.imshow\n    '
    n_channels = max((x.shape[0] for x in images_np))
    assert ((n_channels == 3) or (n_channels == 1)), 'images should have 1 or 3 channels'
    images_np = [(x if (x.shape[0] == n_channels) else np.concatenate([x, x, x], axis=0)) for x in images_np]
    grid = get_image_grid(images_np, nrow)
    plt.figure(figsize=((len(images_np) + factor), (12 + factor)))
    if (images_np[0].shape[0] == 1):
        plt.imshow(grid[0], cmap='gray', interpolation=interpolation)
    else:
        plt.imshow(grid.transpose(1, 2, 0), interpolation=interpolation)
    plt.show()
    return grid

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 4:------------------- similar code ------------------ index = 19, score = 7.0 
def plot_xo_2stage_decoder_kind(extension):
    attn_yolo_path = os.path.join(data_dir, 'xo/pretrained/yolo/run_search_yolo-xo-continue_env=xo_decoder-kind=attn_alg=yolo-xo-continue_duration=long_seed=0_2018_06_07_12_12_19')
    mlp_yolo_path = os.path.join(data_dir, 'xo/pretrained/yolo/run_search_yolo-xo-continue_env=xo_decoder-kind=mlp_alg=yolo-xo-continue_duration=long_seed=0_2018_06_07_12_17_35')
    seq_yolo_path = os.path.join(data_dir, 'xo/pretrained/yolo/run_search_yolo-xo-continue_env=xo_decoder-kind=seq_alg=yolo-xo-continue_duration=long_seed=0_2018_06_07_12_13_03')
    attn_simple_path = os.path.join(data_dir, 'xo/full/simple/run_search_conv-xo_env=xo_decoder-kind=attn_alg=simple-xo_duration=long_seed=0_2018_06_08_17_49_36')
    mlp_simple_path = os.path.join(data_dir, 'xo/full/simple/run_search_conv-xo_env=xo_decoder-kind=mlp_alg=simple-xo_duration=long_seed=0_2018_06_08_17_50_40')
    seq_simple_path = os.path.join(data_dir, 'xo/full/simple/run_search_conv-xo_env=xo_decoder-kind=seq_alg=simple-xo_duration=long_seed=0_2018_06_08_17_50_20')
    plt.figure(figsize=(5, 3.5))
    ax = plt.gca()
    measure = 'math_accuracy'
    (x, y, *yerr) = get_arithmetic_data([attn_yolo_path], 'n_train', measure, 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='attn-yolo')
    attn_colour = line.lines[0].get_c()
    (x, y, *yerr) = get_arithmetic_data([mlp_yolo_path], 'n_train', measure, 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='mlp-yolo')
    mlp_colour = line.lines[0].get_c()
    (x, y, *yerr) = get_arithmetic_data([seq_yolo_path], 'n_train', measure, 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='seq-yolo')
    seq_colour = line.lines[0].get_c()
    measure = 'math_accuracy'
    (x, y, *yerr) = get_arithmetic_data([attn_simple_path], 'n_train', measure, 0, 'ci95')
    ax.errorbar(x, y, yerr=yerr, label='attn-simple', c=attn_colour, ls='--')
    (x, y, *yerr) = get_arithmetic_data([mlp_simple_path], 'n_train', measure, 0, 'ci95')
    ax.errorbar(x, y, yerr=yerr, label='mlp-simple', c=mlp_colour, ls='--')
    (x, y, *yerr) = get_arithmetic_data([seq_simple_path], 'n_train', measure, 0, 'ci95')
    ax.errorbar(x, y, yerr=yerr, label='seq-simple', c=seq_colour, ls='--')
    ax.set_ylabel('Accuracy', fontsize=12)
    ax.set_xlabel('\\# Training Samples', fontsize=12)
    ax.tick_params(axis='both', labelsize=14)
    ax.set_ylim((0.0, 1.05))
    ax.set_xticks(x)
    plt.legend()
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ( ... ):
    plt.figure

idx = 5:------------------- similar code ------------------ index = 65, score = 7.0 
def plot_action_mask(plot_env, meta_actor_critic, hidden_size, device):
    meta_recurrent_hidden_states = torch.zeros(1, hidden_size, device=device)
    masks = torch.zeros(1, 1, device=device)
    base_heat_map = np.zeros((plot_env.height, plot_env.width))
    arm_heat_map = np.zeros((plot_env.height, plot_env.width))
    plot_env.reset()
    plot_env.agent_orientation = 3
    plot_env.target_pos = np.array([3, 5])
    for col in range(1, (plot_env.width - 1)):
        if (col == (plot_env.width // 2)):
            continue
        if (col > (plot_env.width // 2)):
            plot_env.door_state = plot_env.door_max_state
        for row in range(1, (plot_env.height - 1)):
            plot_env.agent_pos = np.array([row, col])
            observations = [plot_env.get_state()]
            batch = batch_obs(observations)
            for sensor in batch:
                batch[sensor] = batch[sensor].to(device)
            with torch.no_grad():
                (_, _, _, _, _, _, action_mask_probs) = meta_actor_critic.act(batch, meta_recurrent_hidden_states, masks)
            base_heat_map[(row, col)] = (action_mask_probs[0][0].item() + action_mask_probs[0][2].item())
            arm_heat_map[(row, col)] = (action_mask_probs[0][1].item() + action_mask_probs[0][2].item())
            print(row, col, action_mask_probs)
    print(arm_heat_map)
    plt.figure(0)
    plt.imshow(base_heat_map, cmap='hot', interpolation='nearest')
    plt.figure(1)
    plt.imshow(arm_heat_map, cmap='hot', interpolation='nearest')
    plt.show()
    assert False

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure
    assert False

idx = 6:------------------- similar code ------------------ index = 64, score = 7.0 
def test_phase(predictor, test, args):
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize, repeat=False, shuffle=False)
    chainer.serializers.load_npz(os.path.join(args.out, 'predictor.npz'), predictor)
    model = MCSampler(predictor, mc_iteration=args.mc_iteration, activation=[F.identity, F.exp], reduce_mean=None, reduce_var=None)
    if (args.gpu >= 0):
        chainer.backends.cuda.get_device_from_id(args.gpu).use()
        model.to_gpu()
    infer = Inferencer(test_iter, model, device=args.gpu)
    (pred, epistemic_uncert, aleatory_uncert, _) = infer.run()
    x = test.x.ravel()
    t = test.t.ravel()
    pred = pred.ravel()
    epistemic_uncert = epistemic_uncert.ravel()
    aleatory_uncert = aleatory_uncert.ravel()
    plt.rcParams['font.size'] = 18
    plt.figure(figsize=(13, 5))
    ax = sns.scatterplot(x=x, y=pred, color='blue', s=75)
    ax.errorbar(x, pred, yerr=epistemic_uncert, fmt='none', capsize=10, ecolor='gray', linewidth=1.5)
    ax.plot(x, t, color='red', linewidth=1.5)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_xlim((- 10), 10)
    ax.set_ylim((- 15), 15)
    plt.legend(['Ground-truth', 'Prediction', 'Epistemic uncertainty'])
    plt.title('Result on testing data set')
    plt.tight_layout()
    plt.savefig(os.path.join(args.out, 'eval_epistemic.png'))
    plt.close()
    plt.rcParams['font.size'] = 18
    plt.figure(figsize=(13, 5))
    ax = sns.scatterplot(x=x, y=pred, color='blue', s=75)
    ax.errorbar(x, pred, yerr=aleatory_uncert, fmt='none', capsize=10, ecolor='gray', linewidth=1.5)
    ax.plot(x, t, color='red', linewidth=1.5)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_xlim((- 10), 10)
    ax.set_ylim((- 15), 15)
    plt.legend(['Ground-truth', 'Prediction', 'Aleatoric uncertainty'])
    plt.title('Result on testing data set')
    plt.tight_layout()
    plt.savefig(os.path.join(args.out, 'eval_aleatoric.png'))
    plt.close()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 7:------------------- similar code ------------------ index = 63, score = 7.0 
def show(image):
    plt.figure(figsize=(15, 15))
    plt.imshow(image, interpolation='nearest')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ( ... ):
    plt.figure

idx = 8:------------------- similar code ------------------ index = 62, score = 7.0 
def plot_confusion_matrix(self, name):
    (cm, target_names) = self.confusion_matrix(name)
    df_cm = pd.DataFrame(cm, index=[i for i in target_names], columns=[i for i in target_names])
    plt.figure(figsize=(20, 20))
    ax = sn.heatmap(df_cm, annot=True, square=True, fmt='d', cmap='YlGnBu', mask=(cm == 0), linecolor='black', linewidths=0.01)
    ax.set_ylabel('True')
    ax.set_xlabel('Predicted')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 9:------------------- similar code ------------------ index = 61, score = 7.0 
if (__name__ == '__main__'):
    HOME = os.environ['HOME']
    rgbdir = (HOME + '/myDataset/NYU_v2/')
    depdir = (HOME + '/myDataset/NYU_v2/')
    trainrgb = '../datasets/nyu_path/train_rgb_12k.txt'
    traindep = '../datasets/nyu_path/train_depth_12k.txt'
    valrgb = '../datasets/nyu_path/valid_rgb.txt'
    valdep = '../datasets/nyu_path/valid_depth.txt'
    kwargs = {'min_depth': 0.72, 'max_depth': 10.0, 'flip': True, 'scale': True, 'rotate': True, 'jitter': True, 'crop': True}
    train_dataset = NYUDataset(rgbdir, depdir, trainrgb, traindep, mode='train', **kwargs)
    val_dataset = NYUDataset(rgbdir, depdir, valrgb, valdep, mode='val', **kwargs)
    trainloader = DataLoader(train_dataset, 20, shuffle=True, num_workers=4, pin_memory=True, drop_last=False)
    valloader = DataLoader(val_dataset, 20, shuffle=True, num_workers=4, pin_memory=True, drop_last=False)
    (image, label) = train_dataset[2000]
    image_npy = image.numpy().transpose(1, 2, 0)
    label_npy = label.numpy().squeeze()
    print(image.shape, label.shape)
    print(label.max())
    plt.figure()
    plt.subplot(1, 2, 1)
    plt.imshow(image_npy)
    plt.subplot(1, 2, 2)
    plt.imshow(label_npy, cmap='jet')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
if:
    plt.figure()

idx = 10:------------------- similar code ------------------ index = 60, score = 7.0 
def plot_relative_error(data_dir='heterogeneous_example_data'):
    if (data_dir[(- 1)] != '/'):
        data_dir += '/'
    data_filename = (data_dir + 'heterogeneous_example_error_data.json')
    data = json.load(open(data_filename))
    plotting_data = []
    for n in data.keys():
        for i in data[n].keys():
            d = {'Sample Size': int(n), 'Iteration': int(i)}
            true_effects = data[n][str(i)]['true_effects']
            estimated_effects = data[n][str(i)]['estimated_effects']
            error = (np.array(true_effects) - np.array(estimated_effects))
            relative_error = (np.linalg.norm(error) / np.linalg.norm(true_effects))
            d['Relative Error'] = relative_error
            plotting_data.append(d)
    plotting_df = pd.DataFrame(plotting_data)
    plt.figure(figsize=(18, 8))
    ax = plt.gca()
    grid = sns.lineplot(x='Sample Size', y='Relative Error', data=plotting_df, marker='o', ax=ax)
    sample_sizes = [int(n) for n in data.keys()]
    ax.set_xticks(sample_sizes)
    ax.set_xticklabels(([''] + sample_sizes[1:]), rotation=45)
    ax.set_xlim([0, (max(sample_sizes) + 100)])
    sns.despine()
    plt.title('Relative Error of Causal Forest')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 11:------------------- similar code ------------------ index = 57, score = 7.0 
def show_mask(mask):
    plt.figure(figsize=(10, 10))
    plt.imshow(mask, cmap='gray')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ( ... ):
    plt.figure

idx = 12:------------------- similar code ------------------ index = 56, score = 7.0 
def visualize_classification_scores(data, other_data_dicts, dict_key, data_name, save_path):
    "\n    Visualization of classification scores per dataset.\n\n    Parameters:\n        data (list): Classification scores.\n        other_data_dicts (dictionary of dictionaries): Dictionary of key-value pairs per dataset\n        dict_key (string): Dictionary key to plot\n        data_name (str): Original trained dataset's name.\n        save_path (str): Saving path.\n    "
    data = [y for x in data for y in x]
    plt.figure(figsize=(20, 20))
    plt.hist(data, label=data_name, alpha=1.0, bins=20, color=colors[0])
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        other_data = [y for x in other_data_dict[dict_key] for y in x]
        plt.hist(other_data, label=other_data_name, alpha=0.5, bins=20, color=colors[c])
        c += 1
    plt.title('Dataset classification', fontsize=title_font_size)
    plt.xlabel('Classification confidence', fontsize=axes_font_size)
    plt.ylabel('Number of images', fontsize=axes_font_size)
    plt.legend(loc=0)
    plt.xlim(left=(- 0.0), right=1.05)
    plt.savefig(os.path.join(save_path, (((data_name + '_') + ','.join(list(other_data_dicts.keys()))) + '_classification_scores.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 13:------------------- similar code ------------------ index = 55, score = 7.0 
def plot(self):
    plt.figure(figsize=(6, 6))
    plt.plot(self.results['precision'], self.results['recall'], '.')
    plt.xlabel('precision')
    plt.ylabel('recall')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ( ... ):
    plt.figure

idx = 14:------------------- similar code ------------------ index = 54, score = 7.0 
def draw_predict_with_error(fig_id=None, data=None, error=None, filename=None, pathsave=None):
    plt.figure(fig_id)
    plt.plot(data[0])
    plt.plot(data[1])
    plt.ylabel('Real value')
    plt.xlabel('Point')
    plt.legend([('Predict y... RMSE= ' + str(error[0])), ('Test y... MAE= ' + str(error[1]))], loc='upper right')
    plt.savefig(((pathsave + filename) + '.png'))
    plt.close()
    return None

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure
    return None

idx = 15:------------------- similar code ------------------ index = 53, score = 7.0 
def show(image):
    plt.figure(figsize=(15, 15))
    plt.imshow(image, interpolation='nearest')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ( ... ):
    plt.figure

idx = 16:------------------- similar code ------------------ index = 23, score = 7.0 
def plot_data(path):
    plt.figure()
    with open(path, 'r') as file:
        lines = file.readlines()
        for line in lines:
            if (line[0] in '0123456789'):
                num_list = [float(x) for x in line.split()]
                plot(num_list)
                plt.title('Performance')
                plt.xlabel('Epoch')
                plt.ylabel('Accuracy')
                plt.savefig(path.replace('.txt', '.jpg'))
            else:
                continue

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ( ... ):
    plt.figure()

idx = 17:------------------- similar code ------------------ index = 24, score = 7.0 
def plot_grid_multiple(ca_list, shape=None, slice=(- 1), titles=None, colormap='Greys', vmin=None, vmax=None):
    cmap = plt.get_cmap(colormap)
    for i in range(0, len(ca_list)):
        plt.figure(i)
        if (titles is not None):
            plt.title(titles[i])
        activities = list(ca_list[i])
        if (shape is not None):
            activities = np.array(activities).reshape((len(activities), shape[0], shape[1]))[slice]
        plt.imshow(activities, interpolation='none', cmap=cmap, vmin=vmin, vmax=vmax)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    for  ...  in:
        plt.figure

idx = 18:------------------- similar code ------------------ index = 25, score = 7.0 
def visualize_weibull_outlier_probabilities(data_outlier_probs, other_data_outlier_probs_dict, data_name, save_path, tailsize):
    "\n    Visualization of Weibull CDF outlier probabilites.\n\n    Parameters:\n        data_outlier_probs (np.array): Outlier probabilities for each input of the trained dataset's validation set.\n        other_data_outlier_probs_dict (dictionary): Outlier probabilities for each input of an unseen dataset.\n        data_name (str): Original trained dataset's name.\n        save_path (str): Saving path.\n        tailsize (int): Fitted Weibull model's tailsize.\n    "
    data_outlier_probs = np.concatenate(data_outlier_probs, axis=0)
    data_weights = (np.ones_like(data_outlier_probs) / float(len(data_outlier_probs)))
    plt.figure(figsize=(20, 20))
    plt.hist(data_outlier_probs, label=data_name, weights=data_weights, bins=50, color=colors[0], alpha=1.0, edgecolor='white', linewidth=5)
    c = 0
    for (other_data_name, other_data_outlier_probs) in other_data_outlier_probs_dict.items():
        other_data_outlier_probs = np.concatenate(other_data_outlier_probs, axis=0)
        other_data_weights = (np.ones_like(other_data_outlier_probs) / float(len(other_data_outlier_probs)))
        plt.hist(other_data_outlier_probs, label=other_data_name, weights=other_data_weights, bins=50, color=colors[c], alpha=0.5, edgecolor='white', linewidth=5)
        c += 1
    plt.title(('Outlier probabilities: tailsize ' + str(tailsize)), fontsize=title_font_size)
    plt.xlabel('Outlier probability according to Weibull CDF', fontsize=axes_font_size)
    plt.ylabel('Percentage', fontsize=axes_font_size)
    plt.xlim(left=(- 0.05), right=1.05)
    plt.ylim(bottom=(- 0.05), top=1.05)
    plt.legend(loc=0)
    plt.savefig(os.path.join(save_path, (((((data_name + '_') + ','.join(list(other_data_outlier_probs_dict.keys()))) + '_weibull_outlier_probabilities_tailsize_') + str(tailsize)) + '.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 19:------------------- similar code ------------------ index = 45, score = 7.0 
def test_phase(predictor, test, args):
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize, repeat=False, shuffle=False)
    chainer.serializers.load_npz(os.path.join(args.out, 'predictor.npz'), predictor)
    model = MCSampler(predictor, mc_iteration=args.mc_iteration, activation=partial(F.softmax, axis=1), reduce_mean=partial(F.argmax, axis=1), reduce_var=partial(F.mean, axis=1))
    if (args.gpu >= 0):
        chainer.backends.cuda.get_device_from_id(args.gpu).use()
        model.to_gpu()
    infer = Inferencer(test_iter, model, device=args.gpu)
    (pred, uncert) = infer.run()
    os.makedirs(args.out, exist_ok=True)
    match = (pred == test.labels)
    accuracy = (np.sum(match) / len(match))
    arr = [uncert[match], uncert[np.logical_not(match)]]
    plt.rcParams['font.size'] = 18
    plt.figure(figsize=(13, 5))
    ax = sns.violinplot(data=arr, inner='quartile', palette='Blues', orient='h', cut=0)
    ax.set_xlabel('Predicted variance')
    ax.set_yticklabels([('Correct prediction\n(n=%d)' % len(arr[0])), ('Wrong prediction\n(n=%d)' % len(arr[1]))])
    plt.title(('Accuracy=%.3f' % accuracy))
    plt.tight_layout()
    plt.savefig(os.path.join(args.out, 'eval.png'))
    plt.close()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 20:------------------- similar code ------------------ index = 27, score = 7.0 
def random_torque(driver, port, idn):
    " Read the entire control table and randomly sampled torque commands to the DXL.\n\n    This is done 'N' times and timed. Relevant data is plotted.\n    "
    dxl.write_torque_mode_enable(driver, port, idn, 1)
    times = []
    vals_dict = {'present_pos': ((2 * pi) / 3.0), 'current': 0}
    actions = []
    currents = []
    for i in range(1000):
        t1 = time.time()
        if (vals_dict['present_pos'] < (pi / 3.0)):
            action = 1000
            dxl.write_torque(driver, port, idn, action)
            time.sleep(0.001)
        elif (vals_dict['present_pos'] > pi):
            action = (- 1000)
            dxl.write_torque(driver, port, idn, action)
            time.sleep(0.001)
        else:
            action = int((np.random.uniform((- 1), 1) * 1000))
        dxl.write_torque(driver, port, idn, action)
        vals_dict = dxl.read_vals(driver, port, idn)
        actions.append(action)
        currents.append(vals_dict['current'])
        times.append((time.time() - t1))
    dxl.write_torque(driver, port, idn, 0)
    print(np.mean(times))
    print(currents[:10])
    plt.xcorr(currents, actions)
    plt.figure()
    plt.plot(np.cumsum(times), actions, label='actions')
    plt.plot(np.cumsum(times), currents, label='currents')
    plt.legend()
    plt.figure()
    plt.plot(times)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure()

idx = 21:------------------- similar code ------------------ index = 43, score = 7.0 
@staticmethod
def show_plot(h):
    plt.style.use('ggplot')
    plt.figure()
    plt.plot(np.arange(0, ep), h.history['loss'], label='train_loss')
    plt.plot(np.arange(0, ep), h.history['val_loss'], label='val_loss')
    plt.plot(np.arange(0, ep), h.history['acc'], label='acc')
    plt.plot(np.arange(0, ep), h.history['val_acc'], label='val_acc')
    plt.title('AMINJAMAL')
    plt.xlabel('Epoch #')
    plt.ylabel('Loss/ACC')
    plt.legend()
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ( ... ):
    plt.figure()

idx = 22:------------------- similar code ------------------ index = 42, score = 7.0 
def visualize_classification_scores(data, other_data_dicts, dict_key, data_name, save_path):
    "\n    Visualization of classification scores per dataset.\n\n    Parameters:\n        data (list): Classification scores.\n        other_data_dicts (dictionary of dictionaries): Dictionary of key-value pairs per dataset\n        dict_key (string): Dictionary key to plot\n        data_name (str): Original trained dataset's name.\n        save_path (str): Saving path.\n    "
    data = [y for x in data for y in x]
    plt.figure(figsize=(20, 20))
    plt.hist(data, label=data_name, alpha=1.0, bins=20, color=colors[0])
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        other_data = [y for x in other_data_dict[dict_key] for y in x]
        plt.hist(other_data, label=other_data_name, alpha=0.5, bins=20, color=colors[c])
        c += 1
    plt.title('Dataset classification', fontsize=title_font_size)
    plt.xlabel('Classification confidence', fontsize=axes_font_size)
    plt.ylabel('Number of images', fontsize=axes_font_size)
    plt.legend(loc=0)
    plt.xlim(left=(- 0.0), right=1.05)
    plt.savefig(os.path.join(save_path, (((data_name + '_') + ','.join(list(other_data_dicts.keys()))) + '_classification_scores.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 23:------------------- similar code ------------------ index = 28, score = 7.0 
if (__name__ == '__main__'):
    instruments_table = FlowCal.excel_ui.read_table(filename='experiment.xlsx', sheetname='Instruments', index_col='ID')
    beads_table = FlowCal.excel_ui.read_table(filename='experiment.xlsx', sheetname='Beads', index_col='ID')
    samples_table = FlowCal.excel_ui.read_table(filename='experiment.xlsx', sheetname='Samples', index_col='ID')
    (beads_samples, mef_transform_fxns) = FlowCal.excel_ui.process_beads_table(beads_table=beads_table, instruments_table=instruments_table, verbose=True, plot=True, plot_dir='plot_beads')
    samples = FlowCal.excel_ui.process_samples_table(samples_table=samples_table, instruments_table=instruments_table, mef_transform_fxns=mef_transform_fxns, verbose=True, plot=True, plot_dir='plot_samples')
    sample_ids = ['S00{:02}'.format(n) for n in range(1, (10 + 1))]
    dapg = samples_table.loc[(sample_ids, 'DAPG (uM)')]
    cmap = mpl.cm.get_cmap('gray_r')
    norm = mpl.colors.LogNorm(vmin=1.0, vmax=3500.0)
    colors = [cmap(norm((dapg_i + 4.0))) for dapg_i in dapg]
    plt.figure(figsize=(6, 3.5))
    FlowCal.plot.hist1d([samples[s_id] for s_id in sample_ids], channel='FL1', histtype='step', bins=128, edgecolor=colors)
    plt.ylim((0, 2500))
    plt.xlim((0, 50000.0))
    plt.xlabel('FL1  (Molecules of Equivalent Fluorescein, MEFL)')
    plt.legend(['{:.1f} $\\mu M$ DAPG'.format(i) for i in dapg], loc='upper left', fontsize='small')
    plt.tight_layout()
    plt.savefig('histograms.png', dpi=200)
    plt.close()
    samples_fluorescence = [FlowCal.stats.mean(samples[s_id], channels='FL1') for s_id in sample_ids]
    min_fluorescence = FlowCal.stats.mean(samples['min'], channels='FL1')
    max_fluorescence = FlowCal.stats.mean(samples['max'], channels='FL1')
    dapg_color = '#ffc400'
    plt.figure(figsize=(3, 3))
    plt.plot(dapg, samples_fluorescence, marker='o', color=dapg_color)
    plt.axhline(min_fluorescence, color='gray', linestyle='--', zorder=(- 1))
    plt.text(s='Min', x=200.0, y=160.0, ha='left', va='bottom', color='gray')
    plt.axhline(max_fluorescence, color='gray', linestyle='--', zorder=(- 1))
    plt.text(s='Max', x=(- 0.7), y=5200.0, ha='left', va='top', color='gray')
    plt.yscale('log')
    plt.ylim((50.0, 10000.0))
    plt.xscale('symlog')
    plt.xlim(((- 1.0), 1000.0))
    plt.xlabel('DAPG Concentration ($\\mu M$)')
    plt.ylabel('FL1 Fluorescence (MEFL)')
    plt.tight_layout()
    plt.savefig('dose_response.png', dpi=200)
    plt.close()

    def dapg_sensor_output(dapg_concentration):
        mn = 86.0
        mx = 3147.0
        K = 20.0
        n = 3.57
        if (dapg_concentration <= 0):
            return mn
        else:
            return (mn + ((mx - mn) / (1 + ((K / dapg_concentration) ** n))))
    autofluorescence = FlowCal.stats.mean(samples['min'], channels='FL1')

    def dapg_sensor_cellular_fluorescence(dapg_concentration):
        return (dapg_sensor_output(dapg_concentration) + autofluorescence)
    plt.figure(figsize=(4, 3.5))
    FlowCal.plot.violin_dose_response(data=[samples[s_id] for s_id in sample_ids], channel='FL1', positions=dapg, min_data=samples['min'], max_data=samples['max'], model_fxn=dapg_sensor_cellular_fluorescence, violin_kwargs={'facecolor': dapg_color, 'edgecolor': 'black'}, violin_width_to_span_fraction=0.075, xscale='log', yscale='log', ylim=(10.0, 30000.0), draw_model_kwargs={'color': 'gray', 'linewidth': 3, 'zorder': (- 1), 'solid_capstyle': 'butt'})
    plt.xlabel('DAPG Concentration ($\\mu M$)')
    plt.ylabel('FL1 Fluorescence (MEFL)')
    plt.tight_layout()
    plt.savefig('dose_response_violin.png', dpi=200)
    plt.close()
    print('\nDone.')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
if:
    plt.figure

idx = 24:------------------- similar code ------------------ index = 29, score = 7.0 
def visualize_weibull_outlier_probabilities(data_outlier_probs, other_data_outlier_probs_dict, data_name, save_path, tailsize):
    "\n    Visualization of Weibull CDF outlier probabilites.\n\n    Parameters:\n        data_outlier_probs (np.array): Outlier probabilities for each input of the trained dataset's validation set.\n        other_data_outlier_probs_dict (dictionary): Outlier probabilities for each input of an unseen dataset.\n        data_name (str): Original trained dataset's name.\n        save_path (str): Saving path.\n        tailsize (int): Fitted Weibull model's tailsize.\n    "
    data_outlier_probs = np.concatenate(data_outlier_probs, axis=0)
    data_weights = (np.ones_like(data_outlier_probs) / float(len(data_outlier_probs)))
    plt.figure(figsize=(20, 20))
    plt.hist(data_outlier_probs, label=data_name, weights=data_weights, bins=50, color=colors[0], alpha=1.0, edgecolor='white', linewidth=5)
    c = 0
    for (other_data_name, other_data_outlier_probs) in other_data_outlier_probs_dict.items():
        other_data_outlier_probs = np.concatenate(other_data_outlier_probs, axis=0)
        other_data_weights = (np.ones_like(other_data_outlier_probs) / float(len(other_data_outlier_probs)))
        plt.hist(other_data_outlier_probs, label=other_data_name, weights=other_data_weights, bins=50, color=colors[c], alpha=0.5, edgecolor='white', linewidth=5)
        c += 1
    plt.title(('Outlier probabilities: tailsize ' + str(tailsize)), fontsize=title_font_size)
    plt.xlabel('Outlier probability according to Weibull CDF', fontsize=axes_font_size)
    plt.ylabel('Percentage', fontsize=axes_font_size)
    plt.xlim(left=(- 0.05), right=1.05)
    plt.ylim(bottom=(- 0.05), top=1.05)
    plt.legend(loc=0)
    plt.savefig(os.path.join(save_path, (((((data_name + '_') + ','.join(list(other_data_outlier_probs_dict.keys()))) + '_weibull_outlier_probabilities_tailsize_') + str(tailsize)) + '.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 25:------------------- similar code ------------------ index = 39, score = 7.0 
def visualize_entropy_histogram(data, other_data_dicts, max_entropy, dict_key, data_name, save_path):
    "\n    Visualization of the entropy the datasets.\n\n    Parameters:\n        data (list):\n        other_data_dicts (dictionary of dictionaries): Dictionary of key-value pairs per dataset\n        dict_key (str): Dictionary key to plot\n        data_name (str): Original trained dataset's name.\n        save_path (str): Saving path.\n    "
    data = [x for x in data]
    plt.figure(figsize=(20, 20))
    plt.hist(data, label=data_name, alpha=1.0, bins=25, color=colors[0])
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        other_data = [x for x in other_data_dict[dict_key]]
        plt.hist(other_data, label=other_data_name, alpha=0.5, bins=25, color=colors[c])
        c += 1
    plt.title('Dataset classification entropy', fontsize=title_font_size)
    plt.xlabel('Classification entropy', fontsize=axes_font_size)
    plt.ylabel('Number of images', fontsize=axes_font_size)
    plt.legend(loc=0)
    plt.xlim(left=(- 0.0), right=max_entropy)
    plt.savefig(os.path.join(save_path, (((data_name + '_') + ','.join(list(other_data_dicts.keys()))) + '_classification_entropies.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 26:------------------- similar code ------------------ index = 38, score = 7.0 
def plot_action_mask(plot_env, meta_actor_critic, hidden_size, device):
    meta_recurrent_hidden_states = torch.zeros(1, hidden_size, device=device)
    masks = torch.zeros(1, 1, device=device)
    base_heat_map = np.zeros((plot_env.height, plot_env.width))
    arm_heat_map = np.zeros((plot_env.height, plot_env.width))
    plot_env.reset()
    plot_env.agent_orientation = 3
    plot_env.target_pos = np.array([3, 5])
    for col in range(1, (plot_env.width - 1)):
        if (col == (plot_env.width // 2)):
            continue
        if (col > (plot_env.width // 2)):
            plot_env.door_state = plot_env.door_max_state
        for row in range(1, (plot_env.height - 1)):
            plot_env.agent_pos = np.array([row, col])
            observations = [plot_env.get_state()]
            batch = batch_obs(observations)
            for sensor in batch:
                batch[sensor] = batch[sensor].to(device)
            with torch.no_grad():
                (_, _, _, _, _, _, action_mask_probs) = meta_actor_critic.act(batch, meta_recurrent_hidden_states, masks)
            base_heat_map[(row, col)] = (action_mask_probs[0][0].item() + action_mask_probs[0][2].item())
            arm_heat_map[(row, col)] = (action_mask_probs[0][1].item() + action_mask_probs[0][2].item())
            print(row, col, action_mask_probs)
    print(arm_heat_map)
    plt.figure(0)
    plt.imshow(base_heat_map, cmap='hot', interpolation='nearest')
    plt.figure(1)
    plt.imshow(arm_heat_map, cmap='hot', interpolation='nearest')
    plt.show()
    assert False

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure
    assert False

idx = 27:------------------- similar code ------------------ index = 31, score = 7.0 
if (__name__ == '__main__'):
    HOME = os.environ['HOME']
    rgbdir = (HOME + '/myDataset/KITTI/raw_data_KITTI/')
    depdir = (HOME + '/myDataset/KITTI/datasets_KITTI/')
    trainrgb = '../datasets/kitti_path/eigen_train_files.txt'
    traindep = '../datasets/kitti_path/eigen_train_depth_files.txt'
    valrgb = '../datasets/kitti_path/eigen_test_files.txt'
    valdep = '../datasets/kitti_path/eigen_test_depth_files.txt'
    kwargs = {'min_depth': 1.8, 'max_depth': 80.0, 'flip': True, 'scale': True, 'rotate': True, 'jitter': True, 'crop': True}
    train_dataset = KITTIDataset(rgbdir, depdir, trainrgb, traindep, mode='train', **kwargs)
    val_dataset = KITTIDataset(rgbdir, depdir, valrgb, valdep, mode='val', **kwargs)
    trainloader = DataLoader(train_dataset, 10, shuffle=True, num_workers=4, pin_memory=True, drop_last=False)
    valloader = DataLoader(val_dataset, 10, shuffle=True, num_workers=4, pin_memory=True, drop_last=False)
    (image, label) = train_dataset[400]
    image_npy = image.numpy().transpose(1, 2, 0)
    label_npy = label.numpy().squeeze()
    print(image.shape, label.shape)
    print(label.max())
    plt.figure()
    plt.subplot(1, 2, 1)
    plt.imshow(image_npy)
    plt.subplot(1, 2, 2)
    plt.imshow(label_npy, cmap='plasma')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
if:
    plt.figure()

idx = 28:------------------- similar code ------------------ index = 35, score = 7.0 
def plot_spec(M):
    M = np.flip(M, axis=0)
    plt.figure(figsize=(18, 4))
    plt.imshow(M, interpolation='nearest', aspect='auto')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ( ... ):
    plt.figure

idx = 29:------------------- similar code ------------------ index = 17, score = 7.0 
def train_phase(predictor, train, valid, args):
    plt.rcParams['font.size'] = 18
    plt.figure(figsize=(13, 5))
    ax = sns.scatterplot(x=train.x.ravel(), y=train.y.ravel(), color='blue', s=55, alpha=0.3)
    ax.plot(train.x.ravel(), train.t.ravel(), color='red', linewidth=2)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_xlim((- 10), 10)
    ax.set_ylim((- 15), 15)
    plt.legend(['Ground-truth', 'Observation'])
    plt.title('Training data set')
    plt.tight_layout()
    plt.savefig(os.path.join(args.out, 'train_dataset.png'))
    plt.close()
    train_iter = chainer.iterators.SerialIterator(train, args.batchsize, shuffle=True)
    valid_iter = chainer.iterators.SerialIterator(valid, args.batchsize, repeat=False, shuffle=False)
    lossfun = noised_mean_squared_error
    accfun = (lambda y, t: F.mean_absolute_error(y[0], t))
    model = Regressor(predictor, lossfun=lossfun, accfun=accfun)
    if (args.gpu >= 0):
        chainer.backends.cuda.get_device_from_id(args.gpu).use()
        model.to_gpu()
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)
    if (args.decay > 0):
        optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(args.decay))
    updater = training.updaters.StandardUpdater(train_iter, optimizer, device=args.gpu)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)
    trainer.extend(extensions.Evaluator(valid_iter, model, device=args.gpu))
    trainer.extend(extensions.dump_graph('main/loss'))
    frequency = (args.epoch if (args.frequency == (- 1)) else max(1, args.frequency))
    trainer.extend(extensions.snapshot(), trigger=(frequency, 'epoch'))
    trainer.extend(extensions.LogReport())
    if (args.plot and extensions.PlotReport.available()):
        trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], 'epoch', file_name='loss.png'))
        trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], 'epoch', file_name='accuracy.png'))
        trainer.extend(extensions.PlotReport(['main/predictor/sigma', 'validation/main/predictor/sigma'], 'epoch', file_name='sigma.png'))
    trainer.extend(extensions.PrintReport(['epoch', 'iteration', 'main/loss', 'validation/main/loss', 'main/accuracy', 'validation/main/accuracy', 'main/predictor/sigma', 'validation/main/predictor/sigma', 'elapsed_time']))
    trainer.extend(extensions.ProgressBar())
    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)
    trainer.run()
    chainer.serializers.save_npz(os.path.join(args.out, 'predictor.npz'), predictor)

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 30:------------------- similar code ------------------ index = 68, score = 7.0 
if (__name__ == '__main__'):
    import tensorflow as tf
    import matplotlib.pyplot as plt
    from astropy.utils.data import get_pkg_data_filename
    image_file = '/home/eric/Downloads/abell_2744_RGB.fits'
    image_data = fits.getdata(image_file, ext=0)
    plt.figure()
    plt.imshow(image_data, cmap='gray')
    plt.colorbar()
    plt.show()
    n = 32
    dset = FITSDataset(fits_file=image_file, postprocessing='tile_pad', n_samples_per_image=n, tile_shape=(1000, 1000), force_memmap=False)
    print(dset.depth)
    sess = tf.Session()
    with sess.as_default():
        dset.visualize(n)

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
if:
    plt.figure()

idx = 31:------------------- similar code ------------------ index = 15, score = 7.0 
def plot_learning_curve(self, estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(0.1, 1.0, 5)):
    print('Drawing curve, depending on your datasets size, this may take several minutes to several hours.')
    plt.figure()
    plt.title(title)
    if (ylim is not None):
        plt.ylim(*ylim)
    plt.xlabel('Training examples')
    plt.ylabel('Score')
    (train_sizes, train_scores, test_scores) = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()
    plt.fill_between(train_sizes, (train_scores_mean - train_scores_std), (train_scores_mean + train_scores_std), alpha=0.1, color='r')
    plt.fill_between(train_sizes, (test_scores_mean - test_scores_std), (test_scores_mean + test_scores_std), alpha=0.1, color='g')
    plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')
    plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')
    plt.legend(loc='best')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure()

idx = 32:------------------- similar code ------------------ index = 84, score = 7.0 
if (__name__ == '__main__'):
    if (not os.path.exists(beads_plot_dir)):
        os.makedirs(beads_plot_dir)
    if (not os.path.exists(samples_plot_dir)):
        os.makedirs(samples_plot_dir)
    print('\nProcessing calibration beads...')
    print('Loading file "{}"...'.format(beads_filename))
    beads_sample = FlowCal.io.FCSData(beads_filename)
    min_beads_sample = FlowCal.io.FCSData(min_beads_filename)
    max_beads_sample = FlowCal.io.FCSData(max_beads_filename)
    print('Performing data transformation...')
    beads_sample = FlowCal.transform.to_rfi(beads_sample)
    min_beads_sample = FlowCal.transform.to_rfi(min_beads_sample)
    max_beads_sample = FlowCal.transform.to_rfi(max_beads_sample)
    print('Performing gating...')
    beads_sample_gated = FlowCal.gate.start_end(beads_sample, num_start=250, num_end=100)
    min_beads_sample_gated = FlowCal.gate.start_end(min_beads_sample, num_start=250, num_end=100)
    max_beads_sample_gated = FlowCal.gate.start_end(max_beads_sample, num_start=250, num_end=100)
    beads_sample_gated = FlowCal.gate.high_low(beads_sample_gated, channels=['FSC', 'SSC'])
    min_beads_sample_gated = FlowCal.gate.high_low(min_beads_sample_gated, channels=['FSC', 'SSC'])
    max_beads_sample_gated = FlowCal.gate.high_low(max_beads_sample_gated, channels=['FSC', 'SSC'])
    density_gate_output = FlowCal.gate.density2d(data=beads_sample_gated, channels=['FSC', 'SSC'], gate_fraction=0.85, sigma=5.0, full_output=True)
    beads_sample_gated = density_gate_output.gated_data
    gate_contour = density_gate_output.contour
    min_density_gate_output = FlowCal.gate.density2d(data=min_beads_sample_gated, channels=['FSC', 'SSC'], gate_fraction=0.85, sigma=5.0, full_output=True)
    min_beads_sample_gated = min_density_gate_output.gated_data
    min_gate_contour = min_density_gate_output.contour
    max_density_gate_output = FlowCal.gate.density2d(data=max_beads_sample_gated, channels=['FSC', 'SSC'], gate_fraction=0.85, sigma=5.0, full_output=True)
    max_beads_sample_gated = max_density_gate_output.gated_data
    max_gate_contour = max_density_gate_output.contour
    print('Plotting density plot and histogram...')
    density_params = {}
    density_params['mode'] = 'scatter'
    density_params['xlim'] = [90, 1023]
    density_params['ylim'] = [90, 1023]
    density_params['sigma'] = 5.0
    plot_filename = '{}/density_hist_{}.png'.format(beads_plot_dir, 'beads')
    min_plot_filename = '{}/min_density_hist_{}.png'.format(beads_plot_dir, 'beads')
    max_plot_filename = '{}/max_density_hist_{}.png'.format(beads_plot_dir, 'beads')
    FlowCal.plot.density_and_hist(beads_sample, beads_sample_gated, density_channels=['FSC', 'SSC'], hist_channels=['FL1', 'FL3'], gate_contour=gate_contour, density_params=density_params, savefig=plot_filename)
    FlowCal.plot.density_and_hist(min_beads_sample, min_beads_sample_gated, density_channels=['FSC', 'SSC'], hist_channels=['FL1', 'FL3'], gate_contour=min_gate_contour, density_params=density_params, savefig=min_plot_filename)
    FlowCal.plot.density_and_hist(max_beads_sample, max_beads_sample_gated, density_channels=['FSC', 'SSC'], hist_channels=['FL1', 'FL3'], gate_contour=max_gate_contour, density_params=density_params, savefig=max_plot_filename)
    print('\nCalculating standard curve for channel FL1...')
    mef_transform_fxn = FlowCal.mef.get_transform_fxn(beads_sample_gated, mef_channels='FL1', mef_values=mefl_values, clustering_channels=['FL1', 'FL3'], verbose=True, plot=True, plot_dir=beads_plot_dir, plot_filename='beads')
    min_mef_transform_fxn = FlowCal.mef.get_transform_fxn(min_beads_sample_gated, mef_channels='FL1', mef_values=min_mefl_values, clustering_channels=['FL1', 'FL3'], verbose=True, plot=True, plot_dir=beads_plot_dir, plot_filename='min_beads')
    max_mef_transform_fxn = FlowCal.mef.get_transform_fxn(max_beads_sample_gated, mef_channels='FL1', mef_values=max_mefl_values, clustering_channels=['FL1', 'FL3'], verbose=True, plot=True, plot_dir=beads_plot_dir, plot_filename='max_beads')
    print('\nProcessing cell samples...')
    samples = []
    for (sample_id, sample_filename) in enumerate(samples_filenames):
        print('\nLoading file "{}"...'.format(sample_filename))
        sample = FlowCal.io.FCSData(sample_filename)
        print('Performing data transformation...')
        sample = FlowCal.transform.to_rfi(sample)
        sample = mef_transform_fxn(sample, channels=['FL1'])
        print('Performing gating...')
        sample_gated = FlowCal.gate.start_end(sample, num_start=250, num_end=100)
        sample_gated = FlowCal.gate.high_low(sample_gated, channels=['FSC', 'SSC', 'FL1'])
        density_gate_output = FlowCal.gate.density2d(data=sample_gated, channels=['FSC', 'SSC'], gate_fraction=0.85, full_output=True)
        sample_gated = density_gate_output.gated_data
        gate_contour = density_gate_output.contour
        print('Plotting density plot and histogram...')
        density_params = {}
        density_params['mode'] = 'scatter'
        hist_params = {}
        hist_params['xlabel'] = ('FL1 ' + '(Molecules of Equivalent Fluorescein, MEFL)')
        plot_filename = '{}/density_hist_{}.png'.format(samples_plot_dir, 'S{:03}'.format((sample_id + 1)))
        FlowCal.plot.density_and_hist(sample, sample_gated, density_channels=['FSC', 'SSC'], hist_channels=['FL1'], gate_contour=gate_contour, density_params=density_params, hist_params=hist_params, savefig=plot_filename)
        samples.append(sample_gated)
    print('\nProcessing control samples...')
    min_sample = FlowCal.io.FCSData(min_sample_filename)
    max_sample = FlowCal.io.FCSData(max_sample_filename)
    min_sample = FlowCal.transform.to_rfi(min_sample)
    max_sample = FlowCal.transform.to_rfi(max_sample)
    min_sample = min_mef_transform_fxn(min_sample, channels=['FL1'])
    max_sample = max_mef_transform_fxn(max_sample, channels=['FL1'])
    min_sample_gated = FlowCal.gate.start_end(min_sample, num_start=250, num_end=100)
    max_sample_gated = FlowCal.gate.start_end(max_sample, num_start=250, num_end=100)
    min_sample_gated = FlowCal.gate.high_low(min_sample_gated, channels=['FSC', 'SSC', 'FL1'])
    max_sample_gated = FlowCal.gate.high_low(max_sample_gated, channels=['FSC', 'SSC', 'FL1'])
    min_density_gate_output = FlowCal.gate.density2d(data=min_sample_gated, channels=['FSC', 'SSC'], gate_fraction=0.85, full_output=True)
    min_sample_gated = min_density_gate_output.gated_data
    min_gate_contour = min_density_gate_output.contour
    max_density_gate_output = FlowCal.gate.density2d(data=max_sample_gated, channels=['FSC', 'SSC'], gate_fraction=0.85, full_output=True)
    max_sample_gated = max_density_gate_output.gated_data
    max_gate_contour = max_density_gate_output.contour
    min_plot_filename = '{}/density_hist_min.png'.format(samples_plot_dir)
    max_plot_filename = '{}/density_hist_max.png'.format(samples_plot_dir)
    FlowCal.plot.density_and_hist(min_sample, min_sample_gated, density_channels=['FSC', 'SSC'], hist_channels=['FL1'], gate_contour=min_gate_contour, density_params=density_params, hist_params=hist_params, savefig=min_plot_filename)
    FlowCal.plot.density_and_hist(max_sample, max_sample_gated, density_channels=['FSC', 'SSC'], hist_channels=['FL1'], gate_contour=max_gate_contour, density_params=density_params, hist_params=hist_params, savefig=max_plot_filename)
    cmap = mpl.cm.get_cmap('gray_r')
    norm = mpl.colors.LogNorm(vmin=1.0, vmax=3500.0)
    colors = [cmap(norm((dapg_i + 4.0))) for dapg_i in dapg]
    plt.figure(figsize=(6, 3.5))
    FlowCal.plot.hist1d(samples, channel='FL1', histtype='step', bins=128, edgecolor=colors)
    plt.ylim((0, 2500))
    plt.xlim((0, 50000.0))
    plt.xlabel('FL1  (Molecules of Equivalent Fluorescein, MEFL)')
    plt.legend(['{} $\\mu M$ DAPG'.format(i) for i in dapg], loc='upper left', fontsize='small')
    plt.tight_layout()
    plt.savefig('histograms.png', dpi=200)
    plt.close()
    samples_fluorescence = [FlowCal.stats.mean(s, channels='FL1') for s in samples]
    min_fluorescence = FlowCal.stats.mean(min_sample_gated, channels='FL1')
    max_fluorescence = FlowCal.stats.mean(max_sample_gated, channels='FL1')
    dapg_color = '#ffc400'
    plt.figure(figsize=(3, 3))
    plt.plot(dapg, samples_fluorescence, marker='o', color=dapg_color)
    plt.axhline(min_fluorescence, color='gray', linestyle='--', zorder=(- 1))
    plt.text(s='Min', x=200.0, y=160.0, ha='left', va='bottom', color='gray')
    plt.axhline(max_fluorescence, color='gray', linestyle='--', zorder=(- 1))
    plt.text(s='Max', x=(- 0.7), y=5200.0, ha='left', va='top', color='gray')
    plt.yscale('log')
    plt.ylim((50.0, 10000.0))
    plt.xscale('symlog')
    plt.xlim(((- 1.0), 1000.0))
    plt.xlabel('DAPG Concentration ($\\mu M$)')
    plt.ylabel('FL1 Fluorescence (MEFL)')
    plt.tight_layout()
    plt.savefig('dose_response.png', dpi=200)
    plt.close()

    def dapg_sensor_output(dapg_concentration):
        mn = 86.0
        mx = 3147.0
        K = 20.0
        n = 3.57
        if (dapg_concentration <= 0):
            return mn
        else:
            return (mn + ((mx - mn) / (1 + ((K / dapg_concentration) ** n))))
    autofluorescence = FlowCal.stats.mean(min_sample_gated, channels='FL1')

    def dapg_sensor_cellular_fluorescence(dapg_concentration):
        return (dapg_sensor_output(dapg_concentration) + autofluorescence)
    plt.figure(figsize=(4, 3.5))
    FlowCal.plot.violin_dose_response(data=samples, channel='FL1', positions=dapg, min_data=min_sample_gated, max_data=max_sample_gated, model_fxn=dapg_sensor_cellular_fluorescence, violin_kwargs={'facecolor': dapg_color, 'edgecolor': 'black'}, violin_width_to_span_fraction=0.075, xscale='log', yscale='log', ylim=(10.0, 30000.0), draw_model_kwargs={'color': 'gray', 'linewidth': 3, 'zorder': (- 1), 'solid_capstyle': 'butt'})
    plt.xlabel('DAPG Concentration ($\\mu M$)')
    plt.ylabel('FL1 Fluorescence (MEFL)')
    plt.tight_layout()
    plt.savefig('dose_response_violin.png', dpi=200)
    plt.close()
    print('\nDone.')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
if:
    plt.figure

idx = 33:------------------- similar code ------------------ index = 101, score = 7.0 
def plot_xo_full_decoder_kind(extension):
    attn_yolo_path = os.path.join(data_dir, 'xo/full/2stage/run_search_yolo-xo-2stage_env=xo_decoder-kind=attn_alg=yolo-xo-2stage_duration=long_seed=0_2018_06_07_12_29_32')
    mlp_yolo_path = os.path.join(data_dir, 'xo/full/2stage/run_search_yolo-xo-2stage_env=xo_decoder-kind=mlp_alg=yolo-xo-2stage_duration=long_seed=0_2018_06_07_12_30_29')
    seq_yolo_path = os.path.join(data_dir, 'xo/full/2stage/run_search_yolo-xo-2stage_env=xo_decoder-kind=seq_alg=yolo-xo-2stage_duration=long_seed=0_2018_06_07_12_30_00')
    attn_simple_path = os.path.join(data_dir, 'xo/full/simple/run_search_conv-xo_env=xo_decoder-kind=attn_alg=simple-xo_duration=long_seed=0_2018_06_08_17_49_36')
    mlp_simple_path = os.path.join(data_dir, 'xo/full/simple/run_search_conv-xo_env=xo_decoder-kind=mlp_alg=simple-xo_duration=long_seed=0_2018_06_08_17_50_40')
    seq_simple_path = os.path.join(data_dir, 'xo/full/simple/run_search_conv-xo_env=xo_decoder-kind=seq_alg=simple-xo_duration=long_seed=0_2018_06_08_17_50_20')
    plt.figure(figsize=(5, 3.5))
    ax = plt.gca()
    measure = 'math_accuracy'
    (x, y, *yerr) = get_arithmetic_data([attn_yolo_path], 'n_train', measure, 1, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='attn-yolo')
    attn_colour = line.lines[0].get_c()
    (x, y, *yerr) = get_arithmetic_data([mlp_yolo_path], 'n_train', measure, 1, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='mlp-yolo')
    mlp_colour = line.lines[0].get_c()
    (x, y, *yerr) = get_arithmetic_data([seq_yolo_path], 'n_train', measure, 1, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='seq-yolo')
    seq_colour = line.lines[0].get_c()
    measure = 'math_accuracy'
    (x, y, *yerr) = get_arithmetic_data([attn_simple_path], 'n_train', measure, 0, 'ci95')
    ax.errorbar(x, y, yerr=yerr, label='attn-simple', c=attn_colour, ls='--')
    (x, y, *yerr) = get_arithmetic_data([mlp_simple_path], 'n_train', measure, 0, 'ci95')
    ax.errorbar(x, y, yerr=yerr, label='mlp-simple', c=mlp_colour, ls='--')
    (x, y, *yerr) = get_arithmetic_data([seq_simple_path], 'n_train', measure, 0, 'ci95')
    ax.errorbar(x, y, yerr=yerr, label='seq-simple', c=seq_colour, ls='--')
    ax.set_ylabel('Accuracy', fontsize=12)
    ax.set_xlabel('\\# Training Samples', fontsize=12)
    ax.tick_params(axis='both', labelsize=14)
    ax.set_ylim((0.0, 1.05))
    ax.set_xticks(x)
    plt.legend()
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ( ... ):
    plt.figure

idx = 34:------------------- similar code ------------------ index = 97, score = 7.0 
def visualize_means(means, classes_order, data_name, save_path, name):
    '\n    Visualization of means, e.g. of latent code z.\n\n    Parameters:\n        means (torch.Tensor): 2-D Tensor with one mean z vector per class.\n        classes_order (dict): Defines mapping between integer indices and class names (strings).\n        data_name (str): Dataset name. Used for naming.\n        save_path (str): Saving path.\n        name (str): Name for type of mean, e.g. "z".\n    '
    classes_order = sorted(classes_order)
    classes = []
    for key in classes_order:
        classes.append(key)
    plt.figure(figsize=(20, 20))
    ax = sns.heatmap(means.cpu().numpy(), cmap='BrBG')
    ax.set_title(data_name, fontsize=title_font_size)
    ax.set_xlabel((name + ' mean activations'), fontsize=axes_font_size)
    ax.set_yticklabels(classes, rotation=0)
    plt.savefig(os.path.join(save_path, (name + '_mean_activations.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 35:------------------- similar code ------------------ index = 96, score = 7.0 
def threshold_map(self, metric):
    lin = np.linspace(0, 1, 64)
    triang = tri.Triangulation(self.results.threshold1.values, self.results.threshold2.values)
    interpolator = tri.LinearTriInterpolator(triang, self.results[metric])
    (Xi, Yi) = np.meshgrid(lin, lin)
    zi = interpolator(Xi, Yi)
    plt.figure(figsize=(6, 6))
    img = plt.imshow(zi[::(- 1)], extent=[0, 1, 0, 1])
    plt.colorbar(img)
    plt.xlabel('threshold1')
    plt.ylabel('threshold2')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 36:------------------- similar code ------------------ index = 5, score = 7.0 
def save_plot_figure(performance_list, name='performance.jpg'):
    plt.figure()
    plot(performance_list)
    plt.title('Performance')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.savefig(name)

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure()

idx = 37:------------------- similar code ------------------ index = 93, score = 7.0 
def visualize_means(means, num_classes, data_name, save_path, name):
    '\n    Visualization of means, e.g. of latent code z.\n\n    Parameters:\n        means (torch.Tensor): 2-D Tensor with one mean z vector per class.\n        num_classes (int): Defines number of classes.\n        data_name (str): Dataset name. Used for naming.\n        save_path (str): Saving path.\n        name (str): Name for type of mean, e.g. "z".\n    '
    classes = np.arange(0, num_classes)
    plt.figure(figsize=(20, 20))
    ax = sns.heatmap(means.cpu().numpy(), cmap='BrBG')
    ax.set_title(data_name, fontsize=title_font_size)
    ax.set_xlabel((name + ' mean activations'), fontsize=axes_font_size)
    ax.set_yticklabels(classes, rotation=0)
    plt.savefig(os.path.join(save_path, (name + '_mean_activations.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 38:------------------- similar code ------------------ index = 6, score = 7.0 
def make_plot(df, fit_params):
    v_min = (df.volume.min() * 0.99)
    v_max = (df.volume.max() * 1.01)
    v_fitting = np.linspace(v_min, v_max, num=50)
    e_fitting = murnaghan(v_fitting, *fit_params)
    plt.figure(figsize=(8.0, 6.0))
    loc = df.converged
    plt.plot(df[loc].volume, df[loc].energy, 'o')
    loc = [(not b) for b in df.converged]
    plt.plot(df[loc].volume, df[loc].energy, 'o', c='grey')
    plt.plot(v_fitting, e_fitting, '--')
    plt.xlabel('volume [$\\mathrm{\\AA}^3$]')
    plt.ylabel('energy [eV]')
    plt.tight_layout()
    plt.savefig('murn.pdf')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 39:------------------- similar code ------------------ index = 8, score = 7.0 
def test_phase(predictor, test, args):
    test_iter = chainer.iterators.SerialIterator(test, args.batchsize, repeat=False, shuffle=False)
    chainer.serializers.load_npz(os.path.join(args.out, 'predictor.npz'), predictor)
    model = MCSampler(predictor, mc_iteration=args.mc_iteration, activation=None, reduce_mean=None, reduce_var=None)
    if (args.gpu >= 0):
        chainer.backends.cuda.get_device_from_id(args.gpu).use()
        model.to_gpu()
    infer = Inferencer(test_iter, model, device=args.gpu)
    (pred, uncert) = infer.run()
    x = test.x.ravel()
    t = test.t.ravel()
    pred = pred.ravel()
    uncert = uncert.ravel()
    plt.rcParams['font.size'] = 18
    plt.figure(figsize=(13, 5))
    ax = sns.scatterplot(x=x, y=pred, color='blue', s=75)
    ax.errorbar(x, pred, yerr=uncert, fmt='none', capsize=10, ecolor='gray', linewidth=1.5)
    ax.plot(x, t, color='red', linewidth=1.5)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_xlim((- 10), 10)
    ax.set_ylim((- 15), 15)
    plt.legend(['Ground-truth', 'Prediction', 'Predicted variance'])
    plt.title('Result on testing data set')
    plt.tight_layout()
    plt.savefig(os.path.join(args.out, 'eval.png'))
    plt.close()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 40:------------------- similar code ------------------ index = 9, score = 7.0 
def train_phase(predictor, train, valid, args):
    plt.rcParams['font.size'] = 18
    plt.figure(figsize=(13, 5))
    ax = sns.scatterplot(x=train.x.ravel(), y=train.y.ravel(), color='blue', s=55, alpha=0.3)
    ax.plot(train.x.ravel(), train.t.ravel(), color='red', linewidth=2)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_xlim((- 10), 10)
    ax.set_ylim((- 15), 15)
    plt.legend(['Ground-truth', 'Observation'])
    plt.title('Training data set')
    plt.tight_layout()
    plt.savefig(os.path.join(args.out, 'train_dataset.png'))
    plt.close()
    train_iter = chainer.iterators.SerialIterator(train, args.batchsize, shuffle=True)
    valid_iter = chainer.iterators.SerialIterator(valid, args.batchsize, repeat=False, shuffle=False)
    model = Regressor(predictor)
    if (args.gpu >= 0):
        chainer.backends.cuda.get_device_from_id(args.gpu).use()
        model.to_gpu()
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(model)
    if (args.decay > 0):
        optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(args.decay))
    updater = training.updaters.StandardUpdater(train_iter, optimizer, device=args.gpu)
    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)
    trainer.extend(extensions.Evaluator(valid_iter, model, device=args.gpu))
    trainer.extend(extensions.dump_graph('main/loss'))
    frequency = (args.epoch if (args.frequency == (- 1)) else max(1, args.frequency))
    trainer.extend(extensions.snapshot(), trigger=(frequency, 'epoch'))
    trainer.extend(extensions.LogReport())
    if (args.plot and extensions.PlotReport.available()):
        trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], 'epoch', file_name='loss.png'))
        trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], 'epoch', file_name='accuracy.png'))
    trainer.extend(extensions.PrintReport(['epoch', 'iteration', 'main/loss', 'validation/main/loss', 'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))
    trainer.extend(extensions.ProgressBar())
    if args.resume:
        chainer.serializers.load_npz(args.resume, trainer)
    trainer.run()
    chainer.serializers.save_npz(os.path.join(args.out, 'predictor.npz'), predictor)

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 41:------------------- similar code ------------------ index = 10, score = 7.0 
def visualize_recon_loss_histogram(data, other_data_dicts, max_recon_loss, dict_key, data_name, save_path):
    "\n    Visualization of the entropy the datasets.\n\n    Parameters:\n        data (list):\n        other_data_dicts (dictionary of dictionaries): Dictionary of key-value pairs per dataset\n        dict_key (str): Dictionary key to plot\n        data_name (str): Original trained dataset's name.\n        save_path (str): Saving path.\n    "
    data = [x for x in data]
    plt.figure(figsize=(20, 20))
    plt.hist(data, label=data_name, alpha=1.0, bins=25, color=colors[0])
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        other_data = [x for x in other_data_dict[dict_key]]
        plt.hist(other_data, label=other_data_name, alpha=0.5, bins=25, color=colors[c])
        c += 1
    plt.title('Dataset reconstruction', fontsize=title_font_size)
    plt.xlabel('Reconstruction loss (nats)', fontsize=axes_font_size)
    plt.ylabel('Number of images', fontsize=axes_font_size)
    plt.legend(loc=0)
    plt.xlim(left=(- 0.0), right=max_recon_loss)
    plt.savefig(os.path.join(save_path, (((data_name + '_') + ','.join(list(other_data_dicts.keys()))) + '_reconstruction_losses.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 42:------------------- similar code ------------------ index = 87, score = 7.0 
def visualize_classification_uncertainty(data_mus, data_sigmas, other_data_dicts, other_data_mu_key, other_data_sigma_key, data_name, num_samples, save_path):
    "\n    Visualization of prediction uncertainty computed over multiple samples for each input.\n\n    Parameters:\n        data_mus (list or torch.Tensor): Encoded mu values for trained dataset's validation set.\n        data_sigmas (list or torch.Tensor): Encoded sigma values for trained dataset's validation set.\n        other_data_dicts (dictionary of dictionaries): A dataset with values per dictionary, among them mus and sigmas\n        other_data_mu_key (str): Dictionary key for the mus\n        other_data_sigma_key (str): Dictionary key for the sigmas\n        data_name (str): Original dataset's name.\n        num_samples (int): Number of used samples to obtain prediction values.\n        save_path (str): Saving path.\n    "
    data_mus = [y for x in data_mus for y in x]
    data_sigmas = [y for x in data_sigmas for y in x]
    plt.figure(figsize=(20, 14))
    plt.scatter(data_mus, data_sigmas, label=data_name, s=75, c=colors[0], alpha=1.0)
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        other_data_mus = [y for x in other_data_dict[other_data_mu_key] for y in x]
        other_data_sigmas = [y for x in other_data_dict[other_data_sigma_key] for y in x]
        plt.scatter(other_data_mus, other_data_sigmas, label=other_data_name, s=75, c=colors[c], alpha=0.3, marker='*')
        c += 1
    plt.xlabel('Prediction mean', fontsize=axes_font_size)
    plt.ylabel('Prediction standard deviation', fontsize=axes_font_size)
    plt.xlim(left=(- 0.05), right=1.05)
    plt.ylim(bottom=(- 0.05), top=0.55)
    plt.legend(loc=1, fontsize=legend_font_size)
    plt.savefig(os.path.join(save_path, (((((data_name + '_vs_') + ','.join(list(other_data_dicts.keys()))) + '_classification_uncertainty_') + str(num_samples)) + '_samples.pdf')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 43:------------------- similar code ------------------ index = 73, score = 7.0 
def effects_histogram(true_effects, estimated_effects=None, num_bins=100, title=None, x_range=None, y_max=None, show_avg=True):
    plt.figure(figsize=(18, 8))
    if (title is None):
        if (estimated_effects is None):
            title = 'True Heterogeneous Effects'
        else:
            title = 'True and Estimated Heterogeneous Effects'
    plt.title(title, pad=20)
    if ((estimated_effects is not None) and (x_range is None)):
        x_range = np.percentile(np.concatenate((true_effects, estimated_effects)), (5, 95))
    true_effects_mean = np.mean(true_effects)
    plt.axvline(x=true_effects_mean, color='C0', linestyle='--')
    plt.hist(true_effects, bins=num_bins, range=x_range, density=True, alpha=0.5, color='C0', label='True Effects')
    if (estimated_effects is not None):
        estimated_effects_mean = np.mean(estimated_effects)
        plt.axvline(x=estimated_effects_mean, color='C1', linestyle='--')
        plt.hist(estimated_effects, bins=num_bins, range=x_range, density=True, alpha=0.5, color='C1', label='Estimated Effects')
    if y_max:
        plt.ylim(top=(y_max * 1.05))
    if show_avg:
        plt.text(true_effects_mean, (plt.ylim()[1] * 0.95), '  True ATE', color='C0', fontsize=18)
        if (estimated_effects is not None):
            plt.text(estimated_effects_mean, (plt.ylim()[1] * 0.9), '  Estimated ATE', color='C1', fontsize=18)
    plt.xlabel('Effect Size')
    plt.ylabel('Density')
    plt.legend(fontsize=18)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 44:------------------- similar code ------------------ index = 86, score = 7.0 
def visualize_correlation_metrics(spearman_scores, kendall_scores, pearson_scores, model_name, year):
    '\n    Visualize the scores of correlation metrics with respect to k-worst bpes you tried\n    :param spearman_scores: The spearman correlation scores of Q1 and k-worst bpes each time\n    :param kendall_scores: The Kendall correlation scores of Q1 and k-worst bpes each time\n    :param pearson_scores: The Pearson correlation scores of Q1 and k-worst bpes each time\n    :param model_name: Name of Language Model you used (BERT or GPT2). It is used on the output file name\n    :param year: The corresponding year of the data\n    '
    x_ticks = [i for i in range(1, (MAX_BPES_TO_SEARCH + 1))]
    plt.figure(figsize=(26, 8))
    y_max = max(spearman_scores)
    x_pos = spearman_scores.index(y_max)
    x_max = x_ticks[x_pos]
    plt.subplot(1, 3, 1)
    plt.plot(x_ticks, spearman_scores, 'bo')
    plt.annotate('{0:.2f} K={1:d}'.format(y_max, x_max), xy=(x_max, y_max), xytext=(x_max, (y_max + 0.005)))
    plt.title('Spearman')
    plt.xlabel('# of worst words')
    y_max = max(kendall_scores)
    x_pos = kendall_scores.index(y_max)
    x_max = x_ticks[x_pos]
    plt.subplot(1, 3, 2)
    plt.plot(x_ticks, kendall_scores, 'bo')
    plt.annotate('{0:.2f} K={1:d}'.format(y_max, x_max), xy=(x_max, y_max), xytext=(x_max, (y_max + 0.005)))
    plt.title('Kendall')
    plt.xlabel('# of worst words')
    y_max = max(pearson_scores)
    x_pos = pearson_scores.index(y_max)
    x_max = x_ticks[x_pos]
    plt.subplot(1, 3, 3)
    plt.plot(x_ticks, pearson_scores, 'bo')
    plt.annotate('{0:.2f} K={1:d}'.format(y_max, x_max), xy=(x_max, x_max), xytext=(x_max, (y_max + 0.005)))
    plt.title('Pearson')
    plt.xlabel('# of worst words')
    path_to_save = os.path.join(OUTPUT_DIR, 'Q1 - {0:s}  {1:s}.png'.format(model_name, year))
    plt.savefig(path_to_save)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 45:------------------- similar code ------------------ index = 11, score = 7.0 
def dvf_statistics(setting, dvf, spacing=None, im_info=None, stage=None):
    im_info_su = {'data': im_info['data'], 'deform_exp': im_info['deform_exp'], 'type_im': im_info['type_im'], 'cn': im_info['cn'], 'dsmooth': im_info['dsmooth'], 'stage': stage, 'padto': im_info['padto']}
    max_dvf = np.max(setting['deform_exp'][im_info['deform_exp']]['MaxDeform'])
    import matplotlib.pyplot as plt
    plt.figure()
    plt.hist(np.ravel(dvf), log=True, bins=np.arange((- max_dvf), (max_dvf + 1)))
    plt.draw()
    plt.savefig(su.address_generator(setting, 'DVF_histogram', **im_info_su))
    plt.close()
    jac = ip.calculate_jac(dvf, spacing)
    sitk.WriteImage(sitk.GetImageFromArray(jac.astype(np.float32)), su.address_generator(setting, 'Jac', **im_info_su))
    jac_hist_max = 3
    jac_hist_min = (- 1)
    step_h = 0.2
    if (np.max(jac) > jac_hist_max):
        jac_hist_max = np.ceil(np.max(jac))
    if (np.min(jac) < jac_hist_min):
        jac_hist_min = np.floor(np.min(jac))
    plt.figure()
    plt.hist(np.ravel(jac), log=True, bins=np.arange(jac_hist_min, (jac_hist_max + step_h), step_h))
    plt.title('min(Jac)={:.2f}, max(Jac)={:.2f}'.format(np.min(jac), np.max(jac)))
    plt.draw()
    plt.savefig(su.address_generator(setting, 'Jac_histogram', **im_info_su))
    plt.close()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure()

idx = 46:------------------- similar code ------------------ index = 34, score = 7.0 
def visualize_entropy_histogram(data, other_data_dicts, max_entropy, dict_key, data_name, save_path):
    "\n    Visualization of the entropy the datasets.\n\n    Parameters:\n        data (list):\n        other_data_dicts (dictionary of dictionaries): Dictionary of key-value pairs per dataset\n        dict_key (str): Dictionary key to plot\n        data_name (str): Original trained dataset's name.\n        save_path (str): Saving path.\n    "
    data = [x for x in data]
    plt.figure(figsize=(20, 20))
    plt.hist(data, label=data_name, alpha=1.0, bins=25, color=colors[0])
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        other_data = [x for x in other_data_dict[dict_key]]
        plt.hist(other_data, label=other_data_name, alpha=0.5, bins=25, color=colors[c])
        c += 1
    plt.title('Dataset classification entropy', fontsize=title_font_size)
    plt.xlabel('Classification entropy', fontsize=axes_font_size)
    plt.ylabel('Number of images', fontsize=axes_font_size)
    plt.legend(loc=0)
    plt.xlim(left=(- 0.0), right=max_entropy)
    plt.savefig(os.path.join(save_path, (((data_name + '_') + ','.join(list(other_data_dicts.keys()))) + '_classification_entropies.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 47:------------------- similar code ------------------ index = 12, score = 7.0 
def scatter3d_and_projections(data_list, channels=[0, 1, 2], xscale='logicle', yscale='logicle', zscale='logicle', xlabel=None, ylabel=None, zlabel=None, xlim=None, ylim=None, zlim=None, color=None, figsize=None, savefig=None, **kwargs):
    "\n    Plot a 3D scatter plot and 2D projections from FCSData objects.\n\n    `scatter3d_and_projections` creates a 3D scatter plot and three 2D\n    projected scatter plots in four different axes for each FCSData object\n    in `data_list`, in the same figure.\n\n    Parameters\n    ----------\n    data_list : FCSData object, or list of FCSData objects\n        Flow cytometry data to plot.\n    channels : list of int, list of str\n        Three channels to use for the plot.\n    savefig : str, optional\n        The name of the file to save the figure to. If None, do not save.\n\n    Other parameters\n    ----------------\n    xscale : str, optional\n        Scale of the x axis, either ``linear``, ``log``, or ``logicle``.\n    yscale : str, optional\n        Scale of the y axis, either ``linear``, ``log``, or ``logicle``.\n    zscale : str, optional\n        Scale of the z axis, either ``linear``, ``log``, or ``logicle``.\n    xlabel : str, optional\n        Label to use on the x axis. If None, attempts to extract channel\n        name from last data object.\n    ylabel : str, optional\n        Label to use on the y axis. If None, attempts to extract channel\n        name from last data object.\n    zlabel : str, optional\n        Label to use on the z axis. If None, attempts to extract channel\n        name from last data object.\n    xlim : tuple, optional\n        Limits for the x axis. If None, attempts to extract limits from the\n        range of the last data object.\n    ylim : tuple, optional\n        Limits for the y axis. If None, attempts to extract limits from the\n        range of the last data object.\n    zlim : tuple, optional\n        Limits for the z axis. If None, attempts to extract limits from the\n        range of the last data object.\n    color : matplotlib color or list of matplotlib colors, optional\n        Color for the scatter plot. It can be a list with the same length\n        as `data_list`. If `color` is not specified, elements from\n        `data_list` are plotted with colors taken from the module-level\n        variable `cmap_default`.\n    figsize : tuple, optional\n        Figure size. If None, use matplotlib's default.\n    kwargs : dict, optional\n        Additional parameters passed directly to matploblib's ``scatter``.\n\n    Notes\n    -----\n    `scatter3d_and_projections` uses matplotlib's ``scatter``, with the 3D\n    scatter plot using a 3D projection. Additional keyword arguments\n    provided to `scatter3d_and_projections` are passed directly to\n    ``scatter``.\n\n    "
    if (len(channels) != 3):
        raise ValueError('three channels need to be specified')
    plt.figure(figsize=figsize)
    plt.subplot(221)
    scatter2d(data_list, channels=[channels[0], channels[2]], xscale=xscale, yscale=zscale, xlabel=xlabel, ylabel=zlabel, xlim=xlim, ylim=zlim, color=color, **kwargs)
    ax_3d = plt.gcf().add_subplot(222, projection='3d')
    scatter3d(data_list, channels=channels, xscale=xscale, yscale=yscale, zscale=zscale, xlabel=xlabel, ylabel=ylabel, zlabel=zlabel, xlim=xlim, ylim=ylim, zlim=zlim, color=color, **kwargs)
    plt.subplot(223)
    scatter2d(data_list, channels=[channels[0], channels[1]], xscale=xscale, yscale=yscale, xlabel=xlabel, ylabel=ylabel, xlim=xlim, ylim=ylim, color=color, **kwargs)
    plt.subplot(224)
    scatter2d(data_list, channels=[channels[2], channels[1]], xscale=zscale, yscale=yscale, xlabel=zlabel, ylabel=ylabel, xlim=zlim, ylim=ylim, color=color, **kwargs)
    if (savefig is not None):
        plt.tight_layout()
        plt.savefig(savefig, dpi=savefig_dpi)
        plt.close()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 48:------------------- similar code ------------------ index = 75, score = 7.0 
def visualize_classification_uncertainty(data_mus, data_sigmas, other_data_dicts, other_data_mu_key, other_data_sigma_key, data_name, num_samples, save_path):
    "\n    Visualization of prediction uncertainty computed over multiple samples for each input.\n\n    Parameters:\n        data_mus (list or torch.Tensor): Encoded mu values for trained dataset's validation set.\n        data_sigmas (list or torch.Tensor): Encoded sigma values for trained dataset's validation set.\n        other_data_dicts (dictionary of dictionaries): A dataset with values per dictionary, among them mus and sigmas\n        other_data_mu_key (str): Dictionary key for the mus\n        other_data_sigma_key (str): Dictionary key for the sigmas\n        data_name (str): Original dataset's name.\n        num_samples (int): Number of used samples to obtain prediction values.\n        save_path (str): Saving path.\n    "
    data_mus = [y for x in data_mus for y in x]
    data_sigmas = [y for x in data_sigmas for y in x]
    plt.figure(figsize=(20, 14))
    plt.scatter(data_mus, data_sigmas, label=data_name, s=75, c=colors[0], alpha=1.0)
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        other_data_mus = [y for x in other_data_dict[other_data_mu_key] for y in x]
        other_data_sigmas = [y for x in other_data_dict[other_data_sigma_key] for y in x]
        plt.scatter(other_data_mus, other_data_sigmas, label=other_data_name, s=75, c=colors[c], alpha=0.3, marker='*')
        c += 1
    plt.xlabel('Prediction mean', fontsize=axes_font_size)
    plt.ylabel('Prediction standard deviation', fontsize=axes_font_size)
    plt.xlim(left=(- 0.05), right=1.05)
    plt.ylim(bottom=(- 0.05), top=0.55)
    plt.legend(loc=1, fontsize=legend_font_size)
    plt.savefig(os.path.join(save_path, (((((data_name + '_vs_') + ','.join(list(other_data_dicts.keys()))) + '_classification_uncertainty_') + str(num_samples)) + '_samples.pdf')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 49:------------------- similar code ------------------ index = 74, score = 7.0 
def visualize_reconstruction_classification(data, other_data_dicts, dict_key, data_name, thresholds, save_path, autoregression=False):
    "\n    Visualization of percentage of datasets considered as statistical outliers evaluated for different\n    entropy thresholds.\n\n    Parameters:\n        data (list): Dataset outlier percentages per rejection prior value for the trained dataset's validation set.\n        other_data_dicts (dictionary of dictionaries):\n            Dataset outlier percentages per rejection prior value for an unseen dataset.\n        dict_key (str): Dictionary key of the values to visualize\n        data_name (str): Original trained dataset's name.\n        thresholds (list): List of integers with rejection prior values.\n        save_path (str): Saving path.\n    "
    lw = 10
    plt.figure(figsize=(20, 20))
    plt.plot(thresholds, data, label=data_name, color=colors[0], linestyle='solid', linewidth=lw)
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        plt.plot(thresholds, other_data_dict[dict_key], label=other_data_name, color=colors[c], linestyle=linestyles[(c % len(linestyles))], linewidth=lw)
        c += 1
    if autoregression:
        plt.xlabel('Dataset reconstruction loss (bits per dim)', fontsize=axes_font_size)
    else:
        plt.xlabel('Dataset reconstruction loss (nats)', fontsize=axes_font_size)
    plt.ylabel('Percentage of dataset outliers', fontsize=axes_font_size)
    plt.xlim(left=(- 0.05), right=thresholds[(- 1)])
    plt.ylim(bottom=(- 0.05), top=1.05)
    plt.legend(loc=0, fontsize=(legend_font_size - 15))
    plt.savefig(os.path.join(save_path, ((((data_name + '_') + ','.join(list(other_data_dicts.keys()))) + '_reconstruction_loss_outlier_classification') + '.pdf')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 50:------------------- similar code ------------------ index = 81, score = 7.0 
def visualize_entropy_classification(data, other_data_dicts, dict_key, data_name, thresholds, save_path):
    "\n    Visualization of percentage of datasets considered as statistical outliers evaluated for different\n    entropy thresholds.\n\n    Parameters:\n        data (list): Dataset outlier percentages per rejection prior value for the trained dataset's validation set.\n        other_data_dicts (dictionary of dictionaries):\n            Dataset outlier percentages per rejection prior value for an unseen dataset.\n        dict_key (str): Dictionary key of the values to visualize\n        data_name (str): Original trained dataset's name.\n        thresholds (list): List of integers with rejection prior values.\n        save_path (str): Saving path.\n    "
    lw = 10
    plt.figure(figsize=(20, 20))
    plt.plot(thresholds, data, label=data_name, color=colors[0], linestyle='solid', linewidth=lw)
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        plt.plot(thresholds, other_data_dict[dict_key], label=other_data_name, color=colors[c], linestyle=linestyles[(c % len(linestyles))], linewidth=lw)
        c += 1
    plt.xlabel('Predictive entropy', fontsize=axes_font_size)
    plt.ylabel('Percentage of dataset outliers', fontsize=axes_font_size)
    plt.xlim(left=(- 0.05), right=thresholds[(- 1)])
    plt.ylim(bottom=(- 0.05), top=1.05)
    plt.legend(loc=0, fontsize=(legend_font_size - 15))
    plt.savefig(os.path.join(save_path, ((((data_name + '_') + ','.join(list(other_data_dicts.keys()))) + '_entropy_outlier_classification') + '.pdf')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 51:------------------- similar code ------------------ index = 80, score = 7.0 
def read_time(driver, port, idn):
    " Read the entire control table of the DXL MX-64AT device 'N' times and plot the mean & percentile time taken. "
    times = []
    for i in range(1000):
        t1 = time.time()
        dxl.read_vals(driver, port, idn)
        times.append((time.time() - t1))
    print(np.mean(times))
    print(np.percentile(times, 99))
    plt.figure()
    plt.plot(times)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure()

idx = 52:------------------- similar code ------------------ index = 79, score = 7.0 
def show_plot(price, firstIndicator, secondIndicator, dates, symbol='', label1='', label2=''):
    "Displays a chart of the price and indicators for a stock\n\n    Args:\n        price(Pandas series): Series containing a stock's prices\n        firstIndicator(Pandas series): Series containing a technical indicator, such as 50-day moving average\n        secondIndicator(Pandas series): Series containing a technical indicator, such as 200-day moving average\n        dates(Pandas series): Series containing the dates that correspond to the prices and indicators\n        label1(str): Chart label of the first technical indicator\n        label2(str): Chart label of the first technical indicator\n\n    Returns:\n        True if the stock's current price is higher than it was five years ago, or the stock IPO'd within the last five years\n        False otherwise\n    "
    plt.figure(figsize=(10, 5))
    plt.title(symbol)
    plt.plot(dates, price, label='Closing prices')
    plt.plot(dates, firstIndicator, label=label1)
    plt.plot(dates, secondIndicator, label=label2)
    plt.yticks(np.arange(price.min(), price.max(), step=((price.max() - price.min()) / 15.0)))
    plt.legend()
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 53:------------------- similar code ------------------ index = 78, score = 7.0 
def error_bar_plot(experiment_data, results, title='', ylabel=''):
    true_effect = experiment_data.true_effects.mean()
    estimators = list(results.keys())
    x = list(estimators)
    y = [results[estimator].ate for estimator in estimators]
    cis = [((np.array(results[estimator].ci) - results[estimator].ate) if (results[estimator].ci is not None) else [0, 0]) for estimator in estimators]
    err = [[abs(ci[0]) for ci in cis], [abs(ci[1]) for ci in cis]]
    plt.figure(figsize=(12, 5))
    (_, caps, _) = plt.errorbar(x, y, yerr=err, fmt='o', markersize=8, capsize=5)
    for cap in caps:
        cap.set_markeredgewidth(2)
    plt.plot(x, ([true_effect] * len(x)), label='True Effect')
    plt.legend(fontsize=12, loc='lower right')
    plt.ylabel(ylabel)
    plt.title(title)

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 54:------------------- similar code ------------------ index = 82, score = 7.0 
def visualize_openset_classification(data, other_data_dicts, dict_key, data_name, thresholds, save_path, tailsize):
    "\n    Visualization of percentage of datasets considered as statistical outliers evaluated for different\n    Weibull CDF rejection priors.\n\n    Parameters:\n        data (list): Dataset outlier percentages per rejection prior value for the trained dataset's validation set.\n        other_data_dicts (dictionary of dictionaries):\n            Dataset outlier percentages per rejection prior value for an unseen dataset.\n        dict_key (str): Dictionary key of the values to visualize\n        data_name (str): Original trained dataset's name.\n        thresholds (list): List of integers with rejection prior values.\n        save_path (str): Saving path.\n        tailsize (int): Weibull model's tailsize.\n    "
    lw = 10
    plt.figure(figsize=(20, 20))
    plt.plot(thresholds, data, label=data_name, color=colors[0], linestyle='solid', linewidth=lw)
    c = 0
    for (other_data_name, other_data_dict) in other_data_dicts.items():
        plt.plot(thresholds, other_data_dict[dict_key], label=other_data_name, color=colors[c], linestyle=linestyles[(c % len(linestyles))], linewidth=lw)
        c += 1
    plt.xlabel('Weibull CDF outlier rejection prior $\\Omega_t$', fontsize=axes_font_size)
    plt.ylabel('Percentage of dataset outliers', fontsize=axes_font_size)
    plt.xlim(left=(- 0.05), right=1.05)
    plt.ylim(bottom=(- 0.05), top=1.05)
    plt.legend(loc=0, fontsize=(legend_font_size - 15))
    plt.savefig(os.path.join(save_path, ((((((data_name + '_') + ','.join(list(other_data_dicts.keys()))) + '_outlier_classification') + '_tailsize_') + str(tailsize)) + '.pdf')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.7272727272727273 
def  ... ():
    plt.figure

idx = 55:------------------- similar code ------------------ index = 1, score = 6.0 
def plot_ur5_reacher(env, batch_size, shared_returns, plot_running):
    'Helper process for visualize the tasks and episodic returns.\n\n    Args:\n        env: An instance of ReacherEnv\n        batch_size: An int representing timesteps_per_batch provided to the PPO learn function\n        shared_returns: A manager dictionary object containing `episodic returns` and `episodic lengths`\n        plot_running: A multiprocessing Value object containing 0/1.\n            1: Continue plotting, 0: Terminate plotting loop\n    '
    print('Started plotting routine')
    import matplotlib.pyplot as plt
    plt.ion()
    time.sleep(5.0)
    fig = plt.figure(figsize=(20, 6))
    ax1 = fig.add_subplot(131)
    (hl1,) = ax1.plot([], [], markersize=10, marker='o', color='r')
    (hl2,) = ax1.plot([], [], markersize=10, marker='o', color='b')
    ax1.set_xlabel('X', fontsize=14)
    h = ax1.set_ylabel('Y', fontsize=14)
    h.set_rotation(0)
    ax3 = fig.add_subplot(132)
    (hl3,) = ax3.plot([], [], markersize=10, marker='o', color='r')
    (hl4,) = ax3.plot([], [], markersize=10, marker='o', color='b')
    ax3.set_xlabel('Z', fontsize=14)
    h = ax3.set_ylabel('Y', fontsize=14)
    h.set_rotation(0)
    ax2 = fig.add_subplot(133)
    (hl11,) = ax2.plot([], [])
    count = 0
    old_size = len(shared_returns['episodic_returns'])
    while plot_running.value:
        plt.suptitle('Reward: {:.2f}'.format(env._reward_.value), x=0.375, fontsize=14)
        hl1.set_ydata([env._x_target_[1]])
        hl1.set_xdata([env._x_target_[2]])
        hl2.set_ydata([env._x_[1]])
        hl2.set_xdata([env._x_[2]])
        ax1.set_ylim([env._end_effector_low[1], env._end_effector_high[1]])
        ax1.set_xlim([env._end_effector_low[2], env._end_effector_high[2]])
        ax1.set_title('X-Y plane', fontsize=14)
        ax1.set_xlim(ax1.get_xlim()[::(- 1)])
        ax1.set_ylim(ax1.get_ylim()[::(- 1)])
        hl3.set_ydata([env._x_target_[1]])
        hl3.set_xdata([env._x_target_[0]])
        hl4.set_ydata([env._x_[1]])
        hl4.set_xdata([env._x_[0]])
        ax3.set_ylim([env._end_effector_low[1], env._end_effector_high[1]])
        ax3.set_xlim([env._end_effector_low[0], env._end_effector_high[0]])
        ax3.set_title('Y-Z plane', fontsize=14)
        ax3.set_xlim(ax3.get_xlim()[::(- 1)])
        ax3.set_ylim(ax3.get_ylim()[::(- 1)])
        copied_returns = copy.deepcopy(shared_returns)
        if ((not copied_returns['write_lock']) and (len(copied_returns['episodic_returns']) > old_size)):
            returns = np.array(copied_returns['episodic_returns'])
            old_size = len(copied_returns['episodic_returns'])
            window_size_steps = 5000
            x_tick = 1000
            if copied_returns['episodic_lengths']:
                ep_lens = np.array(copied_returns['episodic_lengths'])
            else:
                ep_lens = (batch_size * np.arange(len(returns)))
            cum_episode_lengths = np.cumsum(ep_lens)
            if (cum_episode_lengths[(- 1)] >= x_tick):
                steps_show = np.arange(x_tick, (cum_episode_lengths[(- 1)] + 1), x_tick)
                rets = []
                for i in range(len(steps_show)):
                    rets_in_window = returns[((cum_episode_lengths > max(0, ((x_tick * (i + 1)) - window_size_steps))) * (cum_episode_lengths < (x_tick * (i + 1))))]
                    if rets_in_window.any():
                        rets.append(np.mean(rets_in_window))
                hl11.set_xdata((np.arange(1, (len(rets) + 1)) * x_tick))
                ax2.set_xlim([x_tick, (len(rets) * x_tick)])
                hl11.set_ydata(rets)
                ax2.set_ylim([np.min(rets), (np.max(rets) + 50)])
        time.sleep(0.01)
        fig.canvas.draw()
        fig.canvas.flush_events()
        count += 1

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    plt
     ...  =  ... .figure

idx = 56:------------------- similar code ------------------ index = 51, score = 6.0 
def plot_returns(env, batch_size, shared_returns, plot_running):
    'Plots episodic returns\n\n    Args:\n        env: An instance of DoubleInvertedPendulumEnv\n        batch_size: An int representing timesteps_per_batch provided to the PPO learn function\n        shared_returns: A manager dictionary object containing `episodic returns` and `episodic lengths`\n        plot_running: A multiprocessing Value object containing 0/1.\n            1: Continue plotting, 0: Terminate plotting loop\n    '
    print('Started plotting routine')
    import matplotlib.pyplot as plt
    plt.ion()
    time.sleep(5.0)
    fig = plt.figure(figsize=(20, 6))
    ax = fig.add_subplot(111)
    (hl11,) = ax.plot([], [])
    fig.suptitle('Simulated Double Pendulum', fontsize=14)
    ax.set_title('Learning Curve')
    ax.set_xlabel('Time Step')
    ax.set_ylabel('Average Returns')
    count = 0
    old_size = len(shared_returns['episodic_returns'])
    returns = []
    while plot_running.value:
        if ((count % 20) == 0):
            if (len(shared_returns['episodic_returns']) > old_size):
                returns.append(np.mean(shared_returns['episodic_returns'][(- (len(shared_returns['episodic_returns']) - old_size)):]))
                old_size = len(shared_returns['episodic_returns'])
                hl11.set_ydata(returns)
                hl11.set_xdata((batch_size * np.arange(len(returns))))
                ax.set_ylim([np.min(returns), np.max(returns)])
                ax.set_xlim([0, int((len(returns) * batch_size))])
                fig.canvas.draw()
                fig.canvas.flush_events()
        time.sleep(0.01)
        fig.canvas.draw()
        fig.canvas.flush_events()
        count += 1

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    plt
     ...  =  ... .figure

idx = 57:------------------- similar code ------------------ index = 36, score = 6.0 
def plot_ur5_reacher(env, batch_size, shared_returns, plot_running):
    'Helper process for visualize the tasks and episodic returns.\n\n    Args:\n        env: An instance of ReacherEnv\n        batch_size: An int representing timesteps_per_batch provided to the PPO learn function\n        shared_returns: A manager dictionary object containing `episodic returns` and `episodic lengths`\n        plot_running: A multiprocessing Value object containing 0/1.\n            1: Continue plotting, 0: Terminate plotting loop\n    '
    print('Started plotting routine')
    import matplotlib.pyplot as plt
    plt.ion()
    time.sleep(5.0)
    fig = plt.figure(figsize=(20, 6))
    ax1 = fig.add_subplot(131)
    (hl1,) = ax1.plot([], [], markersize=10, marker='o', color='r')
    (hl2,) = ax1.plot([], [], markersize=10, marker='o', color='b')
    ax1.set_xlabel('X', fontsize=14)
    h = ax1.set_ylabel('Y', fontsize=14)
    h.set_rotation(0)
    ax3 = fig.add_subplot(132)
    (hl3,) = ax3.plot([], [], markersize=10, marker='o', color='r')
    (hl4,) = ax3.plot([], [], markersize=10, marker='o', color='b')
    ax3.set_xlabel('Z', fontsize=14)
    h = ax3.set_ylabel('Y', fontsize=14)
    h.set_rotation(0)
    ax2 = fig.add_subplot(133)
    (hl11,) = ax2.plot([], [])
    count = 0
    old_size = len(shared_returns['episodic_returns'])
    while plot_running.value:
        plt.suptitle('Reward: {:.2f}'.format(env._reward_.value), x=0.375, fontsize=14)
        hl1.set_ydata([env._x_target_[1]])
        hl1.set_xdata([env._x_target_[2]])
        hl2.set_ydata([env._x_[1]])
        hl2.set_xdata([env._x_[2]])
        ax1.set_ylim([env._end_effector_low[1], env._end_effector_high[1]])
        ax1.set_xlim([env._end_effector_low[2], env._end_effector_high[2]])
        ax1.set_title('X-Y plane', fontsize=14)
        ax1.set_xlim(ax1.get_xlim()[::(- 1)])
        ax1.set_ylim(ax1.get_ylim()[::(- 1)])
        hl3.set_ydata([env._x_target_[1]])
        hl3.set_xdata([env._x_target_[0]])
        hl4.set_ydata([env._x_[1]])
        hl4.set_xdata([env._x_[0]])
        ax3.set_ylim([env._end_effector_low[1], env._end_effector_high[1]])
        ax3.set_xlim([env._end_effector_low[0], env._end_effector_high[0]])
        ax3.set_title('Y-Z plane', fontsize=14)
        ax3.set_xlim(ax3.get_xlim()[::(- 1)])
        ax3.set_ylim(ax3.get_ylim()[::(- 1)])
        copied_returns = copy.deepcopy(shared_returns)
        if ((not copied_returns['write_lock']) and (len(copied_returns['episodic_returns']) > old_size)):
            returns = np.array(copied_returns['episodic_returns'])
            old_size = len(copied_returns['episodic_returns'])
            window_size_steps = 5000
            x_tick = 1000
            if copied_returns['episodic_lengths']:
                ep_lens = np.array(copied_returns['episodic_lengths'])
            else:
                ep_lens = (batch_size * np.arange(len(returns)))
            cum_episode_lengths = np.cumsum(ep_lens)
            if (cum_episode_lengths[(- 1)] >= x_tick):
                steps_show = np.arange(x_tick, (cum_episode_lengths[(- 1)] + 1), x_tick)
                rets = []
                for i in range(len(steps_show)):
                    rets_in_window = returns[((cum_episode_lengths > max(0, ((x_tick * (i + 1)) - window_size_steps))) * (cum_episode_lengths < (x_tick * (i + 1))))]
                    if rets_in_window.any():
                        rets.append(np.mean(rets_in_window))
                hl11.set_xdata((np.arange(1, (len(rets) + 1)) * x_tick))
                ax2.set_xlim([x_tick, (len(rets) * x_tick)])
                hl11.set_ydata(rets)
                ax2.set_ylim([np.min(rets), (np.max(rets) + 50)])
        time.sleep(0.01)
        fig.canvas.draw()
        fig.canvas.flush_events()
        count += 1

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    plt
     ...  =  ... .figure

idx = 58:------------------- similar code ------------------ index = 100, score = 6.0 
def plot_dxl_reacher(env, batch_size, shared_returns, plot_running):
    ' Visualizes the DXL reacher task and plots episodic returns\n\n    Args:\n        env: An instance of DxlReacher1DEnv\n        batch_size: An int representing timesteps_per_batch provided to the PPO learn function\n        shared_returns: A manager dictionary object containing `episodic returns` and `episodic lengths`\n        plot_running: A multiprocessing Value object containing 0/1.\n            1: Continue plotting, 0: Terminate plotting loop\n    '
    print('Started plotting routine')
    import matplotlib.pyplot as plt
    plt.ion()
    time.sleep(5.0)
    fig = plt.figure(figsize=(20, 6))
    ax1 = fig.add_subplot(121)
    (hl1,) = ax1.plot([], [], markersize=10, marker='o', color='r')
    (hl2,) = ax1.plot([], [], markersize=10, marker='o', color='b')
    ax1.set_xlabel('X')
    ax1.set_ylabel('Y')
    ax2 = fig.add_subplot(122)
    (hl11,) = ax2.plot([], [])
    fig.suptitle('DXL Reacher', fontsize=14)
    ax2.set_title('Learning Curve')
    ax2.set_xlabel('Time Step')
    ax2.set_ylabel('Average Returns')
    count = 0
    old_size = len(shared_returns['episodic_returns'])
    while plot_running.value:
        hl1.set_ydata([1])
        hl1.set_xdata([env._target_pos_.value])
        hl2.set_ydata([1])
        hl2.set_xdata([env._present_pos_[(- 1)]])
        ax1.set_ylim([0, 2])
        ax1.set_xlim([env.angle_low, env.angle_high])
        ax1.set_title(('Current Reward: ' + str(env._reward_.value)))
        ax1.set_xlim(ax1.get_xlim()[::(- 1)])
        ax1.set_ylim(ax1.get_ylim()[::(- 1)])
        copied_returns = copy.deepcopy(shared_returns)
        if ((not copied_returns['write_lock']) and (len(copied_returns['episodic_returns']) > old_size)):
            returns = np.array(copied_returns['episodic_returns'])
            old_size = len(copied_returns['episodic_returns'])
            window_size_steps = 5000
            x_tick = 1000
            if copied_returns['episodic_lengths']:
                ep_lens = np.array(copied_returns['episodic_lengths'])
            else:
                ep_lens = (batch_size * np.arange(len(returns)))
            cum_episode_lengths = np.cumsum(ep_lens)
            if (cum_episode_lengths[(- 1)] >= x_tick):
                steps_show = np.arange(x_tick, (cum_episode_lengths[(- 1)] + 1), x_tick)
                rets = []
                for i in range(len(steps_show)):
                    rets_in_window = returns[((cum_episode_lengths > max(0, ((x_tick * (i + 1)) - window_size_steps))) * (cum_episode_lengths < (x_tick * (i + 1))))]
                    if rets_in_window.any():
                        rets.append(np.mean(rets_in_window))
                hl11.set_xdata((np.arange(1, (len(rets) + 1)) * x_tick))
                ax2.set_xlim([x_tick, (len(rets) * x_tick)])
                hl11.set_ydata(rets)
                ax2.set_ylim([np.min(rets), (np.max(rets) + 50)])
        time.sleep(0.01)
        fig.canvas.draw()
        fig.canvas.flush_events()
        count += 1

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    plt
     ...  =  ... .figure

idx = 59:------------------- similar code ------------------ index = 66, score = 6.0 
def plot_dxl_tracker(env, batch_size, shared_returns, plot_running):
    ' Visualizes the DXL tracker task and plots episodic returns\n\n    Args:\n        env: An instance of DxlTracker1DEnv\n        batch_size: An int representing timesteps_per_batch provided to the PPO learn function\n        shared_returns: A manager dictionary object containing `episodic returns` and `episodic lengths`\n        plot_running: A multiprocessing Value object containing 0/1.\n            1: Continue plotting, 0: Terminate plotting loop\n    '
    print('Started plotting routine')
    import matplotlib.pyplot as plt
    plt.ion()
    time.sleep(5.0)
    fig = plt.figure(figsize=(20, 6))
    ax1 = fig.add_subplot(121)
    (hl1,) = ax1.plot([], [], markersize=10, marker='o', color='r')
    (hl2,) = ax1.plot([], [], markersize=10, marker='o', color='b')
    ax1.set_xlabel('X')
    ax1.set_ylabel('Y')
    ax2 = fig.add_subplot(122)
    (hl11,) = ax2.plot([], [])
    fig.suptitle('DXL Tracker', fontsize=14)
    ax2.set_title('Learning Curve')
    ax2.set_xlabel('Time Step')
    ax2.set_ylabel('Average Returns')
    count = 0
    old_size = len(shared_returns['episodic_returns'])
    while plot_running.value:
        hl1.set_ydata([1])
        hl1.set_xdata([env._target_pos_.value])
        hl2.set_ydata([1])
        hl2.set_xdata([env._present_pos_[(- 1)]])
        ax1.set_ylim([0, 2])
        ax1.set_xlim([env.angle_low, env.angle_high])
        ax1.set_title(('Current Reward: ' + str(env._reward_.value)))
        ax1.set_xlim(ax1.get_xlim()[::(- 1)])
        ax1.set_ylim(ax1.get_ylim()[::(- 1)])
        copied_returns = copy.deepcopy(shared_returns)
        if ((not copied_returns['write_lock']) and (len(copied_returns['episodic_returns']) > old_size)):
            returns = np.array(copied_returns['episodic_returns'])
            old_size = len(copied_returns['episodic_returns'])
            window_size_steps = 5000
            x_tick = 1000
            if copied_returns['episodic_lengths']:
                ep_lens = np.array(copied_returns['episodic_lengths'])
            else:
                ep_lens = (batch_size * np.arange(len(returns)))
            cum_episode_lengths = np.cumsum(ep_lens)
            if (cum_episode_lengths[(- 1)] >= x_tick):
                steps_show = np.arange(x_tick, (cum_episode_lengths[(- 1)] + 1), x_tick)
                rets = []
                for i in range(len(steps_show)):
                    rets_in_window = returns[((cum_episode_lengths > max(0, ((x_tick * (i + 1)) - window_size_steps))) * (cum_episode_lengths < (x_tick * (i + 1))))]
                    if rets_in_window.any():
                        rets.append(np.mean(rets_in_window))
                hl11.set_xdata((np.arange(1, (len(rets) + 1)) * x_tick))
                ax2.set_xlim([x_tick, (len(rets) * x_tick)])
                hl11.set_ydata(rets)
                ax2.set_ylim([np.min(rets), (np.max(rets) + 50)])
        time.sleep(0.01)
        fig.canvas.draw()
        fig.canvas.flush_events()
        count += 1

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    plt
     ...  =  ... .figure

idx = 60:------------------- similar code ------------------ index = 102, score = 6.0 
def plot_at_northpole(lons, lats, data, fig_name, ylabel=None, magnify=None):
    "\n    Plot grid data at the northpole. Note: The value of the grid data was magnified 1000 times during the plot process. \n\n    Usage:\n    plot_at_northpole(lons,lats,data,fig_name)\n    plot_at_northpole(lons,lats,data,fig_name,1e3)\n\n    Inputs:\n    lons -> [float array] logitudes\n    lats -> [float array] latitudes\n    data -> [float 2d array] grid data\n    fig_name -> [str] figure name\n    ylabel -> [str] ylabel, such as '$10^{-3}$ [mm w.e.]'\n\n    Parameters:\n    magnify -> [optional, float, default = None] If None, 1 is taken.\n    \n    Outputs: A png(200dpi) image stored in the figures directory \n    "
    fig_dir = 'figures/'
    if (not path.exists(fig_dir)):
        makedirs(fig_dir)
    plt.clf()
    fig = plt.figure(dpi=200)
    proj = ccrs.NearsidePerspective(0, 90, 2000000.0)
    ax = fig.add_subplot(1, 1, 1, projection=proj)
    gl = ax.gridlines(xlocs=np.linspace(0, 360, 7), ylocs=np.linspace(90, 0, 10), linestyle='--', alpha=0.7)
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    ax.add_feature(cfeature.COASTLINE)
    ax.add_feature(cfeature.RIVERS)
    ax.add_feature(cfeature.LAKES)
    if (magnify is None):
        magnify = 1
    (XX, YY) = np.meshgrid(lons, lats)
    Z = (data * magnify)
    abs_Z_max = np.abs(Z).max()
    Z_levels = np.linspace((- abs_Z_max), abs_Z_max, 61)
    CS = ax.contourf(XX, YY, Z, levels=Z_levels, extend='both', cmap=plt.cm.RdBu_r, zorder=0, transform=ccrs.PlateCarree())
    cbar = plt.colorbar(CS, extend='both', format='%.0f', shrink=0.9)
    cbar.ax.set_ylabel(ylabel, fontsize=8)
    cbar.ax.tick_params(labelsize=8)
    return plt.savefig(fig_name, bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.5833333333333334 
def  ... ():
    plt
     ...  =  ... .figure

idx = 61:------------------- similar code ------------------ index = 49, score = 7.0 
def plot_true_and_predicted_spectra(true_dense_spectra, generated_dense_spectra, plot_mode_key=PlotModeKeys.PREDICTED_SPECTRUM, output_filename='', rescale_mz_axis=False):
    'Generates a plot comparing a true and predicted mass spec spectra.\n\n  If output_filename given, saves a png file of the spectra, with the\n  true spectrum above and predicted spectrum below.\n\n  Args:\n    true_dense_spectra : np.array representing the true mass spectra\n    generated_dense_spectra : np.array representing the predicted mass spectra\n    plot_mode_key: a PlotModeKeys instance\n    output_filename : str path for saving generated image.\n    rescale_mz_axis: Setting to rescale m/z axis according to highest m/z peak\n        location.\n\n  Returns:\n    np.array of the bits of the generated matplotlib plot.\n  '
    if (not rescale_mz_axis):
        x_array = np.arange(SPECTRA_PLOT_PEAK_LOC_LIMIT)
        bar_width = SPECTRA_PLOT_BAR_LINE_WIDTH
        mz_max = SPECTRA_PLOT_PEAK_LOC_LIMIT
    else:
        mz_max = max(max(np.nonzero(true_dense_spectra)[0]), max(np.nonzero(generated_dense_spectra)[0]))
        if ((mz_max + SPECTRA_PLOT_MZ_MAX_OFFSET) < ms_constants.MAX_PEAK_LOC):
            mz_max += SPECTRA_PLOT_MZ_MAX_OFFSET
        else:
            mz_max = ms_constants.MAX_PEAK_LOC
        x_array = np.arange(mz_max)
        true_dense_spectra = true_dense_spectra[:mz_max]
        generated_dense_spectra = generated_dense_spectra[:mz_max]
        bar_width = ((SPECTRA_PLOT_BAR_LINE_WIDTH * mz_max) / ms_constants.MAX_PEAK_LOC)
    figure = plt.figure(figsize=SPECTRA_PLOT_FIGURE_SIZE, dpi=300)
    ax_main = figure.add_subplot(111, frameon=False)
    ax_main.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')
    ax_main.set_xlabel(SPECTRA_PLOT_X_AXIS_LABEL)
    ax_main.set_ylabel(SPECTRA_PLOT_Y_AXIS_LABEL)
    if six.PY2:
        ax_top = figure.add_subplot(211, axisbg=SPECTRA_PLOT_BACKGROUND_COLOR)
    else:
        ax_top = figure.add_subplot(211, facecolor=SPECTRA_PLOT_BACKGROUND_COLOR)
    bar_top = ax_top.bar(x_array, true_dense_spectra, bar_width, color=SPECTRA_PLOT_TRUE_SPECTRA_COLOR, edgecolor=SPECTRA_PLOT_TRUE_SPECTRA_COLOR)
    ax_top.set_ylim((0, SPECTRA_PLOT_INTENSITY_LIMIT))
    plt.setp(ax_top.get_xticklabels(), visible=False)
    ax_top.grid(color=SPECTRA_PLOT_GRID_COLOR, linewidth=SPECTRA_PLOT_BAR_GRID_LINE_WIDTH)
    if six.PY2:
        ax_bottom = figure.add_subplot(212, axisbg=SPECTRA_PLOT_BACKGROUND_COLOR)
    else:
        ax_bottom = figure.add_subplot(212, facecolor=SPECTRA_PLOT_BACKGROUND_COLOR)
    figure.subplots_adjust(hspace=0.0)
    bar_bottom = ax_bottom.bar(x_array, generated_dense_spectra, bar_width, color=SPECTRA_PLOT_PREDICTED_SPECTRA_COLOR, edgecolor=SPECTRA_PLOT_PREDICTED_SPECTRA_COLOR)
    ax_bottom.set_ylim((SPECTRA_PLOT_INTENSITY_LIMIT, 0))
    ax_bottom.set_xlim(0, mz_max)
    yticks_bottom = ax_bottom.yaxis.get_major_ticks()
    yticks_bottom[0].label1.set_visible(False)
    ax_bottom.grid(color=SPECTRA_PLOT_GRID_COLOR, linewidth=SPECTRA_PLOT_BAR_GRID_LINE_WIDTH)
    for ax in [ax_top, ax_bottom]:
        ax.minorticks_on()
        ax.tick_params(axis='y', which='minor', left='off')
        ax.tick_params(axis='y', which='minor', right='off')
    ax_top.tick_params(axis='x', which='minor', top='off')
    if (plot_mode_key == PlotModeKeys.PREDICTED_SPECTRUM):
        ax_top.legend((bar_top, bar_bottom), (SPECTRA_PLOT_ACTUAL_SPECTRA_LEGEND_TEXT, SPECTRA_PLOT_PREDICTED_SPECTRA_LEGEND_TEXT), **SPECTRA_PLOT_PLACE_LEGEND_ABOVE_CHART_KWARGS)
    elif (plot_mode_key == PlotModeKeys.LIBRARY_MATCHED_SPECTRUM):
        ax_top.legend((bar_top, bar_bottom), (SPECTRA_PLOT_QUERY_SPECTRA_LEGEND_TEXT, SPECTRA_PLOT_LIBRARY_MATCH_SPECTRA_LEGEND_TEXT), **SPECTRA_PLOT_PLACE_LEGEND_ABOVE_CHART_KWARGS)
    figure.canvas.draw()
    data = np.fromstring(figure.canvas.tostring_rgb(), dtype=np.uint8, sep='')
    try:
        data = np.reshape(data, SPECTRA_PLOT_DIMENSIONS_RGB)
    except ValueError:
        raise ValueError('The shape of the np.array generated from the data does not match the values in SPECTRA_PLOT_DIMENSIONS_RGB : {}'.format(SPECTRA_PLOT_DIMENSIONS_RGB))
    if output_filename:
        if ((not output_filename.endswith('.png')) or output_filename.endswith('.eps')):
            output_filename += '.png'
        with tf.gfile.GFile(output_filename, 'wb') as out:
            image = PilImage.fromarray(data).convert('RGB')
            image.save(out, dpi=(SPECTRA_PLOT_DPI, SPECTRA_PLOT_DPI))
    tf.logging.info('Shape of spectra plot data {} '.format(np.shape(data)))
    plt.close(figure)
    return data

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure

idx = 62:------------------- similar code ------------------ index = 47, score = 7.0 
def save_figs(filename=None, open=True, folder='.', save_latest=True):
    '\n\n    :param filename: (optional) path to pdf file for saving.\n    :param open: boolean flag for opening pdf file after saving\n    :param folder: folder to save pdf files (default=current dir)\n    :param save_latest: boolean flag for creating a "latest.pdf" in folder directory, symlinked to latest plots.\n    :return:\n    '
    if (filename is None):
        filename = time.strftime('%Y%m%d-%H%M%S.pdf')
    if (folder is not None):
        if (not os.path.exists(folder)):
            os.makedirs(folder, exist_ok=True)
        filename = os.path.join(folder, filename)
    fn = os.path.join(os.getcwd(), filename)
    pp = PdfPages(fn)
    for i in plt.get_fignums():
        plt.figure(i).tight_layout()
        pp.savefig(plt.figure(i))
        plt.close(plt.figure(i))
    pp.close()
    if save_latest:
        try:
            latest_path = os.path.join(folder, 'latest.pdf')
            if os.path.exists(latest_path):
                os.remove(latest_path)
            os.symlink(filename, latest_path)
            if open:
                _open_figs(latest_path)
            return
        except OSError:
            warnings.warn('Cannot create symbolic link in Windows without administrator privileges. Skipping.')
    if open:
        _open_figs(filename)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for  ...  in:
        plt.figure

idx = 63:------------------- similar code ------------------ index = 30, score = 6.0 
def plot_memory_usage(self, plot_file):
    '\n        Plots memory usage for each operator in the schedule as a stacked bar chart.\n        :param plot_file: Output file\n        '
    import matplotlib.pyplot as plt
    labels = []
    input_sizes = []
    output_sizes = []
    other_sizes = []
    schedule = self._execution_schedule_info()
    peak_mem_use = 0
    for (op, working_set, mem_use, _, _) in schedule:
        input_size = TFLiteModel._cum_tensor_sizes(op.non_empty_inputs)
        output_size = op.output.size
        other_size = TFLiteModel._cum_tensor_sizes((t for t in working_set if ((t not in op.non_empty_inputs) and (t != op.output))))
        assert (((input_size + output_size) + other_size) == mem_use)
        peak_mem_use = max(peak_mem_use, mem_use)
        labels.append(op.output.name)
        input_sizes.append(input_size)
        output_sizes.append(output_size)
        other_sizes.append(other_size)
    input_sizes = (np.array(input_sizes) / 1024)
    output_sizes = (np.array(output_sizes) / 1024)
    other_sizes = (np.array(other_sizes) / 1024)
    peak_mem_use /= 1024
    fig = plt.figure(figsize=(max((len(labels) / 3.5), 6), 8))
    fig.tight_layout()
    ax = fig.gca()
    x = np.arange(0, len(labels))
    ax.bar(x, input_sizes, color='#D95319', label='Operator inputs')
    ax.bar(x, output_sizes, bottom=input_sizes, color='#EDB120', label='Operator outputs')
    ax.bar(x, other_sizes, bottom=(input_sizes + output_sizes), color='#0072BD', label='Other tensors')
    ax.set_xticks(x)
    ax.set_xlabel('Operators')
    ax.set_ylabel('Memory usage (KB)')
    ax.set_ylim([0, (peak_mem_use + 10)])
    ax.set_xticklabels(labels, rotation=90)
    ax.legend()
    plt.savefig(plot_file, bbox_inches='tight', dpi=300)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure

idx = 64:------------------- similar code ------------------ index = 2, score = 6.0 
def ddk_gaussian(filter_type, D, visible=None):
    "\n    Given a specific type of DDK filter and the maximum SHC degree number, evaluate the 'equivalent' Gaussian filter radius. \n    Different Gaussian filter radius corresponds to a different correlation between DDK filer and Gaussian filter.\n    According to the rule that the largest correlation corresponds to the 'equivalent' radius, this program try to find out and visualize them.\n\n    Usage:\n    ddk_gausian('DDK5',96)\n    ddk_gausian('DDK3',60,'visible')\n\n    Inputs:\n    filter_type -> [str] Types of DDK filter. Available options are 'DDK1' to 'DDK8'.\n    D -> [int] Degree and order of SHC\n\n    Parameters:\n    visible -> [optional, str, default = None] If None, the visualization of the Point Spreading Function(PSF), DDK filtered PSF, Gaussian filtered PSF at the northpole as well as their cross section will be closed. If 'visible', they will be visualized by outputing an image.\n    \n    Outputs:\n    Print the 'equivalent' Gaussian filter radius and the correlation coefficient between the given DDK filer and the Gaussian filter with the 'equivalent' radius. \n    Also, images of the Point Spreading Function(PSF), DDK filtered PSF, Gaussian filtered PSF at the northpole as well as their cross section can be generated in the figures directory. \n    "
    cap_at_equator_grids_class = SHGrid.from_cap(0.1, 0, 0, D)
    cap_at_equator_coeffs_class = cap_at_equator_grids_class.expand()
    cap_at_northpole_coeffs_class = cap_at_equator_coeffs_class.rotate(0, 90, 0)
    cap_at_northpole_grids_class = cap_at_northpole_coeffs_class.expand()
    cap_at_northpole_coeffs = cap_at_northpole_coeffs_class.coeffs
    cap_at_northpole_grids = cap_at_northpole_grids_class.data
    filt_SHC_ddk = filter_ddk(filter_type, cap_at_northpole_coeffs)
    ddk_power = np.sum((filt_SHC_ddk ** 2))
    corrs = []
    rs = range(20, 550, 5)
    for r in rs:
        filt_SHC_gau = filter_gaussian(r, cap_at_northpole_coeffs)
        gau_power = np.sum((filt_SHC_gau ** 2))
        ddk_gau_crosspower = np.sum((filt_SHC_ddk * filt_SHC_gau))
        alpha = (ddk_gau_crosspower / np.sqrt((ddk_power * gau_power)))
        corrs.append(alpha)
    corrs = np.array(corrs)
    max_corrs_index = np.argmax(corrs)
    filt_SHC_gau = filter_gaussian(rs[max_corrs_index], cap_at_northpole_coeffs)
    print('Correlation: {:.4f}\nApproximate equivalent Gaussian filter radius for {:s}: {:d}'.format(corrs[max_corrs_index], filter_type, rs[max_corrs_index]))
    thetas = np.arange(0, 20, 0.1)
    (value, value_ddk, value_gau) = ([], [], [])
    for theta in thetas:
        value.append(MakeGridPoint(cap_at_northpole_coeffs, (90 - theta), 0))
        value_ddk.append(MakeGridPoint(filt_SHC_ddk, (90 - theta), 0))
        value_gau.append(MakeGridPoint(filt_SHC_gau, (90 - theta), 0))
    value = np.array(value)
    value_ddk = np.array(value_ddk)
    value_gau = np.array(value_gau)
    if (visible is not None):
        fig_dir = 'figures/'
        if (not path.exists(fig_dir)):
            makedirs(fig_dir)
        cap_at_northpole_grids_ddk = MakeGridDH(filt_SHC_ddk, sampling=2)
        cap_at_northpole_grids_gau = MakeGridDH(filt_SHC_gau, sampling=2)
        (lons, lats) = (cap_at_northpole_grids_class.lons(), cap_at_northpole_grids_class.lats())
        fig_name = 'raw_ddk_gau.png'
        (fig_name1, fig_name2, fig_name3) = ('psf_raw.png', 'psf_ddk.png', 'psf_gau.png')
        magnify = 1000.0
        plot_at_northpole(lons, lats, cap_at_northpole_grids, (fig_dir + fig_name1), '[mm w.e.]', magnify)
        plot_at_northpole(lons, lats, cap_at_northpole_grids_ddk, (fig_dir + fig_name2), '[mm w.e.]', magnify)
        plot_at_northpole(lons, lats, cap_at_northpole_grids_gau, (fig_dir + fig_name3), '[mm w.e.]', magnify)
        fig = plt.figure(dpi=200)
        ax = fig.add_subplot(111)
        ax.plot(thetas, (value * magnify), 'y', label='raw')
        ax.plot(thetas, (value_ddk * magnify), 'b--', label='ddk')
        ax.plot(thetas, (value_gau * magnify), 'r--', label='gaussian')
        ax.set_xlabel('$\\theta$ [deg]', size='small')
        ax.set_ylabel('[mm w.e.]', size='small')
        ax.legend()
        plt.savefig((fig_dir + fig_name), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = plt.figure

idx = 65:------------------- similar code ------------------ index = 3, score = 6.0 
def plot(array):
    fig = plt.figure(figsize=(30, 5))
    ax = fig.add_subplot(111)
    ax.xaxis.label.set_color('grey')
    ax.yaxis.label.set_color('grey')
    ax.xaxis.label.set_fontsize(23)
    ax.yaxis.label.set_fontsize(23)
    ax.tick_params(axis='x', colors='grey', labelsize=23)
    ax.tick_params(axis='y', colors='grey', labelsize=23)
    plt.plot(array)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = plt.figure

idx = 66:------------------- similar code ------------------ index = 18, score = 6.0 
def animate_plot1D(x, y, save=False, interval=50, dpi=80):
    if isinstance(y[0], State):
        y = get_activities_over_time_as_list(y)
    fig1 = plt.figure()
    (line,) = plt.plot(x, y[0])

    def update_line(activity):
        line.set_data(x, activity)
        return (line,)
    ani = animation.FuncAnimation(fig1, update_line, frames=y, blit=True, interval=interval)
    if save:
        ani.save('plot.gif', dpi=dpi, writer='imagemagick')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 67:------------------- similar code ------------------ index = 13, score = 6.0 
def plot_progress(self):
    '\n        Should probably by improved\n        :return:\n        '
    try:
        font = {'weight': 'normal', 'size': 18}
        matplotlib.rc('font', **font)
        fig = plt.figure(figsize=(30, 24))
        ax = fig.add_subplot(111)
        ax2 = ax.twinx()
        x_values = list(range((self.epoch + 1)))
        ax.plot(x_values, self.all_tr_losses, color='b', ls='-', label='loss_tr')
        ax.plot(x_values, self.all_val_losses, color='r', ls='-', label='loss_val, train=False')
        if (len(self.all_val_losses_tr_mode) > 0):
            ax.plot(x_values, self.all_val_losses_tr_mode, color='g', ls='-', label='loss_val, train=True')
        if (len(self.all_val_eval_metrics) == len(x_values)):
            ax2.plot(x_values, self.all_val_eval_metrics, color='g', ls='--', label='evaluation metric')
        ax.set_xlabel('epoch')
        ax.set_ylabel('loss')
        ax2.set_ylabel('evaluation metric')
        ax.legend()
        ax2.legend(loc=9)
        fig.savefig(join(self.output_folder, 'progress.png'))
        plt.close()
    except IOError:
        self.print_to_log_file('failed to plot: ', sys.exc_info())

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    try:
         ...  = plt.figure

idx = 68:------------------- similar code ------------------ index = 4, score = 6.0 
def plot_arithmetic(extension):
    yolo_path = os.path.join(data_dir, 'arithmetic/2stage/run_search_sample_complexity-size=14_colour=False_task=arithmetic_alg=yolo_math_2stage_duration=long_seed=0_2018_05_15_00_32_28')
    simple_path = os.path.join(data_dir, 'arithmetic/simple/run_search_sample_complexity-size=14_colour=False_task=arithmetic_alg=yolo_math_simple_duration=long_seed=0_2018_05_15_00_01_16')
    simple_2stage_path = os.path.join(data_dir, 'arithmetic/simple_2stage/run_search_sample_complexity-size=14_colour=False_task=arithmetic_alg=yolo_math_simple_2stage_duration=long_seed=0_2018_05_15_12_59_19')
    fig = plt.figure(figsize=(5, 3.5))
    ax = plt.gca()
    measure = 'math_accuracy'
    (x, y, *yerr) = get_arithmetic_data([yolo_path], 'n_train', measure, 1, 'ci95')
    label = 'SI-AIR'
    ax.errorbar(x, y, yerr=yerr, label=label)
    (x, y, *yerr) = get_arithmetic_data([simple_path], 'n_train', measure, 0, 'ci95')
    label = 'Conv'
    ax.errorbar(x, y, yerr=yerr, label=label)
    (x, y, *yerr) = get_arithmetic_data([simple_2stage_path], 'n_train', measure, 0, 'ci95')
    label = 'Conv - 2stage'
    ax.errorbar(x, y, yerr=yerr, label=label)
    ax.set_ylabel('Accuracy', fontsize=12)
    ax.set_xlabel('# Training Samples / 1000', fontsize=12)
    ax.set_title('Arithmetic - Between 1 and 11 numbers', fontsize=12)
    ax.tick_params(axis='both', labelsize=14)
    ax.set_ylim((0.0, 1.05))
    ax.set_xticks(x)
    ax.set_xticklabels((np.array(x) / 1000).astype('i'))
    plt.legend(loc='upper left')
    plot_path = os.path.join(plot_dir, ('arithmetic/main.' + extension))
    os.makedirs(os.path.dirname(plot_path), exist_ok=True)
    plt.subplots_adjust(left=0.12, bottom=0.14, right=0.98, top=0.91)
    fig.savefig(plot_path)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = plt.figure

idx = 69:------------------- similar code ------------------ index = 14, score = 6.0 
def plot_performance_quad(returns, fig_path=None, fig_name='heat_map_quad', font_size=20):

    def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):
        new_cmap = colors.LinearSegmentedColormap.from_list('trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval), cmap(np.linspace(minval, maxval, n)))
        return new_cmap
    fig = plt.figure(figsize=(16, 9))
    fig.suptitle(returns.name, fontsize=16)
    gs = gridspec.GridSpec(2, 2, wspace=0.2, hspace=0.3)
    ax_heatmap = plt.subplot(gs[(0, 0)])
    ax_monthly = plt.subplot(gs[(0, 1)])
    ax_box_plot = plt.subplot(gs[(1, 0)])
    ax_yearly = plt.subplot(gs[(1, 1)])
    monthly_ret_table = pf.timeseries.aggregate_returns(returns, 'monthly')
    monthly_ret_table = monthly_ret_table.unstack().round(3)
    ax = plt.gca()
    cmap = cm.viridis
    new_cmap = truncate_colormap(cmap, 0.2, 0.8)
    sns.heatmap((monthly_ret_table.fillna(0) * 100.0), annot=True, annot_kws={'size': font_size}, alpha=1.0, center=0.0, cbar=False, mask=monthly_ret_table.isna(), cmap=new_cmap, ax=ax_heatmap)
    ax_heatmap.set_xticklabels(np.arange(0.5, 12.5, step=1))
    ax_heatmap.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)
    ylabels = ax_heatmap.get_yticklabels()
    ax_heatmap.set_yticklabels(ylabels, rotation=45)
    ax_heatmap.set_xlabel('')
    ax_heatmap.set_ylabel('')
    pf.plotting.plot_monthly_returns_dist(returns, ax=ax_monthly)
    ax_monthly.xaxis.set_major_formatter(FormatStrFormatter('%.1f%%'))
    ax_monthly.set_xlabel('')
    leg1 = ax_monthly.legend(['mean'], framealpha=0.0, prop={'size': font_size})
    for text in leg1.get_texts():
        text.set_label('mean')
    df_weekly = pf.timeseries.aggregate_returns(returns, convert_to='weekly')
    df_monthly = pf.timeseries.aggregate_returns(returns, convert_to='monthly')
    pf.plotting.plot_return_quantiles(returns, df_weekly, df_monthly, ax=ax_box_plot)
    pf.plotting.plot_annual_returns(returns, ax=ax_yearly)
    _ = ax_yearly.legend(['mean'], framealpha=0.0, prop={'size': font_size})
    ax_yearly.xaxis.set_major_formatter(FormatStrFormatter('%.1f%%'))
    plt.xticks(rotation=45)
    ax_yearly.set_xlabel('')
    ax_yearly.set_ylabel('')
    for ax in [ax_box_plot, ax_heatmap, ax_monthly, ax_yearly]:
        for item in (([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels()) + ax.get_yticklabels()):
            item.set_fontsize(font_size)
    for items in (ax_yearly.get_yticklabels() + ax_heatmap.get_yticklabels()):
        items.set_fontsize((font_size - 5))
    if (fig_path is not None):
        if Path.is_dir(fig_path):
            plt.savefig((fig_path / fig_name), dpi=600, bbox_inches='tight', transparent=True)
        return fig

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():

     ...  = plt.figure

idx = 70:------------------- similar code ------------------ index = 26, score = 6.0 
def SITKshow(img, title=None, margin=0.05, dpi=80):
    import matplotlib.pyplot as plt
    nda = sitk.GetArrayViewFromImage(img)
    spacing = img.GetSpacing()
    ysize = nda.shape[0]
    xsize = nda.shape[1]
    figsize = ((((1 + margin) * ysize) / dpi), (((1 + margin) * xsize) / dpi))
    fig = plt.figure(title, figsize=figsize, dpi=dpi)
    ax = fig.add_axes([margin, margin, (1 - (2 * margin)), (1 - (2 * margin))])
    extent = (0, (xsize * spacing[1]), 0, (ysize * spacing[0]))
    t = ax.imshow(nda, extent=extent, interpolation='hamming', cmap='gray', origin='lower')
    if title:
        plt.title(title)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure

idx = 71:------------------- similar code ------------------ index = 7, score = 6.0 
def draw_voxel_model(voxels, is_show=True, save_path=None):
    import matplotlib.pyplot as plt
    if (not is_show):
        import matplotlib
        matplotlib.use('Agg')
    from mpl_toolkits.mplot3d import Axes3D
    from matplotlib.colors import LightSource
    import seaborn as sns
    color_num = voxels.max()
    current_palette = sns.color_palette(as_cmap=True)
    colors = np.empty(voxel.shape, dtype=object)
    for i in range(color_num):
        colors[(voxel == (i + 1))] = current_palette[i]
    fig = plt.figure()
    ax = fig.gca(projection='3d')
    ax.voxels(voxels, facecolors=colors, lightsource=LightSource(azdeg=315, altdeg=45))
    if is_show:
        plt.show()
    if (save_path is not None):
        plt.savefig(save_path, transparent=True)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 72:------------------- similar code ------------------ index = 20, score = 6.0 
def save_attention(attn, path):
    fig = plt.figure(figsize=(12, 6))
    plt.imshow(attn.T, interpolation='nearest', aspect='auto')
    fig.savefig(f'{path}.png', bbox_inches='tight')
    plt.close(fig)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure

idx = 73:------------------- similar code ------------------ index = 21, score = 6.0 
def _plot_reconstruction(self, updater, fetched):
    inp = fetched['inp']
    output = fetched['output']
    (_, image_height, image_width, _) = inp.shape
    obj = fetched['obj'].reshape(self.N, (- 1))
    anchor_box = updater.network.anchor_box
    (yt, xt, ys, xs) = np.split(fetched['normalized_box'], 4, axis=(- 1))
    (yt, xt, ys, xs) = coords_to_pixel_space(yt, xt, ys, xs, (image_height, image_width), anchor_box, top_left=True)
    box = np.concatenate([yt, xt, ys, xs], axis=(- 1))
    box = box.reshape(self.N, (- 1), 4)
    on_colour = np.array(to_rgb('xkcd:azure'))
    off_colour = np.array(to_rgb('xkcd:red'))
    for (n, (pred, gt)) in enumerate(zip(output, inp)):
        fig = plt.figure(figsize=(5, 5))
        ax = plt.gca()
        self.imshow(ax, gt)
        ax.set_axis_off()
        for (o, (top, left, height, width)) in zip(obj[n], box[n]):
            if ((not self.show_zero_boxes) and (o <= 1e-06)):
                continue
            colour = ((o * on_colour) + ((1 - o) * off_colour))
            rect = patches.Rectangle((left, top), width, height, linewidth=2, edgecolor=colour, facecolor='none')
            ax.add_patch(rect)
        plt.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01, wspace=0.1, hspace=0.1)
        self.savefig(('ground_truth/' + str(n)), fig, updater, is_dir=False)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    for in:
         ...  = plt.figure

idx = 74:------------------- similar code ------------------ index = 22, score = 6.0 
@staticmethod
def _tsplot(y, lags=None, figsize=(16, 9), style='bmh'):
    if (not isinstance(y, pd.Series)):
        y = pd.Series(y)
    with plt.style.context(style):
        _ = plt.figure(figsize=figsize)
        mpl.rcParams['font.sans-serif'] = 'Roboto Condensed'
        mpl.rcParams['font.family'] = 'sans-serif'
        layout = (3, 2)
        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)
        acf_ax = plt.subplot2grid(layout, (1, 0))
        pacf_ax = plt.subplot2grid(layout, (1, 1))
        qq_ax = plt.subplot2grid(layout, (2, 0))
        pp_ax = plt.subplot2grid(layout, (2, 1))
        y.plot(ax=ts_ax)
        ts_ax.set_title('Time Series Analysis Plots')
        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)
        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)
        sm.qqplot(y, line='s', ax=qq_ax)
        qq_ax.set_title('QQ Plot')
        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)
        plt.tight_layout()
    return

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = plt.figure
    return

idx = 75:------------------- similar code ------------------ index = 33, score = 6.0 
def plot_3d_func(X, Y, Z, zlabel, figsize):
    'Plot a 3 dimensional function.\n\n    Plots a 3 dimensional function, where X, Y, Z form a meshgrid, with the\n    usual functional relationship: z = f(x, y).\n\n    Args:\n        X (np.array): Meshgrid on first dimension.\n        Y (np.array): Meshgrid on second dimension.\n        Z (np.array): Meshgrid on outcome dimensions.\n        zlabel (str): Name of z-axis.\n        figsize (tuple): Figure size.\n\n    Returns:\n        ax (matplotlib.axis): The finished plot.\n\n    '
    mpl.rcParams.update({'font.family': 'stix'})
    mpl.rcParams.update({'font.size': 30})
    plt.rcParams.update({'font.size': 22})
    fig = plt.figure()
    ax = fig.gca(projection=Axes3D.name)
    ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel(zlabel)
    ax.set_zlim((0, 5))
    ax.yaxis.labelpad = 30
    ax.zaxis.labelpad = 10
    ax.xaxis.labelpad = 30
    ax.view_init(30, 240)
    ax.grid(False)
    ax.xaxis.pane.set_edgecolor('black')
    ax.yaxis.pane.set_edgecolor('black')
    ax.xaxis.pane.fill = False
    ax.yaxis.pane.fill = False
    ax.zaxis.pane.fill = False
    plt.rcParams['figure.figsize'] = [figsize[0], figsize[1]]

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 76:------------------- similar code ------------------ index = 99, score = 6.0 
def plot_warp(warp2d, gray_image0, depth_map0, gray_image1):
    from tadataka.interpolation import interpolation
    from tadataka.coordinates import image_coordinates
    from tadataka.utils import is_in_image_range
    from matplotlib import pyplot as plt
    us0 = image_coordinates(depth_map0.shape)
    depths0 = depth_map0.flatten()
    (us1, depths1) = warp2d(us0, depths0)
    mask = is_in_image_range(us1, depth_map0.shape)
    fig = plt.figure()
    E = photometric_error(warp2d, gray_image0, depth_map0, gray_image1)
    fig.suptitle('photometric error = {:.3f}'.format(E))
    ax = fig.add_subplot(221)
    ax.set_title('t0 intensities')
    ax.imshow(gray_image0, cmap='gray')
    ax = fig.add_subplot(223)
    ax.set_title('t0 depth')
    ax.imshow(depth_map0, cmap='gray')
    ax = fig.add_subplot(222)
    ax.set_title('t1 intensities')
    ax.imshow(gray_image1, cmap='gray')
    ax = fig.add_subplot(224)
    ax.set_title('predicted t1 intensities')
    (height, width) = gray_image1.shape
    ax.scatter(us1[(mask, 0)], us1[(mask, 1)], c=gray_image0[(us0[(mask, 1)], us0[(mask, 0)])], s=0.5, cmap='gray')
    ax.set_xlim(0, width)
    ax.set_ylim(height, 0)
    ax.set_aspect('equal')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 77:------------------- similar code ------------------ index = 98, score = 6.0 
def plot_prior(image, depth_map_true, depth_map_pred, variance_map_pred, image_cmap='gray', depth_cmap='RdBu'):
    fig = plt.figure()
    fig.suptitle('Prior')
    vmin = min(np.min(depth_map_true), np.min(depth_map_pred))
    vmax = max(np.max(depth_map_true), np.max(depth_map_pred))
    norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)
    mapper = ScalarMappable(norm=norm, cmap=depth_cmap)
    ax = fig.add_subplot(221)
    ax.set_title('frame')
    ax.imshow(image, cmap=image_cmap)
    ax = fig.add_subplot(222)
    ax.set_title('ground truth depth map')
    im = ax.imshow(depth_map_true, norm=norm, cmap=depth_cmap)
    ax = fig.add_subplot(223)
    ax.set_title('prior depth map')
    im = ax.imshow(depth_map_pred, norm=norm, cmap=depth_cmap)
    plot_with_bar(ax, im)
    ax = fig.add_subplot(224)
    ax.set_title('prior variance map')
    im = ax.imshow(variance_map_pred, cmap=depth_cmap)
    plot_with_bar(ax, im)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 78:------------------- similar code ------------------ index = 94, score = 6.0 
@exp.automain
def compute_components(n_components, batch_size, learning_rate, method, reduction, alpha, step_size, n_jobs, n_epochs, verbose, source, _run):
    basedir = join(_run.observers[0].basedir, str(_run._id))
    artifact_dir = join(basedir, 'artifacts')
    if (not os.path.exists(artifact_dir)):
        os.makedirs(artifact_dir)
    if (source == 'hcp'):
        train_size = None
        smoothing_fwhm = 3
        test_size = 2
        data_dir = get_data_dirs()[0]
        mask = fetch_hcp_mask()
        masker = MultiRawMasker(mask_img=mask, smoothing_fwhm=smoothing_fwhm, detrend=True, standardize=True)
        mapping = json.load(open(join(data_dir, 'HCP_unmasked/mapping.json'), 'r'))
        data = sorted(list(mapping.values()))
        data = list(map((lambda x: join(data_dir, x)), data))
        data = pd.DataFrame(data, columns=['filename'])
    else:
        smoothing_fwhm = 6
        train_size = 4
        test_size = 4
        raw_res_dir = join(get_output_dir(), 'unmasked', source)
        try:
            (masker, data) = get_raw_rest_data(raw_res_dir)
        except ValueError:
            raw_res_dir = join(get_output_dir(), 'unmask', source)
            (masker, data) = get_raw_rest_data(raw_res_dir)
    (train_imgs, test_imgs) = train_test_split(data, test_size=test_size, random_state=0, train_size=train_size)
    train_imgs = train_imgs['filename'].values
    test_imgs = test_imgs['filename'].values
    cb = rfMRIDictionaryScorer(test_imgs, info=_run.info)
    dict_fact = fMRIDictFact(method=method, mask=masker, verbose=verbose, n_epochs=n_epochs, n_jobs=n_jobs, random_state=1, n_components=n_components, smoothing_fwhm=smoothing_fwhm, learning_rate=learning_rate, batch_size=batch_size, reduction=reduction, step_size=step_size, alpha=alpha, callback=cb)
    dict_fact.fit(train_imgs)
    dict_fact.components_img_.to_filename(join(artifact_dir, 'components.nii.gz'))
    fig = plt.figure()
    display_maps(fig, dict_fact.components_img_)
    plt.savefig(join(artifact_dir, 'components.png'))
    (fig, ax) = plt.subplots(1, 1)
    ax.plot(cb.cpu_time, cb.score, marker='o')
    _run.info['time'] = cb.cpu_time
    _run.info['score'] = cb.score
    _run.info['iter'] = cb.iter
    plt.savefig(join(artifact_dir, 'score.png'))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 79:------------------- similar code ------------------ index = 92, score = 6.0 
def __call__(self, u_key, depth_key):
    u_ref = self.warp(u_key, depth_key)
    fig = plt.figure()
    ax = fig.add_subplot(121)
    ax.set_title('keyframe')
    ax.imshow(self.image_key)
    ax.scatter(u_key[0], u_key[1], c='red')
    ax = fig.add_subplot(122)
    ax.set_title('reference frame')
    ax.imshow(self.image_ref)
    ax.scatter(u_ref[0], u_ref[1], c='red')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 80:------------------- similar code ------------------ index = 91, score = 6.0 
def plot_addition_old(extension):
    yolo_path = os.path.join(data_dir, 'addition/2stage/run_search_sample_complexity_experiment_yolo_air_VS_nips_2018_addition_14x14_kind=long_cedar_seed=0_2018_05_14_03_04_29')
    yolo_supplement_path = os.path.join(data_dir, 'addition/2stage/run_search_supplement_sample_complexity_experiment_yolo_air_VS_nips_2018_addition_14x14_kind=supplement_seed=0_2018_05_14_14_18_26')
    simple_path = os.path.join(data_dir, 'addition/simple/run_search_sample_complexity-size=14_colour=False_task=addition_alg=yolo_math_simple_duration=long_seed=0_2018_05_14_23_59_50')
    simple_2stage_path = os.path.join(data_dir, 'addition/simple_2stage/run_search_sample_complexity-size=14_colour=False_task=addition_alg=yolo_math_simple_2stage_duration=long_seed=0_2018_05_15_12_55_38')
    fig = plt.figure(figsize=(5, 3.5))
    ax = plt.gca()
    measure = 'math_accuracy'
    (x, y, *yerr) = get_arithmetic_data([yolo_path, yolo_supplement_path], 'n_train', measure, 1, 'ci95')
    label = 'SI-AIR'
    ax.errorbar(x, y, yerr=yerr, label=label)
    (x, y, *yerr) = get_arithmetic_data([simple_path], 'n_train', measure, 0, 'ci95')
    label = 'Conv'
    ax.errorbar(x, y, yerr=yerr, label=label)
    (x, y, *yerr) = get_arithmetic_data([simple_2stage_path], 'n_train', measure, 0, 'ci95')
    label = 'Conv - 2stage'
    ax.errorbar(x, y, yerr=yerr, label=label)
    ax.set_ylabel('Accuracy', fontsize=12)
    ax.set_xlabel('\\# Training Samples / 1000', fontsize=12)
    ax.set_title('Addition - Between 1 and 11 numbers', fontsize=12)
    ax.tick_params(axis='both', labelsize=14)
    ax.set_ylim((0.0, 1.05))
    ax.set_xticks(x)
    ax.set_xticklabels((np.array(x) / 1000).astype('i'))
    plt.legend(loc='upper left')
    plot_path = os.path.join(plot_dir, ('addition/main.' + extension))
    os.makedirs(os.path.dirname(plot_path), exist_ok=True)
    plt.subplots_adjust(left=0.12, bottom=0.14, right=0.98, top=0.91)
    fig.savefig(plot_path)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = plt.figure

idx = 81:------------------- similar code ------------------ index = 90, score = 6.0 
def __init__(self):
    self.printing_voronoi = False
    self.robots = {}
    self.physics_time = 0.05
    rospy.init_node('simulator')
    self.vis_time = 0.05
    self.robot_pose_service = rospy.Service('set_robot_pose', SetRobotPose, self.robot_service)
    self.occ_grid_topic = ''
    self.tesselation_topic = ''
    self.robot_param = ''
    self.occ_grid_subscriber = None
    self.tesselation_subscriber = None
    self.voronoi_collection = None
    self.voronoi_axes = None
    self.voronoi_should_draw = False
    self.plot_handle = None
    self.obstacle_collection = None
    self.obstacle_axes = None
    self.fig = plt.figure(1)
    plt.gca().set_aspect('equal', adjustable='box')
    plt.axis([0, 20, 0, 20])
    self.occ_grid = OccGrid('static_map', self.fig)
    self.occ_grid.get_occ_grid()
    self.fig.canvas.draw()
    self.read_simulator_params()
    self.read_robot_parameters()
    self.physics_t = threading.Thread(target=self.physics_thread)
    self.physics_t.daemon = True
    self.visual_t = threading.Thread(target=self.visual_thread)
    self.visual_t.daemon = True
    self.loop_time = 0

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
 = plt.figure

idx = 82:------------------- similar code ------------------ index = 89, score = 6.0 
def poincare_plot(activities, timesteps, xlabel=None, ylabel=None, xlim=None, ylim=None, title=None):
    '\n    Create a Poincar plot.\n\n    :param activities: A list of activities. If the values of this list are also lists, then each will\n                       be plotted as a separate series.\n\n    :param timesteps: The number of timesteps in the trajectory to consider, starting from the end.\n\n    :param xlabel: A string representing the label of the x-axis.\n\n    :param ylabel: A string representing the label of the y-axis.\n\n    :param xlim: A 2-tuple of numbers, representing the limits of the x-axis.\n\n    :param ylim: A 2-tuple of numbers, or a list of at most two 2-tuples of numbers, representing the\n                 limits of the y-axis.\n\n    :param title: The plot title.\n    '
    cm = plt.get_cmap('gist_rainbow')
    fig = plt.figure()
    ax = fig.add_subplot(111)
    is_multiseries = isinstance(activities[0], (list, np.ndarray))
    if is_multiseries:
        ax.set_prop_cycle(color=[cm(((1.0 * i) / len(activities))) for i in range(len(activities))])
    else:
        activities = [activities]
    for a in activities:
        x = []
        y = []
        for t in range((timesteps - 1)):
            x.append(a[(- timesteps):][t])
            y.append(a[(- timesteps):][(t + 1)])
        plt.scatter(x, y, s=1)
    if xlim:
        plt.xlim(xlim)
    if ylim:
        plt.ylim(ylim)
    if xlabel:
        plt.xlabel(xlabel)
    if ylabel:
        plt.ylabel(ylabel)
    if title:
        plt.title(title)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 83:------------------- similar code ------------------ index = 88, score = 6.0 
def save(self):
    if (self._test_valid_step() is True):
        fig = plt.figure(figsize=self.figsize)
        plt.plot(self.step[1:], self.value[1:], '-')
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(self.path, (self.name + self.postfix)), bbox_inches='tight')
        plt.close(fig)
        fig = plt.figure(figsize=self.figsize)
        plt.plot(self.step[1:], self.ma_value[1:], '-')
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(self.path, ((self.name + '_ma') + self.postfix)), bbox_inches='tight')
        plt.close(fig)
    else:
        fig = plt.figure(figsize=self.figsize)
        plt.plot(self.value[1:], '-')
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(self.path, (self.name + self.postfix)), bbox_inches='tight')
        plt.close(fig)
        fig = plt.figure(figsize=self.figsize)
        plt.plot(self.ma_value[1:], '-')
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(self.path, ((self.name + '_ma') + self.postfix)), bbox_inches='tight')
        plt.close(fig)
    np.savez(os.path.join(self.path, (self.name + '.npz')), value=self.value[1:], ma_value=self.ma_value, original_value=self.original_value)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
    if:
         ...  = plt.figure

idx = 84:------------------- similar code ------------------ index = 85, score = 6.0 
def plot_per_image(rgb, ground_truth, pred, pred_entropy, probs, mask, dataset, exp_name, idx, deterministic=False):
    if (dataset == 'camvid'):
        (H, W) = (360, 480)
    im_ratio = float((H / W))
    rgb = rgb.view(3, H, W).permute(1, 2, 0).numpy()
    ground_truth = ground_truth.view(H, W).numpy()
    pred = pred.view(H, W).numpy()
    fig = plt.figure(1, figsize=(12, 2))
    ax1 = plt.subplot(151)
    im1 = ax1.imshow(rgb)
    ax1.axis('off')
    ax2 = plt.subplot(152)
    im2 = ax2.imshow(ground_truth)
    ax2.axis('off')
    ax3 = plt.subplot(153)
    im3 = ax3.imshow(pred)
    ax3.axis('off')
    if (not deterministic):
        pred_entropy = pred_entropy.view(H, W).numpy()
        probs = probs.numpy()
        mask = mask.view(1, (- 1)).numpy()
        ax4 = plt.subplot(154)
        im4 = ax4.imshow(pred_entropy, vmin=0.0, vmax=np.log(11.0))
        ax4.axis('off')
        cb4 = fig.colorbar(im4, ax=ax4, fraction=(0.046 * im_ratio), pad=0.04)
        cb4.ax.tick_params(labelsize=0)
        cb4.ax.tick_params(size=0)
        (true_freq, pred_freq, calibration) = calibration_per_image(probs, mask, ground_truth)
        print('Img: {} || Calibration: {:.5f}'.format(idx, calibration))
        ax5 = plt.subplot(155)
        ax5.set_aspect(im_ratio)
        ax5.xaxis.set_tick_params(labelsize=5)
        ax5.yaxis.set_tick_params(labelsize=5)
        ax5.plot(pred_freq, true_freq, color='red')
        ax5.plot([0.0, 1.0], [0.0, 1.0], 'g--')
        np.savetxt('{}_{}_calibration_score_{}.txt'.format(dataset, exp_name, idx), [calibration])
    plt.savefig('{}_{}_results_test_pred_{}.pdf'.format(dataset, exp_name, idx), bbox_inches='tight', pad_inches=0.1)
    plt.close()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure

idx = 85:------------------- similar code ------------------ index = 83, score = 6.0 
def visualize_confusion(writer, step, matrix, class_dict, save_path):
    '\n    Visualization of confusion matrix. Is saved to hard-drive and TensorBoard.\n\n    Parameters:\n        writer (tensorboard.SummaryWriter): TensorBoard SummaryWriter instance.\n        step (int): Counter usually specifying steps/epochs/time.\n        matrix (numpy.array): Square-shaped array of size class x class.\n            Should specify cross-class accuracies/confusion in percent\n            values (range 0-1).\n        class_dict (dict): Dictionary specifying class names as keys and\n            corresponding integer labels/targets as values.\n        save_path (str): Path used for saving\n    '
    all_categories = sorted(class_dict, key=class_dict.get)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    cax = ax.matshow(matrix)
    fig.colorbar(cax, boundaries=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])
    ax.set_xticklabels(([''] + all_categories), rotation=90)
    ax.set_yticklabels(([''] + all_categories))
    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.grid(False)
    plt.tight_layout()
    writer.add_figure('Training data', fig, global_step=str(step))
    plt.savefig(os.path.join(save_path, (('confusion_epoch_' + str(step)) + '.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 86:------------------- similar code ------------------ index = 76, score = 6.0 
def visualize_dataset_in_2d_embedding(writer, encoding_list, dataset_name, save_path, task=1):
    '\n    Visualization of 2-D latent embedding. Is saved to both hard-disc as well as TensorBoard.\n\n    Parameters:\n        writer (tensorboard.SummaryWriter): TensorBoard SummaryWriter instance.\n        encoding_list (list): List of Tensors containing encoding values\n        dataset_name (str): Dataset name.\n        save_path (str): Path used for saving.\n        task (int): task counter. Used for naming.\n    '
    num_classes = len(encoding_list)
    encoded_classes = []
    for i in range(len(encoding_list)):
        if isinstance(encoding_list[i], torch.Tensor):
            encoded_classes.append(([i] * encoding_list[i].size(0)))
        else:
            device = torch.device(('cuda' if torch.cuda.is_available() else 'cpu'))
            encoding_list[i] = torch.Tensor(encoding_list[i]).to(device)
            encoded_classes.append(([i] * 0))
    encoded_classes = np.concatenate(np.asarray(encoded_classes), axis=0)
    encoding = torch.cat(encoding_list, dim=0)
    if (encoding.size(1) != 2):
        print('Skipping visualization of latent space because it is not 2-D')
        return
    encoded_dim1 = np.squeeze(encoding.narrow(1, 0, 1).cpu().numpy())
    encoded_dim2 = np.squeeze(encoding.narrow(1, 1, 1).cpu().numpy())
    xlabel = 'z dimension 1'
    ylabel = 'z dimension 2'
    my_cmap = ListedColormap(sns.color_palette('Paired', num_classes).as_hex())
    fig = plt.figure(figsize=(20, 20))
    plt.scatter(encoded_dim1, encoded_dim2, c=encoded_classes, cmap=my_cmap)
    plt.xlabel(xlabel, fontsize=axes_font_size)
    plt.ylabel(ylabel, fontsize=axes_font_size)
    plt.xticks(fontsize=ticks_font_size)
    plt.yticks(fontsize=ticks_font_size)
    cbar = plt.colorbar(ticks=np.linspace(0, (num_classes - 1), num_classes))
    cbar.ax.set_yticklabels([str(i) for i in range(num_classes)])
    cbar.ax.tick_params(labelsize=legend_font_size)
    plt.tight_layout()
    writer.add_figure('latent_embedding', fig, global_step=task)
    plt.savefig(os.path.join(save_path, (((dataset_name + '_latent_2d_embedding_task_') + str(task)) + '.png')), bbox_inches='tight')

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure

idx = 87:------------------- similar code ------------------ index = 72, score = 6.0 
def plot_rolling_beta(self, **kwargs):
    (beta, rolling_window) = self.vix_beta(**kwargs)
    pc = PlotConstants()
    with plt.style.context('bmh'):
        _ = plt.figure(figsize=pc.fig_size, dpi=600, facecolor='None', edgecolor='None')
        gs = gridspec.GridSpec(1, 1, wspace=0.5, hspace=0.25)
        ax_beta = plt.subplot(gs[:])
        ax_beta = beta.plot(lw=1.5, ax=ax_beta, grid=True, alpha=0.4, color=pc.color_yellow, title='VIX beta to S&P500 - {} days rolling window'.format(rolling_window))
        ax_beta.set_ylabel('Beta')
        ax_beta.axhline(beta.mean(), color='k', ls='--', lw=0.75, alpha=1.0)
        chart_format([ax_beta], pc.color_light)
        plt.autoscale(enable=True, axis='x', tight=True)
    return ax_beta

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    with:
         ...  = plt.figure

idx = 88:------------------- similar code ------------------ index = 71, score = 6.0 
def plot_random_spec_img(pic, true_label):
    '\n    Take first hyperspectral image from dataset and plot spectral data distribution\n    Arguements pic = list of images in size (?, height, width, bands), where ? represents any number > 0\n                true_labels = lists of ground truth corrospond to pic\n    '
    pic = pic[0]
    from matplotlib import pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    from numpy import mean, argmax
    print(('Image Shape: ' + str(pic.shape)))
    print(('Label of this image is -> ' + str(true_label[0])))
    title = argmax(true_label[0], axis=0)
    mean_value = mean(pic)
    pic[(pic < mean_value)] = 0
    x = []
    y = []
    z = []
    for z1 in range(pic.shape[0]):
        for x1 in range(pic.shape[1]):
            for y1 in range(pic.shape[2]):
                if (pic[(z1, x1, y1)] != 0):
                    z.append(z1)
                    x.append(x1)
                    y.append(y1)
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.set_title(('True class = ' + str(title)))
    ax.scatter(x, y, z, color='#0606aa', marker='o', s=0.5)
    ax.set_xlabel('X Label')
    ax.set_ylabel('Spectral Label')
    ax.set_zlabel('Y Label')
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 89:------------------- similar code ------------------ index = 50, score = 6.0 
def plot_addition(extension):
    air_fixed_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=fixed_alg=air-math_duration=long_seed=0_2018_07_29_09_58_58/')
    baseline_fixed_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=fixed_alg=baseline-math_duration=long_seed=0_2018_07_28_22_01_21/')
    ground_truth_fixed_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=fixed_alg=ground-truth-math_duration=long_seed=0_2018_07_28_22_02_14/')
    simple_fixed_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=fixed_alg=simple-math_duration=long_seed=0_2018_07_28_22_03_18/')
    yolo_air_fixed_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=fixed_alg=yolo-air-math_duration=long_seed=0_2018_07_28_22_04_04/')
    baseline_raw_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=raw_alg=baseline-math_duration=long_seed=0_2018_07_28_22_00_58/')
    ground_truth_raw_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=raw_alg=ground-truth-math_duration=long_seed=0_2018_07_28_22_01_56/')
    simple_raw_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=raw_alg=simple-math_duration=long_seed=0_2018_07_28_22_02_59/')
    air_unfixed_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=unfixed_alg=air-math_duration=long_seed=0_2018_07_29_09_58_32/')
    baseline_unfixed_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=unfixed_alg=baseline-math_duration=long_seed=0_2018_07_28_22_01_39/')
    ground_truth_unfixed_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=unfixed_alg=ground-truth-math_duration=long_seed=0_2018_07_28_22_02_30/')
    simple_unfixed_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=unfixed_alg=simple-math_duration=long_seed=0_2018_07_28_22_03_41/')
    yolo_air_unfixed_path = os.path.join(data_dir, 'addition/stage2/run_search_addition-stage2_env=task=arithmetic2_run-kind=unfixed_alg=yolo-air-math_duration=long_seed=0_2018_07_28_22_04_21/')
    fig = plt.figure(figsize=(5, 4.5))
    ax = plt.gca()
    (x, y, *yerr) = get_arithmetic_data([baseline_raw_path], 'n_train', '_test_math_accuracy', 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='ConnComp', marker='o', ls='-')
    (x, y, *yerr) = get_arithmetic_data([ground_truth_raw_path], 'n_train', '_test_math_accuracy', 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='TrueBB', marker='^', ls='-')
    (x, y, *yerr) = get_arithmetic_data([simple_raw_path], 'n_train', '_test_math_accuracy', 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='ConvNet', marker='v', ls='-')
    (x, y, *yerr) = get_arithmetic_data([yolo_air_fixed_path], 'n_train', '_test_math_accuracy', 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='SPAIR - Fixed', marker='v', ls='-')
    yolo_air_color = line.lines[0].get_c()
    (x, y, *yerr) = get_arithmetic_data([yolo_air_unfixed_path], 'n_train', '_test_math_accuracy', 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='SPAIR - Unfixed', marker='v', ls='--', color=yolo_air_color)
    (x, y, *yerr) = get_arithmetic_data([air_fixed_path], 'n_train', '_test_math_accuracy', 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='AIR - Fixed', marker='o', ls='-')
    air_color = line.lines[0].get_c()
    (x, y, *yerr) = get_arithmetic_data([air_unfixed_path], 'n_train', '_test_math_accuracy', 0, 'ci95')
    line = ax.errorbar(x, y, yerr=yerr, label='AIR - Unfixed', marker='o', ls='--', color=air_color)
    ax.set_ylabel('Accuracy')
    ax.set_xlabel('\\# Training Examples / 1000')
    ax.tick_params(axis='both')
    ax.set_ylim((0.0, 1.05))
    ax.set_xscale('log')
    ax.set_xticks(x)
    ax.set_xticklabels((np.array(x) / 1000).astype('i'))
    plt.legend(loc='lower center', handlelength=2.5, bbox_to_anchor=(0.5, 1.01), ncol=3, columnspacing=1)
    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.99, top=0.82)
    plot_path = os.path.join(plot_dir, ('addition/main.' + extension))
    os.makedirs(os.path.dirname(plot_path), exist_ok=True)
    fig.savefig(plot_path)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = plt.figure

idx = 90:------------------- similar code ------------------ index = 37, score = 6.0 
def plot_num_recall(recalls, proposal_nums):
    'Plot Proposal_num-Recalls curve.\n\n    Args:\n        recalls(ndarray or list): shape (k,)\n        proposal_nums(ndarray or list): same shape as `recalls`\n    '
    if isinstance(proposal_nums, np.ndarray):
        _proposal_nums = proposal_nums.tolist()
    else:
        _proposal_nums = proposal_nums
    if isinstance(recalls, np.ndarray):
        _recalls = recalls.tolist()
    else:
        _recalls = recalls
    import matplotlib.pyplot as plt
    f = plt.figure()
    plt.plot(([0] + _proposal_nums), ([0] + _recalls))
    plt.xlabel('Proposal num')
    plt.ylabel('Recall')
    plt.axis([0, proposal_nums.max(), 0, 1])
    f.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 91:------------------- similar code ------------------ index = 59, score = 6.0 
def run(self, total_steps):
    ' Runs PPO\n\n        Args:\n            total_steps (int): total number of environment steps to run for\n        '
    N = self.num_workers
    T = self.worker_steps
    E = self.opt_epochs
    A = self.venv.action_space.n
    while (self.taken_steps < total_steps):
        progress = (self.taken_steps / total_steps)
        (obs, rewards, masks, actions, steps) = self.interact()
        ob_shape = obs.size()[2:]
        ep_reward = self.test()
        self.reward_histr.append(ep_reward)
        self.steps_histr.append(self.taken_steps)
        group_size = (len(self.steps_histr) // self.plot_points)
        if (self.plot_reward and ((len(self.steps_histr) % (self.plot_points * 10)) == 0) and (group_size >= 10)):
            (x_means, _, y_means, y_stds) = mean_std_groups(np.array(self.steps_histr), np.array(self.reward_histr), group_size)
            fig = plt.figure()
            fig.set_size_inches(8, 6)
            plt.ticklabel_format(axis='x', style='sci', scilimits=((- 2), 6))
            plt.errorbar(x_means, y_means, yerr=y_stds, ecolor='xkcd:blue', fmt='xkcd:black', capsize=5, elinewidth=1.5, mew=1.5, linewidth=1.5)
            plt.title('Training progress')
            plt.xlabel('Total steps')
            plt.ylabel('Episode reward')
            plt.savefig(self.plot_path, dpi=200)
            plt.clf()
            plt.close()
            plot_timer = 0
        obs_ = obs.view(((((T + 1) * N),) + ob_shape))
        obs_ = Variable(obs_)
        (_, values) = self.policy(obs_)
        values = values.view((T + 1), N, 1)
        (advantages, returns) = gae(rewards, masks, values, self.gamma, self.lambd)
        self.policy_old.load_state_dict(self.policy.state_dict())
        for e in range(E):
            self.policy.zero_grad()
            MB = (steps // self.minibatch_steps)
            b_obs = Variable(obs[:T].view(((steps,) + ob_shape)))
            b_rewards = Variable(rewards.view(steps, 1))
            b_masks = Variable(masks.view(steps, 1))
            b_actions = Variable(actions.view(steps, 1))
            b_advantages = Variable(advantages.view(steps, 1))
            b_returns = Variable(returns.view(steps, 1))
            b_inds = np.arange(steps)
            np.random.shuffle(b_inds)
            for start in range(0, steps, self.minibatch_steps):
                mb_inds = b_inds[start:(start + self.minibatch_steps)]
                mb_inds = cuda_if(torch.from_numpy(mb_inds).long(), self.cuda)
                (mb_obs, mb_rewards, mb_masks, mb_actions, mb_advantages, mb_returns) = [arr[mb_inds] for arr in [b_obs, b_rewards, b_masks, b_actions, b_advantages, b_returns]]
                (mb_pis, mb_vs) = self.policy(mb_obs)
                (mb_pi_olds, mb_v_olds) = self.policy_old(mb_obs)
                (mb_pi_olds, mb_v_olds) = (mb_pi_olds.detach(), mb_v_olds.detach())
                losses = self.objective(self.clip_func(progress), mb_pis, mb_vs, mb_pi_olds, mb_v_olds, mb_actions, mb_advantages, mb_returns)
                (policy_loss, value_loss, entropy_loss) = losses
                loss = ((policy_loss + (value_loss * self.value_coef)) + (entropy_loss * self.entropy_coef))
                set_lr(self.optimizer, self.lr_func(progress))
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm(self.policy.parameters(), self.max_grad_norm)
                self.optimizer.step()
        self.taken_steps += steps
        print(self.taken_steps)
        torch.save({'policy': self.policy.state_dict()}, (('./save/PPO_' + self.env_name) + '.pt'))

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    while:
        if:
             ...  = plt.figure()

idx = 92:------------------- similar code ------------------ index = 58, score = 6.0 
def plot_comut(self, fig=None, spec=None, x_padding=0, y_padding=0, tri_padding=0, heights=None, hspace=0.2, subplot_hspace=None, widths=None, wspace=0.2, structure=None, figsize=(10, 6)):
    "plot the CoMut object\n\n        Params:\n        -----------\n        fig: `~.figure.Figure`\n            The figure on which to create the CoMut plot. If no fig\n            is passed, it will be created.\n\n        spec: gridspec\n            The gridspec on which to create the CoMut plot. If no spec\n            is passed, one will be created.\n\n        x_padding, y_padding: float, optional (default 0)\n            The space between patches in the CoMut plot in the x and y\n            direction.\n\n        tri_padding: float\n            If there are two values for a sample in a category, the spacing\n            between the triangles that represent each value.\n\n        heights: dict\n            The relative heights of all the plots. Dict should have keys as\n            plot names and values as relative height.\n\n            Height values for each plot type default to the following:\n                Number of categories for categorical data\n                1 for continuous data\n                3 for bar plots\n                1 for sample indicator\n\n            Example:\n            --------\n            heights = {'plot1': 3, 'plot2': 5, 'plot3': 7}\n            CoMut.plot_comut(heights=heights)\n\n        hspace: float, default 0.2\n            The distance between different plots in the CoMut plot.\n\n        widths: list-like\n            The relative widths of plots in the x direction. Valid only\n            if side bar plots are added. Defaults to 5 for the central CoMut\n            and 1 for each side plot.\n\n            Example:\n            --------\n            widths = [0.5, 5]\n            CoMut.plot_comut(widths=heights)\n\n        wspace: float, default 0.2\n            The distance between different plots in the x-direction\n            (ie side bar plots)\n\n        structure: list-like\n            List containing desired CoMut structure. Must be provided\n            as list of lists (see example). Default structure is to place\n            each plot in its own list.\n\n            Example:\n            --------\n            # plot plot1 and plot2 in a separate subplot from plot4, don't plot\n            # plot3.\n            structure = [('plot1', 'plot2'), ('plot4')]\n            CoMut.plot_comut(structure=structure)\n\n        sublot_hspace: float\n            The distance between plots in a subplot. The scale for\n            subplot_hspace and hspace are not the same.\n\n        figsize (float, float), optional, default: (10,6)\n            width, height of CoMut figure in inches. Only valid if fig argument\n            is None.\n\n        Returns:\n        -------\n        self: CoMut object\n            CoMut object with updated axes and figure attributes.\n\n        Example\n        --------\n        # create CoMut object\n        ex_comut = comut.CoMut()\n\n        # add mutation data\n        ex_comut.add_categorical_data(mutation_data, name='mutation')\n\n        # add clinical data\n        ex_comut.add_categorical_data(tumor_stage, name='tumor_stage')\n        ex_comut.add_continuous_data(purity_data, name='purity')\n\n        # plot CoMut data\n        ex_comut.plot_comut()\n\n        # ex_comut.axes will be a dictionary with keys 'mutation', 'tumor_stage',\n        # and 'purity', with values equal to the plotted axes."
    if (structure is None):
        structure = [[plot] for plot in self._plots]
    if (heights is None):
        heights = {}
    num_subplots = len(structure)
    heights = self._get_height_spec(structure, heights)
    plot_heights = [sum(height) for height in heights][::(- 1)]
    if (fig is None):
        fig = plt.figure(figsize=figsize)
    if (widths is None):
        (widths, comut_idx) = self._get_default_widths_and_comut_loc()
    else:
        (_, comut_idx) = self._get_default_widths_and_comut_loc()
    num_cols = len(widths)
    if (spec is None):
        spec = gridspec.GridSpec(ncols=num_cols, nrows=num_subplots, figure=fig, height_ratios=plot_heights, width_ratios=widths, hspace=hspace, wspace=wspace)
    else:
        spec = gridspec.GridSpecFromSubplotSpec(ncols=num_cols, nrows=num_subplots, height_ratios=plot_heights, width_ratios=widths, hspace=hspace, wspace=wspace, subplot_spec=spec)
    for (i, (plot, height)) in enumerate(zip(structure, heights)):
        if (i == 0):
            sharex = None
            first_plot = plot[0]
        else:
            sharex = self.axes[first_plot]
        if (len(plot) == 1):
            plot_name = plot[0]
            ax = fig.add_subplot(spec[(((num_subplots - i) - 1), comut_idx)], sharex=sharex)
            ax = self._plot_data_on_axis(ax=ax, plot_name=plot_name, x_padding=x_padding, y_padding=y_padding, tri_padding=tri_padding)
            side_plots = self._side_plots[plot_name]
            (left_idx, right_idx) = (1, 1)
            for (side_name, side_plot) in side_plots.items():
                position = side_plot['position']
                if (position == 'left'):
                    sideplot_idx = (comut_idx - left_idx)
                    left_idx += 1
                elif (position == 'right'):
                    sideplot_idx = (comut_idx + right_idx)
                    right_idx += 1
                side_ax = fig.add_subplot(spec[(((num_subplots - i) - 1), sideplot_idx)])
                side_ax = self._plot_side_bar_data(side_ax, side_name, y_padding=y_padding, **side_plot)
                side_ax.set_ylim(ax.get_ylim())
        else:
            num_plots = len(plot)
            height = height[::(- 1)]
            subplot_spec = gridspec.GridSpecFromSubplotSpec(ncols=1, nrows=num_plots, hspace=subplot_hspace, subplot_spec=spec[(((num_subplots - i) - 1), comut_idx)], height_ratios=height)
            for (j, plot_name) in enumerate(plot):
                ax = fig.add_subplot(subplot_spec[(((num_plots - j) - 1), 0)], sharex=sharex)
                ax = self._plot_data_on_axis(ax=ax, plot_name=plot_name, x_padding=x_padding, y_padding=y_padding, tri_padding=tri_padding)
                if self._side_plots[plot_name]:
                    raise ValueError('Side bar plot for {} cannot be created. Plots within a subplot cannot have a side plot.'.format(plot_name))
    self.axes[first_plot].set_xticks(np.arange(0.5, (len(self.samples) + 0.5)))
    self.axes[first_plot].set_xticklabels(self.samples, rotation=90)
    self.axes[first_plot].get_xaxis().set_visible(True)
    self.axes[first_plot].tick_params(axis='x', which='both', bottom=False, length=0)
    self.figure = fig
    return self

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
    if:
         ...  = plt.figure

idx = 93:------------------- similar code ------------------ index = 52, score = 6.0 
def draw_figure_to_plt(distance_matrix, embeddings, names, label_size=14):
    fig = plt.figure(figsize=(((15 / 25.0) * len(embeddings)), ((15 / 25.0) * len(embeddings))))
    ax = plt.gca()
    plt.imshow(distance_matrix, cmap='viridis_r')
    ax.set_xticks(np.arange(len(embeddings)))
    ax.set_yticks(np.arange(len(embeddings)))
    ax.set_xticklabels(names)
    ax.set_yticklabels(names)
    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')
    try:
        for (i, e) in enumerate(embeddings):
            arr_img = get_image(e)
            imagebox = OffsetImage(arr_img, zoom=0.18)
            imagebox.image.axes = ax
            xy = (i, i)
            ab = AnnotationBbox(imagebox, xy, frameon=False)
            ax.add_artist(ab)
    except FileNotFoundError:
        print('Could not find an icon for a taxonomy entry. Have you downloaded the iconic_taxa directory in ./static?')
    plt.tick_params(axis='both', which='major', labelsize=label_size)
    plt.tight_layout()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure

idx = 94:------------------- similar code ------------------ index = 40, score = 6.0 
def plot_comparison(extension):
    yolo_air_path = os.path.join(data_dir, 'comparison/run_search_yolo-air-run_env=size=14-in-colour=False-task=arithmetic-ops=addition_alg=yolo-air_duration=long_seed=0_2018_07_16_13_46_48/')
    air_path = os.path.join(data_dir, 'comparison/run_search_air-run_env=size=14-in-colour=False-task=arithmetic-ops=addition_alg=attend-infer-repeat_duration=long_seed=0_2018_07_24_13_02_34')
    dair_path = os.path.join(data_dir, 'comparison/run_search_dair-run_env=size=14-in-colour=False-task=arithmetic-ops=addition_alg=attend-infer-repeat_duration=long_seed=0_2018_07_10_09_22_24')
    baseline_path = os.path.join(data_dir, 'comparison/run_search_comparison-baseline_env=size=14-in-colour=False-task=arithmetic-ops=addition_alg=baseline_duration=oak_seed=0_2018_07_20_11_15_24/')
    fig = plt.figure(figsize=(5, 3.5))
    ax = plt.gca()
    y_func = (lambda y: (100 * y))
    (x, y, *yerr) = get_arithmetic_data([yolo_air_path], 'n_digits', '_test_AP', 0, 'ci95', y_func=y_func)
    line = ax.errorbar(x, y, yerr=yerr, label='SPAIR', marker='o', ls='-')
    line.lines[0].get_c()
    (x, y, *yerr) = get_arithmetic_data([air_path], 'n_digits', '_test_AP', 0, 'ci95', y_func=y_func)
    line = ax.errorbar(x, y, yerr=yerr, label='AIR', marker='^', ls='-.')
    line.lines[0].get_c()
    (x, y, *yerr) = get_arithmetic_data([dair_path], 'n_digits', 'AP', 0, 'ci95', y_func=y_func)
    line = ax.errorbar(x, y, yerr=yerr, label='DAIR', marker='v', ls='--')
    line.lines[0].get_c()
    (x, y, *yerr) = get_arithmetic_data([baseline_path], 'n_digits', '_test_AP', 0, 'ci95', y_func=y_func)
    ax.plot(x, y, label='ConnComp', marker='s', ls=':')
    ax.set_ylabel('Average Precision', fontsize=12)
    ax.set_xlabel('\\# Digits in Image', fontsize=12)
    ax.tick_params(axis='both', labelsize=14)
    ax.set_ylim((0.0, 105.0))
    ax.set_xticks(x)
    plt.legend(loc='upper right', handlelength=4)
    plt.subplots_adjust(left=0.12, bottom=0.13, right=0.99, top=0.99)
    plot_path = os.path.join(plot_dir, ('comparison/main.' + extension))
    os.makedirs(os.path.dirname(plot_path), exist_ok=True)
    fig.savefig(plot_path)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = plt.figure

idx = 95:------------------- similar code ------------------ index = 41, score = 6.0 
def find_lr(self, num_iters=1000, init_value=1e-06, final_value=10.0, beta=0.98):
    '\n        stolen and adapted from here: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n        :param num_iters:\n        :param init_value:\n        :param final_value:\n        :param beta:\n        :return:\n        '
    import math
    self._maybe_init_amp()
    mult = ((final_value / init_value) ** (1 / num_iters))
    lr = init_value
    self.optimizer.param_groups[0]['lr'] = lr
    avg_loss = 0.0
    best_loss = 0.0
    losses = []
    log_lrs = []
    for batch_num in range(1, (num_iters + 1)):
        loss = (self.run_iteration(self.tr_gen, do_backprop=True, run_online_evaluation=False).data.item() + 1)
        avg_loss = ((beta * avg_loss) + ((1 - beta) * loss))
        smoothed_loss = (avg_loss / (1 - (beta ** batch_num)))
        if ((batch_num > 1) and (smoothed_loss > (4 * best_loss))):
            break
        if ((smoothed_loss < best_loss) or (batch_num == 1)):
            best_loss = smoothed_loss
        losses.append(smoothed_loss)
        log_lrs.append(math.log10(lr))
        lr *= mult
        self.optimizer.param_groups[0]['lr'] = lr
    import matplotlib.pyplot as plt
    lrs = [(10 ** i) for i in log_lrs]
    fig = plt.figure()
    plt.xscale('log')
    plt.plot(lrs[10:(- 5)], losses[10:(- 5)])
    plt.savefig(join(self.output_folder, 'lr_finder.png'))
    plt.close()
    return (log_lrs, losses)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 96:------------------- similar code ------------------ index = 46, score = 6.0 
def imshow_rgb(images, nrow, ncol):
    '\n    Parameters\n    ----------\n    images : numpy.ndarray \n             shape [h, w, c]\n    '
    (h, w, c) = images.shape
    fig = plt.figure()
    plt.imshow(images)
    return fig

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 97:------------------- similar code ------------------ index = 44, score = 6.0 
def plot_distance_hist(origin, moved):
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.set_title('moved distance')
    ax.hist(np.sum(np.power((moved - origin), 2), axis=1), bins=20)
    plt.show()

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

idx = 98:------------------- similar code ------------------ index = 95, score = 5.0 
def get_ptcloud_img(ptcloud):
    fig = plt.figure(figsize=(8, 8))
    (x, z, y) = ptcloud.transpose(1, 0)
    ax = fig.gca(projection=Axes3D.name, adjustable='box')
    ax.axis('off')
    ax.axis('scaled')
    ax.view_init(30, 45)
    (max, min) = (np.max(ptcloud), np.min(ptcloud))
    ax.set_xbound(min, max)
    ax.set_ybound(min, max)
    ax.set_zbound(min, max)
    ax.scatter(x, y, z, zdir='z', c=x, cmap='jet')
    fig.canvas.draw()
    img = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')
    img = img.reshape((fig.canvas.get_width_height()[::(- 1)] + (3,)))
    return img

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ( ... ):
     ...  = plt.figure

idx = 99:------------------- similar code ------------------ index = 70, score = 5.0 
def plot_param_count(optimizer, result):
    all_dim_values = result.x_iters
    losses = result.func_vals
    par_cnt_scheme = optimizer.adapt_param['par_cnt_scheme']
    param_thr = optimizer.adapt_param['param_thr']
    all_par_dicts = []
    bad_param_cnt_inds = []
    bad_param_dict = []
    for (ind, dim_values) in enumerate(all_dim_values):
        (_, par_dict) = check_parameter_count_for_sample(dim_values, optimizer.hyper_param_names, param_thr, par_cnt_scheme=par_cnt_scheme)
        all_par_dicts.append(par_dict)
        if ((par_dict['total'] < (param_thr * 0.95)) or (par_dict['total'] > param_thr)):
            bad_param_cnt_inds.append(ind)
            bad_param_dict.append(par_dict)
    sorted_inds = np.argsort(losses)
    sorted_losses = [losses[ind] for ind in sorted_inds]
    sorted_par_dicts = [all_par_dicts[ind] for ind in sorted_inds]
    cnn_cnts = np.array([par_dict['cnn'] for par_dict in sorted_par_dicts])
    lstm_cnts = np.array([par_dict['lstm'] for par_dict in sorted_par_dicts])
    ff_cnts = np.array([par_dict['ff'] for par_dict in sorted_par_dicts])
    n_exps = len(sorted_losses)
    exps_range = np.arange(n_exps)
    font = {'size': 16}
    matplotlib.rc('font', **font)
    mpl_fig = plt.figure()
    ax_loss = mpl_fig.add_subplot(211)
    ax_loss.scatter(exps_range, sorted_losses)
    ax_loss.set_ylabel('Validation loss')
    ax = mpl_fig.add_subplot(212, sharex=ax_loss)
    width = 0.5
    p1 = ax.bar(exps_range, cnn_cnts, width)
    p2 = ax.bar(exps_range, lstm_cnts, width, bottom=cnn_cnts)
    p3 = ax.bar(exps_range, ff_cnts, width, bottom=(lstm_cnts + cnn_cnts))
    ax.legend(['CNN', 'LSTM', 'FC'], loc='upper right')
    ax.set_ylabel('Trainable parameter count (in millions)')
    ax.set_xlabel('Experiments ordered by loss')
    y_ticks = np.arange(0, param_thr, 2000000.0)
    y_tickslabels = np.arange(0, (param_thr / 1000000.0), (2000000.0 / 1000000.0))
    y_tickslabels = [str(lab) for lab in y_tickslabels]
    ax.set_yticks(y_ticks)
    ax.set_yticklabels(y_tickslabels)
    tmp = [ind for (ind, loss) in enumerate(sorted_losses) if (loss >= 0.17)]
    ax_loss.axvline((tmp[0] - 0.5), linestyle='-', lw=1)
    ax.axvline((tmp[0] - 0.5), linestyle='-', lw=1)

------------------- similar code (pruned) ------------------ score = 0.46153846153846156 
def  ... ():
     ...  = plt.figure()

